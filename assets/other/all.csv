"Document Title","Authors","Author Affiliations","Publication Title","Date Added To Xplore","Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN","ISBNs","DOI","Funding Information","PDF Link","Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms","Article Citation Count","Patent Citation Count","Reference Count","License","Online Date","Issue Date","Meeting Date","Publisher","Document Identifier"
"A 1024-Member Ensemble Data Assimilation with 3.5-Km Mesh Global Weather Simulations","H. Yashiro; K. Terasaki; Y. Kawai; S. Kudo; T. Miyoshi; T. Imamura; K. Minami; H. Inoue; T. Nishiki; T. Saji; M. Satoh; H. Tomita","National Institute for Environmental Studies (NIES), Tsukuba, Japan; RIKEN Center for Computational Science (R-CCS), Kobe, Japan; RIKEN Center for Computational Science (R-CCS), Kobe, Japan; RIKEN Center for Computational Science (R-CCS), Kobe, Japan; RIKEN Center for Computational Science (R-CCS), Kobe, Japan; RIKEN Center for Computational Science (R-CCS), Kobe, Japan; RIKEN Center for Computational Science (R-CCS), Kobe, Japan; Technical Computing Solution Unit, Fujitsu Ltd., Chiba, Japan; Technical Computing Solution Unit, Fujitsu Ltd., Chiba, Japan; Metro Inc., Numazu, Japan; Atmosphere and Ocean Research Institute (AORI), The University of Tokyo, Kashiwa, Japan; RIKEN Center for Computational Science (R-CCS), Kobe, Japan","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,10,"Numerical weather prediction (NWP) supports our daily lives. Weather models require higher spatiotemporal resolutions to prepare for extreme weather disasters and reduce the uncertainty of predictions. The accuracy of the initial state of the weather simulation is also critical; thus, we need more advanced data assimilation (DA) technology. By combining resolution and ensemble size, we have achieved the world’s largest weather DA experiment using a global cloud-resolving model and an ensemble Kalman filter method. The number of grid points was $\sim$4.4 trillion, and 1.3 PiB of data was passed from the model simulation part to the DA part. We adopted a data-centric application design and approximate computing to speed up the overall system of DA. Our DA system, named NICAM-LETKF, scales to 131,072 nodes (6,291,456 cores) of the supercomputer Fugaku with a sustained performance of 29 PFLOPS and 79 PFLOPS for the simulation and DA parts, respectively.","","978-1-7281-9998-6","10.1109/SC41405.2020.00005","RIKEN; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355206","numerical weather prediction;data assimilation;Fugaku","Computational modeling;Weather forecasting;Predictive models;Data models;Numerical models;Meteorology;Data assimilation","clouds;data assimilation;disasters;geophysics computing;Kalman filters;parallel machines;weather forecasting","1024-member ensemble data assimilation;numerical weather prediction;weather models;higher spatiotemporal resolutions;extreme weather disasters;initial state;weather simulation;advanced data assimilation technology;ensemble size;global cloud-resolving model;ensemble Kalman filter method;model simulation part;data-centric application design;mesh global weather;NICAM-LETKF;computer speed 79.0 PFLOPS;computer speed 29.0 PFLOPS","",6.0,"",38.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Processing Full-Scale Square Kilometre Array Data on the Summit Supercomputer","R. Wang; R. Tobar; M. Dolensky; T. An; A. Wicenec; C. Wu; F. Dulwich; N. Podhorszki; V. Anantharaj; E. Suchyta; B. Lao; S. Klasky","Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, United States; International Centre for Radio Astronomy Research, The University of Western Australia, Perth, Australia; International Centre for Radio Astronomy Research, The University of Western Australia, Perth, Australia; Radio Astronomy Division, Shanghai Astronomical Observatory, Shanghai, China; International Centre for Radio Astronomy Research, The University of Western Australia, Perth, Australia; International Centre for Radio Astronomy Research, The University of Western Australia, Perth, Australia; Department of Engineering Science, Oxford e-Research Centre, University of Oxford, Oxford, United Kingdom; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, United States; National Center for Computational Sciences, Oak Ridge National Laboratory, Oak Ridge, United States; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, United States; Radio Astronomy Division, Shanghai Astronomical Observatory, Shanghai, China; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, United States","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,12,"This work presents a workflow for simulating and processing the full-scale low-frequency telescope data of the Square Kilometre Array (SKA) Phase 1. The SKA project will enter the construction phase soon, and once completed, it will be the world's largest radio telescope and one of the world's largest data generators. The authors used Summit to mimic an endto-end SKA workflow, simulating a dataset of a typical 6 hour observation and then processing that dataset with an imaging pipeline. This workflow was deployed and run on 4,560 compute nodes, and used 27,360 GPUs to generate 2.6 PB of data. This was the first time that radio astronomical data were processed at this scale. Results show that the workflow has the capability to process one of the key SKA science cases, an Epoch of Reionization observation. This analysis also helps reveal critical design factors for the next-generation radio telescopes and the required dedicated processing facilities.","","978-1-7281-9998-6","10.1109/SC41405.2020.00006","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355269","SKA;Summit;extreme-scale workflow;DALiuGE;ADIOS2;OSKAR2","Phased arrays;Radio astronomy;Pipelines;Telescopes;Supercomputers;Arrays;Next generation networking","astronomy computing;parallel machines;radiotelescopes","Summit supercomputer;full-scale low-frequency telescope data;Square Kilometre Array Phase 1;SKA project;construction phase;endto-end SKA workflow;radio astronomical data;key SKA science cases;next-generation radio telescopes;processing facilities;full-scale Square Kilometre Array data;critical design factors","",4.0,"",30.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Toward Realization of Numerical Towing-Tank Tests by Wall-Resolved Large Eddy Simulation based on 32 Billion Grid Finite-Element Computation","C. Kato; Y. Yamade; K. Nagano; K. Kumahata; K. Minami; T. Nishikawa","Institute of Industrial Science, The University of Tokyo, Tokyo, Japan; Mizuho Information &, Research Institute, Inc, Tokyo, Japan; Mizuho Information &, Research Institute, Inc, Tokyo, Japan; RIKEN Center for Computational Science, Hyogo, Japan; RIKEN Center for Computational Science, Hyogo, Japan; Shipbuilding Research Centre of Japan, Tokyo, Japan","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"To realize numerical towing-tank tests by substantially shortening the time to the solution, a general-purpose Finite-Element flow solver, named FrontFlow/blue (FFB), has been fully optimized so as to achieve maximum possible sustained memory throughputs with three of its four hot kernels. A single-node sustained performance of 179.0 GFLOPS, which corresponds to 5.3% of the peak performance, has been achieved on Fugaku, the next flagship computer of Japan. A weak-scale benchmark test has confirmed that FFB runs with a parallel efficiency of over 85% up to 5,505,024 compute cores, and an overall sustained performance of16.7 PFLOPS has been achieved. As a result, the time needed for large-eddy simulation using 32 billion grids has been significantly reduced from almost two days to only 37 min., or by a factor of 71. This has clearly indicated that a numerical towing-tank could actually be built for ship hydrodynamics within a few years.","","978-1-7281-9998-6","10.1109/SC41405.2020.00007","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355268","Very-large-scale Industrial Applications;Fluid-flow Simulations;Time-to-Solution;Hot Kernel Optimization;Sustained Memory Throughputs;Weak-scale Benchmark Test Performance attributes are: time-to-solution and scalability of an implicit method;and they were measured by FLOPS counts for the whole application except I/O run on a part of the full system under installation with single precision","Computational modeling;Benchmark testing;Throughput;Extraterrestrial measurements;Finite element analysis;Numerical models;Kernel","finite element analysis;flow simulation;geophysics computing;hydrodynamics;parallel machines;ships","numerical towing-tank tests;wall-resolved large eddy simulation;FFB;maximum possible sustained memory throughputs;single-node sustained performance;179.0 GFLOPS;peak performance;flagship computer;compute cores;grid finite element computation;computer speed 179.0 GFLOPS;time 37.0 min","",1.0,"",37.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Accelerating Large-Scale Excited-State GW Calculations on Leadership HPC Systems","M. D. Ben; C. Yang; Z. Li; F. H. d. Jornada; S. G. Louie; J. Deslippe","Lawrence Berkeley National Laboratory, Berkeley, California USA; Lawrence Berkeley National Laboratory, Berkeley, California USA; Department of Physics, University of California at Berkeley, California USA; Department of Materials Science and Engineering, Stanford University, Stanford, California USA; Department of Physics, University of California at Berkeley, California USA; Lawrence Berkeley National Laboratory, Berkeley, California USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,11,"Large-scale GW calculations are the state-of-the-art approach to accurately describe many-body excited-state phenomena in complex materials. This is critical for novel device design but due to their extremely high computational cost, these calculations often run at a limited scale. In this paper, we present algorithm and implementation advancements made in the materials science code BerkeleyGW to scale calculations to the order of over 10,000 electrons utilizing the entire Summit at OLCF. Excellent strong and weak scaling is observed, and a 105.9 PFLOP/s double-precision performance is achieved on 27,648 V100 GPUs, reaching 52.7% of the peak. This work for the first time demonstrates the possibility to perform GW calculations at such scale within minutes on current HPC systems, and leads the way for future efficient HPC software development in materials, physical, chemical, and engineering sciences.","","978-1-7281-9998-6","10.1109/SC41405.2020.00008","U.S. Department of Energy; Office of Science; Basic Energy Sciences; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355291","electronic structure;excited states;GW method;GPU acceleration;divacancy defects;quantum computing","Materials science and technology;Leadership;High performance computing;Software;Computational efficiency;Acceleration;Chemicals","excited states;mainframes;parallel processing","large-scale excited-state GW calculations;leadership HPC systems;many-body excited-state phenomena;complex materials;device design;materials science code BerkeleyGW;HPC software development","",7.0,"",44.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Pushing the Limit of Molecular Dynamics with Ab Initio Accuracy to 100 Million Atoms with Machine Learning","W. Jia; H. Wang; M. Chen; D. Lu; L. Lin; R. Car; E. Weinan; L. Zhang","University of California, Berkeley, Berkeley, USA; Laboratory of Computational Physics, Institute of Applied Physics and Computational Mathematics, Beijing, China; CAPT, HEDPS,College of Engineering, Peking University, Beijing, China; CAPT, HEDPS,College of Engineering, Peking University, Beijing, China; Lawrence Berkeley National Laboratory, Berkeley, USA; Princeton University, Princeton, USA; Princeton University, Princeton, USA; Princeton University, Princeton, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"For 35 years, ab initio molecular dynamics (AIMD) has been the method of choice for modeling complex atomistic phenomena from first principles. However, most AIMD applications are limited by computational cost to systems with thousands of atoms at most. We report that a machine learning based simulation protocol (Deep Potential Molecular Dynamics), while retaining ab initio accuracy, can simulate more than 1 nanosecond-long trajectory of over 100 million atoms per day, using a highly optimized code (GPU DeePMD-kit) on the Summit supercomputer. Our code can efficiently scale up to the entire Summit supercomputer, attaining 91 PFLOPS in double precision (45.5% of the peak) and 162/275 PFLOPS in mixed-single/half precision. The great accomplishment of this work is that it opens the door to simulating unprecedented size and time scales with ab initio accuracy. It also poses new challenges to the next-generation supercomputer for a better integration of machine learning and physical modeling.","","978-1-7281-9998-6","10.1109/SC41405.2020.00009","National Science Foundation; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355242","Deep potential molecular dynamics;ab initio molecular dynamics;machine learning;GPU;heterogeneous architecture;Summit","Protocols;Computational modeling;High performance computing;Machine learning;Supercomputers;Trajectory;Next generation networking","ab initio calculations;learning (artificial intelligence);molecular dynamics method;parallel machines;physics computing","machine learning;ab initio molecular dynamics;AIMD;GPU DeePMD-kit;supercomputer;PFLOPS;deep potential molecular dynamics","",31.0,"",75.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Scalable Knowledge Graph Analytics at 136 Petaflop/s","R. Kannan; P. Sao; H. Lu; D. Herrmannova; V. Thakkar; R. Patton; R. Vuduc; T. Potok","Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Georgia Institute of Technology, Atlanta, GA, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Georgia Institute of Technology, Atlanta, GA, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"We are motivated by newly proposed methods for data mining large-scale corpora of scholarly publications, such as the full biomedical literature, which may consist of tens of millions of papers spanning decades of research. In this setting, analysts seek to discover how concepts relate to one another. They construct graph representations from annotated text databases and then formulate the relationship-mining problem as one of computing all-pairs shortest paths (APSP), which becomes a significant bottleneck. In this context, we present a new high-performance algorithm and implementation of the Floyd-Warshall algorithm for distributed-memory parallel computers accelerated by GPUs, which we call DSNAPSHOT (Distributed Accelerated Semiring All-Pairs Shortest Path). For our largest experiments, we ran DSNAPSHOT on a connected input graph with millions of vertices using 4, 096nodes (24,576GPUs) of the Oak Ridge National Laboratory's Summit supercomputer system. We find DSNAPSHOT achieves a sustained performance of $136\times 10^{15}$ floating-point operations per second (136petaflop/s) at a parallel efficiency of 90% under weak scaling and, in absolute speed, 70% of the best possible performance given our computation (in the single-precision tropical semiring or “min-plus” algebra). Looking forward, we believe this novel capability will enable the mining of scholarly knowledge corpora when embedded and integrated into artificial intelligence-driven natural language processing workflows at scale.","","978-1-7281-9998-6","10.1109/SC41405.2020.00010","Battelle; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355266","Shortest path problem;High Performance Computing;Parallel Algorithms","Knowledge engineering;Databases;High performance computing;Supercomputers;Natural language processing;Acceleration;Data mining","artificial intelligence;data mining;directed graphs;floating point arithmetic;natural language processing;parallel machines;text analysis","APSP;significant bottleneck;high-performance algorithm;petaflop;Oak Ridge National Laboratory summit supercomputer system;distributed accelerated semiring all-pairs shortest path;all-pairs shortest paths;relationship-mining problem;annotated text databases;graph representations;biomedical literature;scholarly publications;large-scale corpora;data mining;newly proposed methods;scalable knowledge graph analytics;scholarly knowledge corpora;tropical semiring;possible performance;weak scaling;parallel efficiency;floating-point operations;sustained performance;GPU;connected input graph;largest experiments;DSNAPSHOT;distributed-memory parallel computers;Floyd-Warshall algorithm","",2.0,"",46.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"A Parallel Framework for Constraint-Based Bayesian Network Learning via Markov Blanket Discovery","A. Srivastava; S. P. Chockalingam; S. Aluru","Georgia Institute of Technology, Atlanta, Georgia, USA; Georgia Institute of Technology, Atlanta, Georgia, USA; Georgia Institute of Technology, Atlanta, Georgia, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Bayesian networks (BNs) are a widely used graphical model in machine learning. As learning the structure of BNs is NP-hard, high-performance computing methods are necessary for constructing large-scale networks. In this paper, we present a parallel framework to scale BN structure learning algorithms to tens of thousands of variables. Our framework is applicable to learning algorithms that rely on the discovery of Markov blankets (MBs) as an intermediate step. We demonstrate the applicability of our framework by parallelizing three different algorithms: Grow-Shrink (GS), Incremental Association MB (IAMB), and Interleaved IAMB (Inter-IAMB). Our implementations are able to construct BNs from real data sets with tens of thousands of variables and thousands of observations in less than a minute on 1024 cores, with a speedup of up to 845X and 82.5% efficiency. Furthermore, we demonstrate using simulated data sets that our proposed parallel framework can scale to BNs of even higher dimensionality.","","978-1-7281-9998-6","10.1109/SC41405.2020.00011","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355320","Bayesian networks;constraint-based learning;parallel machine learning;gene networks","Machine learning algorithms;Graphical models;High performance computing;Machine learning;Markov processes;Bayes methods","Bayes methods;belief networks;learning (artificial intelligence);Markov processes;parallel processing","Inter-IAMB;GS;grow-shrink;interleaved IAMB;incremental association MB;constraint-based Bayesian network learning;Markov blankets;BN structure learning algorithms;large-scale networks;high-performance computing methods;machine learning;graphical model;BNs;Markov blanket discovery;parallel framework","",1.0,"",51.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Recurrent Neural Network Architecture Search for Geophysical Emulation","R. Maulik; R. Egele; B. Lusch; P. Balaprakash","Argonne National Laboratory; École Polytechnique; Argonne National Laboratory; Argonne National Laboratory","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Developing surrogate geophysical models from data is a key research topic in atmospheric and oceanic modeling because of the large computational costs associated with numerical simulation methods. Researchers have started applying a wide range of machine learning models, in particular neural networks, to geophysical data for forecasting without these constraints. Constructing neural networks for forecasting such data is nontrivial, however, and often requires trial and error. To address these limitations, we focus on developing proper-orthogonal-decomposition-based long short-term memory networks (PODLSTMs). We develop a scalable neural architecture search for generating stacked LSTMs to forecast temperature in the NOAA Optimum Interpolation Sea-Surface Temperature data set. Our approach identifies POD-LSTMs that are superior to manually designed variants and baseline time-series prediction methods. We also assess the scalability of different architecture search strategies on up to 512 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.","","978-1-7281-9998-6","10.1109/SC41405.2020.00012","U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355238","Recurrent neural networks;Emulation;Geophysics","Temperature distribution;Recurrent neural networks;Atmospheric modeling;Computational modeling;Computer architecture;Data models;Numerical models","interpolation;learning (artificial intelligence);neural nets;ocean temperature;recurrent neural nets;time series","recurrent neural network architecture search;geophysical emulation;surrogate geophysical models;numerical simulation methods;machine learning models;geophysical data;proper-orthogonal-decomposition-based;short-term memory networks;scalable neural architecture search;stacked LSTMs;NOAA Optimum Interpolation Sea-Surface Temperature data;baseline time-series prediction methods;architecture search strategies","",10.0,"",48.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"MESHFREEFLOWNET: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework","C. “. Jiang; S. Esmaeilzadeh; K. Azizzadenesheli; K. Kashinath; M. Mustafa; H. A. Tchelepi; P. Marcus; M. Prabhat; A. Anandkumar","University of California, Berkeley, CA, USA; Stanford University, Stanford, CA, USA; Purdue University, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Stanford University, Stanford, CA, USA; University of California, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; NVIDIA, Santa Clara, CA, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"We propose MESHFREEFLOWNET, a novel deep learning-based super-resolution framework to generate continuous (grid-free) spatio-temporal solutions from the low-resolution inputs. While being computationally efficient, MESHFREEFLOWNET accurately recovers the fine-scale quantities of interest. MESHFREEFLOWNET allows for: (i) the output to be sampled at all spatio-temporal resolutions, (ii) a set of Partial Differential Equation (PDE) constraints to be imposed, and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal domains owing to its fully convolutional encoder. We empirically study the performance of MESHFREEFLOWNET on the task of super-resolution of turbulent flows in the Rayleigh-Bénard convection problem. Across a diverse set of evaluation metrics, we show that MESHFREEFLOWNET significantly outperforms existing baselines. Furthermore, we provide a large scale implementation of MESHFREEFLOWNET and show that it efficiently scales across large clusters, achieving 96.80% scaling efficiency on up to 128 GPUs and a training time of less than 4 minutes. We provide an opensource implementation of our method that supports arbitrary combinations of PDE constraints.","","978-1-7281-9998-6","10.1109/SC41405.2020.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355293","Super-Resolution;PDEs;Physics-Constrained;Deep Neural Networks","Training;Measurement;Partial differential equations;High performance computing;Superresolution;Neural networks;Task analysis","computational fluid dynamics;deep learning (artificial intelligence);partial differential equations;physics computing;Rayleigh-Benard convection;turbulence","spatio-temporal resolutions;MESHFREEFLOWNET;physics-constrained deep continuous space-time super-resolution framework;deep learning-based super-resolution framework;partial differential equation;turbulent flows;Rayleigh-Bénard convection problem;convolutional encoder","",12.0,"",67.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Improving All-to-Many Personalized Communication in Two-Phase I/O","Q. Kang; R. Ross; R. Latham; S. Lee; A. Agrawal; A. Choudhary; W. -k. Liao","Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, IL, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, IL, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, IL, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, IL, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"As modern parallel computers enter the exascale era, the communication cost for redistributing requests becomes a significant bottleneck in MPIIO routines. The communication kernel for request redistribution, which has an all-to-many personalized communication pattern for application programs with a large number of noncontiguous requests, plays an essential role in the overall performance. This paper explores the available communication kernels for two-phase I/O communication. We generalize the spread-out algorithm to adapt to the all-to-many communication pattern of two-phase I/O by reducing the communication straggler effect. Communication throttling methods that reduce communication contention for asynchronous MPI implementation are adopted to improve communication performance further. Experimental results are presented using different communication kernels running on Cray XC40 Cori and IBM AC922 Summit supercomputers with different I/O patterns. Our study shows that adjusting communication kernel algorithms for different I/O patterns can improve the end-to-end performance up to 10 times compared with default MPI-IO implementations.","","978-1-7281-9998-6","10.1109/SC41405.2020.00014","U.S. Department of Energy; Oak Ridge National Laboratory; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355310","MPI-IO;ROMIO;two-phase I/O;communication traffic throttling","Network topology;Exascale computing;Metadata;Topology;Data communication;Kernel","application program interfaces;data handling;input-output programs;message passing;parallel machines;parallel programming;storage management","modern parallel computers;exascale era;communication cost;significant bottleneck;MPIIO routines;request redistribution;personalized communication pattern;application programs;noncontiguous requests;available communication kernels;spread-out algorithm;communication straggler effect;communication contention;asynchronous MPI implementation;communication performance;communication kernel algorithms;end-to-end performance;Cray XC40 Cori;IBM AC922 summit supercomputers","",5.0,"",29.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Taming I/O Variation on QoS-Less HPC Storage: What Can Applications Do?","Z. Qiao; Q. Liu; N. Podhorszki; S. Klasky; J. Chen","New Jersey Institute of Technology, Newark, NJ, USA; New Jersey Institute of Technology, Newark, NJ, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"As high-performance computing (HPC) is being scaled up to exascale to accommodate new modeling and simulation needs, I/O has continued to be a major bottleneck in the end-to-end scientific processes. Nevertheless, prior work in this area mostly aimed to maximize the average performance, and there has been a lack of study and solutions that can manage I/O performance variation on HPC systems. This work aims to take advantage of the storage characteristics and explore application level solutions that are interference-aware. In particular, we monitor the performance of data analytics and estimate the state of shared storage resources using discrete fourier transform (DFT). If heavy I/O interference is predicted to occur at a given timestep, data analytics can dynamically adapt to the environment by lowering the accuracy and performing partial or no augmentation from the shared storage, dictated by an augmentation-bandwidth plot. We evaluate three data analytics, XGC, GenASiS, and Jet, on Chameleon, and quantitatively demonstrate that both the average and variation of I/O performance can be vastly improved using our dynamic augmentation, with the mean and variance improved by as much as 67% and 96%, respectively, while maintaining acceptable outcome of data analysis.","","978-1-7281-9998-6","10.1109/SC41405.2020.00015","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355326","High performance computing;data analysis;data storage","Data analysis;Computational modeling;Discrete Fourier transforms;Quality of service;Interference;Load management;Monitoring","data analysis;discrete Fourier transforms;input-output programs;optimisation;parallel processing;quality of service;storage management","HPC systems;storage characteristics;application level solutions;interference-aware;data analytics;shared storage resources;augmentation-bandwidth plot;dynamic augmentation;data analysis;high-performance computing;end-to-end scientific processes;I/O performance variation;discrete Fourier transform;DFT;heavy I/O interference;XGC;GenASiS","",1.0,"",58.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"BORA: A Bag Optimizer for Robotic Analysis","J. Zhang; T. Xie; Y. Jing; Y. Song; G. Hu; S. Chen; S. Yin","University of Chinese Academy of Science, China; Department of Computer Science, San Diego State University, CA, USA; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; Department of Computer Science, West Chester University, PA, USA; University of Chinese Academy of Science, China","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"We present BORA (Bag Optimizer for Robotic Analysis), a file system middleware that optimizes the acquisition of bags, which are specially formatted files used to store timestamped ROS (robot operating system) messages. BORA sits between ROS and an existing file system to conduct semantic-aware data pre-processing. In particular, it categorizes ROS bag data into multiple groups with each having a distinct label. BORA predigests data index constructions and reduces file open time via a hash-based label management scheme. It is also capable of providing ROS analytic applications with only data needed without a sequence of data searching and locating operations. We implement a BORA prototype, which is then integrated into three computing platforms: a single-node server, a four-node PVFS storage cluster, and a Tianhe-1A Supercomputer storage subsystem. Next, we evaluate the BORA prototype on the three platforms using four real-world ROS applications. Our experimental results show that compared to a traditional bag management scheme BORA improves data acquisition performance by up to 11x. In addition, it offers up to 10x data acquisition performance improvement and 3,100x bags open improvement under a swarm robotics data analysis scenario where data is retrieved across multiple bags simultaneously.","","978-1-7281-9998-6","10.1109/SC41405.2020.00016","China Postdoctoral Science Foundation; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355296","","Data analysis;File systems;Software algorithms;Data acquisition;Prototypes;Robot sensing systems;Middleware","control engineering computing;data acquisition;data analysis;data structures;middleware;parallel processing;robots;storage management","bag optimizer for robotic analysis;bags open improvement;supercomputer storage subsystem;multiple bags;swarm robotics data analysis scenario;data acquisition performance;traditional bag management scheme BORA;real-world ROS applications;four-node PVFS storage cluster;single-node server;BORA prototype;ROS analytic applications;hash-based label management scheme;file open time;data index constructions;distinct label;ROS bag data;semantic-aware data pre-processing;existing file system;robot operating system;timestamped ROS messages;file system middleware","",1.0,"",36.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Density Matrix Quantum Circuit Simulation via the BSP Machine on Modern GPU Clusters","A. Li; O. Subasi; X. Yang; S. Krishnamoorthy","Pacific Northwest National Laboratory (PNNL), Richland, WA, USA; Pacific Northwest National Laboratory (PNNL), Richland, WA, USA; Lehigh University, Bethlehem, PA, USA; Washington State University, Pullman, WA, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"As quantum computers evolve, simulations of quantum programs on classical computers will be essential in validating quantum algorithms, understanding the effect of system noise, and designing applications for future quantum computers. In this paper, we first propose a new multi-GPU programming methodology called MG-BSP which constructs a virtual BSP machine on top of modern multi-GPU platforms, and apply this methodology to build a multi-GPU density matrix quantum simulator called DM-Sim. We propose a new formulation that can significantly reduce communication overhead, and show that this formula transformation can conserve the semantics despite noise being introduced. We build the tool-chain for the simulator to run open standard quantum assembly code, execute synthesized quantum circuits, and perform ultra-deep and largescale simulations. We evaluated DM-Sim on several state-of-the-art multi-GPU platforms including NVIDIA's PascaUVolta DGX1, DGX-2, and ORNL's Summit supercomputer. In particular, we have demonstrated the simulation of one million general gates in 94 minutes on DGX-2, far deeper circuits than has been demonstrated in prior works. Our simulator is more than 10x faster with respect to the corresponding state-vector quantum simulators on GPUs and other platforms. The DM-Sim simulator is released at: http:llgithub.comlpnnllDM-Sim.","","978-1-7281-9998-6","10.1109/SC41405.2020.00017","Office of Science; Advanced Scientific Computing Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355323","","Computational modeling;Graphics processing units;Programming;Supercomputers;Integrated circuit modeling;Quantum circuit;Standards","graphics processing units;multi-threading;parallel processing;quantum computing;virtual machines","open standard quantum assembly code;DGX-2;DM-Sim simulator;modern GPU clusters;quantum programs;quantum algorithms;quantum computers;multiGPU programming methodology;MG-BSP;virtual BSP machine;state vector quantum simulators;multiGPU density matrix quantum circuit simulation;NVIDIA's PascaUVolta DGX1;ORNL's Summit supercomputer;parallel threads","",7.0,"",86.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Efficient 2D Tensor Network Simulation of Quantum Systems","Y. Pang; T. Hao; A. Dugad; Y. Zhou; E. Solomonik","Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Simulation of quantum systems is challenging due to the exponential size of the state space. Tensor networks provide a systematically improvable approximation for quantum states. 2D tensor networks such as Projected Entangled Pair States (PEPS) are well-suited for key classes of physical systems and quantum circuits. However, direct contraction of PEPS networks has exponential cost, while approximate algorithms require computations with large tensors. We propose new scalable algorithms and software abstractions for PEPS-based methods, accelerating the bottleneck operation of contraction and refactorization of a tensor subnetwork. We employ randomized SVD with an implicit matrix to reduce cost and memory footprint asymptotically. Further, we develop a distributed-memory PEPS library and study accuracy and efficiency of alternative algorithms for PEPS contraction and evolution on the Stampede2 supercomputer. We also simulate a popular near-term quantum algorithm, the Variational Quantum Eigensolver (VQE), and benchmark Imaginary Time Evolution (ITE), which compute ground states of Hamiltonians.","","978-1-7281-9998-6","10.1109/SC41405.2020.00018","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355283","Numerical simulation;Parallel algorithms;Quantum mechanics;Quantum computing","Quantum system;Tensors;Software algorithms;Two dimensional displays;Approximation algorithms;Supercomputers;Libraries","approximation theory;eigenvalues and eigenfunctions;ground states;quantum computing;quantum entanglement;singular value decomposition;tensors","quantum systems;exponential size;tensor networks;quantum states;Projected Entangled Pair States;physical systems;quantum circuits;PEPS networks;scalable algorithms;software abstractions;PEPS-based methods;tensor subnetwork;memory footprint;Stampede2 supercomputer;near-term quantum algorithm;variational quantum eigensolver;2D tensor network simulation","",7.0,"",67.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"VERITAS: Accurately Estimating the Correct Output on Noisy Intermediate-Scale Quantum Computers","T. Patel; D. Tiwari","Northeastern University; Northeastern University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"Noisy Intermediate-Scale Quantum (NISQ) machines are being increasingly used to develop quantum algorithms and establish use cases for quantum computing. However, these devices are highly error-prone and produce output, which can be far from the correct output of the quantum algorithm. In this paper, we propose VERITAS, an end-to-end approach toward designing quantum experiments, executing experiments, and correcting outputs produced by quantum circuits post their execution such that the correct output of the quantum algorithm can be accurately estimated.","","978-1-7281-9998-6","10.1109/SC41405.2020.00019","Northeastern University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355231","Quantum Computing;Computer Errors;Error Analysis;Error Probability;Error Correction","Estimation error;Quantum algorithm;Filtration;High performance computing;Error correction;Noise measurement;Quantum circuit","inference mechanisms;quantum computing;quantum noise;speech processing","noisy intermediate-scale quantum computers;noisy intermediate-scale quantum machines;quantum algorithm;quantum computing;VERITAS;end-to-end approach;quantum experiments;quantum circuits","",13.0,"",42.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise Sparsity","C. Guo; B. Y. Hsueh; J. Leng; Y. Qiu; Y. Guan; Z. Wang; X. Jia; X. Li; M. Guo; Y. Zhu","Shanghai Jiao Tong University; NVIDIA; Shanghai Qi Zhi Institute, Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; NVIDIA; NVIDIA; NVIDIA; Shanghai Qi Zhi Institute, Shanghai Jiao Tong University; University of Rochester","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Network pruning can reduce the high computation cost of deep neural network (DNN) models. However, to maintain their accuracies, sparse models often carry randomly-distributed weights, leading to irregular computations. Consequently, sparse models cannot achieve meaningful speedup on commodity hardware (e.g., GPU) built for dense matrix computations. As such, prior works usually modify or design completely new sparsity-optimized architectures for exploiting sparsity. We propose an algorithm-software co-designed pruning method that achieves latency speedups on existing dense architectures. Our work builds upon the insight that the matrix multiplication generally breaks the large matrix into multiple smaller tiles for parallel execution. We propose a tiling-friendly “tile-wise” sparsity pattern, which maintains a regular pattern at the tile level for efficient execution but allows for irregular, arbitrary pruning at the global scale to maintain the high accuracy. We implement and evaluate the sparsity pattern on GPU tensor core, achieving a 1.95× speedup over the dense model.","","978-1-7281-9998-6","10.1109/SC41405.2020.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355304","","Tensors;Computational modeling;High performance computing;Neural networks;Graphics processing units;Computer architecture;Sparse matrices","computational complexity;image representation;iterative methods;matrix algebra;matrix multiplication;neural nets;optimisation;sparse matrices","hardware-support;tile-wise sparsity;network pruning;high computation cost;deep neural network models;sparse models;randomly-distributed weights;irregular computations;meaningful speedup;commodity hardware;dense matrix computations;sparsity-optimized architectures;exploiting sparsity;pruning method;latency speedups;dense architectures;matrix multiplication;multiple smaller tiles;tiling-friendly tile-wise;tile level;irregular pruning;arbitrary pruning;sparsity pattern;dense model;sparse DNN models","",18.0,"",70.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Sparse GPU Kernels for Deep Learning","T. Gale; M. Zaharia; C. Young; E. Elsen","Stanford University, United States of America; Stanford University, United States of America; Google Brain, United States of America; DeepMind, United Kingdom","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Scientific workloads have traditionally exploited high levels of sparsity to accelerate computation and reduce memory requirements. While deep neural networks can be made sparse, achieving practical speedups on GPUs is difficult because these applications have relatively moderate levels of sparsity that are not sufficient for existing sparse kernels to outperform their dense counterparts. In this work, we study sparse matrices from deep learning applications and identify favorable properties that can be exploited to accelerate computation. Based on these insights, we develop high-performance GPU kernels for two sparse matrix operations widely applicable in neural networks: sparse matrix-dense matrix multiplication and sampled dense- dense matrix multiplication. Our kernels reach 27% of singleprecision peak on Nvidia V100 GPUs. Using our kernels, we demonstrate sparse Transformer and MobileNet models that achieve 1.2-2.1× speedups and up to 12.8× memory savings without sacrificing accuracy.","","978-1-7281-9998-6","10.1109/SC41405.2020.00021","Facebook; Google; Toyota Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355309","Neural networks;sparse matrices;graphics processing units","Deep learning;Neural networks;Memory management;Graphics processing units;Sparse matrices;Acceleration;Kernel","deep learning (artificial intelligence);graphics processing units;matrix multiplication;neural nets;sparse matrices","sparse GPU kernels;scientific workloads;memory requirements;deep neural networks;sparse kernels;sparse matrices;deep learning applications;high-performance GPU kernels;sparse matrix operations;matrix-dense matrix multiplication;Nvidia V100 GPU","",30.0,"",55.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"SpTFS: Sparse Tensor Format Selection for MTTKRP via Deep Learning","Q. Sun; Y. Liu; M. Dun; H. Yang; Z. Luan; L. Gan; G. Yang; D. Qian","Beihang University, China; Beihang University, China; Beihang University, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Wuxi, China; Beihang University, China; Tsinghua University, China; Tsinghua University, China; Beihang University, China","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Canonical polyadic decomposition (CPD) is one of the most common tensor computations adopted in many scientific applications. The major bottleneck of CPD is matricized tensor times Khatri-Rao product (MTTKRP). To optimize the performance of MTTKRP, various sparse tensor formats have been proposed such as CSF and HiCOO. However, due to the spatial complexity of the tensors, no single format fits all tensors. To address this problem, we propose SpTFS, a framework that automatically predicts the optimal storage format for an input sparse tensor. Specifically, SpTFS leverages a set of sampling methods to lower the sparse tensor to fix-sized matrices and specific features. Then, TnsNet combines CNN and the feature layer to accurately predict the optimal format. The experimental results show that SpTFS achieves prediction accuracy of 92.7% and 96% on CPU and GPU respectively.","","978-1-7281-9998-6","10.1109/SC41405.2020.00022","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355324","MTTKRP;Sparse Tensor;Format Selection;Convolutional Neural Network","Tensors;High performance computing;Neural networks;Graphics processing units;Sampling methods;Sparse matrices;Matrix decomposition","learning (artificial intelligence);matrix decomposition;sparse matrices;storage management;tensors","MTTKRP;deep learning;canonical polyadic decomposition;CPD;common tensor computations;scientific applications;tensor times Khatri-Rao product;sparse tensor formats;single format;optimal storage format;input sparse tensor;SpTFS leverages;fix-sized matrices;optimal format","",2.0,"",63.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Scaling Distributed Deep Learning Workloads beyond the Memory Capacity with KARMA","M. Wahib; H. Zhang; T. T. Nguyen; A. Drozd; J. Domke; L. Zhang; R. Takano; S. Matsuoka","RIKEN Center for Computational Science, Kobe, Japan; miHoYo Inc, Tokyo Institute of Technology; National Institute of Advanced Industrial Science and Technology, Japan; RIKEN Center for Computational Science, Kobe, Japan; RIKEN Center for Computational Science, Kobe, Japan; Tokyo Institute of Technology, Tokyo, Japan; National Institute of Advanced Industrial Science and Technology, Japan; Tokyo Institute of Technology, Tokyo, Japan","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"The dedicated memory of hardware accelerators can be insufficient to store all weights and/or intermediate states of large deep learning models. Although model parallelism is a viable approach to reduce the memory pressure issue, significant modification of the source code and considerations for algorithms are required. An alternative solution is to use out-of-core methods instead of, or in addition to, data parallelism. We propose a performance model based on the concurrency analysis of out-of-core training behavior, and derive a strategy that combines layer swapping and redundant recomputing. We achieve an average of 1. 52x speedup in six different models over the state-of-the-art out-of-core methods. We also introduce the first method to solve the challenging problem of out-of-core multi-node training by carefully pipelining gradient exchanges and performing the parameter updates on the host. Our data parallel out-of-core solution can outperform complex hybrid model parallelism in training large models, e.g. Megatron-LM and Turning-NLG.","","978-1-7281-9998-6","10.1109/SC41405.2020.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355256","Deep Neural Networks;Out-of-core;GPUs","Training;Deep learning;Computational modeling;High performance computing;Memory management;Data models;Pipeline processing","concurrency control;deep learning (artificial intelligence);parallel processing;source code (software)","distributed deep learning workloads;memory capacity;KARMA;hardware accelerators;source code;data parallelism;concurrency analysis;out-of-core training behavior;redundant recomputing;out-of-core multinode training;layer swapping","",3.0,"",45.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"ZeRO: Memory optimizations Toward Training Trillion Parameter Models","S. Rajbhandari; J. Rasley; O. Ruwase; Y. He","NA; NA; NA; NA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8. 3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create Turing-NLG, the world's largest language model at the time (17B parameters) with record breaking accuracy.","","978-1-7281-9998-6","10.1109/SC41405.2020.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355301","","Training;Solid modeling;Computational modeling;Memory management;Redundancy;Parallel processing;Data models","circuit optimisation;graphics processing units;integrated circuit modelling;integrated memory circuits;learning (artificial intelligence)","memory optimizations;deep learning models;model parallelism;device memory;Zero Redundancy Optimizer;training speed;memory redundancies;model-parallel training;computational granularity;trillion parameter models;ZeRO;Turing-NLG;GPU;Petaflops","",46.0,"",32.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Kraken: Memory-Efficient Continual Learning for Large-Scale Real-Time Recommendations","M. Xie; K. Ren; Y. Lu; G. Yang; Q. Xu; B. Wu; J. Lin; H. Ao; W. Xu; J. Shu","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Kuaishou Technology, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Kuaishou Technology, Beijing, China; Kuaishou Technology, Beijing, China; Kuaishou Technology, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Kuaishou Technology, Beijing, China; Kuaishou Technology, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,17,"Modern recommendation systems in industry often use deep learning (DL) models that achieve better model accuracy with more data and model parameters. However, current opensource DL frameworks, such as TensorFlow and PyTorch, show relatively low scalability on training recommendation models with terabytes of parameters. To efficiently learn large-scale recommendation models from data streams that generate hundreds of terabytes training data daily, we introduce a continual learning system called Kraken. Kraken contains a special parameter server implementation that dynamically adapts to the rapidly changing set of sparse features for the continual training and serving of recommendation models. Kraken provides a sparsity-aware training system that uses different learning optimizers for dense and sparse parameters to reduce memory overhead. Extensive experiments using real-world datasets confirm the effectiveness and scalability of Kraken. Kraken can benefit the accuracy of recommendation tasks with the same memory resources, or trisect the memory usage while keeping model performance.","","978-1-7281-9998-6","10.1109/SC41405.2020.00025","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355295","Systems for Machine Learning;Continual Learning;Recommendation System","Training;Adaptation models;Scalability;Data models;Real-time systems;Servers;Task analysis","learning (artificial intelligence);recommender systems","Kraken;memory-efficient continual learning;real-time recommendations;deep learning models;model parameters;large-scale recommendation models;data streams;continual learning system;special parameter server implementation;continual training;sparsity-aware training system;dense parameters;sparse parameters;opensource DL frameworks","",3.0,"",55.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Cell-List based Molecular Dynamics on Many-Core Processors: A Case Study on Sunway TaihuLight Supercomputer","X. Duan; P. Gao; M. Zhang; T. Zhang; H. Meng; Y. Li; B. Schmidt; H. Fu; L. Gan; W. Xue; W. Liu; G. Yang","National Supercomputing Center in Wuxi, China; National Supercomputing Center in Wuxi, China; National Supercomputing Center in Wuxi, China; National Supercomputing Center in Wuxi, China; National Supercomputing Center, China; National Supercomputing Center, China; Johannes-Gutenberg University of Mainz, Germany; National Supercomputing Center, China; National Supercomputing Center, China; National Supercomputing Center, China; National Supercomputing Center, China; National Supercomputing Center, China","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,12,"Molecular dynamics (MD) simulations are playing an increasingly important role in several research areas. The most frequently used potentials in MD simulations are pair-wise potentials. Due to the memory wall, computing pair-wise potentials on many-core processors are usually memory bounded. In this paper, we take the SW26010 processor as an exemplary platform to explore the possibility to break the memory bottleneck by improving data reusage via cell-list-based methods. We use cell-lists instead of neighbor-lists in the potential computation, and apply a number of novel optimization methods. Theses methods include: an adaptive replica arrangement strategy, a parameter profile data structure, and a particle-cell cutoff checking filter. An incremental cell-list building method is also realized to accelerate the construction of cell-lists. Furthermore, we have established an open source standalone framework, ESMD, featuring the techniques above. Experiments show that ESMD is 50~170% faster than previous ports on a single node, and can scale to 1,024 nodes with a weak scalibility of 95%.","","978-1-7281-9998-6","10.1109/SC41405.2020.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355271","Molecular Dynamics;Sunway TaihuLight;Neighbor-list-free method","Program processors;Microprocessors;Computational modeling;Switched mode power supplies;Optimization methods;Computer architecture;Supercomputers","coprocessors;data structures;molecular dynamics method;multiprocessing systems;optimisation;parallel machines","many-core processors;SW26010 processor;exemplary platform;memory bottleneck;data reusage;cell-list-based methods;cell-lists;neighbor-lists;potential computation;novel optimization methods;theses methods include;adaptive replica arrangement strategy;parameter profile data structure;particle-cell cutoff;incremental cell-list building method;cell-list based molecular dynamics;Sunway TaihuLight supercomputer;frequently used potentials;MD simulations;pair-wise potentials;memory wall","",1.0,"",22.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Evaluation of a Minimally Synchronous Algorithm for 2:1 Octree Balance","H. Suh; T. Isaac","School of Computational Science and Engineering Georgia Institute of Technology, Atlanta, Georgia; School of Computational Science and Engineering Georgia Institute of Technology, Atlanta, Georgia","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,12,"The p4est library implements octree-based adaptive mesh refinement (AMR) and has demonstrated parallel scalability beyond 100,000 MPI processes in previous weak scaling studies. This work focuses on the strong scalability of mesh adaptivity in p4est, where the communication pattern of the existing 2:1-balance is a latency bottleneck. The sorting-based algorithm of Malhotra and Biros has balanced communication, but synchronizes all processes. We propose an algorithm that combines sorting and neighbor-to-neighbor exchange to minimize the number of processes each process synchronizes with. We measure the performance of these algorithms on several test problems on Stampede2 at TACC. Both the parallel-sorting and minimally-synchronous algorithms significantly outperform the existing algorithm and have nearly identical performance out to 1,024 Xeon Phi KNL nodes, meaning the asymptotic advantage of the minimally-synchronous algorithm does not translate to improved performance at this scale. We conclude by showing that global metadata communication will limit future strong scaling.","","978-1-7281-9998-6","10.1109/SC41405.2020.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355233","","Scalability;High performance computing;Octrees;Metadata;Libraries;Synchronization;Sorting","mesh generation;message passing;meta data;octrees;parallel algorithms;parallel processing;sorting;synchronisation","minimally synchronous algorithm;2:1 octree balance;p4est library;adaptive mesh refinement;parallel scalability;100 MPI processes;000 MPI processes;previous weak scaling studies;strong scalability;mesh adaptivity;communication pattern;sorting-based algorithm;synchronizes all processes;neighbor-to-neighbor exchange;processes each process;Stampede2;parallel-sorting algorithms;minimally-synchronous algorithms;1 Xeon Phi KNL nodes;024 Xeon Phi KNL nodes;minimally-synchronous algorithm;future strong scaling","","","",30.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Distributed-Memory DMRG via Sparse and Dense Parallel Tensor Contractions","R. Levy; E. Solomonik; B. K. Clark","Department of Physics, Institute for Condensed Matter Theory; Department of Computer Science, University of Illinois at Urbana-Champaign, IL, USA; Department of Physics, Institute for Condensed Matter Theory","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"The density matrix renormalization group (DMRG) algorithm is a powerful tool for solving eigenvalue problems to model quantum systems. DMRG relies on tensor contractions and dense linear algebra to compute properties of condensed matter physics systems. However, its efficient parallel implementation is challenging due to limited concurrency, large memory footprint, and tensor sparsity. We mitigate these problems by implementing two new parallel approaches that handle block sparsity arising in DMRG, via Cyclops, a distributed memory tensor contraction library. We benchmark their performance on two physical systems using the Blue Waters and Stampede2 supercomputers. Our DMRG performance is improved by up to 5.9X in runtime and 99X in processing rate over ITensor, at roughly comparable computational resource use. This enables higher accuracy calculations via larger tensors for quantum state approximation. We demonstrate that despite having limited concurrency, DMRG is weakly scalable with the use of efficient parallel tensor contraction mechanisms.","","978-1-7281-9998-6","10.1109/SC41405.2020.00028","National Science Foundation; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355235","DMRG;tensor networks;tensor contractions;sparse tensors;quantum systems;Cyclops Tensor Framework","Concurrent computing;Tensors;Runtime;Benchmark testing;Tools;Supercomputers;Water resources","eigenvalues and eigenfunctions;linear algebra;parallel algorithms;parallel processing;physics computing;quantum chemistry;renormalisation;tensors","distributed-memory DMRG;dense parallel tensor contractions;density matrix renormalization group algorithm;eigenvalue problems;quantum systems;dense linear algebra;condensed matter physics systems;efficient parallel implementation;concurrency;memory footprint;tensor sparsity;parallel approaches;block sparsity;distributed memory tensor contraction library;physical systems;DMRG performance;roughly comparable computational resource use;larger tensors;efficient parallel tensor contraction mechanisms","",5.0,"",47.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"TAGO: Rethinking Routing Design in High Performance Reconfigurable Networks","M. Y. Teh; Y. -H. Hung; G. Michelogiannakis; S. Yan; M. Glick; J. Shalf; K. Bergman","Columbia University; Columbia University; Lawrence Berkeley National Lab; Columbia University; Columbia University; Lawrence Berkeley National Lab; Columbia University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"Many reconfigurable network topologies have been proposed in the past. However, efficient routing on top of these flexible interconnects still presents a challenge. In this work, we reevaluate key principles that have guided the designs of many routing protocols on static networks, and see how well those principles apply on reconfigurable network topologies. Based on a theoretical analysis of key properties that routing in a reconfigurable network should satisfy to maximize performance, we propose a topology-aware, globally-direct oblivious (TAGO) routing protocol for reconfigurable topologies. Our proposed routing protocol is simple in design and yet, when deployed in conjunction with a reconfigurable network topology, improves throughput by up to 2.2× compared to established routing protocols and even comes within 10% of the throughput of impractical adaptive routing that has instant global congestion information.","","978-1-7281-9998-6","10.1109/SC41405.2020.00029","Office of Science; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355260","Adaptive Routing;Oblivious Routing;Reconfigurable Networks;Bandwidth Steering","Adaptive systems;Network topology;High performance computing;Routing;Throughput;Routing protocols;Topology","routing protocols;telecommunication network topology","reconfigurable network topology;reconfigurable topologies;adaptive routing;routing design;high performance reconfigurable networks;static networks;topology-aware globally-direct oblivious routing protocol;TAGO routing protocol;global congestion information;global congestion information","","","",76.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Architecture and Performance Studies of 3D-Hyper-FleX-LION for Reconfigurable All-to-All HPC Networks","G. Liu; R. Proietti; M. Fariborz; P. Fotouhi; X. Xiao; S. J. Ben Yoo","Electrical and Computer Engineering Department, University of California, Davis, Davis, USA; Electrical and Computer Engineering Department, University of California, Davis, Davis, USA; Electrical and Computer Engineering Department, University of California, Davis, Davis, USA; Electrical and Computer Engineering Department, University of California, Davis, Davis, USA; Electrical and Computer Engineering Department, University of California, Davis, Davis, USA; Electrical and Computer Engineering Department, University of California, Davis, Davis, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"While the Fat-Tree network topology represents the dominant state-of-art solution for large-scale HPC networks, its scalability in terms of power, latency, complexity, and cost is significantly challenged by the ever-increasing communication bandwidth among tens of thousands of heterogeneous computing nodes. We propose 3D-Hyper-FleX-LION, a flat hybrid electronic-photonic interconnect network that leverages the multichannel nature of modern multi-terabit switch ASICs (with 100 Gb/s granularity) and a reconfigurable all-to-all photonic fabric called Flex-LIONS. Compared to a Fat-Tree network interconnecting the same number of nodes and with the same oversubscription ratio, the proposed 3D-Hyper-FleX-LION offers a 20% smaller diameter, 3x lower power consumption, 10X fewer cable connections, and 4x reduction in the number of transceivers. When bandwidth reconfiguration capabilities of Flex-LIONS are exploited for non-uniform traffic workloads, simulation results indicate that 3D-Hyper-FleX-LION can achieve up to 4x improvement in energy efficiency for synthetic traffic workloads with high locality compared to Fat-Tree.","","978-1-7281-9998-6","10.1109/SC41405.2020.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355261","Computer network;Reconfigurable architectures;Optical interconnections;Photonic integrated circuits;Wavelength routing","Optical polarization;Power demand;Optical switches;Bandwidth;Computer architecture;Energy efficiency;Photonics","application specific integrated circuits;multiprocessor interconnection networks;optical interconnections;parallel processing;photonic switching systems;telecommunication network topology;telecommunication traffic;transceivers","Fat-Tree network topology;large-scale HPC networks;3D-Hyper-FleX-LION;Flex-LIONS;Reconfigurable All-to-All HPC Networks;flat hybrid electronic-photonic interconnect network;modern multi-terabit switch ASICs","",11.0,"",72.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"FatPaths: Routing in Supercomputers and Data Centers when Shortest Paths Fall Short","M. Besta; M. Schneider; M. Konieczny; K. Cynk; E. Henriksson; S. D. Girolamo; A. Singla; T. Hoefler","Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Faculty of Computer Science, Electronics and Telecommunications, AGH-UST; Faculty of Computer Science, Electronics and Telecommunications, AGH-UST; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,18,"We introduce FatPaths: a simple, generic, and robust routing architecture that enables state-of-the-art low-diameter topologies such as Slim Fly to achieve unprecedented performance. FatPaths targets Ethernet stacks in both HPC supercomputers as well as cloud data centers and clusters. FatPaths exposes and exploits the rich (“fat”) diversity of both minimal and non-minimal paths for high-performance multi-pathing. Moreover, FatPaths uses a redesigned “purified” transport layer that removes virtually all TCP performance issues (e.g., the slow start), and incorporates flowlet switching, a technique used to prevent packet reordering in TCP networks, to enable very simple and effective load balancing. Our design enables recent low-diameter topologies to outperform powerful Clos designs, achieving 15% higher net throughput at 2” lower latency for comparable cost. FatPaths will significantly accelerate Ethernet clusters that form more than 50% of the Top500 list and it may become a standard routing scheme for modern topologies.Extended paper version: https://arxiv.org/abs/1906.10885","","978-1-7281-9998-6","10.1109/SC41405.2020.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355307","","Data centers;Network topology;Ethernet;Routing;Load management;Supercomputers;Topology","cloud computing;computer centres;local area networks;mainframes;packet switching;parallel machines;resource allocation;telecommunication network routing;telecommunication network topology;transport protocols","routing architecture;low-diameter topologies;HPC supercomputers;cloud data centers;FatPaths;high-performance multipathing;load balancing;routing scheme;Ethernet clusters;TCP networks;Clos designs;flowlet switching;packet reordering;redesigned purified transport layer;Ethernet stacks","",7.0,"",163.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"SCALANA: Automating Scaling Loss Detection with Graph Analysis","Y. Jin; H. Wang; T. Yu; X. Tang; T. Hoefler; X. Liu; J. Zhai","Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; ETH Zürich; North Carolina State University; Tsinghua University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Scaling a parallel program to modern supercomputers is challenging due to inter-process communication, Amdahl’s law, and resource contention. Performance analysis tools for finding such scaling bottlenecks either base on profiling or tracing. Profiling incurs low overheads but does not capture detailed dependencies needed for root-cause analysis. Tracing collects all information at prohibitive overheads. In this work, we design SCALANA that uses static analysis techniques to achieve the best of both worlds - it enables the analyzability of traces at a cost similar to profiling. SCALANA first leverages static compiler techniques to build a Program Structure Graph, which records the main computation and communication patterns as well as the program’s control structures. At runtime, we adopt lightweight techniques to collect performance data according to the graph structure and generate a Program Performance Graph. With this graph, we propose a novel approach, called backtracking root cause detection, which can automatically and efficiently detect the root cause of scaling loss. We evaluate SCALANA with real applications. Results show that our approach can effectively locate the root cause of scaling loss for real applications and incurs 1.73parcent overhead on average for up to 2,048 processes. We achieve up to 11.11parcent performance improvement by fixing the root causes detected by SCALANA on 2,048 processes.","","978-1-7281-9998-6","10.1109/SC41405.2020.00032","National Natural Science Foundation of China; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355294","Performance Analysis;Scalability Bottleneck;Root-Cause Detection;Static Analysis","Backtracking;Runtime;Scalability;Static analysis;Tools;Supercomputers;Performance analysis","graph theory;parallel programming;program compilers;program diagnostics","static compiler;graph analysis;parallel program;modern supercomputers;inter-process communication;Amdahl's law;resource contention;root-cause analysis;tracing;static analysis;static compiler;program structure graph;lightweight techniques;program performance graph;backtracking root cause detection;SCALANA;scaling loss detection automation","",2.0,"",66.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"ZeroSpy: Exploring Software Inefficiency with Redundant Zeros","X. You; H. Yang; Z. Luan; D. Qian; X. Liu","Beihang University, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Wuxi, China; Beihang University, China; Beihang University, China; North Carolina State University, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Redundant zeros cause inefficiencies in which the zero values are loaded and computed repeatedly, resulting in unnecessary memory traffic and identity computation that waste memory bandwidth and CPU resources. optimizing compilers is difficult in eliminating these zero-related inefficiencies due to limitations in static analysis. Hardware approaches, in contrast, optimize inefficiencies without code modification, but are not widely adopted in commodity processors. In this paper, we propose ZeroSpy - a fine-grained profiler to identify redundant zeros caused by both inappropriate use of data structures and useless computation. ZeroSpy also provides intuitive optimization guidance by revealing the locations where the redundant zeros happen in source lines and calling contexts. The experimental results demonstrate ZeroSpy is capable of identifying redundant zeros in programs that have been highly optimized for years. Based on the optimization guidance revealed by ZeroSpy, we can achieve significant speedups after eliminating redundant zeros.","","978-1-7281-9998-6","10.1109/SC41405.2020.00033","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355303","Redundant Zero;Software Inefficiency;Performance Profiling and optimization","Optimizing compilers;Memory management;Redundancy;Static analysis;Production;Software;Optimization","data structures;optimising compilers;redundancy","redundant zeros;ZeroSpy;zero values;memory traffic;identity computation;waste memory bandwidth;software inefficiency;CPU resources;fine-grained profiler;optimizing compilers;data structures","",2.0,"",62.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"DRCCTPROF: A Fine-Grained Call Path Profiler for ARM-Based Clusters","Q. Zhao; X. Liu; M. Chabbi","William & Mary; North Carolina State University; Scalable Machines Research","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"ARM is an attractive CPU architecture for exascale systems because of its energy efficiency. As a recent entry into the HPC paradigm, ARM lags in its software stack, especially in the performance tooling aspect. Notably, there is a lack of fine-grained measurement tools to analyze fully optimized HPC binary executables on ARM processors. In this paper, we introduce DRCCTPROF — a fine-grained call path profiling framework for binaries running on ARM architectures. The unique ability of DRCCTPROF is to obtain full calling context at any and every machine instruction that executes, which provides more detailed diagnostic feedback for performance optimization and correctness tools. Furthermore, DRCCTPROF not only associates any instruction with source code along the call path, but also associates memory access instructions back to the constituent data object. Finally, DRCCTPROF incurs moderate overhead and provides a compact view to visualize the profiles collected from parallel executions.","","978-1-7281-9998-6","10.1109/SC41405.2020.00034","Google; Google; Thomas F. and Kate Miller Jeffress Memorial Trust; Bank of America; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355248","Fine-grained analysis;ARM;performance analysis;debugging;high-performance computing","Context;Instruction sets;Computer architecture;Tools;Software;Optimization;Monitoring","microprocessor chips;parallel processing","ARM architectures;machine instruction;performance optimization;correctness tools;memory access instructions;fine-grained call path profiler;ARM-based clusters;CPU architecture;exascale systems;energy efficiency;software stack;performance tooling aspect;fine-grained measurement tools;fully optimized HPC binary executables;ARM processors;fine-grained call path profiling framework;DRCCTPROF","",1.0,"",73.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"RLScheduler: An Automated HPC Batch Job Scheduler Using Reinforcement Learning","D. Zhang; D. Dai; Y. He; F. S. Bao; B. Xie","Computer Science Department, University of North Carolina at Charlotte; Computer Science Department, University of North Carolina at Charlotte; Computer Science Department, Iowa State University; Computer Science Department, Iowa State University; Oak Ridge National Laboratory, Oak Ridge Leadership Computing Facility","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Today's high-performance computing (HPC) platforms are still dominated by batch jobs. Accordingly, effective batch job scheduling is crucial to obtain high system efficiency. Existing HPC batch job schedulers typically leverage heuristic priority functions to prioritize and schedule jobs. But, once configured and deployed by the experts, such priority functions can hardly adapt to the changes of job loads, optimization goals, or system settings, potentially leading to degraded system efficiency when changes occur. To address this fundamental issue, we present RLScheduler, an automated HPC batch job scheduler built on reinforcement learning. RLScheduler relies on minimal manual interventions or expert knowledge, but can learn high-quality scheduling policies via its own continuous `trial and error'. We introduce a new kernel-based neural network structure and trajectory filtering mechanism in RLScheduler to improve and stabilize the learning process. Through extensive evaluations, we confirm that RLScheduler can learn high-quality scheduling policies towards various workloads and various optimization goals with relatively low computation cost. Moreover, we show that the learned models perform stably even when applied to unseen workloads, making them practical for production use.","","978-1-7281-9998-6","10.1109/SC41405.2020.00035","National Science Foundation; Oak Ridge National Laboratory; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355253","","Schedules;Processor scheduling;Neural networks;Reinforcement learning;Production;Trajectory;Optimization","learning (artificial intelligence);neural nets;optimisation;parallel processing;scheduling","RLScheduler;automated HPC batch job scheduler;reinforcement learning;high-performance computing platforms;HPC batch job schedulers;high-quality scheduling policies;kernel-based neural network structure;trajectory filtering mechanism;optimization goals","",24.0,"",46.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Alita: Comprehensive Performance Isolation through Bias Resource Management for Public Clouds","Q. Chen; S. Xue; S. Zhao; S. Chen; Y. Wu; Y. Xu; Z. Song; T. Ma; Y. Yang; M. Guo","Alibaba Cloud, China; Alibaba Cloud, China; Alibaba Cloud, China; Alibaba Cloud, China; Alibaba Cloud, China; Alibaba Cloud, China; Alibaba Cloud, China; Alibaba Cloud, China; Alibaba Cloud, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"The tenants of public cloud platforms share hard-ware resources on the same node, resulting in the potential for performance interference (or malicious attacks). A tenant is able to degrade the performance of its neighbors on the same node significantly through overuse of the shared memory bus, last level cache (LLC)/memory bandwidth, and power. To eliminate such unfairness we propose Alita, a runtime system consisting of an online interference identifier and adaptive interference eliminator. The interference identifier monitors hardware and system-level event statistics to identify resource polluters. The eliminator improves the performance of normal applications by throttling only the resource usage of polluters. Specifically, Alita adopts bus lock sparsification, bias LLC/bandwidth isolation, and selective power throttling to throttle the resource usage of polluters. Results for an experimental platform and in-production cloud platform with 30,000 nodes demonstrate that Alita significantly improves the performance of co-located virtual machines in the presence of resource polluters based on system-level knowledge.","","978-1-7281-9998-6","10.1109/SC41405.2020.00036","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355282","","Cloud computing;Runtime;Adaptive systems;Interference;Hardware;Virtual machining;Monitoring","cache storage;cloud computing;resource allocation;shared memory systems;virtual machines","Alita;comprehensive performance isolation;bias resource management;public cloud platforms;performance interference;malicious attacks;runtime system;online interference identifier;adaptive interference eliminator;system-level event statistics;resource polluters;bus lock sparsification;virtual machines","",6.0,"",49.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"HPC I/O Throughput Bottleneck Analysis with Explainable Local Models","M. Isakov; E. d. Rosario; S. Madireddy; P. Balaprakash; P. Carns; R. B. Ross; M. A. Kinsy","Department of Electrical and Computer Engineering, Adaptive and Secure Computing Systems (ASCS) Laboratory, Texas A & M University, College Station, TX; Department of Electrical and Computer Engineering, Adaptive and Secure Computing Systems (ASCS) Laboratory, Texas A & M University, College Station, TX; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Department of Electrical and Computer Engineering, Adaptive and Secure Computing Systems (ASCS) Laboratory, Texas A & M University, College Station, TX","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"With the growing complexity of high-performance computing (HPC) systems, achieving high performance can be difficult because of I/O bottlenecks. We analyze multiple years' worth of Darshan logs from the Argonne Leadership Computing Facility's Theta supercomputer in order to understand causes of poor I/O throughput. We present Gauge: a data-driven diagnostic tool for exploring the latent space of supercomputing job features, understanding behaviors of clusters of jobs, and interpreting I/O bottlenecks. We find groups of jobs that at first sight are highly heterogeneous but share certain behaviors, and analyze these groups instead of individual jobs, allowing us to reduce the workload of domain experts and automate I/O performance analysis. We conduct a case study where a system owner using Gauge was able to arrive at several clusters that do not conform to conventional I/O behaviors, as well as find several potential improvements, both on the application level and the system level.","","978-1-7281-9998-6","10.1109/SC41405.2020.00037","U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355272","HPC;I/O;diagnostics;machine learning;clustering","Leadership;Tools;Writing;Throughput;Supercomputers;Space exploration;Tuning","computational complexity;data handling;input-output programs;mainframes;multiprocessing systems;parallel machines;parallel processing;performance evaluation;program diagnostics","explainable local models;high-performance computing systems;Argonne Leadership Computing Facility;Gauge;data-driven diagnostic tool;latent space;performance analysis;system owner;system level;supercomputing job features;Theta supercomputer;HPC I/O throughput bottleneck analysis;Darshan logs","",5.0,"",32.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"A Hierarchical and Load-Aware Design for Large Message Neighborhood Collectives","S. M. Ghazimirsaeed; Q. Zhou; A. Ruhela; M. Bayatpour; H. Subramoni; D. K. D. Panda","The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"The MPI-3.0 standard introduced neighborhood collective to support sparse communication patterns used in many applications. In this paper, we propose a hierarchical and distributed graph topology that considers the physical topology of the system and the virtual communication pattern of processes to improve the performance of large message neighborhood collectives. Moreover, we propose two design alternatives on top of the hierarchical design: 1. LAG-H: assumes the same communication load for all processes, 2. LAW-H: considers the communication load of processes for fair distribution of load between them. We propose a mathematical model to determine the communication capacity of each process. Then, we use the derived capacity to fairly distribute the load between processes. Our experimental results on up to 28,672 processes show up to 9x speedup for various process topologies. We also observe up to 8.2% performance gain and 34x speedup for NAS-DT and SpMM, respectively.","","978-1-7281-9998-6","10.1109/SC41405.2020.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355240","MPI neighborhood collective;virtual topology;communication pattern","Network topology;High performance computing;Performance gain;Topology;Mathematical model;Standards","application program interfaces;graph theory;message passing","message neighborhood collectives;sparse communication patterns;distributed graph topology;physical topology;virtual communication pattern;hierarchical design;communication load;fair distribution;communication capacity;process topologies;performance gain;load-aware design;MPI-3.0 standard;hierarchical graph topology;LAG-H;LAW-H;mathematical model;NAS-DT;SpMM;message passing interface","","","",29.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"An In-Depth Analysis of the Slingshot Interconnect","D. De Sensi; S. Di Girolamo; K. H. McMahon; D. Roweth; T. Hoefler","Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Hewlett Packard Enterprise; Hewlett Packard Enterprise; Department of Computer Science, ETH Zurich","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"The interconnect is one of the most critical components in large scale computing systems, and its impact on the performance of applications is going to increase with the system size. In this paper, we will describe SLINGSHOT, an interconnection network for large scale computing systems. SLINGSHOT is based on high-radix switches, which allow building exascale and hyper-scale datacenters networks with at most three switch-to-switch hops. Moreover, SLINGSHOT provides efficient adaptive routing and congestion control algorithms, and highly tunable traffic classes. SLINGSHOT uses an optimized Ethernet protocol, which allows it to be interoperable with standard Ethernet devices while providing high performance to HPC applications. We analyze the extent to which SLINGSHOT provides these features, evaluating it on microbenchmarks and on several applications from the datacenter and AI worlds, as well as on HPC applications. We find that applications running on SLINGSHOT are less affected by congestion compared to previous generation networks.","","978-1-7281-9998-6","10.1109/SC41405.2020.00039","European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355230","interconnection network;dragonfly;exascale;datacenters;congestion","Multiprocessor interconnection;Ethernet;Switches;Routing;Supercomputers;Resource management;Standards","computer centres;local area networks;multiprocessor interconnection networks;parallel processing;protocols;telecommunication congestion control;telecommunication network routing;telecommunication switching","interconnection network;large scale computing systems;high-radix switches;hyper-scale datacenters networks;switch-to-switch hops;highly tunable traffic classes;HPC applications;slingshot interconnect;adaptive routing;congestion control algorithms;Ethernet protocol;Ethernet devices;AI worlds","",26.0,"",74.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"CAB-MPI: Exploring Interprocess Work-Stealing towards Balanced MPI Communication","K. Ouyang; M. Si; A. Hori; Z. Chen; P. Balaji","Computer Science and Engineering, University of California, Riverside, USA; Mathematics and Computer Science, Argonne National Laboratory, USA; RIKEN Center for Computational Science, RIKEN, Japan; Computer Science and Engineering, University of California, Riverside, USA; Mathematics and Computer Science, Argonne National Laboratory, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Load balance is essential for high-performance applications. Unbalanced communication can cause severe performance degradation, even in computation-balanced BSP applications. Designing communication-balanced applications is challenging, however, because of the diverse communication implementations at the underlying runtime system. In this paper, we address this challenge through an interprocess workstealing scheme based on process-memory-sharing techniques. We present CAB-MPI, an MPI implementation that can identify idle processes inside MPI and use these idle resources to dynamically balance communication workload on the node. We design throughput-optimized strategies to ensure efficient stealing of the data movement tasks. We demonstrate the benefit of work stealing through several internal processes in MPI, including intranode data transfer, pack/unpack for noncontiguous communication, and computation in one-sided accumulates. The implementation is evaluated through a set of microbenchmarks and proxy applications on Intel Xeon and Xeon Phi platforms.","","978-1-7281-9998-6","10.1109/SC41405.2020.00040","Argonne National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355278","Work stealing;MPI;load balance;communication","Degradation;Runtime;High performance computing;Data transfer;Task analysis;Optimization","application program interfaces;message passing;multiprocessing systems;performance evaluation;resource allocation","Intel Xeon platforms;Xeon Phi platforms;proxy applications;noncontiguous communication;communication workload;idle resources;idle processes;MPI implementation;process-memory-sharing techniques;underlying runtime system;computation-balanced BSP applications;high-performance applications;load balance;balanced MPI communication;interprocess work-stealing;CAB-MPI","",2.0,"",46.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Petascale XCT: 3D Image Reconstruction with Hierarchical Communications on Multi-GPU Nodes","M. Hidayetoğlu; T. Bicer; S. G. de Gonzalo; B. Ren; V. De Andrade; D. Gursoy; R. Kettimuthu; I. T. Foster; W. -m. W. Hwu","University of Illinois at Urbana-Champaign, USA; Argonne National Laboratory, IL, USA; Barcelona Supercomputing Center, Spain; College of William & Mary, VA, USA; Argonne National Laboratory, IL, USA; Argonne National Laboratory, IL, USA; Argonne National Laboratory, IL, USA; Argonne National Laboratory, IL, USA; University of Illinois at Urbana-Champaign, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"X-ray computed tomography is a commonly used technique for noninvasive imaging at synchrotron facilities. Iterative tomographic reconstruction algorithms are often preferred for recovering high quality 3D volumetric images from 2D X-ray images, however, their use has been limited to small/medium datasets due to their computational requirements. In this paper, we propose a high-performance iterative reconstruction system for terabyte(s)-scale 3D volumes. Our design involves three novel optimizations: (1) optimization of (back)projection operators by extending the 2D memory-centric approach to 3D;(2) performing hierarchical communications by exploiting “fat-node” architecture with many GPUs; 3) utilization of mixed-precision types while preserving convergence rate and quality. We extensively evaluate the proposed optimizations and scaling on the Summit supercomputer. Our largest reconstruction is a mouse brain volume with 9×11K×11K voxels, where the total reconstruction time is under three minutes using 24,576 GPUs, reaching 65 PFLOPS: 34% of Summit's peak performance.","","978-1-7281-9998-6","10.1109/SC41405.2020.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355274","","Three-dimensional displays;Two dimensional displays;Synchrotrons;Supercomputers;Image reconstruction;Optimization;X-ray imaging","brain;computerised tomography;graphics processing units;image reconstruction;iterative methods","2D memory-centric approach;hierarchical communications;convergence rate;mouse brain volume;total reconstruction time;petascale XCT;3D image reconstruction;multiGPU nodes;X-ray computed tomography;noninvasive imaging;synchrotron facilities;iterative tomographic reconstruction algorithms;high quality 3D volumetric images;2D X-ray images;computational requirements;high-performance iterative reconstruction system;Summit peak performance;computer speed 65.0 PFLOPS","",7.0,"",42.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Multi-Node Multi-GPU Diffeomorphic Image Registration for Large-Scale Imaging Problems","M. Brunn; N. Himthani; G. Biros; M. Mehl; A. Mang","Computer Science, University of Stuttgart, Stuttgart, DE; Oden Institute, University of Texas, Austin, TX, US; Oden Institute, University of Texas, Austin, TX, US; Computer Science, University of Stuttgart, Stuttgart, DE; Mathematics, University of Houston, Houston, TX, US","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,17,"We present a Gauss-Newton-Krylov solver for large deformation diffeomorphic image registration. We extend the publicly available CLAIRE library to multi-node multi-graphics processing unit (GPUs) systems and introduce novel algorithmic modifications that significantly improve performance. Our contributions comprise (i) a new preconditioner for the reduced-space Gauss-Newton Hessian system, (ii) a highly-optimized multi-node multi-GPU implementation exploiting device direct communication for the main computational kernels (interpolation, high-order finite difference operators and Fast-Fourier-Transform), and (iii) a comparison with state-of-the-art CPU and GPU implementations. We solve a 2563-resolution image registration problem in five seconds on a single NVIDIA Tesla V100, with a performance speedup of 70% compared to the state-of-the-art. In our largest run, we register 20483 resolution images (25B unknowns; approximately 152$\times$ larger than the largest problem solved in state-of-the-art GPU implementations) on 64 nodes with 256 GPUs on TACC’s Longhorn system.","","978-1-7281-9998-6","10.1109/SC41405.2020.00042","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355267","diffeomorphic image registration;multi-node multi-GPU;Gauss’Newton’Krylov solver;PDE-constrained optimization;preconditioning","Performance evaluation;Image registration;Image resolution;Graphics processing units;Imaging;IP networks;Kernel","computer graphics;graphics processing units;image registration;image resolution;Newton method;optimisation","resolution image registration problem;state-of-the-art GPU implementations;multinode multiGPU diffeomorphic image registration;large-scale imaging problems;Gauss-Newton-Krylov;deformation diffeomorphic image registration;publicly available CLAIRE library;multigraphics processing unit systems;reduced-space Gauss-Newton Hessian system;highly-optimized multinode multiGPU implementation;high-order finite difference operators;TACC Longhorn system","",2.0,"",74.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"SegAlign: A Scalable GPU-Based Whole Genome Aligner","S. D. Goenka; Y. Turakhia; B. Paten; M. Horowitz","Stanford University; University of California, Santa Cruz; University of California, Santa Cruz; Stanford University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"Pairwise Whole Genome Alignment (WGA) is a crucial first step to understanding evolution at the DNA sequence-level. Pairwise WGA of thousands of currently available species genomes could help make biological discoveries, however, computing them for even a fraction of the millions of possible pairs is prohibitive - WGA of a single pair of vertebrate genomes (human-mouse) takes 11 hours on a 96-core Amazon Web Services (AWS) instance (c5.24xlarge). This paper presents SegAlign - a scalable, GPU-accelerated system for computing pairwise WGA. SegAlign is based on the standard seed-filter-extend heuristic, in which the filtering stage dominates the runtime (e.g. 98% for human-mouse WGA), and is accelerated using GPU(s). Using three vertebrate genome pairs, we show that SegAlign provides a speedup of up to 14× on an 8-GPU, 64-core AWS instance (p3.16xlarge) for WGA and nearly 2.3× reduction in dollar cost. SegAlign also allows parallelization over multiple GPU nodes and scales efficiently.","","978-1-7281-9998-6","10.1109/SC41405.2020.00043","Stanford SystemX Alliance; National Human Genome Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355243","Whole Genome Alignment;Graphics Processing Unit (GPU);Comparative Genomics;Apache Spark","Runtime;Web services;High performance computing;Genomics;Sparks;Bioinformatics;Standards","bioinformatics;DNA;genetics;genomics;graphics processing units;Web services","SegAlign;GPU-accelerated system;pairwise WGA;vertebrate genome pairs;AWS instance;Pairwise Whole Genome Alignment;DNA sequence-level;biological discoveries;Amazon Web Services instance;seed-filter-extend heuristic;time 11.0 hour","",4.0,"",58.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"TOSS-2020: A Commodity Software Stack for HPC","E. A. León; T. D’Hooge; N. Hanford; I. Karlin; R. Pankajakshan; J. Foraker; C. Chambreau; M. L. Leininger","Livermore Computing Lawrence Livermore National Laboratory, Livermore, California, USA; Livermore Computing Lawrence Livermore National Laboratory, Livermore, California, USA; Livermore Computing Lawrence Livermore National Laboratory, Livermore, California, USA; Livermore Computing Lawrence Livermore National Laboratory, Livermore, California, USA; Livermore Computing Lawrence Livermore National Laboratory, Livermore, California, USA; Livermore Computing Lawrence Livermore National Laboratory, Livermore, California, USA; Livermore Computing Lawrence Livermore National Laboratory, Livermore, California, USA; Livermore Computing Lawrence Livermore National Laboratory, Livermore, California, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"The simulation environment of any HPC platform is key to the performance, portability, and productivity of scientific applications. This environment has traditionally been provided by platform vendors, presenting challenges for HPC centers and users including platform-specific software that tend to stagnate over the lifetime of the system. In this paper, we present the Tri-Laboratory Operating System Stack (TOSS), a production simulation environment based on Linux and open source software, with proprietary software components integrated as needed. TOSS, focused on mid-to-large scale commodity HPC systems, provides a common simulation environment across system architectures, reduces the learning curve on new systems, and benefits from a lineage of past experience and bug fixes. To further the scope and applicability of TOSS, we demonstrate its feasibility and effectiveness on a leadership-class supercomputer architecture. Our evaluation, relative to the vendor stack, includes an analysis of resource manager complexity, system noise, networking, and application performance.","","978-1-7281-9998-6","10.1109/SC41405.2020.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355285","Scientific computing;Accelerator architectures;Parallel architectures;Multicore processing;Multiprocessor interconnection networks;Parallel machines;Supercomputers;Processor scheduling;Cluster computing;High performance computing;Software performance;Software reusability;System software;Operating systems;Utility programs;Programming environments;Runtime;Runtime environment;Software libraries","Productivity;Operating systems;Linux;Systems architecture;Supercomputers;Libraries;Open source software","Linux;object-oriented programming;parallel processing;public domain software;resource allocation;software architecture","platform-specific software;production simulation environment;open source software;proprietary software components;system architectures;vendor stack;system noise;TOSS-2020;commodity software stack;productivity;scientific applications;platform vendors;tri-laboratory operating system stack;mid-to-large scale commodity HPC systems;Linux;learning curve;leadership-class supercomputer architecture;resource manager complexity;networking","","","",62.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"GPU Lifetimes on Titan Supercomputer: Survival Analysis and Reliability","G. Ostrouchov; D. Maxwell; R. A. Ashraf; C. Engelmann; M. Shankar; J. H. Rogers","Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; National Center for Computational Sciences, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; National Center for Computational Sciences, Oak Ridge National Laboratory, Oak Ridge, TN, USA; National Center for Computational Sciences, Oak Ridge National Laboratory, Oak Ridge, TN, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"The Cray XK7 Titan was the top supercomputer system in the world for a long time and remained critically important throughout its nearly seven year life. It was an interesting machine from a reliability viewpoint as most of its power came from 18,688 GPUs whose operation was forced to execute three rework cycles, two on the GPU mechanical assembly and one on the GPU circuitboards. We write about the last rework cycle and a reliability analysis of over 100,000 years of GPU lifetimes during Titan's 6-year-long productive period. Using time between failures analysis and statistical survival analysis techniques, we find that GPU reliability is dependent on heat dissipation to an extent that strongly correlates with detailed nuances of the cooling architecture and job scheduling. We describe the history, data collection, cleaning, and analysis and give recommendations for future supercomputing systems. We make the data and our analysis codes publicly available.","","978-1-7281-9998-6","10.1109/SC41405.2020.00045","U.S. Department of Energy; Battelle; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355319","GPU;reliability;supercomputer;NVIDIA;Cray;large-scale systems;log analysis;MTBF;Kaplan-Meier survival;Cox regression;GPU failure data set","Heating systems;Resistors;Graphics processing units;Market research;Supercomputers;Planning;Integrated circuit reliability","cooling;failure analysis;graphics processing units;mainframes;parallel machines;reliability;scheduling","GPU lifetimes;Titan supercomputer;Cray XK7 Titan;rework cycle;GPU mechanical assembly;GPU circuitboards;reliability analysis;failures analysis;statistical survival analysis;GPU reliability;future supercomputing systems;analysis codes;cooling architecture;job scheduling;data collection","",9.0,"",37.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Iris: Allocation Banking and Identity and Access Management for the Exascale Era","G. Torok; M. R. Day; R. J. Hartman-Baker; C. Snavely","National Energy Research Scientific Computing Center, Lawrence Berkeley National Laboratory, Berkeley, United States of America; National Energy Research Scientific Computing Center, Lawrence Berkeley National Laboratory, Berkeley, United States of America; National Energy Research Scientific Computing Center, Lawrence Berkeley National Laboratory, Berkeley, United States of America; National Energy Research Scientific Computing Center, Lawrence Berkeley National Laboratory, Berkeley, United States of America","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,11,"Without a reliable and scalable system for managing authorized users and ensuring they receive their allocated share of computational and storage resources, modern HPC centers would not be able to function. Exascale will amplify these demands with greater machine scale, more users, higher job throughput, and ever-increasing need for management insight and automation throughout the HPC environment. When our legacy system reached retirement age, NERSC took the opportunity to design and build Iris not only to meet our current needs, with 8,000 users and tens of thousands of jobs per day, but also to scale well into the exascale era. In this paper, we describe how we have designed Iris to meet these needs and discuss its key features as well as our implementation experience.","","978-1-7281-9998-6","10.1109/SC41405.2020.00046","Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355247","allocation banking;identity and access management","Process control;Banking;Throughput;Resource management;Reliability;Retirement;Iris recognition","authorisation;parallel processing;resource allocation","Iris;access management;computational storage resources;modern HPC centers;machine scale;job throughput;HPC environment;legacy system;NERSC","",2.0,"",26.0,"USGov","22 Feb 2021","","","IEEE","IEEE Conferences"
"Optimizing Deep Learning Recommender Systems Training on CPU Cluster Architectures","D. Kalamkar; E. Georganas; S. Srinivasan; J. Chen; M. Shiryaev; A. Heinecke","Intel Technology India Private Ltd., India; Intel Corporation, USA; Intel Technology India Private Ltd., India; Intel China Co. Ltd., China; Intel Russia, Russia; Intel Corporation, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"During the last two years, the goal of many researchers has been to squeeze the last bit of performance out of HPC system for AI tasks. Often this discussion is held in the context of how fast ResNet50 can be trained. Unfortunately, ResNet50 is no longer a representative workload in 2020. Thus, we focus on Recommender Systems which account for most of the AI cycles in cloud computing centers. More specifically, we focus on Facebook's DLRM benchmark. By enabling it to run on latest CPU hardware and software tailored for HPC, we are able to achieve up to two-orders of magnitude improvement in performance on a single socket compared to the reference CPU implementation, and high scaling efficiency up to 64 sockets, while fitting ultra-large datasets which cannot be held in single node's memory. Therefore, this paper discusses and analyzes novel optimization and parallelization techniques for the various operators in DLRM. Several optimizations (e.g. tensorcontraction accelerated MLPs, framework MPI progression, BFLOAT16 training with up to 1.8× speed-up) are general and transferable to many other deep learning topologies.","","978-1-7281-9998-6","10.1109/SC41405.2020.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355237","","Training;Deep learning;Sockets;Topology;Artificial intelligence;Recommender systems;Optimization","cloud computing;learning (artificial intelligence);message passing;microprocessor chips;parallel architectures;recommender systems","magnitude improvement;single socket;ultra-large datasets;single node;parallelization techniques;BFLOAT16 training;Recommender Systems training;CPU cluster architectures;HPC system;AI tasks;ResNet50;representative workload;AI cycles;cloud computing centers;scaling efficiency;novel optimization analysis;deep learning topology;Facebook DLRM benchmark","",9.0,"",42.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Herring: Rethinking the Parameter Server at Scale for the Cloud","I. Thangakrishnan; D. Cavdar; C. Karakus; P. Ghai; Y. Selivonchyk; C. Pruce","Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"Training large deep neural networks is time-consuming and may take days or even weeks to complete. Although parameter-server-based approaches were initially popular in distributed training, scalability issues led the field to move towards all-reduce-based approaches. Recent developments in cloud networking technologies, however, such as the Elastic Fabric Adapter (EFA) and Scalable Reliable Datagram (SRD), motivate a re-thinking of the parameter-server approach to address its fundamental inefficiencies. To this end, we introduce a novel communication library, Herring, which is designed to alleviate the performance bottlenecks in parameter-server-based training. We show that gradient reduction with Herring is twice as fast as all-reduce-based methods. We further demonstrate that training deep learning models like $\mathrm{B}\mathrm{E}\mathrm{R}\mathrm{T}_{\mathrm{l}\mathrm{a}\mathrm{r}\mathrm{g}\mathrm{e}}$ using Herring outperforms all-reduce-based training, achieving 85% scaling efficiency on large clusters with up to 2048 NVIDIA V100 GPUs without accuracy drop.","","978-1-7281-9998-6","10.1109/SC41405.2020.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355279","Deep Learning;Data Parallel Training;Scalability;Distributed Parallel Computing","Training;Deep learning;Scalability;Neural networks;Throughput;Libraries;Servers","cloud computing;learning (artificial intelligence);neural nets","Herring;parameter server;deep neural networks;parameter-server-based approaches;distributed training;scalability issues;all-reduce-based approaches;cloud networking technologies;scalable reliable datagram;parameter-server approach;fundamental inefficiencies;parameter-server-based training;all-reduce-based methods;training deep learning models;elastic fabric adapter","",2.0,"",39.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"GEMS: GPU-Enabled Memory-Aware Model-Parallelism System for Distributed DNN Training","A. Jain; A. A. Awan; A. M. Aljuhani; J. M. Hashmi; Q. G. Anthony; H. Subramoni; D. K. Panda; R. Machiraju; A. Parwani","Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Pathology, The Ohio State University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Data-parallelism has become an established paradigm to train DNNs that fit inside GPU memory on large-scale HPC systems. However, model-parallelism is required to train out-of-core DNNs. In this paper, we deal with emerging requirements brought forward by very large DNNs being trained using high-resolution images common in digital pathology. To address these, we propose, design, and implement GEMS; a GPU-Enabled Memory-Aware Model-Parallelism System. We present several design schemes like GEMS-MAST, GEMS-MASTER, and GEMS-Hybrid that offer excellent speedups over state-of-the-art systems like Mesh-TensorFlow and FlexFlow. Furthermore, we combine model-parallelism and data-parallelism to train a 1000-1ayer ResNet-lk model using 1,024 Volta V100 GPUs with 97.32% scaling-efficiency. For the real-world histopathology whole-slide-image (WSI) of 100,000 x 100,000 pixels, we train custom ResNet-110-v2 on image tiles of size 1024 x 1024 and reduce the training time from seven hours to 28 minutes.","","978-1-7281-9998-6","10.1109/SC41405.2020.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355254","DNN;Model Parallelism;Keras;TensorFlow;Eager Execution;MPI","Training;Histopathology;Computational modeling;High performance computing;Graphics processing units;Distributed databases;Data models","acoustic signal processing;graphics processing units;image resolution;learning (artificial intelligence);neural nets;parallel processing;speech recognition","large-scale HPC systems;data-parallelism;1000-1ayer ResNet-lk model;training time;distributed DNN training;GPU memory;GPU-enabled memory-aware model-parallelism system;GEMS-MAST;GEMS-Hybrid;ResNet-110-v2;image tiles","",8.0,"",30.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Experimental Evaluation of NISQ Quantum Computers: Error Measurement, Characterization, and Implications","T. Patel; A. Potharaju; B. Li; R. B. Roy; D. Tiwari","Northeastern University; Northeastern University; Northeastern University; Northeastern University; Northeastern University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Noisy Intermediate-Scale Quantum (NISQ) computers are being increasingly used for executing early-stage quantum programs to establish the practical realizability of existing quantum algorithms. These quantum programs have uses cases in the realm of high-performance computing ranging from molecular chemistry and physics simulations to addressing NP-complete optimization problems. However, NISQ devices are prone to multiple types of errors, which affect the fidelity and reproducibility of the program execution. As the technology is still primitive, our understanding of these quantum machines and their error characteristics is limited. To bridge that understanding gap, this is the first work to provide a systematic and rich experimental evaluation of IBM Quantum Experience (QX) quantum computers of different scales and topologies. Our experimental evaluation uncovers multiple important and interesting aspects of benchmarking and evaluating quantum program on NISQ machines. We have open-sourced our experimental framework and dataset to help accelerate the evaluation of quantum computing systems.","","978-1-7281-9998-6","10.1109/SC41405.2020.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355264","Quantum Computing;Performance Evaluation;Computer Errors;Error Analysis;Error Probability","Computers;Quantum computing;Systematics;Quantum algorithm;Measurement uncertainty;Logic gates;Topology","computational complexity;optimisation;quantum computing;quantum noise","quantum algorithms;high-performance computing;molecular chemistry;NISQ devices;quantum machines;systematic evaluation;IBM Quantum Experience quantum computers;quantum program;NISQ machines;quantum computing systems;NISQ quantum computers;error measurement;noisy intermediate-scale quantum computers;quantum programs","",7.0,"",63.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Co-Design for A64FX Manycore Processor and ”Fugaku”","M. Sato; Y. Ishikawa; H. Tomita; Y. Kodama; T. Odajima; M. Tsuji; H. Yashiro; M. Aoki; N. Shida; I. Miyoshi; K. Hirai; A. Furuya; A. Asato; K. Morita; T. Shimizu","RIKEN Center for Computational Science; RIKEN Center for Computational Science; RIKEN Center for Computational Science; RIKEN Center for Computational Science; RIKEN Center for Computational Science; RIKEN Center for Computational Science; RIKEN Center for Computational Science; FUJITSU LIMITED, Japan; FUJITSU LIMITED, Japan; FUJITSU LIMITED, Japan; FUJITSU LIMITED, Japan; FUJITSU LIMITED, Japan; FUJITSU LIMITED, Japan; FUJITSU LIMITED, Japan; FUJITSU LIMITED, Japan","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"We have been carrying out the FLAGSHIP 2020 Project to develop the Japanese next-generation flagship supercomputer, the Post-K, recently named “Fugaku”. We have designed an original many core processor based on Armv8 instruction sets with the Scalable Vector Extension (SVE), an A64FX processor, as well as a system including interconnect and a storage subsystem with the industry partner, Fujitsu. The “co-design” of the system and applications is a key to making it power efficient and high performance. We determined many architectural parameters by reflecting an analysis of a set of target applications provided by applications teams. In this paper, we present the pragmatic practice of our co-design effort for “Fugaku”. As a result, the system has been proven to be a very power-efficient system, and it is confirmed that the performance of some target applications using the whole system is more than 100 times the performance of the K computer.","","978-1-7281-9998-6","10.1109/SC41405.2020.00051","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355239","exascale computing;co-design;many core processor;high-performance computing","Industries;Instruction sets;High performance computing;Supercomputers;Manycore processors;Next generation networking;Pragmatics","instruction sets;microprocessor chips;multiprocessing systems;performance evaluation;power aware computing","Fugaku;power-efficient system;A64FX manycore processor;Japanese next-generation flagship supercomputer;Armv8 instruction;scalable vector extension;SVE;A64FX processor;storage subsystem;architectural parameters;K computer","",33.0,"",39.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Chronicles of Astra: Challenges and Lessons from the First Petascale Arm Supercomputer","K. Pedretti; A. J. Younge; S. D. Hammond; J. H. Laros III; M. L. Curry; M. J. Aguilar; R. J. Hoekstra; R. Brightwell","Center for Computing Research Sandia National Laboratories, Albuquerque, New Mexico, USA; Center for Computing Research Sandia National Laboratories, Albuquerque, New Mexico, USA; Center for Computing Research Sandia National Laboratories, Albuquerque, New Mexico, USA; Center for Computing Research Sandia National Laboratories, Albuquerque, New Mexico, USA; Center for Computing Research Sandia National Laboratories, Albuquerque, New Mexico, USA; Center for Computing Research Sandia National Laboratories, Albuquerque, New Mexico, USA; Center for Computing Research Sandia National Laboratories, Albuquerque, New Mexico, USA; Center for Computing Research Sandia National Laboratories, Albuquerque, New Mexico, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Arm processors have been explored in HPC for several years, however there has not yet been a demonstration of viability for supporting large-scale production workloads. In this paper, we offer a retrospective on the process of bringing up Astra, the first Petascale supercomputer based on 64-bit Arm processors, and validating its ability to run production HPC applications. Through this process several immature technology gaps were addressed, including software stack enablement, Linux bugs at scale, thermal management issues, power management capabilities, and advanced container support. From this experience, several lessons learned are formulated that contributed to the successful deployment of Astra. These insights can be helpful to accelerate deploying and maturing other first-seen HPC technologies. With Astra now supporting many users running a diverse set of production applications at multi-thousand node scales, we believe this constitutes strong supporting evidence that Arm is a viable technology for even the largest-scale supercomputer deployments.","","978-1-7281-9998-6","10.1109/SC41405.2020.00052","Sandia National Laboratories; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355311","HPC;Arm;Prototype;Supercomputer;Practice","Program processors;Power system management;Linux;Production;Thermal management;Supercomputers;Software","Linux;mainframes;microprocessor chips;parallel machines;thermal management (packaging)","Astra;large-scale production workloads;64-bit Arm processors;immature technology gaps;software stack enablement;Linux bugs;thermal management;power management capabilities;advanced container support;HPC technologies;production applications;multithousand node scales;largest-scale supercomputer deployments;Petascale Arm supercomputer","",5.0,"",58.0,"USGov","22 Feb 2021","","","IEEE","IEEE Conferences"
"PLINER: Isolating Lines of Floating-Point Code for Compiler-Induced Variability","H. Guo; I. Laguna; C. Rubio-González","Department of Computer Science, University of California, Davis, Davis, CA, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Computer Science, University of California, Davis, Davis, CA, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Scientific applications are often impacted by numerical inconsistencies when using different compilers or when a compiler is used with different optimization levels; such inconsistencies hinder reproducibility and can be hard to diagnose. We present PLINER, a tool to automatically pinpoint code lines that trigger compiler-induced variability. PLINER uses a novel approach to enhance floating-point precision at different levels of code granularity, and performs a guided search to identify locations affected by numerical inconsistencies. We demonstrate PLINER on a real-world numerical inconsistency that required weeks to diagnose, which PLINER isolates in minutes. We also evaluate PLiNER on 100 synthetic programs, and the NAS Parallel Benchmarks (NPB). On the synthetic programs, PLiNER detects the affected lines of code 87% of the time while the stateof-the-art approach only detects the affected lines 6% of the time. Furthermore, PLINER successfully isolates all numerical inconsistencies found in the NPB.","","978-1-7281-9998-6","10.1109/SC41405.2020.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355325","reproducibility;numerical reliability;floatingpoint arithmetic;compiler optimizations;scientific computing","Productivity;Program processors;High performance computing;Computer architecture;Tools;Reliability;Optimization","floating point arithmetic;optimisation;parallel processing;program compilers","floating-point code;code lines;floating-point precision;numerical inconsistency;PLiNER;compiler-induced variability;NAS Parallel Benchmarks;NPB","",4.0,"",30.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Tuning Floating-Point Precision Using Dynamic Program Information and Temporal Locality","H. Brunie; C. Iancu; K. Z. Ibrahim; P. Brisk; B. Cook","Lawrence Berkeley National Laboratory; Lawrence Berkeley National Laboratory; Lawrence Berkeley National Laboratory; University of California, Riverside; Lawrence Berkeley National Laboratory","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"We present a methodology for precision tuning of full applications. These techniques must select a search space composed of either variables or instructions and provide a scalable search strategy. In full application settings one cannot assume compiler support for practical reasons. Thus, an additional important challenge is enabling code refactoring. We argue for an instruction-based search space and we show: 1) how to exploit dynamic program information based on call stacks; and 2) how to exploit the iterative nature of scientific codes, combined with temporal locality. We applied the methodology to tune the implementation of scientific codes written in a combination of Python, CUDA, C++ and Fortran, tuning calls to math exp library functions. The iterative search refinement always reduces the search complexity and the number of steps to solution. Dynamic program information increases search efficacy. Using this approach, we obtain application runtime performance improvements up to 27%.","","978-1-7281-9998-6","10.1109/SC41405.2020.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355251","","Tuners;Runtime;Buildings;Graphics processing units;C++ languages;Search problems;Python","dynamic programming;FORTRAN;iterative methods;parallel architectures;program compilers;search problems","tuning floating-point precision;dynamic program information;temporal locality;precision tuning;scalable search strategy;application settings one;additional important challenge;code refactoring;instruction-based search space;scientific codes;iterative search refinement;search complexity;search efficacy;application runtime performance improvements","","","",34.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Scalable yet Rigorous Floating-Point Error Analysis","A. Das; I. Briggs; G. Gopalakrishnan; S. Krishnamoorthy; P. Panchekha","University of Utah; University of Utah; University of Utah; Pacific Northwest National Laboratory; University of Utah","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Automated techniques for rigorous floating-point round-off error analysis are a prerequisite to placing important activities in HPC such as precision allocation, verification, and code optimization on a formal footing. Yet existing techniques cannot provide tight bounds for expressions beyond a few dozen operators-barely enough for HPC. In this work, we offer an approach embedded in a new tool called SATIHE that scales error analysis by four orders of magnitude compared to today's best-of-class tools. We explain how three key ideas underlying SATIHE helps it attain such scale: path strength reduction, bound optimization, and abstraction. SATIHE provides tight bounds and rigorous guarantees on significantly larger expressions with well over a hundred thousand operators, covering important examples including FFT, matrix multiplication, and PDE stencils.","","978-1-7281-9998-6","10.1109/SC41405.2020.00055","U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355314","Floating-point arithmetic;Round-off error;Numerical Analysis;Symbolic Execution;Algorithmic Differentiation;Abstraction;Scalable Analysis","Error analysis;High performance computing;Tools;Resource management;Optimization","error analysis;fast Fourier transforms;floating point arithmetic;formal specification;matrix multiplication;optimisation;program diagnostics;roundoff errors","scalable yet rigorous floating-point error analysis;automated techniques;important activities;HPC;precision allocation;code optimization;formal footing;tight bounds;dozen operators-barely;SATIHE;scales error analysis;best-of-class tools;bound optimization;rigorous guarantees","",13.0,"",47.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"RDMP-KV: Designing Remote Direct Memory Persistence based Key-Value Stores with PMEM","T. Li; D. Shankar; S. Gugnani; X. Lu","Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Byte-addressable persistent memory (PMEM) can be directly manipulated by Remote Direct Memory Access (RDMA) capable networks. However, existing studies to combine RDMA and PMEM can not deliver the desired performance due to their PMEM-oblivious communication protocols. In this paper, we propose novel PMEM-aware RDMA-based communication protocols for persistent key-value stores, referred to as Remote Direct Memory Persistence based Key-Value stores (RDMPKV). RDMP-KV employs a hybrid `server-reply/server-bypass' approach to `durably' store individual key-value objects on PMEM-equipped servers. RDMP-KV's runtime can easily adapt to existing (server-assisted durability) and emerging (appliance durability) RDMA-capable interconnects, while ensuring server scalability through a lightweight consistency scheme. Performance evaluations show that RDMP-KV can improve the server-side performance with different persistent key-value storage architectures by up to 22x, as compared with PMEM-oblivious RDMA-`Server-Reply' protocols. Our evaluations also show that RDMP-KV outperforms a distributed PMEM-based filesystem by up to 65% and a recent RDMA-to-PMEM framework by up to 71%.","","978-1-7281-9998-6","10.1109/SC41405.2020.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355232","Next Generation Networking;RDMA;Data storage systems;Key-value Stores;Persistent Memory","Performance evaluation;Protocols;Runtime;Scalability;Computer architecture;Throughput;Servers","cache storage;computer network performance evaluation;file organisation;protocols","remote direct memory persistence;byte-addressable persistent memory;PMEM-oblivious communication protocols;PMEM-aware RDMA-based communication protocols;persistent key-value stores;key-value objects;PMEM-equipped servers;server-assisted durability;RDMA-capable interconnects;server scalability;server-side performance;distributed PMEM-based filesystem;RDMA-to-PMEM framework;persistent key-value storage architectures;PMEM-oblivious RDMA-server-reply protocols;RDMP-KV runtime;remote direct memory access capable networks;server-reply-server-bypass approach;performance evaluations","",1.0,"",37.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Compiler-Based Timing For Extremely Fine-Grain Preemptive Parallelism","S. Ghosh; M. Cuevas; S. Campanoni; P. Dinda","Department of Computer Science, Northwestern University; Department of Computer Science, Northwestern University; Department of Computer Science, Northwestern University; Department of Computer Science, Northwestern University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"In current operating system kernels and run-time systems, timing is based on hardware timer interrupts, introducing inherent overheads that limit granularity. For example, the scheduling quantum of preemptive threads is limited, resulting in this abstraction being restricted to coarse-grain parallelism. Compiler-based timing replaces interrupts from the hardware timer with callbacks from compiler-injected code. We describe a system that achieves low-overhead timing using whole-program compiler transformations and optimizations combined with kernel and run-time support. A key novelty is new static analyses that achieve predictable, periodic run-time behavior from the transformed code, regardless of control-flow path. We transform the code of a kernel and run-time system to use compiler-based timing and leverage the resulting fine-grain timing to extend an implementation of fibers (cooperatively scheduled threads), attaining what is effectively preemptive scheduling. The result combines the fine granularity of the cooperative fiber model with the ease of programming of the preemptive thread model.","","978-1-7281-9998-6","10.1109/SC41405.2020.00057","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355250","timing;preemptive scheduling;fine-granularity parallelism","Instruction sets;Computational modeling;Parallel processing;Optical fiber networks;Hardware;Timing;Kernel","multi-threading;operating system kernels;processor scheduling;program compilers;program diagnostics","compiler-based timing;fine-grain preemptive parallelism;operating system kernels;run-time system;hardware timer interrupts;coarse-grain parallelism;compiler-injected code;low-overhead timing;whole-program compiler transformations;run-time support;run-time behavior;fine-grain timing;scheduling quantum;preemptive threads;static analyses;control-flow path;cooperatively scheduled threads;preemptive scheduling;cooperative fiber model","",6.0,"",57.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"OMPRacer: A Scalable and Precise Static Race Detector for OpenMP Programs","B. Swain; Y. Li; P. Liu; I. Laguna; G. Georgakoudis; J. Huang","Coderrect Inc, College Station, TX; Computer Science and Engineering, Texas A & M University, College Station, TX; Computer Science and Engineering, Texas A & M University, College Station, TX; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA; Computer Science and Engineering, Texas A & M University, College Station, TX","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"We present OMPRACER, a static tool that uses flow-sensitive, interprocedural analysis to detect data races in OpenMP programs. OMPRACER is fast, scalable, has high code coverage, and supports the most common OpenMP features by combining state-of-the-art pointer analysis, novel value-flow analysis, happens-before tracking, and generalized modelling of OpenMP APIs. Unlike dynamic tools that currently dominate data race detection, OMPRACER achieves almost 100% code coverage using static analysis to detect a broader category of races without running the program or relying on specific input or runtime behaviour. OMPRACER has competitive precision with dynamic tools like Archer and ROMP: passing 105/116 cases in DataRaceBench with a total accuracy of 91%. OMPRACER has been used to analyze several Exascale Computing Project proxy applications containing over 2 million lines of code in under 10 minutes. OMPRACER has revealed previously unknown races in an ECP proxy app and a production simulation for COVID19.","","978-1-7281-9998-6","10.1109/SC41405.2020.00058","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355249","OpenMP;Data race detection;Static analysis;Bug detection;Nondeterminism","Runtime;Exascale computing;Static analysis;Production;Tools;Hardware;Performance analysis","application program interfaces;parallel programming;program compilers;program debugging;program diagnostics;source code (software)","OMPRacer;OpenMP programs;static tool;interprocedural analysis;pointer analysis;value-flow analysis;OpenMP APIs;data race detection;code coverage;static analysis","",7.0,"",40.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"PREEMPT: Scalable Epidemic Interventions Using Submodular Optimization on Multi-GPU Systems","M. Minutoli; P. Sambaturu; M. Halappanavar; A. Tumeo; A. Kalyananaraman; A. Vullikanti","Pacific Northwest National Laboratory, Richland, WA; University of Virginia, Charlottesville, VA; Washington State University, Pullman, WA; Pacific Northwest National Laboratory, Richland, WA; Pacific Northwest National Laboratory, Richland, WA; University of Virginia, Charlottesville, VA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Preventing and slowing the spread of epidemics is achieved through techniques such as vaccination and social distancing. Given practical limitations on the number of vaccines and cost of administration, optimization becomes a necessity. Previous approaches using mathematical programming methods have shown to be effective but are limited by computational costs. In this work, we present PREEMPT, a new approach for intervention via maximizing the influence of vaccinated nodes on the network. We prove submodular properties associated with the objective function of our method so that it aids in construction of an efficient greedy approximation strategy. Consequently, we present a new parallel algorithm based on greedy hill climbing for PREEMPT, and present an efficient parallel implementation for distributed CPU-GPU heterogeneous platforms. Our results demonstrate that PREEMPT is able to achieve a significant reduction (up to 6.75×) in the percentage of people infected and up to 98% reduction in the peak of the infection on a city-scale network. We also show strong scaling results of PREEMPT on up to 128 nodes of the Summit supercomputer. Our parallel implementation is able to significantly reduce time to solution, from hours to minutes on large networks. This work represents a first-of-its-kind effort in parallelizing greedy hill climbing and applying it toward devising effective interventions for epidemics.","","978-1-7281-9998-6","10.1109/SC41405.2020.00059","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355275","Epidemic networks;vaccination;submodular;influence maximization;CPU-GPU;parallel algorithms","Epidemics;Supercomputers;Social factors;Vaccines;Parallel algorithms;Optimization;Mathematical programming","diseases;emergency management;epidemics;graphics processing units;greedy algorithms;mainframes;medical computing;network theory (graphs);optimisation;parallel algorithms;parallel machines","infection;city-scale network;PREEMPT;scalable epidemic interventions;submodular optimization;multiGPU systems;vaccination;social distancing;computational costs;vaccinated nodes;submodular properties;objective function;greedy approximation strategy;parallel algorithm;distributed CPU-GPU heterogeneous platform;greedy approximation strategy;greedy hill climbing;administration cost;Summit supercomputer","",2.0,"",51.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"C-SAW: A Framework for Graph Sampling and Random Walk on GPUs","S. Pandey; L. Li; A. Hoisie; X. S. Li; H. Liu","Stevens Institute of Technology; Brookhaven National Laboratory; Brookhaven National Laboratory; Lawrence Berkeley National Laboratory; Stevens Institute of Technology","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Many applications require to learn, mine, analyze and visualize large-scale graphs. These graphs are often too large to be addressed efficiently using conventional graph processing technologies. Fortunately, recent research efforts find out graph sampling and random walk, which significantly reduce the size of original graphs, can benefit the tasks of learning, mining, analyzing and visualizing large graphs by capturing the desirable graph properties. This paper introduces C-SAW, the first framework that accelerates Sampling and Random Walk framework on GPUs. Particularly, C-SAW makes three contributions: First, our framework provides a generic API which allows users to implement a wide range of sampling and random walk algorithms with ease. Second, offloading this framework on GPU, we introduce warp-centric parallel selection, and two novel optimizations for collision migration. Third, towards supporting graphs that exceed the GPU memory capacity, we introduce efficient data transfer optimizations for out-of-memory and multi-GPU sampling, such as workload-aware scheduling and batched multi-instance sampling. Taken together, our framework constantly outperforms the state of the art projects in addition to the capability of supporting a wide range of sampling and random walk algorithms.","","978-1-7281-9998-6","10.1109/SC41405.2020.00060","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355289","","Processor scheduling;High performance computing;Memory management;Graphics processing units;Data transfer;Task analysis;Optimization","application program interfaces;graph theory;graphics processing units;optimisation;processor scheduling;random processes;sampling methods","C-SAW;graph sampling;large-scale graphs;graph processing;graph properties;data transfer optimizations;multiGPU sampling;multiinstance sampling;random walk;API;warp-centric parallel selection;collision migration;GPU memory capacity;out-of-memory;workload-aware scheduling","",15.0,"",84.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Newton-ADMM: A Distributed GPU-Accelerated Optimizer for Multiclass Classification Problems","C. -H. Fang; S. B. Kylasa; F. Roosta; M. W. Mahoney; A. Grama","Computer Science, Purdue University, West Lafayette, US; Electrical and Computer Engineering, Purdue University, West Lafayette, US; School of Mathematics and Physics, University of Queensland, Brisbane, Australia; Department of Statistics UC Berkeley, ICSI, Berkeley, US; Computer Science, Purdue University, West Lafayette, US","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,12,"First-order optimization techniques, such as stochastic gradient descent (SGD) and its variants, are widely used in machine learning applications due to their simplicity and low per-iteration costs. However, they often require larger numbers of iterations, with associated communication costs in distributed environments. In contrast, Newton-type methods, while having higher per-iteration computation costs, typically require a significantly smaller number of iterations, which directly translates to reduced communication costs. We present a novel distributed optimizer for classification problems, which integrates a GPU-accelerated Newton-type solver with the global consensus formulation of Alternating Direction of Method Multipliers (ADMM). By leveraging the communication efficiency of ADMM, a highly efficient GPUaccelerated inexact-Newton solver, and an effective spectral penalty parameter selection strategy, we show that our proposed method (i) yields better generalization performance on several classification problems; (ii) significantly outperforms state-of-the-art methods in distributed time to solution; and (iii) offers better scaling on large distributed platforms.","","978-1-7281-9998-6","10.1109/SC41405.2020.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355255","Second-Order Method;Newton;ADMM;Convex optimization;Machine Learning;Classification","Scalability;Neural networks;Optimization methods;Machine learning;Convex functions;Newton method;Standards","gradient methods;learning (artificial intelligence);Newton method;optimisation;pattern classification;stochastic processes","Newton-ADMM;distributed GPU-accelerated optimizer;multiclass classification problems;first-order optimization techniques;stochastic gradient descent;SGD;machine learning;distributed environments;global consensus formulation;Alternating Direction of Method Multipliers;GPU accelerated inexact-Newton solver;spectral penalty parameter selection strategy","",1.0,"",30.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Fast Stencil-Code Computation on a Wafer-Scale Processor","K. Rocki; D. V. Essendelft; I. Sharapov; R. Schreiber; M. Morrison; V. Kibardin; A. Portnoy; J. F. Dietiker; M. Syamlal; M. James","Cerebras Systems Inc., Los Altos, California, USA; National Energy Technology Laboratory, Morgantown, West Virginia, USA; Cerebras Systems Inc., Los Altos, California, USA; Cerebras Systems Inc., Los Altos, California, USA; Cerebras Systems Inc., Los Altos, California, USA; Cerebras Systems Inc., Los Altos, California, USA; Cerebras Systems Inc., Los Altos, California, USA; Leidos Research Support Team, Pittsburgh, Pennsylvania, USA; National Energy Technology Laboratory, Morgantown, West Virginia, USA; Cerebras Systems Inc., Los Altos, California, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"The performance of CPU-based and GPU-based systems is often low for PDE codes, where large, sparse, and often structured systems of linear equations must be solved. Iterative solvers are limited by data movement, both between caches and memory and between nodes. Here we describe the solution of such systems of equations on the Cerebras Systems CS-1, a wafer-scale processor that has the memory bandwidth and communication latency to perform well. We achieve 0.86 PFLOPS on a single wafer-scale system for the solution by BiCGStab of a linear system arising from a 7-point finite difference stencil on a 600 × 595 × 1536 mesh, achieving about one third of the machine's peak performance. We explain the system, its architecture and programming, and its performance on this problem and related problems. We discuss issues of memory capacity and floating point precision. We outline plans to extend this work towards full applications.","","978-1-7281-9998-6","10.1109/SC41405.2020.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355322","Algorithms for numerical methods and algebraic systems;Computational fluid dynamics and mechanics;Multi-processor architecture and microarchitecture","Linear systems;Microarchitecture;High performance computing;Heuristic algorithms;Fluid dynamics;Computer architecture;Programming","finite difference methods;graphics processing units;iterative methods;mathematics computing;parallel machines;parallel processing;partial differential equations","fast stencil-code computation;wafer-scale processor;CPU-based;GPU-based systems;PDE codes;linear equations;iterative solvers;data movement;memory bandwidth;single wafer-scale system;linear system;7-point finite difference stencil;machine;memory capacity;floating point precision;cerebras systems CS-1","",18.0,"",19.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"FBLAS: Streaming Linear Algebra on FPGA","T. D. Matteis; J. d. F. Licht; T. Hoefler","Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"Spatial computing architectures pose an attractive alternative to mitigate control and data movement overheads typical of load-store architectures. In practice, these devices are rarely considered in the HPC community due to the steep learning curve, low productivity, and the lack of available libraries for fundamental operations. High-level synthesis (HLS) tools are facilitating hardware programming, but optimizing for these architectures requires factoring in new transformations and resources/performance trade-offs. We present FBLAS, an open-source HLS implementation of BLAS for FPGAs, that enables reusability, portability and easy integration with existing software and hardware codes. FBLAS' implementation allows scaling hardware modules to exploit on-chip resources, and module interfaces are designed to natively support streaming on-chip communications, allowing them to be composed to reduce off-chip communication. With FBLAS, we set a precedent for FPGA library design, and contribute to the toolbox of customizable hardware components necessary for HPC codes to start productively targeting reconfigurable platforms.","","978-1-7281-9998-6","10.1109/SC41405.2020.00063","European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355265","Spatial architectures;high level synthesis;hardware library","Linear algebra;Tools;Libraries;Hardware;System-on-chip;Reconfigurable architectures;Field programmable gate arrays","field programmable gate arrays;high level synthesis;linear algebra;logic design;reconfigurable architectures","hardware codes;hardware modules;on-chip resources;module interfaces;on-chip communications;off-chip communication;FPGA library design;customizable hardware components;HPC codes;linear algebra;spatial computing architectures;data movement;load-store architectures;HPC community;steep learning curve;high-level synthesis;hardware programming;FBLAS implementation;resources-performance trade-offs;open-source HLS implementation","",8.0,"",33.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Massive Parallelization for Finding Shortest Lattice Vectors Based on Ubiquity Generator Framework","N. Tateiwa; Y. Shinano; S. Nakamura; A. Yoshida; S. Kaji; M. Yasuda; K. Fujisawa","Graduate School of Mathematics, Kyushu University, Fukuoka, Japan; Mathematical Algorithmic Intelligence Applied Algorithmic Intelligence Methods (A-IM), Zuse Institute Berlin (ZIB), Berlin, Germany; NTT Secure Platform Laboratories, Tokyo, Japan; Graduate School of Mathematics, Kyushu University, Fukuoka, Japan; Institute of Mathematics for Industry, Kyushu University, Fukuoka, Japan; Department of Mathematics, Rikkyo University, Tokyo, Japan; Institute of Mathematics for Industry, Kyushu University, Fukuoka, Japan","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Lattice-based cryptography has received attention as a next-generation encryption technique, because it is believed to be secure against attacks by classical and quantum computers. Its essential security depends on the hardness of solving the shortest vector problem (SVP). In the cryptography, to determine security levels, it is becoming significantly more important to estimate the hardness of the SVP by high-performance computing. In this study, we develop the world's first distributed and asynchronous parallel SVP solver, the MAssively Parallel solver for SVP (MAP-SVP). It can parallelize algorithms for solving the SVP by applying the Ubiquity Generator framework, which is a generic framework for branch-and-bound algorithms. The MAP-SVP is suitable for massive-scale parallelization, owing to its small memory footprint, low communication overhead, and rapid checkpoint and restart mechanisms. We demonstrate its performance and scalability of the MAP-SVP by using up to 100,032 cores to solve instances of the Darmstadt SVP Challenge.","","978-1-7281-9998-6","10.1109/SC41405.2020.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355284","Lattice based cryptography;Shortest vector problem;Parallel computation;DeepBKZ;ENUM;Ubiquity Generator Framework","Quantum computing;Scalability;High performance computing;Lattices;Generators;Encryption;Next generation networking","cryptography;lattice theory;parallel algorithms;quantum computing;vectors","lattice-based cryptography;next-generation encryption technique;classical computers;quantum computers;lattice vectors;massive parallelization;Darmstadt SVP Challenge;massive-scale parallelization;MAP-SVP;shortest vector problem","","","",42.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Cost-Aware Prediction of Uncorrected DRAM Errors in the Field","I. Boixaderas; D. Zivanovic; S. Moré; J. Bartolome; D. Vicente; M. Casas; P. M. Carpenter; P. Radojković; E. Ayguadé","Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center; Barcelona Supercomputing Center, Universitat Politècnica de Catalunya","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"This paper presents and evaluates a method to predict DRAM uncorrected errors, a leading cause of hardware failures in large-scale HPC clusters. The method uses a random forest classifier, which was trained and evaluated using error logs from two years of production of the MareNostrum 3 supercomputer. By enabling the system to take measures to mitigate node failures, our method reduces lost compute time by up to 57%, a net saving of 21,000 node-hours per year. We release all source code as open source. We also discuss and clarify aspects of methodology that are essential for a DRAM prediction method to be useful in practice. We explain why standard evaluation metrics, such as precision and recall, are insufficient, and base the evaluation on a cost-benefit analysis. This methodology can help ensure that any DRAM error predictor is clear from training bias and has a clear cost-benefit calculation.","","978-1-7281-9998-6","10.1109/SC41405.2020.00065","Ministry of Science and Technology; Generalitat de Catalunya; Ministry of Economy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355321","Memory system;Reliability;Error prediction;Machine learning;Random forest;Cost–benefit analysis","Training;Random access memory;Production;Prediction methods;Throughput;Time measurement;Random forests","cost-benefit analysis;DRAM chips;fault tolerant computing;parallel machines;parallel processing;power aware computing","standard evaluation metrics;cost-benefit analysis;DRAM error predictor;clear cost-benefit calculation;cost-aware prediction;uncorrected DRAM errors;DRAM uncorrected errors;hardware failures;large-scale HPC clusters;random forest classifier;error logs;MareNostrum 3 supercomputer;node failures;compute time;source code;open source;DRAM prediction method","",6.0,"",48.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Task Bench: A Parameterized Benchmark for Evaluating Parallel Runtime Performance","E. Slaughter; W. Wu; Y. Fu; L. Brandenburg; N. Garcia; W. Kautz; E. Marx; K. S. Morris; Q. Cao; G. Bosilca; S. Mirchandaney; W. Leek; S. Treichlerk; P. McCormick; A. Aiken","SLAC National Accelerator Laboratory; Los Alamos National Laboratory; Purdue University; Stanford University; Stanford University; Stanford University; Stanford University; Stanford University; University of Tennessee, Knoxville; University of Tennessee, Knoxville; SLAC National Accelerator Laboratory; NVIDIA; NVIDIA; Los Alamos National Laboratory; Stanford University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"We present Task Bench, a parameterized benchmark designed to explore the performance of distributed programming systems under a variety of application scenarios. Task Bench dramatically lowers the barrier to benchmarking and comparing multiple programming systems by making the implementation for a given system orthogonal to the benchmarks themselves: every benchmark constructed with Task Bench runs on every Task Bench implementation. Furthermore, Task Bench's parameterization enables a wide variety of benchmark scenarios that distill the key characteristics of larger applications. To assess the effectiveness and overheads of the tested systems, we introduce a novel metric, minimum effective task granularity (METG). We conduct a comprehensive study with 15 programming systems on up to 256 Haswell nodes of the Cori supercomputer. Running at scale, 100μs-long tasks are the finest granularity that any system runs efficiently with current technologies. We also study each system's scalability, ability to hide communication and mitigate load imbalance.","","978-1-7281-9998-6","10.1109/SC41405.2020.00066","U.S. Department of Energy; Office of Science; National Science Foundation; National Nuclear Security Administration; Battelle; Office of Science; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355308","","Measurement;Runtime;Programming;Benchmark testing;Space exploration;Task analysis;Usability","benchmark testing;data handling;distributed programming;object-oriented programming;parallel processing;program verification","parameterized benchmark;distributed programming systems;benchmarking;comparing multiple programming systems;benchmark scenarios;minimum effective task granularity;programming systems;parallel runtime performance;task bench parameterization;task bench implementation;METG;Cori supercomputer","",18.0,"",42.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Smart-PGSim: Using Neural Network to Accelerate AC-OPF Power Grid Simulation","W. Dong; Z. Xie; G. Kestor; D. Li","UC Merced, PNNL, California, USA; UC Merced, California, USA; Pacific Northwest National Laboratory, Washington, USA; UC Merced, PNNL, California, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"In this work we address the problem of accelerating complex power-grid simulation through machine learning (ML). Specifically, we develop a framework, Smart-PGSim, which generates multitask-learning (MTL) neural network (NN) models to predict the initial values of variables critical to the problem convergence. MTL models allow information sharing when predicting multiple dependent variables while including customized layers to predict individual variables. We show that, to achieve the required accuracy, it is paramount to embed domain-specific constraints derived from the specific power-grid components in the MTL model. Smart-PGSim then employs the predicted initial values as a high-quality initial condition for the power-grid numerical solver (warm start), resulting in both higher performance compared to state-of-the-art solutions while maintaining the required accuracy. Smart-PGSim brings 2. 60× speedup on average (up to 3. 28×) computed over 10,000 problems, without losing solution optimality.","","978-1-7281-9998-6","10.1109/SC41405.2020.00067","U.S. Department of Energy; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355288","","Analytical models;Computational modeling;Artificial neural networks;Predictive models;Power grids;Numerical models;Acceleration","learning (artificial intelligence);neural nets;optimisation;power grids;power system simulation","individual variables;domain-specific constraints;MTL model;Smart-PGSim;high-quality initial condition;power-grid numerical solver;AC-OPF power grid simulation;complex power-grid simulation;multitask-learning neural network models;problem convergence;multiple dependent variables","",12.0,"",51.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"SEFEE: Lightweight Storage Error Forecasting in Large-Scale Enterprise Storage Systems","A. Yazdi; X. Lin; L. Yang; F. Yan","University of Nevada, Reno, Reno, NV; NetApp, Sunnyvale, CA; University of Nevada, Reno, Reno, NV; University of Nevada, Reno, Reno, NV","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"With the rapid growth in scale and complexity, today's enterprise storage systems need to deal with significant amounts of errors. Existing proactive methods mainly focus on machine learning techniques trained using SMART measurements. However, such methods are usually expensive to use in practice and can only be applied to a limited types of errors with a limited scale. We collected more than 23-million storage events from 87 deployed NetApp-ONTAP systems managing 14,371 disks for two years and propose a lightweight training-free storage error forecasting method SEFEE. SEFEE employs Tensor Decomposition to directly analyze storage error-event logs and perform online error prediction for all error types in all storage nodes. SEFEE explores hidden spatio-temporal information that is deeply embedded in the global scale of storage systems to achieve record breaking error forecasting accuracy with minimal prediction overhead.","","978-1-7281-9998-6","10.1109/SC41405.2020.00068","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355204","Storage failures;error prediction;lightweight forecasting;training-free prediction;tensor decomposition","Training;Tensors;High performance computing;Production;Hardware;Forecasting;Random forests","business data processing;matrix decomposition;spatiotemporal phenomena;storage management;tensors","large-scale enterprise storage systems;proactive methods;lightweight training-free storage error forecasting method;SEFEE;storage error-event logs;online error prediction;error types;storage nodes;global scale;error forecasting accuracy;NetApp-ONTAP systems;tensor decomposition;spatiotemporal information","",1.0,"",43.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Live Forensics for HPC Systems: A Case Study on Distributed Storage Systems","S. Jha; S. Cui; S. S. Banerjee; T. Xu; J. Enos; M. Showerman; Z. T. Kalbarczyk; R. K. Iyer","University of Illinois at Urbana-Champaign, Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, Urbana-Champaign, IL, USA; University of Illinois at Urbana-Champaign, Urbana-Champaign, IL, USA; National Center for Supercomputing Applications (NCSA), Urbana, IL, USA; National Center for Supercomputing Applications (NCSA), Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana-Champaign, IL, USA; National Center for Supercomputing Applications (NCSA), Urbana, IL, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"Large-scale high-performance computing systems frequently experience a wide range of failure modes, such as reliability failures (e.g., hang or crash), and resource overload-related failures (e.g., congestion collapse), impacting systems and applications. Despite the adverse effects of these failures, current systems do not provide methodologies for proactively detecting, localizing, and diagnosing failures. We present Kaleidoscope, a near real-time failure detection and diagnosis framework, consisting of of hierarchical domain-guided machine learning models that identify the failing components, the corresponding failure mode, and point to the most likely cause indicative of the failure in near real-time (within one minute of failure occurrence). Kaleidoscope has been deployed on Blue Waters supercomputer and evaluated with more than two years of production telemetry data. Our evaluation shows that Kaleidoscope successfully localized 99.3% and pinpointed the root causes of 95.8% of 843 real-world production issues, with less than 0.01% runtime overhead.","","978-1-7281-9998-6","10.1109/SC41405.2020.00069","U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research; National Science Foundation; Sandia National Laboratories; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355298","","Production systems;Runtime;Machine learning;Real-time systems;Supercomputers;Reliability;Telemetry","data analysis;distributed databases;failure analysis;fault diagnosis;learning (artificial intelligence);parallel machines;parallel processing;software fault tolerance;system recovery;telemetry","kaleidoscope;real-time failure detection;diagnosis framework;hierarchical domain-guided machine learning models;corresponding failure mode;failure occurrence;live forensics;HPC systems;distributed storage systems;large-scale high-performance computing systems;failure modes;reliability failures;overload-related failures;congestion collapse;diagnosing failures;blue waters supercomputer","",3.0,"",79.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"INEC: Fast and Coherent In-Network Erasure Coding","H. Shi; X. Lu","Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,17,"Erasure coding (EC) is a promising fault tolerance scheme that has been applied to many well-known distributed storage systems. The capability of Coherent EC Calculation and Networking on modern SmartNICs has demonstrated that EC will be an essential feature of in-network computing. In this paper, we propose a set of coherent in-network EC primitives, named INEC. Our analyses based on the proposed α-β performance model demonstrate that INEC primitives can enable different kinds of EC schemes to fully leverage the EC offload capability on modern SmartNICs. We implement INEC on commodity RDMA NICs and integrate it into five state-of-the-art EC schemes. Our experiments show that INEC primitives significantly reduce 50th, 95th, and 99th percentile latencies, and accelerate the end-to-end throughput, write, and degraded read performance of the key-value store co-designed with INEC by up to 99.57%, 47.30%, and 49.55%, respectively.","","978-1-7281-9998-6","10.1109/SC41405.2020.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355252","Next Generation Networking;In-Network Computing;Fault Tolerance;Erasure Coding","Fault tolerance;High performance computing;Computational modeling;Fault tolerant systems;Throughput;Encoding;Next generation networking","software fault tolerance;storage management","in-network computing;in-network EC primitives;α-β performance model;INEC primitives;EC offload capability;modern SmartNICs;commodity RDMA NICs;state-of-the-art EC schemes;erasure coding;fault tolerance scheme;storage systems","",6.0,"",53.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Waiting Game: Optimally Provisioning Fixed Resources for Cloud-Enabled Schedulers","P. Ambati; N. Bashir; D. Irwin; P. Shenoy","University of Massachusetts Amherst; University of Massachusetts Amherst; University of Massachusetts Amherst; University of Massachusetts Amherst","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"While cloud platforms enable users to rent computing resources on demand to execute their jobs, buying fixed resources is still much cheaper than renting if their utilization is high. Thus, optimizing cloud costs requires users to determine how many fixed resources to buy versus rent based on their workload. In this paper, we introduce the concept of a waiting policy for cloud-enabled schedulers, which is the dual of a scheduling policy, and show that the optimal cost depends on it. We define multiple waiting policies and develop simple analytical models to reveal their tradeoff between fixed resource provisioning, cost, and job waiting time. We evaluate the impact of these waiting policies on a year-long production batch workload consisting of 14Mjobs run on a 14.3k-core cluster, and show that a compound waiting policy decreases the cost (by 5%) and mean job waiting time (by 7×) compared to a fixed cluster of the current size.","","978-1-7281-9998-6","10.1109/SC41405.2020.00071","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355280","","Analytical models;Maximum likelihood estimation;High performance computing;Production;Machine learning;Games;Compounds","cloud computing;game theory;resource allocation;scheduling","fixed resources;cloud-enabled schedulers;scheduling policy;optimal cost;multiple waiting policies;fixed resource provisioning;job waiting time;compound waiting policy;fixed cluster;waiting game;cloud platforms;rent computing resources;cloud costs;production batch workload","",7.0,"",30.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Metis: Learning to Schedule Long-Running Applications in Shared Container Clusters at Scale","L. Wang; Q. Weng; W. Wang; C. Chen; B. Li","Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Huawei Technologies Ltd; Hong Kong University of Science and Technology","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,17,"Online cloud services are increasingly deployed as long-running applications (LRAs) in containers. Placing LRA containers is known to be difficult as they often have sophisticated resource interferences and I/O dependencies. Existing schedulers rely on operators to manually express the container scheduling requirements as placement constraints and strive to satisfy as many constraints as possible. Such schedulers, however, fall short in performance as placement constraints only provide qualitative scheduling guidelines and minimizing constraint violations does not necessarily result in the optimal performance.In this work, we present Metis, a general-purpose scheduler that learns to optimally place LRA containers using deep reinforcement learning (RL) techniques. This eliminates the complex manual specification of placement constraints and offers, for the first time, concrete quantitative scheduling criteria. As directly training an RL agent does not scale, we develop a novel hierarchical learning technique that decomposes a complex container placement problem into a hierarchy of subproblems with significantly reduced state and action space. We show that many subproblems have similar structures and can hence be solved by training a unified RL agent offline. Large-scale EC2 deployment shows that compared with the traditional constraint-based schedulers, Metis improves the throughput by up to 61%, optimizes various performance metrics, and easily scales to a large cluster where 3K containers run on over 700 machines.","","978-1-7281-9998-6","10.1109/SC41405.2020.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355246","","Training;Measurement;Schedules;Reinforcement learning;Manuals;Containers;Throughput","cloud computing;deep learning (artificial intelligence);scheduling","Metis;long-running applications;shared container clusters;online cloud services;placing LRA containers;container scheduling requirements;placement constraints;general-purpose scheduler;deep reinforcement learning techniques;complex manual specification;concrete quantitative scheduling criteria;hierarchical learning technique;complex container placement problem;unified RL agent offline;constraint-based schedulers;large-scale EC2 deployment;temperature 3.0 K","",8.0,"",85.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"BATCH: Machine Learning Inference Serving on Serverless Platforms with Adaptive Batching","A. Ali; R. Pinciroli; F. Yan; E. Smirni","University of Nevada, Reno, Reno, NV; William and Mary, Williamsburg, VA; University of Nevada, Reno, Reno, NV; William and Mary, Williamsburg, VA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Serverless computing is a new pay-per-use cloud service paradigm that automates resource scaling for stateless functions and can potentially facilitate bursty machine learning serving. Batching is critical for latency performance and cost-effectiveness of machine learning inference, but unfortunately it is not supported by existing serverless platforms due to their stateless design. Our experiments show that without batching, machine learning serving cannot reap the benefits of serverless computing. In this paper, we present BATCH, a framework for supporting efficient machine learning serving on serverless platforms. BATCH uses an optimizer to provide inference tail latency guarantees and cost optimization and to enable adaptive batching support. We prototype BATCH atop of AWS Lambda and popular machine learning inference systems. The evaluation verifies the accuracy of the analytic optimizer and demonstrates performance and cost advantages over the state-of-the-art method MArk and the state-of-the-practice tool SageMaker.","","978-1-7281-9998-6","10.1109/SC41405.2020.00073","National Science Foundation; Amazon Web Services; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355312","Machine-learning-as-a-service (MLaaS);Inference;Serving;Batching;Cloud;Serverless;Service Level Objective (SLO);Cost-effective;Optimization;Modeling;Prediction","Adaptive systems;High performance computing;Prototypes;Machine learning;Tools;Optimization","cloud computing;inference mechanisms;learning (artificial intelligence)","serverless platforms;serverless computing;pay-per-use cloud service paradigm;inference tail;inference systems;machine learning inference serving;BATCH;adaptive batching;MArk method;SageMaker","",25.0,"",84.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Reducing Communication in Graph Neural Network Training","A. Tripathy; K. Yelick; A. Buluç","Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Graph Neural Networks (GNNs) are powerful and flexible neural networks that use the naturally sparse connectivity information of the data. GNNs represent this connectivity as sparse matrices, which have lower arithmetic intensity and thus higher communication costs compared to dense matrices, making GNNs harder to scale to high concurrencies than convolutional or fully-connected neural networks. We introduce a family of parallel algorithms for training GNNs and show that they can asymptotically reduce communication compared to previous parallel GNN training methods. We implement these algorithms, which are based on 1D, 1. 5D, 2D, and 3D sparse-dense matrix multiplication, using torch.distributed on GPU-equipped clusters. Our algorithms optimize communication across the full GNN training pipeline. We train GNNs on over a hundred GPUs on multiple datasets, including a protein network with over a billion edges.","","978-1-7281-9998-6","10.1109/SC41405.2020.00074","National Science Foundation; National Science Foundation; Advanced Scientific Computing Research; Oak Ridge National Laboratory; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355273","Graph neural networks;distributed training;communication-avoiding algorithms","Training;Proteins;Three-dimensional displays;Two dimensional displays;Clustering algorithms;Graph neural networks;Sparse matrices","graph theory;graphics processing units;learning (artificial intelligence);matrix algebra;matrix multiplication;neural nets;parallel algorithms;sparse matrices","graph neural network training;Graph Neural Networks;naturally sparse connectivity information;sparse matrices;lower arithmetic intensity;higher communication costs;dense matrices;GNNs harder;parallel algorithms;training GNNs;3D sparse-dense matrix multiplication;GNN training pipeline;protein network;parallel GNN training methods","",12.0,"",34.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems","Y. Hu; Z. Ye; M. Wang; J. Yu; D. Zheng; M. Li; Z. Zhang; Z. Zhang; Y. Wang","School of ECE, Cornell University; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; Amazon Web Services; School of ECE, Cornell University; Amazon Web Services","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"Graph neural networks (GNNs) are gaining popularity as a promising approach to machine learning on graphs. Unlike traditional graph workloads where each vertex/edge is associated with a scalar, GNNs attach a feature tensor to each vertex/edge. This additional feature dimension, along with consequently more complex vertex- and edge-wise computations, has enormous implications on locality and parallelism, which existing graph processing systems fail to exploit. This paper proposes FeatGraph to accelerate GNN workloads by co-optimizing graph traversal and feature dimension computation. FeatGraph provides a flexible programming interface to express diverse GNN models by composing coarse-grained sparse templates with fine-grained user-defined functions (UDFs) on each vertex/edge. FeatGraph incorporates optimizations for graph traversal into the sparse templates and allows users to specify optimizations for UDFs with a feature dimension schedule (FDS). FeatGraph speeds up end-to-end GNN training and inference by up to 32× on CPU and 7× on GPU.","","978-1-7281-9998-6","10.1109/SC41405.2020.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355318","","Training;Computational modeling;Programming;Graph neural networks;Acceleration;Kernel;Optimization","graph theory;learning (artificial intelligence);neural nets","FeatGraph;graph neural network systems;machine learning;graph workloads;feature tensor;feature dimension;edge-wise computations;graph processing systems;GNN workloads;feature dimension computation;flexible programming interface;coarse-grained sparse templates;fine-grained user-defined functions;feature dimension schedule;co-optimized graph traversal","",8.0,"",45.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"GE-SpMM: General-Purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks","G. Huang; G. Dai; Y. Wang; H. Yang","Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China; Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,12,"The acceleration of Graph Neural Networks (GNNs) requires efficient and framework-compatible Sparse-Dense Matrix-Matrix Multiplication (SpMM). From the compatibility perspective, the sophisticated sparse matrix representations in state-of-the-art SpMM designs cause heavy preprocessing overhead for the framework. From the efficiency perspective, optimizations for SpMV (Sparse Matrix-Vector) do not apply well to SpMM, leading to redundant and uncoalesced global memory access. We propose GE-SpMM1, which takes the CSR format consistent with GNN frameworks to enable integration without the format transformation overhead. We use Coalesced Row Caching to ensure coalesced access to both sparse and dense data in the global memory. We use Coarse-grained Warp Merging to reduce redundant data loading among GPU warps. Experiments on a real-world graph dataset demonstrate up to 1.41× speedup over Nvidia cuSPARSE [1] and up to 1.81× over GraphBLAST [2]. We embed GE-SpMM in GNN frameworks and get up to 3.67× speedup on popular GNN models like GCN [3] and GraphSAGE [4].","","978-1-7281-9998-6","10.1109/SC41405.2020.00076","Research and Development; National Natural Science Foundation of China; China Postdoctoral Science Foundation; Beijing Innovation Center for Future Chip; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355302","","Training;Merging;Memory management;Loading;Graph neural networks;Sparse matrices;Optimization","graph theory;graphics processing units;matrix multiplication;multiprocessing systems;neural nets;optimisation;sparse matrices;vectors","graph neural networks;compatibility perspective;sophisticated sparse matrix representations;sparse matrix-vector;uncoalesced global memory access;GE-SpMM1;GNN frameworks;format transformation overhead;sparse data;dense data;graph dataset;general-purpose sparse matrix-matrix multiplication;compatible sparse-dense matrix-matrix multiplication;GPU;coarse-grained warp merging;SpMV;optimizations;GraphSAGE;GCN;GNN;Nvidia cuSPARSE","",9.0,"",32.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Alias-Free, Matrix-Free, and Quadrature-Free Discontinuous Galerkin Algorithms for (Plasma) Kinetic Equations","A. Hakim; J. Juno","Princeton Plasma Physics Laboratory, Princeton University, Princeton, NJ; Institute for Research in Electronics and Applied Physics University of Maryland, College Park, MD","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Understanding fundamental kinetic processes is important for many problems, from plasma physics to gas dynamics. A first-principles approach to these problems requires a statistical description via the Boltzmann equation, coupled to appropriate field equations. In this paper we present a novel version of the discontinuous Galerkin (DG) algorithm to solve such kinetic equations. Unlike Monte-Carlo methods, we use a continuum scheme in which we directly discretize the 6D phase-space using discontinuous basis functions. Our DG scheme eliminates counting noise and aliasing errors that would otherwise contaminate the delicate field-particle interactions. We use modal basis functions with reduced degrees of freedom to improve efficiency while retaining a high formal order of convergence. Our implementation incorporates a number of software innovations: use of JIT compiled top-level language, automatically generated computational kernels and a sophisticated shared-memory MPI implementation to handle velocity space parallelization.","","978-1-7281-9998-6","10.1109/SC41405.2020.00077","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355299","Discontinuous Galerkin;kinetic equations;computational physics","Computational modeling;Kinetic theory;Plasmas;Mathematical model;Kernel;Method of moments;Convergence","Boltzmann equation;Galerkin method;message passing;Monte Carlo methods;plasma kinetic theory","modal basis functions;quadrature-free discontinuous Galerkin algorithms;fundamental kinetic processes;plasma physics;gas dynamics;statistical description;Boltzmann equation;appropriate field equations;discontinuous Galerkin algorithm;Monte Carlo methods;continuum scheme;6D phase-space;discontinuous basis functions;DG scheme;delicate field-particle interactions;plasma kinetic equations","",3.0,"",50.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Distributed-Memory Parallel Symmetric Nonnegative Matrix Factorization","S. Eswar; K. Hayashi; G. Ballard; R. Kannan; R. Vuduc; H. Park","Dept. of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA; Dept. of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA; Dept. of Computer Science, Wake Forest University, Winston-Salem, USA; Computational Data Analytics Group, Oak Ridge National Laboratory, Oak Ridge, USA; Dept. of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA; Dept. of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"We develop the first distributed-memory parallel implementation of Symmetric Nonnegative Matrix Factorization (SymNMF), a key data analytics kernel for clustering and dimensionality reduction. Our implementation includes two different algorithms for SymNMF, which give comparable results in terms of time and accuracy. The first algorithm is a parallelization of an existing sequential approach that uses solvers for non symmetric NMF. The second algorithm is a novel approach based on the Gauss-Newton method. It exploits second-order information without incurring large computational and memory costs. We evaluate the scalability of our algorithms on the Summit system at Oak Ridge National Laboratory, scaling up to 128 nodes (4,096 cores) with 70% efficiency. Additionally, we demonstrate our software on an image segmentation task.","","978-1-7281-9998-6","10.1109/SC41405.2020.00078","Battelle; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355277","High performance computing;Newton method;Parallel algorithms;Symmetric Matrices","Symmetric matrices;Scalability;Software algorithms;Clustering algorithms;Software;Task analysis;Kernel","distributed memory systems;image segmentation;matrix decomposition;Newton method","nonsymmetric NMF;computational memory costs;distributed-memory parallel implementation;SymNMF;clustering;dimensionality reduction;parallelization;sequential approach;distributed-memory parallel symmetric nonnegative matrix factorization;data analytics kernel","",2.0,"",34.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Distributed Many-to-Many Protein Sequence Alignment using Sparse Matrices","O. Selvitopi; S. Ekanayake; G. Guidi; G. A. Pavlopoulos; A. Azad; A. Buluç","Computational Research Division, Lawrence Berkeley National Laboratory, USA; Microsoft Corporation, USA; Computational Research Division, Lawrence Berkeley National Laboratory, USA; BSRC “Alexander Fleming”, 34 Fleming Street, Institute for Fundamental Biomedical Research, Vari, Greece; Indiana University, USA; University of California, Berkeley, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Identifying similar protein sequences is a core step in many computational biology pipelines such as detection of homologous protein sequences, generation of similarity protein graphs for downstream analysis, functional annotation, and gene location. Performance and scalability of protein similarity search have proven to be a bottleneck in many bioinformatics pipelines due to increase in cheap and abundant sequencing data. This work presents a new distributed-memory software PASTIS. PASTIS relies on sparse matrix computations for efficient identification of possibly similar proteins. We use distributed sparse matrices for scalability and show that the sparse matrix infrastructure is a great fit for protein similarity search when coupled with a fully-distributed dictionary of sequences that allow remote sequence requests to be fulfilled. Our algorithm incorporates the unique bias in amino acid sequence substitution in search without altering basic sparse matrix model, and in turn, achieves ideal scaling up to millions of protein sequences.","","978-1-7281-9998-6","10.1109/SC41405.2020.00079","Office of Science; Advanced Scientific Computing Research; U.S. Department of Energy; Hellenic Foundation for Research and Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355305","","Scalability;Pipelines;Memory management;Protein sequence;Supercomputers;Software;Sparse matrices","bioinformatics;molecular biophysics;proteins;sparse matrices","protein sequence alignment;homologous protein sequences;protein similarity search;distributed-memory software PASTIS;amino acid sequence substitution;sparse matrix model;gene location","",9.0,"",32.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Runtime-Guided ECC Protection using Online Estimation of Memory Vulnerability","L. Jaulmes; M. Moretó; M. Valero; M. Erez; M. Casas","Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain; Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain; Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain; Electrical and Computer Engineering Department, University of Texas at Austin, Austin, USA; Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Spain","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Diminishing reliability of semiconductor technologies and decreasing power budgets per component hinder designing next-generation high performance computing (HPC) systems. Both constraints strongly impact memory subsystems, as DRAM main memory accounts for up to 30 to 50 percent of a node's overall power consumption, and is the subsystem that is most subject to faults. Improving reliability requires stronger error correcting codes (ECCs), which incur additional power and storage costs. It is critical to develop strategies to uphold memory reliability while minimising these costs, with the goal of improving the power efficiency of computing machines.We introduce a methodology to dynamically estimate the vulnerability of data, and adjust ECC protection accordingly. Our methodology relies on information readily available to runtime systems in task-based dataflow programming models, and the existing Virtualized Error Correcting Code (VECC) schemes to provide adaptable protection. Guiding VECC using vulnerability estimates offers a wide range of reliabilityredundancy trade-offs, as reliable as using expensive offline profiling for guidance and up to to 25% safer than VECC without guidance. Runtime-guided VECC is more efficient than a stronger uniform ECC, reducing DIMM lifetime failure from 1.84% down to 1.26% while increasing DRAM energy consumption by only 1.03×.","","978-1-7281-9998-6","10.1109/SC41405.2020.00080","Ministry of Education; Ministry of Economy; U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355313","Vulnerability;Runtime Systems;Error Correcting Codes;DRAM","Adaptation models;Runtime;Computational modeling;Memory management;Error correction codes;Reliability;Task analysis","data flow computing;DRAM chips;error correction codes;multiprocessing systems;power aware computing;reliability;storage management","runtime-guided ECC protection;online estimation;memory vulnerability;diminishing reliability;semiconductor technologies;power budgets;next-generation high performance computing systems;HPC;memory subsystems;DRAM main memory accounts;error correcting codes;ECCs;memory reliability;power efficiency;runtime systems;task-based dataflow programming models;adaptable protection;vulnerability estimates;runtime-guided VECC;DRAM energy consumption;virtualized error correcting code schemes","",1.0,"",37.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"CRAC: Checkpoint-Restart Architecture for CUDA with Streams and UVM","T. Jain; G. Cooperman","Khoury College of Computer Sciences, Northeastern University, Boston, USA; Khoury College of Computer Sciences, Northeastern University, Boston, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"The share of the top 500 supercomputers with NVIDIA GPUs is now over 25% and continues to grow. While fault tolerance is a critical issue for supercomputing, there does not currently exist an efficient, scalable solution for CUDA applications on NVIDIA GPUs. CRAC (Checkpoint-Restart Architecture for CUDA) is a new checkpoint-restart solution for fault tolerance that supports the full range of CUDA applications. CRAC combines: low runtime overhead (approximately 1% or less); fast checkpoint-restart; support for scalable CUDA streams (for efficient usage of all of the thousands of GPU cores); and support for the full features of Unified Virtual Memory (eliminating the programmer's burden of migrating memory between device and host). CRAC achieves its flexible architecture by segregating application code (checkpointed) and its external GPU communication via non-reentrant CUDA libraries (not checkpointed) within a single process's memory. This eliminates the high overhead of inter-process communication in earlier approaches, and has fewer limitations.","","978-1-7281-9998-6","10.1109/SC41405.2020.00081","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355317","Checkpointing;CUDA;Unified Virtual Memory;Parallel Processing;Split Processes","Fault tolerance;Runtime;Memory management;Fault tolerant systems;Graphics processing units;Supercomputers;Libraries","checkpointing;fault tolerant computing;graphics processing units;parallel architectures;parallel machines","runtime overhead;scalable CUDA streams;CRAC;flexible architecture;application code;nonreentrant CUDA libraries;NVIDIA GPU;fault tolerance;supercomputing;unified virtual memory;checkpoint-restart architecture for CUDA","",5.0,"",47.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"ANT-Man: Towards Agile Power Management in the Microservice Era","X. Hou; C. Li; J. Liu; L. Zhang; Y. Hu; M. Guo","Dept. of Computer Science and Engineering, Shanghai Jiao Tong University; Dept. of Computer Science and Engineering, Shanghai Jiao Tong University; Dept. of Computer Science and Engineering, Shanghai Jiao Tong University; Dept. of Computer Science and Engineering, Shanghai Jiao Tong University; Dept. of Computer Science and Engineering, University of Texas, Dallas; Dept. of Computer Science and Engineering, Shanghai Jiao Tong University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"The emerging trend of decomposing cloud applications into microservices has raised new questions about managing the performance/power trade-off of a datacenter at microsecondscale. We introduce ANT-Man, an Auto, Native and Transparent power Management framework that can exploit fine-grained microservice variability for system efficiency. To achieve this, ANT-Man abstracts away two major sources of latency overhead in traditional hierarchical power management frameworks. First, ANT-Man proposes an auto power budgeting scheme for reducing the power coordination latency at the datacenter level. It can proactively determine the power budget tailored to each individual microservice. Second, ANT-Man proposes a native and transparent power control scheme to overcome the power configuration latency for each microservice. It enables super-fast power budget enforcement with nanosecond-scale performance scaling. Extensive experiments on our prototyped system show that ANT-Man could slash power consumption by $ 7.8\sim 43.5\%$ and in the meantime reduce the $95^{\text{th}}$ tail latency by $ 9.7\sim 12.5\%$ compared to existing techniques.","","978-1-7281-9998-6","10.1109/SC41405.2020.00082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355316","microservice;variability;power management","Power demand;Power system management;Power control;Market research;Large-scale systems;Optimization;Next generation networking","cloud computing;computer centres;power aware computing;power consumption;power control","microservice era;Transparent power Management framework;fine-grained microservice variability;ANT-Man abstracts;traditional hierarchical power management frameworks;auto power budgeting scheme;power coordination;individual microservice;native power control scheme;transparent power control scheme;power configuration;super-fast power budget enforcement;power consumption;towards agile power Management","",9.0,"",79.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Scalable Heterogeneous Execution of a Coupled-Cluster Model with Perturbative Triples","J. Kim; A. Panyala; B. Peng; K. Kowalski; P. Sadayappan; S. Krishnamoorthy","School of Computing, University of Utah, Utah, USA; High-Performance Computing Pacific Northwest National Laboratory, Richland, USA; Chemical Physics and Analysis Pacific Northwest National Laboratory, Richland, USA; Chemical Physics and Analysis Pacific Northwest National Laboratory, Richland, USA; School of Computing, University of Utah, Utah, USA; High-Performance Computing Pacific Northwest National Laboratory, Richland, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"The CCSD(T) coupled-cluster model with perturbative triples is considered a gold standard for computational modeling of the correlated behavior of electrons in molecular systems. A fundamental constraint is the relatively small global-memory capacity in GPUs compared to the main-memory capacity on host nodes, necessitating relatively smaller tile sizes for high-dimensional tensor contractions in NWChem's GPU-accelerated implementation of the CCSD(T) method. A coordinated redesign is described to address this limitation and associated data movement overheads, including a novel fused GPU kernel for a set of tensor contractions, along with inter-node communication optimization and data caching. The new implementation of GPU-accelerated CCSD(T) improves overall performance by 3.4×. Finally, we discuss the trade-offs in using this fused algorithm on current and future supercomputing platforms.","","978-1-7281-9998-6","10.1109/SC41405.2020.00083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355297","","Tensors;Computational modeling;Triples (Data structure);High performance computing;Graphics processing units;Kernel;Optimization","coupled cluster calculations;graphics processing units;mainframes;parallel machines","scalable heterogeneous execution;perturbative triples;gold standard;computational modeling;molecular systems;fundamental constraint;relatively small global-memory capacity;main-memory capacity;host nodes;high-dimensional tensor contractions;NWChem's GPU;coordinated redesign;associated data movement overheads;GPU kernel;inter-node communication optimization;data caching;tensor contractions","",3.0,"",62.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"A Submatrix-Based Method for Approximate Matrix Function Evaluation in the Quantum Chemistry Code CP2K","M. Lass; R. Schade; T. D. Kühne; C. Plessl","Department of Computer Science, Paderborn University, Warburger Str. 100, Paderborn, Germany; Paderborn Center for Parallel Computing, Paderborn University, Warburger Str. 100, Paderborn, Germany; Department of Chemistry, Paderborn University, Warburger Str. 100, Paderborn, Germany; Department of Computer Science, Paderborn University, Warburger Str. 100, Paderborn, Germany","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Electronic structure calculations based on density-functional theory (DFT) represent a significant part of today's HPC workloads and pose high demands on high-performance computing resources. To perform these quantum-mechanical DFT calculations on complex large-scale systems, so-called linear scaling methods instead of conventional cubic scaling methods are required. In this work, we take up the idea of the submatrix method and apply it to the DFT computations in the software package CP2K. For that purpose, we transform the underlying numeric operations on distributed, large, sparse matrices into computations on local, much smaller and nearly dense matrices. This allows us to exploit the full floating-point performance of modern CPUs and to make use of dedicated accelerator hardware, where performance has been limited by memory bandwidth before. We demonstrate both functionality and performance of our implementation and show how it can be accelerated with GPUs and FPGAs.","","978-1-7281-9998-6","10.1109/SC41405.2020.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355245","Accelerator architectures;Approximate computing;Approximation algorithms;Chemistry;Density functional theory;Linear algebra;Open source software;Parallel algorithms;Reconfigurable architectures;Scientific computing;Solid-state physics","Tensors;Temperature;Discrete Fourier transforms;Transforms;Computational efficiency;Sparse matrices;Field programmable gate arrays","density functional theory;quantum chemistry;sparse matrices","quantum-mechanical DFT calculations;linear scaling methods;cubic scaling methods;submatrix method;sparse matrices;floating-point performance;matrix function;quantum chemistry code CP2K;electronic structure calculations;density-functional theory","",1.0,"",34.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Scaling the Hartree-Fock Matrix Build on Summit","G. M. J. Barca; D. L. Poole; J. L. G. Vallejo; M. Alkan; C. Bertoni; A. P. Rendell; M. S. Gordon","Research School of Computer Science, Australian National University, Canberra, Australia; Ames Laboratory, Iowa State University, Ames, IA, United States; Ames Laboratory, Iowa State University, Ames, IA, United States; Ames Laboratory, Iowa State University, Ames, IA, United States; Argonne National Laboratory, Leadership Computing Facility, Lemont, IL, United States; College of Science and Engineering, Flinders University, Adelaide, Australia; Ames Laboratory, Iowa State University, Ames, IA, United States","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Usage of Graphics Processing Units (GPU) has become strategic for simulating the chemistry of large molecular systems, with the majority of top supercomputers utilizing GPUs as their main source of computational horsepower. In this paper, a new fragmentation-based Hartree-Fock matrix build algorithm designed for scaling on many-GPU architectures is presented. The new algorithm uses a novel dynamic load balancing scheme based on a binned shell-pair container to distribute batches of significant shell quartets with the same code path to different GPUs. This maximizes computational throughput and load balancing, and eliminates GPU thread divergence due to integral screening. Additionally, the code uses a novel Fock digestion algorithm to contract electron repulsion integrals into the Fock matrix, which exploits all forms of permutational symmetry and eliminates thread synchronization requirements. The implementation demonstrates excellent scalability on the Summit computer, achieving good strong scaling performance up to 4096 nodes, and linear weak scaling up to 612 nodes.","","978-1-7281-9998-6","10.1109/SC41405.2020.00085","National Nuclear Security Administration; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355281","GPU;Hartree-Fock;Summit","Heuristic algorithms;Instruction sets;Graphics processing units;Throughput;Load management;Supercomputers;Synchronization","graphics processing units;HF calculations;mainframes;parallel architectures;parallel machines;performance evaluation;physics computing;quantum chemistry;resource allocation","fragmentation-based Hartree-Fock matrix build algorithm;shell quartets;computational throughput maximization;Fock digestion algorithm;Summit computer;permutational symmetry;electron repulsion integrals;integral screening;code path;binned shell-pair container;dynamic load balancing scheme;many-GPU architectures;computational horsepower;supercomputers;large molecular systems;graphics processing units","",7.0,"",27.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"MoHA: A Composable System for Efficient In-Situ Analytics on Heterogeneous HPC Systems","H. Xing; G. Agrawal; R. Ramnath","The Ohio State University; Augusta University; The Ohio State University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"Heterogeneous, dense computing architectures consisting of several accelerators, such as GPUs, attached to general-purpose CPUs are now integral High-Performance Computing (HPC) systems. However, these architectures pose severe memory and I/O constraints to computations involving in-situ analytics. This paper introduces MoHA, a framework for in-situ analytics that is designed to efficiently use the limited resources available on heterogeneous platforms. MoHA achieves this efficiency through the extensive use of bitmaps as a compressed or summary representation of simulation outputs. Our specific contributions in this paper include the design of bitmap generation and storage methods suitable for GPUs, the design and efficient implementation of a set of key operators for MoHA, and demonstrations of how several real queries on real datasets can be implemented using these operators. We demonstrate that MoHA reduces I/O transfer as well as overall processing time when compared to a baseline that does not use compressed representations.","","978-1-7281-9998-6","10.1109/SC41405.2020.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355207","Indexes;Data compression;High performance computing;Scientific computing;Query processing;Accelerator architecture","Computational modeling;High performance computing;Memory management;Memory architecture;Bandwidth","parallel processing;storage management","MoHA;bitmap generation;storage methods;GPUs;composable system;heterogeneous HPC systems;computing architectures;general-purpose CPUs;high-performance computing systems;heterogeneous platforms;accelerators;I/O constraints;in-situ analytics;queries;datasets;I/O transfer;processing time","","","",108.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Foresight: Analysis That Matters for Data Reduction","P. Grosset; C. M. Biwer; J. Pulido; A. T. Mohan; A. Biswas; J. Patchett; T. L. Turton; D. H. Rogers; D. Livescu; J. Ahrens","Data Science at Scale, CCS-7 Los Alamos National Laboratory, Los Alamos, USA; Data Science at Scale, CCS-7 Los Alamos National Laboratory, Los Alamos, USA; Data Science at Scale, CCS-7 Los Alamos National Laboratory, Los Alamos, USA; CCS-2: Computational Physics and Methods, Los Alamos National Laboratory, Los Alamos, USA; Data Science at Scale, CCS-7 Los Alamos National Laboratory, Los Alamos, USA; Data Science at Scale, CCS-7 Los Alamos National Laboratory, Los Alamos, USA; Data Science at Scale, CCS-7 Los Alamos National Laboratory, Los Alamos, USA; Data Science at Scale, CCS-7 Los Alamos National Laboratory, Los Alamos, USA; CCS-2: Computational Physics and Methods, Los Alamos National Laboratory, Los Alamos, USA; Data Science at Scale, CCS-7 Los Alamos National Laboratory, Los Alamos, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"As the computation power of supercomputers increases, so does simulation size, which in turn produces orders-of-magnitude more data. Because generated data often exceed the simulation's disk quota, many simulations would stand to benefit from data-reduction techniques to reduce storage requirements. Such techniques include autoencoders, data compression algorithms, and sampling. Lossy compression techniques can significantly reduce data size, but such techniques come at the expense of losing information that could result in incorrect post hoc analysis results. To help scientists determine the best compression they can get while keeping their analyses accurate, we have developed Foresight, an analysis framework that enables users to evaluate how different data-reduction techniques will impact their analyses. We use particle data from a cosmology simulation, turbulence data from Direct Numerical Simulation, and asteroid impact data from xRage to demonstrate how Foresight can help scientists determine the best data-reduction technique for their simulations.","","978-1-7281-9998-6","10.1109/SC41405.2020.00087","Los Alamos National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355315","Data compression;Performance evaluation;Multi-layer neural network","Computational modeling;Neural networks;Numerical simulation;Data models;Supercomputers;Numerical models;Solar system","asteroids;astronomy computing;cosmology;data compression;data reduction","Foresight;data reduction;data compression;lossy compression;data size;post hoc analysis;cosmology simulation;turbulence data;direct numerical simulation;asteroid impact data","",8.0,"",50.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Job Characteristics on Large-Scale Systems: Long-Term Analysis, Quantification, and Implications","T. Patel; Z. Liu; R. Kettimuthu; P. Rich; W. Allcock; D. Tiwari","Northeastern University; Argonne National Laboratory; Argonne National Laboratory; Argonne National Laboratory; Argonne National Laboratory; Northeastern University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,17,"HPC workload analysis and resource consumption characteristics are the key to driving better operation practices, system procurement decisions, and designing effective resource management techniques. Unfortunately, the HPC community does not have easy accessibility to long-term introspective work-load analysis and characterization for production-scale HPC systems. This study bridges this gap by providing detailed long-term quantification, characterization, and analysis of job characteristics on two supercomputers: Intrepid and Mira. This study is one of the largest of its kind - covering trends and characteristics for over three billion compute hours, 750 thousand jobs, and spanning a decade. We confirm several long-held conventional wisdom, and identify many previously undiscovered trends and its implications. We also introduce a learning based technique to predict the resource requirement of future jobs with high accuracy, using features available prior to the job submission and without requiring any application-specific tracing or application-intrusive instrumentation.","","978-1-7281-9998-6","10.1109/SC41405.2020.00088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355229","High Performance Computing;Large-Scale Systems;Monitoring;Queueing Analysis;Statistical Analysis","Procurement;Instruments;Market research;Supercomputers;Large-scale systems;Resource management;Queueing analysis","learning (artificial intelligence);parallel machines;parallel processing;power aware computing;procurement;resource allocation","job submission;job characteristics;large-scale systems;long-term analysis;HPC workload analysis;resource consumption characteristics;operation practices;system procurement decisions;effective resource management techniques;HPC community;easy accessibility;long-term introspective work-load analysis;production-scale HPC systems;long-term quantification;billion compute hours;long-held conventional wisdom;resource requirement;future jobs","",18.0,"",61.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Pencil: A Pipelined Algorithm for Distributed Stencils","H. Wang; A. Chandramowlishwaran","University of California, Irvine; University of California, Irvine","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"Stencil computations are at the core of various Computational Fluid Dynamics (CFD) applications and have been well-studied for several decades. Typically they're highly memory-bound and as a result, numerous tiling algorithms have been proposed to improve its performance. Although efficient, most of these algorithms are designed for single iteration spaces on shared-memory machines. However, in CFD, we are confronted with multi-block structured girds composed of multiple connected iteration spaces distributed across many nodes.In this paper, we propose a pipelined stencil algorithm called Pencil for distributed memory machines that applies to practical CFD problems that span multiple iteration spaces. Based on an in-depth analysis of cache tiling on a single node, we first identify both the optimal combination of MPI and OpenMP for temporal tiling and the best tiling approach, which outperforms the state-of-the-art automatic parallelization tool Pluto by up to $1.92 \times$. Then, we adopt DeepHalo to decouple the multiple connected iteration spaces so that temporal tiling can be applied to each space. Finally, we achieve overlap by pipelining the computation and communication without sacrificing the advantage from temporal cache tiling. Pencil is evaluated using 4 stencils across 6 numerical schemes on two distributed memory machines with Omni-Path and InfiniBand networks. On the Omni-Path system, Pencil exhibits outstanding weak and strong scalability for up to 128 nodes and outperforms MPI+OpenMP Funneled with space tiling by $1.33- 3.41 \times$ on a multi-block grid with 32 nodes.","","978-1-7281-9998-6","10.1109/SC41405.2020.00089","National Science Foundation; University of California; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355259","Computational fluid dynamics;stencils;multiple connected iteration spaces;cache tiling;pipelining;distributed-memory machines","Computational fluid dynamics;Heuristic algorithms;Scalability;Tools;Numerical models;Pluto;Pipeline processing","application program interfaces;cache storage;computational fluid dynamics;distributed memory systems;iterative methods;message passing;optimisation;parallel architectures;shared memory systems","stencil computations;computational fluid dynamics applications;highly memory-bound;numerous tiling algorithms;single iteration spaces;shared-memory machines;multiblock structured girds;multiple connected iteration spaces;pipelined stencil algorithm;Pencil;distributed memory machines;practical CFD problems;multiple iteration spaces;temporal tiling;tiling approach;state-of-the-art automatic parallelization tool Pluto;temporal cache tiling;strong scalability;space tiling;pipelined algorithm;distributed stencils;omni-path system;MPI;OpenMP","",4.0,"",68.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Speeding Up SpMV for Power-Law Graph Analytics by Enhancing Locality & Vectorization","S. Yesil; A. Heidarshenas; A. Morrison; J. Torrellas","Dept. of Computer Science, University of Illinois at Urbana-Champaign; Dept. of Computer Science, University of Illinois at Urbana-Champaign; Blavatnik School of Computer Science, Tel Aviv University; Dept. of Computer Science, University of Illinois at Urbana-Champaign","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Graph analytics applications often target large-scale web and social networks, which are typically power-law graphs. Graph algorithms can often be recast as generalized Sparse Matrix-Vector multiplication (SpMV) operations, making SpMV optimization important for graph analytics. However, executing SpMV on large-scale power-law graphs results in highly irregular memory access patterns with poor cache utilization. Worse, we find that existing SpMV locality and vectorization optimizations are largely ineffective on modern out-of-order (OOO) processors-they are not faster (or only marginally so) than the standard Compressed Sparse Row (CSR) SpMV implementation. To improve performance for power-law graphs on modern OOO processors, we propose Locality-Aware Vectorization (LAV). LAV is a new approach that leverages a graph's power-law nature to extract locality and enable effective vectorization for SpMV-like memory access patterns. LAV splits the input matrix into a dense and a sparse portion. The dense portion is stored in a new representation, which is vectorization-friendly and exploits data locality. The sparse portion is processed using the standard CSR algorithm. We evaluate LAV with several graphs on an Intel Skylake-SP processor, and find that it is faster than CSR (and prior approaches) by an average of 1.5x. LAV reduces the number of DRAM accesses by 35% on average, with only a 3.3% memory overhead.","","978-1-7281-9998-6","10.1109/SC41405.2020.00090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355205","Sparse Matrix Vector Products;Graph Algorithms;Vectorization;SIMD;Locality optimizations","Out of order;Program processors;Social networking (online);Random access memory;Sparse matrices;Optimization;Standards","cache storage;DRAM chips;graph theory;mathematics computing;matrix multiplication;sparse matrices;vectors","power-law graph analytics;graph analytics applications;large-scale web;social networks;power-law graphs;graph algorithms;generalized Sparse Matrix-Vector multiplication operations;SpMV optimization;large-scale power-law graphs results;memory access patterns;cache utilization;vectorization optimizations;out-of-order processors;OOO processors;LAV;SpMV-like memory access patterns;sparse portion;data locality;standard CSR algorithm;SpMV locality;standard compressed sparse row SpMV implementation;locality-aware vectorization;DRAM","",7.0,"",51.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Efficient Tiled Sparse Matrix Multiplication through Matrix Signatures","S. E. Kurt; A. Sukumaran-Rajam; F. Rastello; P. Sadayyapan","University of Utah, Salt Lake City, Utah; Washington State University, Pullman, Washington; Inria, CNRS, Grenoble INP, LIG, Univ. Grenoble Alpes, Grenoble, France; University of Utah, Salt Lake City, Utah","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Tiling is a key technique to reduce data movement in matrix computations. While tiling is well understood and widely used for dense matrix/tensor computations, effective tiling of sparse matrix computations remains a challenging problem. This paper proposes a novel method to efficiently summarize the impact of the sparsity structure of a matrix on achievable data reuse as a one-dimensional signature, which is then used to build an analytical cost model for tile size optimization for sparse matrix computations. The proposed model-driven approach to sparse tiling is evaluated on two key sparse matrix kernels: Sparse Matrix - Dense Matrix Multiplication (SpMM) and Sampled Dense-Dense Matrix Multiplication (SDDMM). Experimental results demonstrate that model-based tiled SpMM and SDDMM achieve high performance relative to the current state-of-the-art.","","978-1-7281-9998-6","10.1109/SC41405.2020.00091","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355290","sparse matrix signature;sparse tiling;SpMM;SpMDM;Sparse Dense Matrix Multiplication;Multi-core","Analytical models;Computational modeling;Two dimensional displays;Data models;Sparse matrices;Kernel;Optimization","mathematics computing;matrix multiplication;optimisation;sparse matrices;tensors","key sparse matrix kernels;sampled dense-dense matrix multiplication;effective tiling;sparse matrix computations;data reuse;tile size optimization;tiled sparse matrix multiplication;sparse matrix-dense matrix multiplication;matrix signatures;data movement reduction;tensor computation;sparsity structure","",4.0,"",42.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"GPU-Trident: Efficient Modeling of Error Propagation in GPU Programs","A. R. Anwer; G. Li; K. Pattabiraman; M. Sullivan; T. Tsai; S. K. S. Hari","University of British Columbia; University of Iowa; University of British Columbia; NVIDIA; NVIDIA; NVIDIA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,15,"Fault injection (FI) techniques are typically used to determine the reliability profiles of programs under soft errors. However, these techniques are highly resource- and time-intensive. Prior research developed a model, TRIDENT to analytically predict Silent Data Corruption (SDC, i.e., incorrect output without any indication) probabilities of single-threaded CPU applications without requiring FIs. Unfortunately, TRIDENT is incompatible with GPU programs, due to their high degree of parallelism and different memory architectures than CPU programs. The main challenge is that modeling error propagation across thousands of threads in a GPU kernel requires enormous amounts of data to be profiled and analyzed, posing a major scalability bottleneck for HPC applications. In this paper, we propose GPU-TRIDENT, an accurate and scalable technique for modeling error propagation in GPU programs. We find that GPU-TRIDENT is 2 orders of magnitude faster than FI-based approaches, and nearly as accurate in determining the SDC rate of GPU programs.","","978-1-7281-9998-6","10.1109/SC41405.2020.00092","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355257","GPU;Error Propagation;Soft Error;Silent Data Corruption;Error Resilience;Program Analysis","Analytical models;Scalability;Graphics processing units;Programming;Data models;Reliability;Kernel","fault tolerance;graphics processing units;parallel processing;radiation hardening (electronics)","GPU-Trident;GPU programs;fault injection techniques;soft errors;single-threaded CPU applications;CPU programs;error propagation;GPU kernel;reliability profiles;silent data corruption","",11.0,"",37.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"GVPROF: A Value Profiler for GPU-Based Clusters","K. Zhou; Y. Hao; J. Mellor-Crummey; X. Meng; X. Liu","Department of Computer Science, Rice University, Houston, USA; Department of Computer Science, North Carolina State University, Raleigh, USA; Department of Computer Science, Rice University, Houston, USA; Department of Computer Science, Rice University, Houston, USA; Department of Computer Science, North Carolina State University, Raleigh, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"GPGPUs are widely used in high-performance computing systems to accelerate scientific and machine learning workloads. Developing efficient GPU kernels is critically important to obtain “bare-metal” performance on GPU-based clusters. In this paper, we describe the design and implementation of GVPROF, the first value profiler that pinpoints value-related inefficiencies in applications running on NVIDIA GPU-based clusters. The novelty of GVPROF resides in its ability to detect temporal and spatial value redundancies, which provides useful information to guide code optimization. GVPROF can monitor production multi-node multi-GPU executions in clusters. Our experiments with well-known GPU benchmarks and HPC applications show that GVPROF incurs acceptable overhead and scales to large executions. Using GVPROF, we optimized several HPC and machine learning workloads on one NVIDIA V100 GPU. In one case study of LAMMPS, optimizations based on information from GVProf led to whole-program speedups ranging from 1.37x on a single GPU to 1.08x on 64 GPUs.","","978-1-7281-9998-6","10.1109/SC41405.2020.00093","U.S. Department of Energy; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355258","High performance computing;Performance analysis;Parallel programming;Supercomputers","Redundancy;Graphics processing units;Machine learning;Production;Kernel;Optimization;Monitoring","graphics processing units;multiprocessing systems;pattern clustering","GPU kernels;value profiler;NVIDIA GPU-based clusters;temporal value redundancies;spatial value redundancies;production multinode multiGPU executions;machine learning workloads;NVIDIA V100 GPU;high-performance computing systems;GVPROF;HPC;LAMMPS;bare-metal performance;code optimization","",8.0,"",60.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"An Efficient and Non-Intrusive GPU Scheduling Framework for Deep Learning Training Systems","S. Wang; O. J. Gonzalez; X. Zhou; T. Williams; B. D. Friedman; M. Havemann; T. Woo","Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Nokia Bell Labs, New Providence, NJ, USA; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Nokia Bell Labs, New Providence, NJ, USA; Nokia Bell Labs, New Providence, NJ, USA; Nokia Bell Labs, New Providence, NJ, USA; Nokia Bell Labs, New Providence, NJ, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"Efficient GPU scheduling is the key to minimizing the execution time of the Deep Learning (DL) training workloads. DL training system schedulers typically allocate a fixed number of GPUs to each job, which inhibits high resource utilization and often extends the overall training time. The recent introduction of schedulers that can dynamically reallocate GPUs has achieved better cluster efficiency. This dynamic nature, however, introduces additional overhead by terminating and restarting jobs or requires modification to the DL training frameworks.We propose and develop an efficient, non-intrusive GPU scheduling framework that employs a combination of an adaptive GPU scheduler and an elastic GPU allocation mechanism to reduce the completion time of DL training workloads and improve resource utilization. Specifically, the adaptive GPU scheduler includes a scheduling algorithm that uses training job progress information to determine the most efficient allocation and reallocation of GPUs for incoming and running jobs at any given time. The elastic GPU allocation mechanism works in concert with the scheduler. It offers a lightweight and nonintrusive method to reallocate GPUs based on a “SideCar” process that temporarily stops and restarts the job's DL training process with a different number of GPUs. We implemented the scheduling framework as plugins in Kubernetes and conducted evaluations on two 16-GPU clusters with multiple training jobs based on TensorFlow. Results show that our proposed scheduling framework reduces the overall execution time and the average job completion time by up to 45% and 63%, respectively, compared to the Kubernetes default scheduler. Compared to a termination based scheduler, our framework reduces the overall execution time and the average job completion time by up to 20% and 37%, respectively.","","978-1-7281-9998-6","10.1109/SC41405.2020.00094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355203","deep learning;GPU clusters;resource scheduling;container;Kubernetes","Training;Deep learning;Scheduling algorithms;Graphics processing units;Process control;Dynamic scheduling;Resource management","deep learning (artificial intelligence);graphics processing units;processor scheduling;resource allocation","nonintrusive GPU scheduling framework;Deep Learning training systems;efficient GPU scheduling;execution time;DL training system schedulers;high resource utilization;training time;cluster efficiency;adaptive GPU scheduler;elastic GPU allocation mechanism;DL training workloads;training job progress information;reallocation;running jobs;16-GPU clusters;multiple training jobs;average job completion time;Kubernetes default scheduler;termination based scheduler","",4.0,"",48.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Preparing Nuclear Astrophysics for Exascale","M. P. Katz; A. Almgren; M. B. Sazo; K. Eiden; K. Gott; A. Harpole; J. M. Sexton; D. E. Willcox; W. Zhang; M. Zingale","NVIDIA Corporation; Center for Computational Sciences and Engineering, Lawrence Berkeley National Laboratory; Department of Physics and Astronomy, Stony Brook University; Department of Astronomy, University of California Berkeley; Center for Computational Sciences and Engineering, Lawrence Berkeley National Laboratory; Department of Physics and Astronomy, Stony Brook University; Center for Computational Sciences and Engineering, Lawrence Berkeley National Laboratory; Center for Computational Sciences and Engineering, Lawrence Berkeley National Laboratory; Center for Computational Sciences and Engineering, Lawrence Berkeley National Laboratory; Department of Physics and Astronomy, Stony Brook University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,12,"Astrophysical explosions such as supernovae are fascinating events that require sophisticated algorithms and substantial computational power to model. Castro and MAESTROeX are nuclear astrophysics codes that simulate thermonuclear fusion in the context of supernovae and X-ray bursts. Examining these nuclear burning processes using high resolution simulations is critical for understanding how these astrophysical explosions occur. In this paper we describe the changes that have been made to these codes to transform them from standard MPI + OpenMP codes targeted at petascale CPU-based systems into a form compatible with the pre-exascale systems now online and the exascale systems coming soon. We then discuss what new science is possible to run on systems such as Summit and Perlmutter that could not have been achieved on the previous generation of supercomputers.","","978-1-7281-9998-6","10.1109/SC41405.2020.00095","Nuclear Physics; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355327","astrophysics;hydrodynamics;computational fluid dynamics;gravity;nuclear reactions;adaptive mesh refinement","Astrophysics;Computational modeling;High performance computing;Transforms;Explosions;Supercomputers;Standards","application program interfaces;astronomy computing;message passing;nucleosynthesis;parallel programming;stellar internal processes;supernovae","high resolution simulations;astrophysical explosions;standard MPI + OpenMP codes;petascale CPU-based systems;pre-exascale systems;nuclear astrophysics;supernovae;sophisticated algorithms;computational power;nuclear astrophysics codes;thermonuclear fusion;X-ray bursts;nuclear burning processes","",4.0,"",33.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"A Performance-Portable Nonhydrostatic Atmospheric Dycore for the Energy Exascale Earth System Model Running at Cloud-Resolving Resolutions","L. Bertagna; O. Guba; M. A. Taylor; J. G. Foucar; J. Larkin; A. M. Bradley; S. Rajamanickam; A. G. Salinger","Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; NVIDIA Corporation, Santa Clara, CA, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"We present an effort to port the nonhydrostatic atmosphere dynamical core of the Energy Exascale Earth System Model (E3SM) to efficiently run on a variety of architectures, including conventional CPU, many-core CPU, and GPU. We specifically target cloud-resolving resolutions of 3 km and 1 km. To express on-node parallelism we use the C++ library Kokkos, which allows us to achieve a performance portable code in a largely architecture-independent way. Our C++ implementation is at least as fast as the original Fortran implementation on IBM Power9 and Intel Knights Landing processors, proving that the code refactor did not compromise the efficiency on CPU architectures. On the other hand, when using the GPUs, our implementation is able to achieve 0.97 Simulated Years Per Day, running on the full Summit supercomputer. To the best of our knowledge, this is the most achieved to date by any global atmosphere dynamical core running at such resolutions.","","978-1-7281-9998-6","10.1109/SC41405.2020.00096","Oak Ridge National Laboratory; Office of Science; Office of Science; Sandia National Laboratories; National Nuclear Security Administration; National Nuclear Security Administration; U.S. Department of Energy; Office of Science; Biological and Environmental Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355270","Atmospheric modeling;Multicore processing;High performance computing","Earth;Atmospheric modeling;Computational modeling;Clouds;Energy resolution;Computer architecture;C++ languages","atmospheric movements;atmospheric techniques;clouds;coprocessors;FORTRAN;geophysics computing;graphics processing units;multiprocessing systems;parallel architectures;parallel processing","performance-portable nonhydrostatic atmospheric dycore;Energy Exascale Earth System Model;cloud-resolving resolutions;nonhydrostatic atmosphere dynamical core;E3SM;conventional CPU;many-core CPU;performance portable code;original Fortran implementation;CPU architectures;global atmosphere dynamical core;C++ library Kokkos;IBM Power9;Intel Knights Landing processors;GPU;full Summit supercomputer","",5.0,"",59.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Acceleration of Fusion Plasma Turbulence Simulations using the Mixed-Precision Communication-Avoiding Krylov Method","Y. Idomura; T. Ina; Y. Ali; T. Imamura","Center for Computational Science and e-Systems, Japan Atomic Energy Agency, Kashiwa, Japan; Center for Computational Science, Riken, Kobe, Japan; Center for Computational Science and e-Systems, Japan Atomic Energy Agency, Kashiwa, Japan; Center for Computational Science, Riken, Kobe, Japan","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,13,"The multi-scale full-f simulation of the next generation experimental fusion reactor ITER based on a five dimensional (5D) gyrokinetic model is one of the most computationally demanding problems in fusion science. In this work, a Gyrokinetic Toroidal 5D Eulerian code (GT5D) is accelerated by a new mixed-precision communication-avoiding (CA) Krylov method. The bottleneck of global collective communication on accelerated computing platforms is resolved using a CA Krylov method. In addition, a new FP16 preconditioner, which is designed using the new support for FP16 SIMD operations on A64FX, reduces both the number of iterations (halo data communication) and the computational cost. The performance of the proposed method for ITER size simulations with ~0.1 trillion grids on 1,440 CPUs/GPUs on Fugaku and Summit shows 2.8× and 1.9× speedups respectively from the conventional non-CA Krylov method, and excellent strong scaling is obtained up to 5,760 CPUs/GPUs.","","978-1-7281-9998-6","10.1109/SC41405.2020.00097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355276","Gyrokinetic simulation;Fusion plasma;ITER;Communication avoiding Krylov method;Mixed-precision computing;Fugaku;Summit","Computational modeling;High performance computing;Fusion reactors;Plasmas;Acceleration;Next generation networking;Hardware acceleration","graphics processing units;parallel machines;parallel processing;physics computing;plasma simulation;plasma toroidal confinement;plasma turbulence;Tokamak devices","mixed-precision communication-avoiding Krylov method;global collective communication;accelerated computing platforms;CA Krylov method;FP16 preconditioner;FP16 SIMD operations;halo data communication;computational cost;ITER size simulations;fusion plasma turbulence simulations;fusion science;Gyrokinetic Toroidal 5D Eulerian code;GT5D;five dimensional gyrokinetic model;5D gyrokinetic model;next generation experimental fusion reactor;multiscale full-f simulation;CPU;GPU;Fugaku;Summit","",1.0,"",42.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Convolutional Neural Network Training with Distributed K-FAC","J. G. Pauloski; Z. Zhang; L. Huang; W. Xu; I. T. Foster","University of Texas at Austin; Texas Advanced Computing Center; Texas Advanced Computing Center; Texas Advanced Computing Center; University of Chicago & Argonne National Laboratory","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,12,"Training neural networks with many processors can reduce time-to-solution; however, it is challenging to maintain convergence and efficiency at large scales. The Kroneckerfactored Approximate Curvature (K-FAC) was recently proposed as an approximation of the Fisher Information Matrix that can be used in natural gradient optimizers. We investigate here a scalable K-FAC design and its applicability in convolutional neural network (CNN) training at scale. We study optimization techniques such as layer-wise distribution strategies, inverse-free second-order gradient evaluation, and dynamic K-FAC update decoupling to reduce training time while preserving convergence. We use residual neural networks (ResNet) applied to the CIFAR10 and ImageNet-1k datasets to evaluate the correctness and scalability of our K-FAC gradient preconditioner. With ResNet-50 on the ImageNet-1k dataset, our distributed K-FAC implementation converges to the 75.9% MLPerf baseline in 18-25% less time than does the classic stochastic gradient descent (SGD) optimizer across scales on a GPU cluster.","","978-1-7281-9998-6","10.1109/SC41405.2020.00098","U.S. Department of Energy; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355234","optimization methods;neural networks;scalability;high performance computing","Training;Scalability;Neural networks;Convolutional neural networks;Optimization;Convergence;Residual neural networks","convolutional neural nets;gradient methods;learning (artificial intelligence);optimisation;stochastic processes","convolutional neural network training;time-to-solution;Kronecker-factored approximate curvature;Fisher information matrix;natural gradient optimizers;scalable K-FAC design;optimization techniques;layer-wise distribution strategies;residual neural networks;ImageNet-1k dataset;K-FAC gradient preconditioner;ResNet-50;stochastic gradient descent optimizer","",6.0,"",42.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"BiQGEMM: Matrix Multiplication with Lookup Table for Binary-Coding-Based Quantized DNNs","Y. Jeon; B. Park; S. J. Kwon; B. Kim; J. Yun; D. Lee","Samsung Research, Seoul, Republic of Korea; Samsung Research, Seoul, Republic of Korea; Samsung Research, Seoul, Republic of Korea; Samsung Research, Seoul, Republic of Korea; Samsung Research, Seoul, Republic of Korea; Samsung Research, Seoul, Republic of Korea","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"The number of parameters in deep neural networks (DNNs) is rapidly increasing to support complicated tasks and to improve model accuracy. Correspondingly, the amount of computations and required memory footprint increase as well. Quantization is an efficient method to address such concerns by compressing DNNs such that computations can be simplified while required storage footprint is significantly reduced. Unfortunately, commercial CPUs and GPUs do not fully support quantization because only fixed data transfers (such as 32 bits) are allowed. As a result, even if weights are quantized (by a non-uniform quantization scheme) into a few bits, CPUs and GPUs may not access multiple quantized weights without memory bandwidth waste. Success of quantization in practice, hence, relies on an efficient computation engine design, especially for matrix multiplication that is a basic computation engine in most DNNs. In this paper, we propose a novel matrix multiplication method, called BiQGEMM, dedicated to quantized DNNs. BiQGEMM can access multiple quantized weights simultaneously in one instruction. In addition, BiQGEMM pre-computes intermediate results that are highly redundant when quantization leads to limited available computation space. Since pre-computed values are stored in lookup tables and reused, BiQGEMM achieves lower amount of overall computations. Our extensive experimental results show that BiQGEMM presents higher performance than conventional schemes when DNNs are quantized.","","978-1-7281-9998-6","10.1109/SC41405.2020.00099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355306","Model Compression;Deep Learning;Machine Learning;AI Inference;Quantization;GEMM;GEMV","Quantization (signal);Memory management;Neural networks;Bandwidth;Data transfer;Computational efficiency;Engines","binary codes;deep learning (artificial intelligence);integrated circuit design;matrix multiplication;memory architecture;microprocessor chips;neural nets;quantisation (signal);table lookup","lookup table;binary-coding-based quantized DNN;deep neural networks;memory footprint;GPU;quantization scheme;BiQGEMM;computation engine design;CPU;general matrix-to-matrix multiplication","",3.0,"",58.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Term Quantization: Furthering Quantization at Run Time","H. T. Kung; B. McDanel; S. Q. Zhang","Harvard University; Franklin and Marshall College; Harvard University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,16,"We present a novel technique, called Term Quantization (TQ), for furthering quantization at run time for improved computational efficiency of deep neural networks (DNNs) already quantized with conventional quantization methods. TQ operates on power-of-two terms in expressions of values. In computing a dot-product computation, TQ dynamically selects a fixed number of largest terms to use from values of the two vectors. By exploiting weight and data distributions typically present in DNNs, TQ has a minimal impact on DNN model performance (e.g., accuracy or perplexity). We use TQ to facilitate tightly synchronized processor arrays, such as systolic arrays, for efficient parallel processing. We evaluate TQ on an MLP for MNIST, multiple CNNs for ImageNet and an LSTM for Wikitext-2. We demonstrate significant reductions in inference computation costs (between 3-10×) compared to conventional uniform quantization for the same level of model performance.","","978-1-7281-9998-6","10.1109/SC41405.2020.00100","Air Force Research Laboratory; MediaTek; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355228","Deep neural network (DNN);quantization;accelerator","Quantization (signal);Computational modeling;Neural networks;Parallel processing;Data models;Arrays;Field programmable gate arrays","convolutional neural nets;deep learning (artificial intelligence);floating point arithmetic;inference mechanisms;matrix algebra;parallel processing;recurrent neural nets;systolic arrays;vectors","deep neural networks;DNN model;quantization methods;power-of-two terms;dot-product computation;inference computation costs;uniform quantization;term quantization;computational efficiency;ImageNet CNN;LSTM;Wikitext-2;MNIST;MLP;synchronized processor arrays;systolic arrays","",4.0,"",48.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Compiling Generalized Histograms for GPU","T. Henriksen; S. Hellfritzsch; P. Sadayappan; C. Oancea","University of Copenhagen, Copenhagen, Denmark; University of Copenhagen, Copenhagen, Denmark; University of Copenhagen, Copenhagen, Denmark; University of Copenhagen, Copenhagen, Denmark","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"We present and evaluate an implementation technique for histogram-like computations on GPUs that ensures both work-efficient asymptotic cost, support for arbitrary associative and commutative operators, and efficient use of hardware-supported atomic operations when applicable. Based on a systematic empirical examination of the design space, we develop a technique that balances conflict rates and memory footprint. We demonstrate our technique both as a library implementation in CUDA, as well as by extending the parallel array language Futhark with a new construct for expressing generalized histograms, and by supporting this construct with several compiler optimizations. We show that our histogram implementation taken in isolation outperforms similar primitives from CUB, and that it is competitive or outperforms the hand-written code of several application benchmarks, even when the latter is specialized for a class of datasets.","","978-1-7281-9998-6","10.1109/SC41405.2020.00101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355244","GPU;parallelism;functional programming","Histograms;Analytical models;Systematics;Parallel programming;Graphics processing units;Benchmark testing;Libraries","graphics processing units;parallel architectures;parallel languages;parallel programming;program compilers","hardware-supported atomic operations;systematic empirical examination;design space;conflict rates;memory footprint;parallel array language Futhark;generalized histograms;compiler optimizations;application benchmarks;GPU;histogram-like computations;work-efficient asymptotic cost;arbitrary associative operators;commutative operators;CUDA","",5.0,"",29.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"CCAMP: An Integrated Translation and optimization Framework for OpenACC and OpenMP","J. Lambert; S. Lee; J. S. Vetter; A. D. Malony","University of Oregon; Oak Ridge National Laboratory; Oak Ridge National Laboratory; University of Oregon","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Heterogeneous computing and exploration into specialized accelerators are inevitable in current and future supercomputers. Although this diversity of devices is promising for performance, the array of architectures presents programming challenges. High-level programming strategies have emerged to face these challenges, such as the OpenMP offloading model and OpenACC. However, the varying levels of support for these standards within vendor-specific and open-source tools, as well as the lack of performance portability across devices, have prevented the standards from achieving their goals. To address these shortcomings, we present CCAMP, an OpenMP and OpenACC interoperable framework. CCAMP provides two primary facilities: language translation between the two standards and device-specific directive optimization within each standard. We show that by using the CCAMP framework, programmers can easily transplant non-portable code into new ecosystems for new architectures. Additionally, by using CCAMP's device-specific directive optimizations, users can achieve optimized performance across architectures using a single source code.","","978-1-7281-9998-6","10.1109/SC41405.2020.00102","National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355287","OpenMP;OpenACC;heterogeneous computing;OpenARC","Performance evaluation;Computer architecture;Tools;Programming;Task analysis;Standards;Optimization","multiprocessing systems;open systems;optimisation;parallel machines;parallel programming;program compilers","integrated translation;single source code;nonportable code;CCAMP framework;language translation;primary facilities;performance portability;open-source tools;vendor-specific;OpenMP offloading model;high-level programming strategies;future supercomputers;current supercomputers;specialized accelerators;OpenACC;optimization framework","",7.0,"",23.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"High-Performance Parallel Graph Coloring with Strong Guarantees on Work, Depth, and Quality","M. Besta; A. Carigiet; K. Janda; Z. Vonarburg-Shmaria; L. Gianinazzi; T. Hoefler","Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, Electronics and Telecommunications, AGH-UST; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,17,"We develop the first parallel graph coloring heuristics with strong theoretical guarantees on work and depth and coloring quality. The key idea is to design a relaxation of the vertex degeneracy order, a well-known graph theory concept, and to color vertices in the order dictated by this relaxation. This introduces a tunable amount of parallelism into the degeneracy ordering that is otherwise hard to parallelize. This simple idea enables significant benefits in several key aspects of graph coloring. For example, one of our algorithms ensures polylogarithmic depth and a bound on the number of used colors that is superior to all other parallelizable schemes, while maintaining workefficiency. In addition to provable guarantees, the developed algorithms have competitive run-times for several real-world graphs, while almost always providing superior coloring quality. Our degeneracy ordering relaxation is of separate interest for algorithms outside the context of coloring.","","978-1-7281-9998-6","10.1109/SC41405.2020.00103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355236","","Heuristic algorithms;High performance computing;Graphics processing units;Color;Parallel processing;Graph theory;Optimization","graph colouring;mathematics computing;parallel processing","high-performance parallel graph coloring;vertex degeneracy order;graph theory;polylogarithmic depth;real-world graphs;superior coloring quality;degeneracy ordering relaxation;color vertices;parallel graph coloring heuristics","",5.0,"",152.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"GraphPi: High Performance Graph Pattern Matching through Effective Redundancy Elimination","T. Shi; M. Zhai; Y. Xu; J. Zhai","Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,14,"Graph pattern matching, which aims to discover structural patterns in graphs, is considered one of the most fundamental graph mining problems in many real applications. Despite previous efforts, existing systems face two main challenges. First, inherent symmetry existing in patterns can introduce a large amount of redundant computation. Second, different matching orders for a pattern have significant performance differences and are quite hard to predict. When these factors are mixed, this problem becomes extremely complicated. High efficient pattern matching remains an open problem currently. To address these challenges, we propose GraphPi, a high performance distributed pattern matching system. GraphPi utilizes a new algorithm based on 2-cycles in group theory to generate multiple sets of asymmetric restrictions, where each set can eliminate redundant computation completely. We further design an accurate performance model to determine the optimal matching order and asymmetric restriction set for efficient pattern matching. We evaluate GraphPi on Tianhe-2A supercomputer. Results show that GraphPi outperforms the state-of-the-art system, by up to $ 105\times$ for 6 real-world graph datasets on a single node. We also scale GraphPi to 1,024 computing nodes (24,576 cores).","","978-1-7281-9998-6","10.1109/SC41405.2020.00104","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355241","Graph mining;pattern matching;automorphisms elimination","Computational modeling;High performance computing;Redundancy;Prediction algorithms;Supercomputers;Optimal matching;Pattern matching","data mining;distributed memory systems;graph theory;matrix multiplication;optimisation;parallel machines;pattern matching","GraphPi;high performance graph pattern matching;redundancy elimination;structural patterns;graph mining problems;open problem;high performance distributed pattern matching system;optimal matching order;asymmetric restriction set;graph datasets;computing nodes;Tianhe-2A supercomputer","",14.0,"",43.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Rocket: Efficient and Scalable All-Pairs Computations on Heterogeneous Platforms","S. Heldens; P. Hijma; B. v. Werkhoven; J. Maassen; H. Bal; R. v. Nieuwpoort","University of Amsterdam; vrije Universiteit Amsterdam; Netherlands eScience Center; Netherlands eScience Center; vrije Universiteit Amsterdam; University of Amsterdam","SC20: International Conference for High Performance Computing, Networking, Storage and Analysis","22 Feb 2021",2020,"","",1,12,"All-pairs compute problems apply a user-defined function to each combination of two items of a given data set. Although these problems present an abundance of parallelism, data reuse must be exploited to achieve good performance. Several researchers considered this problem, either resorting to partial replication with static work distribution or dynamic scheduling with full replication. In contrast, we present a solution that relies on hierarchical multi-level software-based caches to maximize data reuse at each level in the distributed memory hierarchy, combined with a divide-and-conquer approach to exploit data locality, hierarchical work-stealing to dynamically balance the workload, and asynchronous processing to maximize resource utilization. We evaluate our solution using three real-world applications, from digital forensics, localization microscopy, and bioinformatics, on different platforms, from desktop machine to a supercomputer. Results shows excellent efficiency and scalability when scaling to 96 GPUs, even obtaining super-linear speedups due to a distributed cache.","","978-1-7281-9998-6","10.1109/SC41405.2020.00105","Netherlands eScience Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355286","all-pairs computation;heterogeneous computing;GPU;work-stealing;data reuse;distributed cache","Rockets;Scalability;Microscopy;Distributed databases;Parallel processing;Supercomputers;Resource management","cache storage;data handling;distributed memory systems;divide and conquer methods;graphics processing units;multiprocessing systems;parallel machines;parallel processing;resource allocation;shared memory systems","partial replication;static work distribution;dynamic scheduling;hierarchical multilevel software-based caches;data reuse;distributed memory hierarchy;divide-and-conquer approach;data locality;hierarchical work-stealing;localization microscopy;excellent efficiency;distributed cache;rocket;all-pairs computations;heterogeneous platforms;all-pairs compute problems;user-defined function;data set;GPU","","","",27.0,"IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"A Tale of Two C’s: Convergence and Composability","İ. Altintaş","San Diego Supercomputer Center","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1,1,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Cyberinfrastructure is everywhere in diverse forms in service of applications in science, business and society. From IoT to extreme-scale computing data and computing have never been so distributed with the potential for real-time integration into these applications. The common theme to these applications, mostly composed of (big) data-integrated workloads, is their need to run in specialized environments for reasons such as on-demand or 24x7 nature of the tasks they are performing, and difficulties regarding their portability, latency, privacy, and performance optimization. Moreover, in many data-driven scientific applications, there is a need for heterogeneous integration of tasks requiring specialized computing capabilities with traditional high-throughput computing or high-performance computing tasks. Although some key middleware technologies enabled demonstration of standalone heterogeneous applications, such integration requires expertise convergence from a large group of people in very specialized settings. There are still many challenges for streamlined, scalable, repeatable, responsible, and explainable integration of data-integrated applications. Key opportunities for further innovations include intelligent systems and automated workflow management software that can compose and steer dynamic applications that can adapt to changing conditions in a data-driven fashion while integrating many tools to explore, analyze and utilize data. This talk will discuss some examples for data-integrated applications, describe emerging systems that enabled these applications, and overview our recent research to enable composable applications including a convergence application development methodology, intelligent middleware, and workflow composition.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458990","","Data science;Task analysis;Convergence;Supercomputers;Middleware;Distributed processing;Workflow management software","knowledge based systems;middleware;workflow management software","extreme-scale computing data;real-time integration;data-integrated workloads;specialized environments;performance optimization;data-driven scientific applications;heterogeneous integration;specialized computing capabilities;traditional high-throughput computing;high-performance computing tasks;standalone heterogeneous applications;streamlined integration;scalable integration;repeatable integration;explainable integration;data-integrated applications;dynamic applications;data-driven fashion;composable applications;convergence application development methodology;responsible integration;cyberinfrastructure;intelligent systems;automated workflow management software;intelligent middleware;workflow composition","","","","","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"A Tale of Two C’s: Convergence and Composability","İ. Altintaş","San Diego Supercomputer Center","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1,1,"Cyberinfrastructure is everywhere in diverse forms in service of applications in science, business and society. From IoT to extreme-scale computing data and computing have never been so distributed with the potential for real-time integration into these applications. The common theme to these applications, mostly composed of (big) data-integrated workloads, is their need to run in specialized environments for reasons such as on-demand or 24x7 nature of the tasks they are performing, and difficulties regarding their portability, latency, privacy, and performance optimization. Moreover, in many data-driven scientific applications, there is a need for heterogeneous integration of tasks requiring specialized computing capabilities with traditional high-throughput computing or high-performance computing tasks. Although some key middleware technologies enabled demonstration of standalone heterogeneous applications, such integration requires expertise convergence from a large group of people in very specialized settings. There are still many challenges for streamlined, scalable, repeatable, responsible, and explainable integration of data-integrated applications. Key opportunities for further innovations include intelligent systems and automated workflow management software that can compose and steer dynamic applications that can adapt to changing conditions in a data-driven fashion while integrating many tools to explore, analyze and utilize data. This talk will discuss some examples for data-integrated applications, describe emerging systems that enabled these applications, and overview our recent research to enable composable applications including a convergence application development methodology, intelligent middleware, and workflow composition.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460393","","","","","","","","","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Correlation-wise Smoothing: Lightweight Knowledge Extraction for HPC Monitoring Data","A. Netti; D. Tafani; M. Ott; M. Schulz","Technical University of Munich, Garching bei München, Germany; Fujitsu Enabling Software Technology GmbH, München, Germany; Leibniz Supercomputing Centre, Garching bei München, Germany; Technical University of Munich, Garching bei München, Germany","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",2,12,"Modern High-Performance Computing (HPC) and data center operators rely more and more on data analytics techniques to improve the efficiency and reliability of their operations. They employ models that ingest time-series monitoring sensor data and transform it into actionable knowledge for system tuning: a process known as Operational Data Analytics (ODA). However, monitoring data has a high dimensionality, is hardware-dependent and difficult to interpret. This, coupled with the strict requirements of ODA, makes most traditional data mining methods impractical and in turn renders this type of data cumbersome to process. Most current ODA solutions use ad-hoc processing methods that are not generic, are sensible to the sensors' features and are not fit for visualization. In this paper we propose a novel method, called Correlation-wise Smoothing (CS), to extract descriptive signatures from time-series monitoring data in a generic and lightweight way. Our CS method exploits correlations between data dimensions to form groups and produces image-like signatures that can be easily manipulated, visualized and compared. We evaluate the CS method on HPC-ODA, a collection of datasets that we release with this work, and show that it leads to the same performance as most state-of-the-art methods while producing signatures that are up to ten times smaller and up to ten times faster, while gaining visualizability, portability across systems and clear scaling properties.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460454","High-performance computing;Monitoring;Operational data analytics;Time-series analysis;Compression","Data centers;Analytical models;Smoothing methods;Data analysis;Data visualization;Distributed databases;Production","data analysis;data mining;data visualisation;digital signatures;knowledge acquisition;parallel processing;time series","CS method;HPC-ODA;lightweight knowledge extraction;HPC monitoring data;data center operators;data analytics;reliability;operational data analytics;data mining;correlation-wise smoothing;time-series monitoring sensor data;adhoc processing;descriptive signature extraction;image-like signatures;data visualization","",2.0,"",34.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Dancing in the Dark: Profiling for Tiered Memory","J. Choi; S. Blagodurov; H. -W. Tseng","University of California, Riverside; Advanced Micro Devices, Inc.; University of California, Riverside","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",13,22,"With the DDR standard facing density challenges and the emergence of the non-volatile memory technologies such as Cross-Point, phase change, and fast FLASH media, compute and memory vendors are contending with a paradigm shift in the datacenter space. The decades-long status quo of designing servers with DRAM technology as an exclusive memory solution is likely coming to an end. Future systems will increasingly employ tiered memory architectures (TMAs) in which multiple memory technologies work together to satisfy applications' ever-growing demands for more memory, less latency, and greater bandwidth. Exactly how to expose each memory type to software is an open question. Recent systems have focused on hardware caching to leverage faster DRAM memory while exposing slower non-volatile memory to OS-addressable space. The hardware approach that deals with the non-uniformity of TMA, however, requires complex changes to the processor and cannot use fast memory to increase the system's overall memory capacity. Mapping an entire TMA as OS-visible memory alleviates the challenges of the hardware approach but pushes the burden of managing data placement in the TMA to the software layers. The software, however, does not see the memory accesses by default; in order to make informed memory-scheduling decisions, software must rely on hardware methods to gain visibility into the load/store address stream. The OS then uses this information to place data in the most suitable memory location. In this paper, we evaluate different methods of memory-access collection and propose a hybrid tiered-memory approach that offers comprehensive visibility into TMA.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460471","","Nonvolatile memory;Memory management;Memory architecture;Random access memory;Trademarks;Media;Software","cache storage;DRAM chips;memory architecture;random-access storage;storage management","DDR standard;nonvolatile memory technologies;Cross-Point;phase change;FLASH media;paradigm shift;datacenter space;DRAM technology;exclusive memory solution;tiered memory architectures;multiple memory technologies;memory type;hardware caching;hybrid tiered-memory approach;memory-access collection;hardware methods;informed memory-scheduling decisions;memory accesses;software layers;OS-visible memory;TMA;nonuniformity;hardware approach;OS-addressable space;DRAM memory","",1.0,"",59.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Noise-Resilient Empirical Performance Modeling with Deep Neural Networks","M. Ritter; A. Geiß; J. Wehrstein; A. Calotoiu; T. Reimann; T. Hoefler; F. Wolf","Department of Computer Science, Technical University of Darmstadt, Germany; Department of Computer Science, Technical University of Darmstadt, Germany; Department of Computer Science, Technical University of Darmstadt, Germany; Department of Computer Science, ETH Zürich, Switzerland; Department of Computer Science, Technical University of Darmstadt, Germany; Department of Computer Science, ETH Zürich, Switzerland; Department of Computer Science, Technical University of Darmstadt, Germany","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",23,34,"Empirical performance modeling is a proven instrument to analyze the scaling behavior of HPC applications. Using a set of smaller-scale experiments, it can provide important insights into application behavior at larger scales. Extra-P is an empirical modeling tool that applies linear regression to automatically generate human-readable performance models. Similar to other regression-based modeling techniques, the accuracy of the models created by Extra-P decreases as the amount of noise in the underlying data increases. This is why the performance variability observed in many contemporary systems can become a serious challenge. In this paper, we introduce a novel adaptive modeling approach that makes Extra-P more noise resilient, exploiting the ability of deep neural networks to discover the effects of numerical parameters, such as the number of processes or the problem size, on performance when dealing with noisy measurements. Using synthetic analysis and data from three different case studies, we demonstrate that our solution improves the model accuracy at high noise levels by up to 25% while increasing their predictive power by about 15%.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00012","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460451","Performance analysis;performance modeling;deep learning;artificial neural networks;high performance computing;parallel processing","Adaptation models;Analytical models;Computational modeling;Neural networks;Linear regression;Predictive models;Data models","deep learning (artificial intelligence);parallel processing;regression analysis","regression-based modeling techniques;Extra-P;underlying data increases;performance variability;noise resilient;deep neural networks;model accuracy;high noise levels;noise-resilient empirical performance modeling;proven instrument;scaling behavior;HPC applications;smaller-scale experiments;application behavior;empirical modeling tool;linear regression;human-readable performance models","",1.0,"",38.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"SYMBIOSYS: A Methodology for Performance Analysis of Composable HPC Data Services","S. Ramesh; A. D. Malony; P. Carns; R. B. Ross; M. Dorier; J. Soumagne; S. Snyder","University of Oregon; University of Oregon; Argonne National Laboratory; Argonne National Laboratory; Argonne National Laboratory; The HDF Group; Argonne National Laboratory","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",35,45,"Microservices are a powerful new way of building, customizing, and deploying distributed services owing to their flexibility and maintainability. Several large-scale distributed platforms have emerged to serve the growing needs of data-centric workloads and services in commercial computing. Concurrently, high-performance computing (HPC) systems and software are rapidly evolving to meet the demands of diversified applications and heterogeneity. The interplay of hardware factors, software configuration parameters, and the flexibility offered with a microservice architecture makes it nontrivial to estimate the optimal service instantiation for a given application workload. Further, this problem is exacerbated when considering that these services operate in a dynamic and heterogeneous HPC environment. An optimally integrated service can be vastly more performant than a haphazardly integrated one. Existing performance tools for HPC either fail to understand the request-response model of communication inherent to microservices or they operate within a narrow scope, limiting the insight that can be gleaned from employing them in isolation.We propose a methodology for integrated performance analysis of HPC microservices frameworks and applications called SYMBIOSYS. We describe its design and implementation within the context of the Mochi framework. This integration is achieved by combining distributed callpath profiling and tracing with a performance data exchange strategy that collects fine-grained, low-level metrics from the RPC communication library and network layers. The result is a portable, low-overhead performance analysis setup that provides a holistic profile of the dependencies among microservices and how they interact with the Mochi RPC software stack. Using HEPnOS, a production-quality Mochi data service, we demonstrate the low-overhead operation of SYMBIOSYS at scale and use it to identify the root causes of poorly performing service configurations.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00013","Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460467","microservices;storage;performance;tools","Measurement;Distributed processing;Limiting;Distributed databases;Computer architecture;Tools;Software","electronic data interchange;parallel processing;software architecture;software performance evaluation","low-overhead performance analysis setup;Mochi RPC software stack;production-quality Mochi data service;low-overhead operation;SYMBIOSYS;service configurations;composable HPC data services;distributed services;large-scale distributed platforms;data-centric workloads;commercial computing;hardware factors;software configuration parameters;microservice architecture;optimal service instantiation;application workload;dynamic HPC environment;heterogeneous HPC environment;optimally integrated service;performance tools;integrated performance analysis;HPC microservices frameworks;performance data exchange strategy;network layers;HEPnOS;high-performance computing systems;distributed callpath profiling","",1.0,"",30.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Accelerating Distributed-Memory Autotuning via Statistical Analysis of Execution Paths","E. Hutter; E. Solomonik","Department of Computer Science, University of Illinois at Urbana-Champaign; Department of Computer Science, University of Illinois at Urbana-Champaign","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",46,57,"The prohibitive expense of automatic performance tuning at scale has largely limited the use of autotuning to libraries for shared-memory and GPU architectures. We introduce a framework for approximate autotuning that achieves a desired confidence in each algorithm configuration's performance by constructing confidence intervals to describe the performance of individual kernels (subroutines of benchmarked programs). Once a kernel's performance is deemed sufficiently predictable for a set of inputs, subsequent invocations are avoided and replaced with a predictive model of the execution time. We then leverage online execution path analysis to coordinate selective kernel execution and propagate each kernel's statistical profile. This strategy is effective in the presence of frequently-recurring computation and communication kernels, which is characteristic to algorithms in numerical linear algebra. We encapsulate this framework as part of a new profiling tool, Critter, that automates kernel execution decisions and propagates statistical profiles along critical paths of execution. We evaluate performance prediction accuracy obtained by our selective execution methods using state-of-the-art distributed-memory implementations of Cholesky and QR factorization on Stampede2, and demonstrate speed-ups of up to 7.1x with 98% prediction accuracy.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00014","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460475","autotuning;performance profiling;dense linear algebra","Schedules;Tensors;Statistical analysis;Linear algebra;Tools;Prediction algorithms;Libraries","distributed memory systems;graphics processing units;matrix decomposition;shared memory systems;statistical analysis","distributed-memory autotuning;statistical analysis;automatic performance tuning;shared-memory;GPU architectures;approximate autotuning;algorithm configuration;confidence intervals;individual kernels;benchmarked programs;predictive model;execution time;selective kernel execution;communication kernels;numerical linear algebra;profiling tool;kernel execution decisions;statistical profiles;performance prediction accuracy;online execution path analysis;computation kernels;Critter;Cholesky;QR factorization;Stampede2","","","",50.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Optimizing Memory-Compute Colocation for Irregular Applications on a Migratory Thread Architecture","T. B. Rolinger; C. D. Krieger; A. Sussman","Laboratory for Physical Sciences, College Park, MD, USA; Laboratory for Physical Sciences, College Park, MD, USA; University of Maryland, College Park, MD, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",58,67,"The movement of data between memory and processors has become a performance bottleneck for many applications. This is made worse for applications with sparse and irregular memory accesses, as they exhibit weak locality and make poor utilization of cache. As a result, colocating memory and compute is crucial for achieving high performance on irregular applications. There are two paradigms for memory-compute colocation. The first is the conventional approach of moving the data to the compute. The second paradigm is to move the compute to the data, which is less conventional and not as well understood. An example are migratory threads, which physically relocate upon remote accesses to the compute resource that hosts the data. In this paper, we explore the paradigm of moving compute to the data by optimizing memory-compute colocation for irregular applications on a migratory thread architecture. Our optimization method includes both initial data placement as well as data replication. We evaluate our optimization on sparse matrix-vector multiply (SpMV) and sparse matrix-matrix multiply (SpGEMM). Our results show that we can achieve speed-ups as high as 4.2x on SpMV and 6x on SpGEMM when compared to the default data layout. We also highlight that our optimization to improve memory-compute colocation can be applicable to both migratory threads and more conventional systems. To this end, we evaluate our optimization approach on a conventional compute cluster using the Chapel programming language. We demonstrate speed-ups as high as 18x for SpMV.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460497","colocation;optimizations;migratory threads;irregular applications","Distributed processing;Instruction sets;Memory management;Layout;Optimization methods;Load management;Hardware","matrix multiplication;message passing;multiprocessing systems;multi-threading;optimisation;parallel algorithms;parallel programming;software libraries;sparse matrices;vectors","conventional compute cluster;compute resource;migratory threads;colocating memory;irregular memory accesses;sparse memory accesses;processors;migratory thread architecture;irregular applications;memory-compute colocation","",2.0,"",19.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"TileSpMV: A Tiled Algorithm for Sparse Matrix-Vector Multiplication on GPUs","Y. Niu; Z. Lu; M. Dong; Z. Jin; W. Liu; G. Tan","Super Scientific Software Laboratory, China University of Petroleum-Beijing, China; Super Scientific Software Laboratory, China University of Petroleum-Beijing, China; Super Scientific Software Laboratory, China University of Petroleum-Beijing, China; Super Scientific Software Laboratory, China University of Petroleum-Beijing, China; Super Scientific Software Laboratory, China University of Petroleum-Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",68,78,"With the extensive use of GPUs in modern supercomputers, accelerating sparse matrix-vector multiplication (SpMV) on GPUs received much attention in the last couple of decades. A number of techniques, such as increasing utilization of wide vector units, reducing load imbalance and selecting the best formats, have been developed. However, the 2D spatial sparsity structure has not been well exploited in the existing work for SpMV on GPUs. In this paper, we propose an efficient tiled algorithm called TileSpMV for optimizing SpMV on GPUs through exploiting 2D spatial structure of sparse matrices. We first implement seven warp-level SpMV methods for calculating sparse tiles stored in a variety of formats, and then design a selection method to find the best format and SpMV implementation for each tile. We also adaptively extract nonzeros in the very sparse tiles into a separate matrix to maximize the overall performance. The experimental results show that our method is faster than state-of-the-art SpMV methods such as Merge-SpMV, CSR5 and BSR in most matrices of the full SuiteSparse Matrix Collection and delivers up to 2.61x, 3.96x and 426.59x speedups, respectively.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00016","Science Challenge Project; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460505","sparse matrix-vector multiplication;tiling;GPU","Distributed processing;Design methodology;Supercomputers;Sparse matrices;Testing","graphics processing units;mathematics computing;matrix multiplication;sparse matrices;vectors","Merge-SpMV;SuiteSparse matrix collection;TileSpMV;sparse matrix-vector multiplication;GPUs;wide vector units;2D spatial sparsity structure;seven warp-level SpMV methods;sparse tiles;tiled algorithm;load imbalance","",8.0,"",66.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Leveraging PaRSEC Runtime Support to Tackle Challenging 3D Data-Sparse Matrix Problems","Q. Cao; Y. Pei; K. Akbudak; G. Bosilca; H. Ltaief; D. Keyes; J. Dongarra","Innovative Computing Laboratory, University of Tennessee, US; Innovative Computing Laboratory, University of Tennessee, US; ASELSAN Research Center, Turkey; Innovative Computing Laboratory, University of Tennessee, US; Extreme Computing Research Center, Division of Computer, Electrical, and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, KSA; Extreme Computing Research Center, Division of Computer, Electrical, and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, KSA; University of Manchester, UK","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",79,89,"The task-based programming model associated with dynamic runtime systems has gained popularity for challenging problems because of workload imbalance, heterogeneous resources, or extreme concurrency. During the last decade, low-rank matrix approximations-where the main idea consists of exploiting data sparsity, typically by compressing off-diagonal tiles up to an application-specific accuracy threshold-have been adopted to address the curse of dimensionality at extreme scale. In this paper, we create a bridge between the runtime and the linear algebra by communicating knowledge of the data sparsity to the runtime. We design and implement this synergistic approach with high user productivity in mind, in the context of the PaRSEC runtime system and the HiCMA numerical library. This requires extending PaRSEC with new features to integrate rank information into the dataflow so that proper decisions can be made at runtime. We focus on the tile low-rank (TLR) Cholesky factorization for solving 3D data-sparse covariance matrix problems arising in environmental applications. In particular, we employ the 3D exponential model of the Mateŕn matrix kernel, which exhibits challenging nonuniform high ranks in off-diagonal tiles. We first provide dynamic data structure management driven by a performance model to reduce extra floating-point operations. Next, we optimize the memory footprint of the application by relying on a dynamic memory allocator, and supported by a rank-aware data distribution to cope with the workload imbalance. Finally, we expose further parallelism using kernel recursive formulations to shorten the critical path. Our resulting high-performance implementation outperforms existing data-sparse TLR Cholesky factorization by up to 7-fold on a large-scale distributed-memory system, while minimizing the memory footprint up to a 44-fold factor. This multidisciplinary work highlights the need to empower runtime systems beyond their original duty of task scheduling for servicing next-generation low-rank matrix algebra libraries.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460493","Low-rank matrix computations;Task-based programming model;Dynamic runtime system;Asynchronous executions and load balancing;High-performance computing;User productivity;Environmental applications","Solid modeling;Runtime;Three-dimensional displays;Programming;Dynamic scheduling;Matrices;Libraries","approximation theory;covariance matrices;data handling;data structures;distributed memory systems;linear algebra;parallel programming;scheduling;sparse matrices","dynamic runtime systems;workload imbalance;heterogeneous resources;extreme concurrency;low-rank matrix approximations;data sparsity;application-specific accuracy threshold;linear algebra;high user productivity;HiCMA numerical library;tile low-rank Cholesky factorization;data-sparse covariance matrix problems;3D exponential model;Mateŕn matrix kernel;dynamic data structure management;memory footprint;dynamic memory allocator;rank-aware data distribution;high-performance implementation;data-sparse TLR Cholesky factorization;large-scale distributed-memory system;next-generation low-rank matrix algebra libraries;PaRSEC runtime support;task-based programming model;3D data-sparse matrix problems","",4.0,"",33.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Communication-Avoiding and Memory-Constrained Sparse Matrix-Matrix Multiplication at Extreme Scale","M. T. Hussain; O. Selvitopi; A. Buluç; A. Azad","Indiana University, Bloomington, IN; Lawrence Berkeley National Laboratory, Berkeley, CA; Lawrence Berkeley National Laboratory, Berkeley, CA; Indiana University, Bloomington, IN","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",90,100,"We present a distributed-memory algorithm for sparse matrix-matrix multiplication (SpGEMM) of extremely large matrices where the generated output is larger than the aggregated memory of a target supercomputer. We address this challenge by splitting the computation into batches with each batch generating a set of output columns. We developed a distributed symbolic step to understand the memory requirement and determine the number of batches beforehand. We integrated the multiplication in each batch with an existing communication avoiding techniques to reduce the communication overhead while multiplying matrices in a 3-D process grid. Furthermore, we made the in-node computations faster by designing a sort-free SpGEMM and merging algorithm. Incorporating all the proposed approaches, our SpGEMM scales for large protein-similarity networks using 262,144 cores on a Cray XC40 supercomputer while achieving a 10x speedup using 16x more nodes. Our code is available as part of the Combinatorial BLAS library (https://github.com/PASSIONLab/CombBLAS).","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00018","Advanced Scientific Computing Research; Office of Science; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460450","","Proteins;Three-dimensional displays;Social networking (online);Scientific computing;Memory management;Genomics;Parallel processing","graph theory;mathematics computing;matrix algebra;matrix multiplication;multiprocessing systems;parallel machines;proteins;resource allocation;sparse matrices","communication-avoiding;memory-constrained sparse matrix-matrix multiplication;extreme scale;distributed-memory algorithm;generated output;aggregated memory;target supercomputer;output columns;distributed symbolic step;memory requirement;batches beforehand;existing communication;communication overhead;sort-free SpGEMM;SpGEMM scales","",5.0,"",34.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Characterizing Small-Scale Matrix Multiplications on ARMv8-based Many-Core Architectures","W. Yang; J. Fang; D. Dong","College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",101,110,"General Matrix Multiplication (GEMM) is a key subroutine in high-performance computing. There is a large body of work on evaluating and optimizing large-scale matrix multiplication, but how well the small-scale matrix multiplication (SMM) performs is largely unknown, especially for the ARMv8-based many-core architectures. In this work, we evaluate and characterize the performance of SMM subroutines on Phytium 2000 +, an ARMv8-based 64-core architecture. The evaluation work is extensively performed with the mainstream open-source libraries including OpenBLAS, BLIS, BALSFEO, and Eigen. Given various experimental settings, we observe how well the small-scale GEMM routines perform on Phytium 2000 +, and then discuss the impacting factors behind the performance behaviours of SMM. Built on such a basis, we shed light on the performance bottlenecks and practical optimizations on SMM from various angles: (1) mitigating the data packing overhead, (2) processing the edge cases properly, (3) selecting a suitable micro-kernel, and (4) adopting a right parallelization method. The result of our work facilitates users to develop efficient SMM optimizations on ARMv8-based many-core architectures, and embed them into real-world applications.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00019","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460472","Phytium 2000 +;Small-scale GEMM;Performance evaluation and optimization","Deep learning;Distributed processing;Algorithms;Instruction sets;Computer architecture;Benchmark testing;Libraries","belief networks;mathematics computing;matrix multiplication;microprocessor chips;multiprocessing systems;parallel architectures;performance evaluation","ARMv8-based many-core architectures;SMM subroutines;Phytium 2000;ARMv8-based 64-core architecture;evaluation work;small-scale GEMM routines;performance bottlenecks;small-scale matrix multiplications;General Matrix Multiplication;high-performance computing;large-scale matrix multiplication;small-scale matrix multiplication performs","",2.0,"",36.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"DAG-based Scheduling with Resource Sharing for Multi-task Applications in a Polyglot GPU Runtime","A. Parravicini; A. Delamare; M. Arnaboldi; M. D. Santambrogio","Politecnico di Milano, Milan, Italy; Oracle Labs, Zürich, Switzerland; Oracle Labs, Zürich, Switzerland; Politecnico di Milano, Milan, Italy","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",111,120,"GPUs are readily available in cloud computing and personal devices, but their use for data processing acceleration has been slowed down by their limited integration with common programming languages such as Python or Java. Moreover, using GPUs to their full capabilities requires expert knowledge of asynchronous programming. In this work, we present a novel GPU run time scheduler for multi-task GPU computations that transparently provides asynchronous execution, space-sharing, and transfer-computation overlap without requiring in advance any information about the program dependency structure. We leverage the GrCUDA polyglot API to integrate our scheduler with multiple high-level languages and provide a platform for fast prototyping and easy GPU acceleration. We validate our work on 6 benchmarks created to evaluate task-parallelism and show an average of 44% speedup against synchronous execution, with no execution time slowdown compared to hand-optimized host code written using the C++ CUDA Graphs API.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00020","Politecnico di Milano; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460491","GPU;Scheduling;Software Runtime;Hardware Acceleration","Java;Distributed processing;Cloud computing;Runtime;Graphics processing units;Programming;Data processing","application program interfaces;cloud computing;graph theory;graphics processing units;Java;parallel architectures;processor scheduling;program compilers","DAG-based scheduling;resource sharing;multitask applications;polyglot GPU runtime;GPUs;cloud computing;personal devices;data processing acceleration;common programming languages;expert knowledge;asynchronous programming;multitask GPU computations;asynchronous execution;space-sharing;program dependency structure;GrCUDA polyglot API;high-level languages;task-parallelism;synchronous execution;execution time slowdown","",2.0,"",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"CTXBack: Enabling Low Latency GPU Context Switching via Context Flashback","Z. Ji; C. -L. Wang","Department of Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",121,130,"Efficient GPU preemption mechanisms are critical for task prioritization in multitasking environments, especially for latency-sensitive GPU applications. However, due to the large context of GPU kernels, simply borrowing context switching mechanisms from CPU space incurs substantial latency and overhead. To address this problem, we propose CTXBack, which allows a thread block to execute context switching at a preceding instruction with a smaller context. It enables low latency GPU context switching for latency-sensitive applications on shared GPUs. Three complementary ways are proposed to make more preceding instructions into valid points to execute context switching, giving CTXBack a higher chance to find instructions with smaller contexts. Evaluations show that CTXBack reduces the context by 61.0%, which is only 1.09× of the minimum possible context size. With only 0.41% runtime overhead, the preemption latency and resuming time are reduced by 63.1% and 50.0% on average compared to the traditional approach.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00021","Impact Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460478","GPU;GPU sharing;GPU context switching","Context;Distributed processing;Runtime;Graphics processing units;Switches;Multitasking;System-on-chip","graphics processing units;microprocessor chips;multiprogramming","CPU space;low latency GPU context switching;context switching mechanisms;GPU kernels;latency-sensitive GPU applications;GPU preemption mechanisms;context flashback;CTXBack","",2.0,"",30.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Transparent I/O-Aware GPU Virtualization for Efficient Resource Consolidation","N. M. Gonzalez; T. Elengikal","IBM Thomas J. Watson Research Center, New York, United States; IBM Thomas J. Watson Research Center, New York, United States","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",131,140,"Graphics processing units (GPUs) are widely used in high performance computing (HPC) and cloud computing to accelerate workloads. Virtualization provides flexible access to resources while improving utilization and throughput. This is essential to resource disaggregation, which allows ubiquitous access to remote resources among nodes. However, remote GPU virtualization at scale suffers from severe performance degradation due to inter-node communication and resource consolidation overhead, especially for data-intensive workloads.We propose HFGPU, a GPU virtualization solution transparent to application code based on application programming interface (API) remoting. We define a virtual device manager that allows remote GPUs to be seen, managed, and used as though they were local. To perform at scale we combine multi-adapter InfiniBand networking with a novel distributed I/O forwarding mechanism that eliminates consolidation bottlenecks and reduces data movement. Experiments with up to 1024 NVIDIA V100 GPUs demonstrate overhead lower than 1% for data-intensive operations.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00022","Lawrence Livermore National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460488","GPU;virtualization;consolidation;disaggregation;cloud;HPC;HFGPU;OmniGPU;HFCUDA","Performance evaluation;Degradation;Distributed processing;Cloud computing;High performance computing;Graphics processing units;Distributed databases","application program interfaces;cloud computing;graphics processing units;parallel architectures;parallel processing;virtualisation","ubiquitous access;flexible access;cloud;HPC;graphics processing units;resource consolidation;data-intensive operations;1024 NVIDIA V100 GPUs;multiadapter InfiniBand networking;remote GPUs;virtual device manager;application programming interface remoting;application code;GPU virtualization solution;data-intensive workloads;resource consolidation overhead;inter-node communication;severe performance degradation;remote GPU virtualization;remote resources","",1.0,"",28.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Demystifying GPU UVM Cost with Deep Runtime and Workload Analysis","T. Allen; R. Ge","School of Computing, Clemson University, Clemson, SC, USA; School of Computing, Clemson University, Clemson, SC, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",141,150,"With GPUs becoming ubiquitous in HPC systems, NVIDIA's Unified Virtual Memory (UVM) is being adopted as a measure to simplify porting of complex codes to GPU platforms by allowing demand paging between host and device memory without programmer specification. Much like its storage-based counterparts, UVM provides a great deal of added usability at the cost of performance due to the abstraction and fault-handling mechanisms. This is preventing HPC systems from being used efficiently and effectively and decreases the overall value of GPU-based systems.To mitigate the cost of page fault stall time, NVIDIA has introduced a prefetching mechanism to their UVM system. This prefetcher infers data ahead-of-time based on prior page fault history, hoping to satisfy faults before they occur. Such a prefetcher must be cleverly designed and efficient, as it operates under the constraints of a realtime system for providing effective service. Additionally, the workload is quite complex due to the parallel nature of GPU faults, as well as page fault serialization and fault source erasure within the driver. The current prefetching mechanism uses a density-prefetching algorithm to offset the side-effects of receiving page faults in parallel. While this prefetching can be very effective, it also has a negative impact on the performance of GPU oversubscription.In this paper, we provide a deep analysis of the overhead caused by UVM and the primary sources of this overhead. Additionally, we analyze the impact of NVIDIA's prefetching and oversubscription in practice on different workloads, and correlate the performance to the driver implementation and prefetching mechanism. We provide design insights and improvement suggestions for hardware and middleware that would provide new avenues for performance gain.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00023","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460482","GPU;demand paging;UVM;performance","Performance evaluation;Distributed processing;Runtime;Prefetching;Graphics processing units;Performance gain;Hardware","cache storage;computer graphic equipment;coprocessors;graphics processing units;middleware;parallel processing;storage management","density-prefetching algorithm;current prefetching mechanism;fault source erasure;page fault serialization;GPU faults;effective service;realtime system;prior page fault history;data ahead-of-time;prefetcher;UVM system;page fault stall time;GPU-based systems;fault-handling mechanisms;usability;storage-based counterparts;programmer specification;device memory;GPU platforms;complex codes;NVIDIA's Unified Virtual Memory;HPC systems;workload analysis;deep runtime;page faults","",2.0,"",27.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"DUET: A Compiler-Runtime Subgraph Scheduling Approach for Tensor Programs on a Coupled CPU-GPU Architecture","M. Zhang; Z. Hu; M. Li","Microsoft; Beijing University; Beijing University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",151,161,"Deep neural networks (DNNs) are currently the foundation for many artificial intelligence tasks. Existing DL frameworks and compilers often focus on optimizing DL inference speed against CPUs and GPUs in isolation while missing the opportunities to reap the benefits of aggregated computation power from both CPU and GPU. We show that there are DNNs that exhibit complex computation patterns, and different components might be suitable for executing on different types of devices to maximize performance gains. Based on this observation, we present a DNN inference engine, called DUET, that explores potential concurrent execution opportunities on heterogeneous CPU-GPU architecture for DNN inference. In particular, we introduce (i) a coarse-grained partitioning strategy that divides a DNN computation graph into subgraphs that retain high computational granularity with relatively low communication volume, (ii) a compiler-aware profiling method to include DL compiler optimization into the loop to improve scheduling decisions, and (iii) a greedy-correction subgraph scheduling algorithm that automatically maps the DNN computation to CPU and GPU without input from model developers. We evaluate DUET against several DNNs that exhibit complex model structures and compare its performance against existing DL frameworks and the state-of-the-art DNN compiler. The experiment results show that DUET is much faster than existing DL frameworks and obtains 1.5-2.3 times and 1.3-6.4 times speed-ups against the optimized code by the state-of-the-art DNN compiler on GPU and CPU alone, respectively.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460468","DNNs;Efficient Inference;Compiler;Heterogeneous Execution","Solid modeling;Tensors;Scheduling algorithms;Computational modeling;Graphics processing units;Computer architecture;Performance gain","deep learning (artificial intelligence);graph theory;graphics processing units;processor scheduling;program compilers;tensors","compiler-runtime subgraph scheduling approach;tensor programs;coupled CPU-GPU architecture;deep neural networks;artificial intelligence tasks;optimizing DL inference speed;aggregated computation power;exhibit complex computation patterns;DNN inference engine;DUET;potential concurrent execution opportunities;heterogeneous CPU-GPU architecture;coarse-grained partitioning strategy;DNN computation graph;subgraphs;high computational granularity;relatively low communication volume;compiler-aware profiling method;DL compiler optimization;scheduling decisions;greedy-correction subgraph scheduling algorithm;exhibit complex model structures;optimized code;DNN compiler","",3.0,"",48.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"CAGC: A Content-aware Garbage Collection Scheme for Ultra-Low Latency Flash-based SSDs","S. Wu; C. Du; H. Li; H. Jiang; Z. Shen; B. Mao","School of Informatics at Xiamen University, Xiamen, Fujian, China; School of Informatics at Xiamen University, Xiamen, Fujian, China; School of Informatics at Xiamen University, Xiamen, Fujian, China; Department of Computer Science & Engineering, University of Texas-Arlington, Texas, USA; School of Informatics at Xiamen University, Xiamen, Fujian, China; School of Informatics at Xiamen University, Xiamen, Fujian, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",162,171,"With the advent of new flash-based memory technologies with ultra-low latency, directly applying inline data deduplication in flash-based storage devices can degrade the system performance since key deduplication operations lie on the shortened critical write path of such devices. To address the problem, we propose a Content-Aware Garbage Collection scheme (CAGC), which embeds the data deduplication into the data movement workflow of the Garbage Collection (GC) process in ultra-low latency flash-based SSDs. By parallelizing the operations of valid data pages migration, hash computing and flash block erase, the deduplication-induced performance overhead is alleviated and redundant page writes during the GC period are eliminated. To further reduce data writes and write amplification during GC, CAGC separates and stores data pages in different regions based on their reference counts. The performance evaluation of our CAGC prototype implemented in FlashSim shows that CAGC significantly reduces the number of flash blocks erased and data pages migrated during GC, leading to improved user I/O performance and reliability of ultra-low latency flash-based SSDs.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00025","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460458","Ultra-Low Latency Flash-based SSDs;Garbage Collection;Data Deduplication;Reference Count;Data Placement","Performance evaluation;Distributed processing;System performance;Prototypes;Reliability","flash memories;solid state drives;storage management","hash computing;deduplication-induced performance overhead;flash block erase;valid data pages migration;Garbage Collection process;flash-based storage devices;inline data deduplication;flash-based memory technologies;ultra-low latency flash-based SSDs;content-aware garbage collection scheme;CAGC","",1.0,"",42.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"NVMe-CR: A Scalable Ephemeral Storage Runtime for Checkpoint/Restart with NVMe-over-Fabrics","S. Gugnani; T. Li; X. Lu","The Ohio State University; The Ohio State University; University of California, Merced","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",172,181,"Emerging SSDs with NVMe-over-Fabrics (NVMf) support provide new opportunities to significantly improve the performance of IO-intensive HPC applications. However, state-of-the-art parallel filesystems can not extract the best possible performance from fast NVMe SSDs and are not designed for latency-critical ephemeral IO tasks, such as checkpoint/restart. In this paper, we propose a powerful abstraction called microfs to peel away unnecessary software layers and eliminate namespace coordination. Building upon this abstraction, we present the design of NVMe-CR, a scalable ephemeral storage runtime for clusters with disaggregated compute and storage. NVMe-CR proposes techniques like metadata provenance, log record coalescing, and logically isolated shared device access, built around the microfs abstraction, to reduce the overhead of writing millions of concurrent checkpoint files. NVMe-CR utilizes high-density allflash arrays accessible via NVMf to absorb bursty checkpoint IO and increase the progress rates of applications obliviously. Using the ECP CoMD application as a use case, results show that our runtime can achieve near perfect (> 0.96) efficiency at 448 processes and reduce checkpoint overhead by as much as 2x compared to state-of-the-art storage systems.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460479","Checkpoint/Restart;NVMe;NVMf;Exascale","Distributed processing;Runtime;Buildings;Writing;Metadata;Software;Task analysis","checkpointing;flash memories;input-output programs;meta data;parallel processing;storage management","scalable ephemeral storage runtime;NVMe-over-Fabrics support;NVMf;IO-intensive HPC applications;state-of-the-art parallel filesystems;possible performance;fast NVMe SSDs;latency-critical ephemeral IO tasks;powerful abstraction;unnecessary software layers;disaggregated compute;NVMe-CR proposes techniques;microfs abstraction;concurrent checkpoint files;NVMe-CR utilizes high-density allflash arrays;bursty checkpoint IO;ECP CoMD application;checkpoint overhead;state-of-the-art storage systems","","","",40.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Virtual-Link: A Scalable Multi-Producer Multi-Consumer Message Queue Architecture for Cross-Core Communication","Q. Wu; J. Beard; A. Ekanayake; A. Gerstlauer; L. K. John","The University of Texas at Austin; Arm Inc.; The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",182,191,"Cross-core communication is increasingly a bottleneck as the number of processing elements increase per system-on-chip. Typical hardware solutions to cross-core communication are often inflexible; while software solutions are flexible, they have performance scaling limitations. A key problem, as we will show, is that of shared state in software-based message queue mechanisms. This paper proposes Virtual-Link (VL), a novel light-weight communication mechanism with hardware support to facilitate M:N lock-free data movement. VL reduces the amount of coherent shared state, which is a bottleneck for many approaches, to zero. VL provides further latency benefit by keeping data on the fast path (i.e., within the onchip interconnect). VL enables directed cache-injection (stashing) between PEs on the coherence bus, reducing the latency for core-to-core communication. VL is particularly effective for fine-grain tasks on streaming data. Evaluation on a full system simulator with 7 benchmarks shows that VL achieves a 2.09x speedup over state-of-the-art software-based communication mechanisms, while reducing memory traffic by 61%.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460456","","Distributed processing;Computer architecture;Coherence;Benchmark testing;Software;Hardware;Synchronization","cache storage;multiprocessing systems;parallel architectures;queueing theory;system-on-chip","memory traffic reduction;directed cache-injection;lock-free data movement;system-on-chip;hardware solutions;processing elements;cross-core communication;scalable multiproducer multiconsumer message queue architecture;core-to-core communication;coherent shared state;light-weight communication mechanism;VL;Virtual-Link;software-based message queue mechanisms;software solutions","","","",58.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"High-Level Synthesis of Parallel Specifications Coupling Static and Dynamic Controllers","V. G. Castellana; A. Tumeo; F. Ferrandi","High Performance Computing, Pacific Northwest National Laboratory, Richland, Washington, USA; High Performance Computing, Pacific Northwest National Laboratory, Richland, Washington, USA; DEIB, Politecnico di Milano, Milano, Italy","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",192,202,"Conventional High-Level Synthesis (HLS) tools exploit parallelism mostly at the Instruction Level (ILP). They statically schedule the input specifications and build centralized Finite State Machine (FSM) controllers. However, aggressive exploitation of ILP in many applications has diminishing returns and, usually, centralized approaches do not efficiently exploit coarser parallelism, because FSMs are inherently serial. In this paper we present an HLS framework able to synthesize applications that, beside ILP, also expose Task Level Parallelism (TLP). An application can expose TLP through annotations that identify the parallel functions (i.e., tasks). To generate accelerators that efficiently execute concurrent tasks, we need to solve several issues: devise a mechanism to support concurrent execution flows, exploit memory parallelism, and manage synchronization. To support concurrent execution flows, we introduce a novel adaptive controller. The adaptive controller is composed of a set of interacting control elements that independently manage the execution of a task. These control elements check dependencies and resource constraints at runtime, enabling as soon as possible execution. To support parallel access to shared memories and synchronization, we integrate with a novel Hierarchical Memory Interface (HMI). With respect to previous solutions, the proposed interface supports multi-ported memories and atomic memory operations, which commonly occur in parallel programming. Our framework can generate the hardware implementation of C functions by employing two different approaches, depending on its characteristics. If a function exposes TLP, then the framework generates hardware implementations based on the adaptive controller. Otherwise, the framework implements the function through the FSM approach, which is optimized for ILP exploitation. We evaluate our framework on a set of parallel applications and show substantial performance improvements (average speedup of 4.7) with limited area overheads (average area increase of 5.48 times).","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00028","Pacific Northwest National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460500","High Level Synthesis;dataflow;dynamic scheduling;design automation","Annotations;Memory management;Parallel processing;Tools;Dynamic scheduling;Hardware;Finite element analysis","adaptive control;embedded systems;finite state machines;high level synthesis;multiprocessing systems;parallel processing;pipeline processing;reconfigurable architectures;shared memory systems","multiported memories;atomic memory operations;parallel programming;hardware implementation;C functions;FSM approach;ILP exploitation;parallel applications;novel Hierarchical Memory Interface;shared memories;parallel access;resource constraints;interacting control elements;novel adaptive controller;synchronization;memory parallelism;concurrent execution;concurrent tasks;parallel functions;TLP;Task Level Parallelism;HLS framework;coarser parallelism;centralized approaches;aggressive exploitation;Finite State Machine controllers;input specifications;Instruction Level;Conventional High-Level Synthesis tools;dynamic controllers;static controllers;parallel specifications","",5.0,"",20.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"RVMA: Remote Virtual Memory Access","R. E. Grant; M. J. Levenhagen; M. G. F. Dosanjh; P. M. Widener","Sandia National Laboratories, Center for Computational Research; Sandia National Laboratories, Center for Computational Research; Sandia National Laboratories, Center for Computational Research; Sandia National Laboratories, Center for Computational Research","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",203,212,"Remote Direct Memory Access (RDMA) capabilities have been provided by high-end networks for many years, but the network environments surrounding RDMA are evolving. RDMA performance has historically relied on using strict ordering guarantees to determine when data transfers complete, but modern adaptively-routed networks no longer provide those guarantees. RDMA also exposes low-level details about memory buffers: either all clients are required to coordinate access using a single shared buffer, or exclusive resources must be allocatable per-client for an unbounded amount of time. This makes RDMA unattractive for use in many-to-one communication models such as those found in public internet client-server situations. Remote Virtual Memory Access (RVMA) is a novel approach to data transfer which adapts and builds upon RDMA to provide better usability, resource management, and fault tolerance. RVMA provides a lightweight completion notification mechanism which addresses RDMA performance penalties imposed by adaptively-routed networks, enabling high-performance data transfer regardless of message ordering. RVMA also provides receiver-side resource management, abstracting away previously-exposed details from the sender-side and removing the RDMA requirement for exclusive/coordinated resources. RVMA requires only small hardware modifications from current designs, provides performance comparable or superior to traditional RDMA networks, and offers many new features. In this paper, we describe RVMA's receiver-managed resource approach and how it enables a variety of new data-transfer approaches on high-end networks. In particular, we demonstrate how an RVMA NIC could implement the first hardware-based fault tolerant RDMA-like solution. We present the design and validation of an RVMA simulation model in a popular simulation suite and use it to evaluate the advantages of RVMA at large scale. In addition to support for adaptive routing and easy programmability, RVMA can outperform RDMA on a 3D sweep application by 4.4X.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460494","RVMA;RDMA;Networks","Adaptation models;Fault tolerance;Three-dimensional displays;Fault tolerant systems;Memory management;Programming;Data transfer","fault tolerance;file organisation;Internet","receiver-managed resource approach;Remote Direct Memory Access capabilities;adaptive routing;RVMA simulation model;hardware-based fault tolerant RDMA-like solution;RVMA NIC;high-end networks;data-transfer approaches;high-performance data transfer;RDMA performance penalties;resource management;Virtual Memory Access;public internet client-server situations;RDMA unattractive;exclusive resources;memory buffers;modern adaptively-routed networks;network environments","","","",27.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Performance-Portable Graph Coarsening for Efficient Multilevel Graph Analysis","M. S. Gilbert; S. Acer; E. G. Boman; K. Madduri; S. Rajamanickam","Pennsylvania State University, University Park, USA; Sandia National Laboratories, Albuquerque, USA; Sandia National Laboratories, Albuquerque, USA; Pennsylvania State University, University Park, USA; Sandia National Laboratories, Albuquerque, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",213,222,"The multilevel heuristic is an effective strategy for speeding up graph analytics, and graph coarsening is an integral step of multilevel methods. We perform a comprehensive study of multilevel coarsening in this work. We primarily focus on the graphics processing unit (GPU) parallelization of the Heavy Edge Coarsening (HEC) method executed in an iterative setting. We present optimizations for the two phases of coarsening, a fine-to-coarse vertex mapping phase, and a coarse graph construction phase. We also express several other coarsening algorithms using the Kokkos framework and discuss their parallelization. We demonstrate the efficacy of parallelized HEC on an NVIDIA Turing GPU and a 32-core AMD Ryzen processor using multilevel spectral graph partitioning as the primary case study.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00030","National Science Foundation; National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460473","coarsening;multilevel;graph construction;GPU;partitioning","Distributed processing;Graphics processing units;Partitioning algorithms;Iterative methods;Optimization","graph theory;multiprocessing systems","performance-portable graph coarsening;multilevel spectral graph partitioning;parallelized HEC;coarse graph construction phase;fine-to-coarse vertex mapping phase;Heavy Edge Coarsening method;graphics processing unit parallelization;multilevel coarsening;multilevel methods;integral step;graph analytics;multilevel heuristic;efficient multilevel graph analysis","",2.0,"",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Efficient Distributed Algorithms in the k-machine model via PRAM Simulations","J. Augustine; K. Kothapalli; G. Pandurangan","Department of Computer Science & Engineering, Indian Institute of Technology, Madras, Chennai, India; Center for Security, Theory, and Algorithmic Research, International Institute of Information Technology, Hyderabad, Gachibowli, Hyderabad, India; Department of Computer Science, University of Houston, Houston, Texas, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",223,232,"We study several fundamental problems in the k-machine model, a message-passing model for large-scale distributed computations where $k\geq 2$ machines jointly perform computations on a large input of size N, (typically, $N\gg k$). The input is initially partitioned (randomly or in a balanced fashion) among the k machines, a common implementation in many real-world systems. Communication is point-to-point, and the goal is to minimize the number of communication rounds of the computation.Our main result is a general technique for designing efficient deterministic distributed algorithms in the k-machine model using PRAM algorithms. Our technique works by efficiently simulating PRAM algorithms in the k-machine model in a deterministic way. This simulation allows us to arrive at new algorithms in the k-machine model for some problems for which no efficient k-machine algorithms are known before and also improve on existing results in the k-machine model for some problems.While our simulation allows us to obtain k-machine algorithms for any problem with a known PRAM algorithm, we mainly focus on graph problems. For an input graph on n vertices and m edges, we obtain $\tilde{O}(m/k^{2})$ round 4 algorithms for various graph problems such as r-connectivity for $r=1,2,3,4$, minimum spanning tree (MST), maximal independent set (MIS), $(\Delta+1)$-coloring, maximal matching, ear decomposition, and spanners under the assumption that the edges of the input graph are partitioned (randomly, or in an arbitrary, but balanced, fashion) among the k machines. For problems such as connectivity and MST, the above bound is (essentially) the best possible (up to logarithmic factors). Our simulation technique allows us to obtain the first known efficient deterministic algorithms in the k-machine model for other problems with known deterministic PRAM algorithms.4$\tilde{O}$ notation hides a polylog $(.)$ factor and an additive polylog $(.)$ term.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460460","k-machine algorithms;PRAM algorihtms;simulation;deterministic algorithms","Distributed processing;Additives;Computational modeling;Ear;Phase change random access memory;Partitioning algorithms;Computational efficiency","computational complexity;deterministic algorithms;distributed algorithms;graph theory;message passing;parallel algorithms;parallel machines","distributed algorithms;k-machine model;graph problems;k-machine algorithms;deterministic PRAM algorithms;PRAM simulations;message passing;large-scale distributed computations","","","",56.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Euler Meets GPU: Practical Graph Algorithms with Theoretical Guarantees","A. Polak; A. Siwiec; M. Stobierski","Faculty of Mathematics and Computer Science, Jagiellonian University, Kraków, Poland; Faculty of Mathematics and Computer Science, Jagiellonian University, Kraków, Poland; Faculty of Mathematics and Computer Science, Jagiellonian University, Kraków, Poland","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",233,244,"The Euler tour technique is a classical tool for designing parallel graph algorithms, originally proposed for the PRAM model. We ask whether it can be adapted to run efficiently on GPU. We focus on two established applications of the technique: (1) the problem of finding lowest common ancestors (LCA) of pairs of nodes in trees, and (2) the problem of finding bridges in undirected graphs. In our experiments, we compare theoretically optimal algorithms using the Euler tour technique against simpler heuristics supposed to perform particularly well on typical instances. We show that the Euler tour-based algorithms not only fulfill their theoretical promises and outperform practical heuristics on hard instances, but also perform on par with them on easy instances.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460490","graph algorithms;parallel algorithms","Bridges;Distributed processing;Adaptation models;Electric breakdown;Heuristic algorithms;Graphics processing units;Tools","computational complexity;optimisation;parallel algorithms;tree searching;trees (mathematics)","Euler meets GPU;practical graph algorithms;Euler tour technique;parallel graph algorithms;PRAM model;undirected graphs;optimal algorithms;Euler tour-based algorithms","","","",66.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"MultiLogVC: Efficient Out-of-Core Graph Processing Framework for Flash Storage","K. K. Matam; H. Hashemi; M. Annavaram","Facebook Inc., Menlo Park, USA; Electrical Engineering Department, University of Southern California, Los Angeles, USA; Electrical Engineering Department, University of Southern California, Los Angeles, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",245,255,"Graph analytics are at the heart of a broad range of applications such as drug discovery, page ranking, transportation systems, and recommendation models. When graph size exceeds the available memory size in a computing node, out-of-core graph processing is needed. For the widely used out-of-core graph processing systems, the graphs are stored and accessed from a long latency SSD storage, which becomes a significant performance bottleneck. To tackle this long latency this work exploits the key insight that that nearly all graph algorithms have a dynamically varying number of active vertices that must be processed in each iteration. However, existing graph processing frameworks, such as GraphChi, load the entire graph in each iteration even if a small fraction of the graph is active. This limitation is due to the structure of the graph storage used by these systems. In this work, we propose to use a compressed sparse row (CSR) based graph storage that is more amenable for selectively loading only a few active vertices in each iteration. However, CSR based graph processing suffers from random update propagation to many target vertices. To solve this challenge, we propose to use a multi-log update mechanism that logs updates separately, rather than directly update the active edges and vertices in a graph. The multi-log system maintains a separate log per each vertex interval (a group of vertices). This separation enables efficient processing of all updates bound to each vertex interval by just loading the corresponding log. Further, by logging all the updates associated with a vertex interval in one contiguous log this approach reduces read amplification since all the pages in the log will be processed in the next iteration without wasted page reads. Over the current state of the art out-of-core graph processing framework, our evaluation results show that the MultiLogVC framework improves performance by up to 17.84×, 1.19×, 1.65×, 1.38×, 3.15×, and 6.00× for the widely used breadth-first search, pagerank, community detection, graph coloring, maximal independent set, and random-walk applications, respectively.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460461","out-of-core graph processing;graph analytics;SSD storage systems;log storage","Heart;Drugs;Distributed processing;Analytical models;Heuristic algorithms;Computational modeling;Loading","flash memories;graph colouring;iterative methods;random processes;sparse matrices","flash storage;graph analytics;page ranking;transportation systems;graph size;out-of-core graph processing systems;long latency SSD storage;graph algorithms;dynamically varying number;active vertices;compressed sparse row based graph storage;random update propagation;target vertices;multilog update mechanism;active edges;multilog system;separate log;vertex interval;contiguous log;art out-of-core graph processing framework;MultiLogVC framework;pagerank;graph coloring","","","",32.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Networks","M. K. Rahman; M. H. Sujon; A. Azad","Luddy School of Informatics, Computing, and Engineering, Indiana University Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University Bloomington, IN, USA; Luddy School of Informatics, Computing, and Engineering, Indiana University Bloomington, IN, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",256,266,"We develop a fused matrix multiplication kernel that unifies sampled dense-dense matrix multiplication and sparsedense matrix multiplication under a single operation called FusedMM. By using user-defined functions, FusedMM can capture almost all computational patterns needed by popular graph embedding and GNN approaches.FusedMM is an order of magnitude faster than its equivalent kernels in Deep Graph Library. The superior performance of FusedMM comes from the low-level vectorized kernels, a suitable load balancing scheme and an efficient utilization of the memory bandwidth. FusedMM can tune its performance using a code generator and perform equally well on Intel, AMD and ARM processors. FusedMM speeds up an end-to-end graph embedding algorithm by up to 28 x on different processors. The source code is available at https://github.com/HipGraph/FusedMM.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00034","Indiana University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460486","message passing;GNN;graph embedding","Distributed processing;Program processors;Memory management;Bandwidth;Load management;Libraries;Graph neural networks","graph theory;mathematics computing;matrix multiplication;neural nets;parallel processing;resource allocation;sparse matrices;vectors","end-to-end graph;unified SDDMM-SpMM kernel;Graph neural networks;fused matrix multiplication kernel;dense-dense matrix multiplication;sparse-dense matrix multiplication;popular graph embedding;FusedMM;equivalent kernels;Deep Graph Library;low-level vectorized kernels","",2.0,"",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Systemic Assessment of Node Failures in HPC Production Platforms","A. Das; F. Mueller; B. Rountree","North Carolina State University; North Carolina State University; Lawrence Livermore National Laboratory","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",267,276,"Production HPC clusters endure failures reducing computational capability and resource availability. Despite the presence of various failure prediction schemes for large-scale computing systems, a comprehensive understanding of how nodes fail considering various components and layers of the system is required for sustained resilience. This work performs a holistic diagnosis of node failures using a measurement-driven approach on contemporary system logs that can help vendors and system administrators support exascale resilience.Our work shows that external environmental influence is not strongly correlated with node failures in terms of the root cause. Though hardware and software faults trigger failures, the underlying root cause often lies in the application malfunctioning causing the system to fail. Furthermore, lead time enhancements are feasible for nodes showing fail slow characteristics. This study excavates such helpful empirical observations, which could facilitate better failure handling in production systems.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460484","Root Cause;Node Failures;Holistic Analysis","Production systems;Distributed processing;Correlation;Software;Hardware;Character recognition;Resilience","failure analysis;parallel processing;production engineering computing;software fault tolerance","systemic assessment;node failures;HPC production platforms;production HPC clusters;failure prediction schemes;large-scale computing systems;software faults;production systems","",2.0,"",41.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Combining XOR and Partner Checkpointing for Resilient Multilevel Checkpoint/Restart","M. Gholami; F. Schintke","Zuse Institute Berlin; Zuse Institute Berlin","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",277,288,"Checkpoint/restart (C/R) makes large-scale parallel jobs resilient against multiple node failures but typically takes considerable time and storage space. Efficient C/R strategies try to gain high levels of fault-tolerance while keeping the involved I/O and computation low. By combining XOR and partner checkpointing, two relatively weak C/R strategies, we develop and evaluate a stable, scalable, and fast C/R approach (including initialization, checkpointing, version consensus, and recovery mechanisms) that outperforms other C/R methods such as Reed-Solomon checkpointing in terms of stability and performance.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460462","checkpoint/restart;checkpointing;checkpoint;C/R;partner redundancy;XOR;erasure code;erasure coding;combined C/R;combined partner XOR;failures;MTTF;Reed Solomon;Multilevel C/R;Multilevel Checkpoint/Restart;Checkpoint Restart","Checkpointing;Reed-Solomon codes;Fault tolerance;Distributed processing;Limiting;Fault tolerant systems;Receivers","checkpointing;parallel processing;Reed-Solomon codes;software fault tolerance","partner checkpointing;large-scale parallel jobs;multiple node failures;storage space;fault-tolerance;Reed-Solomon checkpointing;XOR checkpointing","","","",42.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Demystifying GPU Reliability: Comparing and Combining Beam Experiments, Fault Simulation, and Profiling","F. F. d. Santos; S. K. S. Hari; P. M. Basso; L. Carro; P. Rech","UFRGS, Brazil; NVIDIA Corporation, United States; UFRGS, Brazil; UFRGS, Brazil; Politecnico di Torino, Italy","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",289,298,"Graphics Processing Units (GPUs) have moved from being dedicated devices for multimedia and gaming applications to general-purpose accelerators employed in High-Performance Computing (HPC) and safety-critical applications such as autonomous vehicles. This market shift led to a burst in the GPU's computing capabilities and efficiency, significant improvements in the programming frameworks and performance evaluation tools, and a concern about their hardware reliability. In this paper, we compare and combine high-energy neutron beam experiments that account for more than 13 million years of natural terrestrial exposure, extensive architectural-level fault simulations that required more than 350 GPU hours (using SASSIFI and NVBitFI), and detailed application-level profiling. Our main goal is to answer one of the fundamental open questions in GPU reliability evaluation: whether fault simulation provides representative results that can be used to predict the failure rates of workloads running on GPUs. We show that, in most cases, fault simulation-based prediction for silent data corruptions is sufficiently close (differences lower than 5×) to the experimentally measured rates. We also analyze the reliability of some of the main GPU functional units (including mixed-precision and tensor cores). We find that the way GPU resources are instantiated plays a critical role in the overall system reliability and that faults outside the functional units generate most detectable errors.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00037","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460470","reliablity;GPU;FIT rate;fault injection;beam experiments;NVIDIA","Performance evaluation;Particle beams;Tensors;Sensitivity;Graphics processing units;Predictive models;Tools","fault simulation;graphics processing units;high energy physics instrumentation computing;neutron beams;parallel processing;performance evaluation;reliability","graphics processing units;general-purpose accelerators;high-performance computing;safety-critical applications;programming frameworks;performance evaluation tools;hardware reliability;high-energy neutron beam experiments;natural terrestrial exposure;extensive architectural-level fault simulations;application-level profiling;GPU reliability evaluation;fault simulation-based prediction;GPU functional units","",7.0,"",36.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Improving checkpointing intervals by considering individual job failure probabilities","A. Frank; M. Baumgartner; R. Salkhordeh; A. Brinkmann","Zentrum für Datenverarbeitung, Johannes Gutenberg University Mainz; Zentrum für Datenverarbeitung, Johannes Gutenberg University Mainz; Zentrum für Datenverarbeitung, Johannes Gutenberg University Mainz; Zentrum für Datenverarbeitung, Johannes Gutenberg University Mainz","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",299,309,"Checkpointing is a popular resilience method in HPC and its efficiency highly depends on the choice of the checkpoint interval. Standard analytical approaches optimize intervals for big, long-running jobs that fail with high probability, while they are unable to minimize checkpointing overheads for jobs with a low or medium probability of failing. Nevertheless, our analysis of batch traces of four HPC systems shows that these jobs are extremely common.We therefore propose an iterative checkpointing algorithm to compute efficient intervals for jobs with a medium risk of failure. The method also supports big and long-running jobs by converging to the results of various traditional methods for these. We validated our algorithm using batch system simulations including traces from four HPC systems and compared it to five alternative checkpoint methods. The evaluations show up to 40% checkpoint savings for individual jobs when using our method, while improving checkpointing costs of complete HPC systems between 2.8% and 24.4% compared to the best alternative approach.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460459","checkpointing;HPC;resilience","Checkpointing;Distributed processing;Runtime;Probability;Iterative algorithms;Systems simulation;Standards","checkpointing;probability","complete HPC systems;checkpointing costs;individual jobs;alternative checkpoint methods;batch system simulations;iterative checkpointing algorithm;medium probability;low probability;checkpointing overheads;standard analytical approaches;resilience method;individual job failure probabilities;checkpointing intervals","",1.0,"",33.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Covirt: Lightweight Fault Isolation and Resource Protection for Co-Kernels","N. Gordon; J. R. Lange","Department of Computer Science, University of Pittsburgh, Pittsburgh, US; Department of Computer Science, University of Pittsburgh, Pittsburgh, US","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",310,319,"The challenges of the exascale era have generated a number of advancements in HPC systems software, with co-kernel architectures emerging as one such novel approach for HPC operating system and runtime (OS/R) design. Cokernels function by running multiple specialized, lightweight OS kernels natively on the same host as a general purpose OS/R. These specialized kernels are able to provide optimized OS/R environments for HPC applications while still retaining access to the full feature set of the co-running general purpose OS/R. While co-kernels are able to effectively optimize for performance, they generally lack effective mechanisms for cross OS/R fault isolation and resource protection. In this paper we present Covirt, a lightweight OS/R protection layer that leverages the hardware virtualization features found on modern CPUs. Covirt interposes a minimal hypervisor layer between a co-kernel OS/R and hardware to prevent OS level faults from impacting other OS/Rs running on the same system. Covirt is different from other virtualization-based approaches due to the level of integration necessary between the co-kernel instances, requiring the support of higher level semantic interfaces between the different OS/Rs. Covirt features a split architecture consisting of a hypervisor and controller module that continuously monitors changes to the underlying resource partitioning and translates those events to hypervisor configuration changes. We have implemented a prototype of Covirt in the context of the Hobbes exascale OS/R stack, specifically targeting the Pisces co-kernel framework and Kitten Lightweight Kernel. Our evaluation shows that Covirt is able to add fault isolation for memory and interrupt processing with minimal performance overheads.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00039","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460476","virtualization;hardware virtualization;cokernels","Distributed processing;Virtual machine monitors;Runtime;Semantics;Prototypes;Computer architecture;Hardware","Linux;multiprocessing systems;operating system kernels;parallel processing;software fault tolerance;virtual machines;virtualisation","Pisces co-kernel framework;Kitten Lightweight Kernel;Covirt;Lightweight fault isolation;resource protection;co-kernels;HPC systems software;co-kernel architectures;HPC operating system;runtime design;multiple specialized OS kernels;lightweight OS kernels;specialized kernels;HPC applications;feature set;hardware virtualization features;minimal hypervisor layer;OS level faults;virtualization-based approaches;co-kernel instances;higher level semantic interfaces;underlying resource partitioning","","","",22.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Introducing Application Awareness Into a Unified Power Management Stack","D. C. Wilson; S. Jana; A. Marathe; S. Brink; C. M. Cantalupo; D. R. Guttman; B. Geltz; L. H. Lawson; A. H. Al-rawi; A. Mohammad; F. Keceli; F. Ardanaz; J. M. Eastep; A. K. Coskun","Boston University; Intel Corporation; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Boston University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",320,329,"Effective power management in a data center is critical to ensure that power delivery constraints are met while maximizing the performance of users' workloads. Power limiting is needed in order to respond to greater-than-expected power demand. HPC sites have generally tackled this by adopting one of two approaches: (1) a system-level power management approach that is aware of the facility or site-level power requirements, but is agnostic to the application demands; OR (2) a job-level power management solution that is aware of the application design patterns and requirements, but is agnostic to the site-level power constraints. Simultaneously incorporating solutions from both domains often leads to conflicts in power management mechanisms. This, in turn, affects system stability and leads to irreproducibility of performance. To avoid this irreproducibility, HPC sites have to choose between one of the two approaches, thereby leading to missed opportunities for efficiency gains.This paper demonstrates the need for the HPC community to collaborate towards seamless integration of system-aware and application-aware power management approaches. This is achieved by proposing a new dynamic policy that inherits the benefits of both approaches from tight integration of a resource manager and a performance-aware job runtime environment. An empirical comparison of this integrated management approach against state-of-the-art solutions exposes the benefits of investing in end-to-end solutions to optimize for system-wide performance or efficiency objectives. With our proposed system-application integrated policy, we observed up to 7% reduction in system time dedicated to jobs and up to 11% savings in compute energy, compared to a baseline that is agnostic to system power and application design constraints.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00040","Lawrence Livermore National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460501","Energy-aware systems;Scheduling;Information resource management","Runtime environment;Protocols;Power demand;Power system management;Power system dynamics;Quality of service;Power system stability","computer centres;optimisation;parallel processing;power aware computing;power consumption;scheduling","resource manager;performance-aware job runtime environment;integrated management approach;end-to-end solutions;system-wide performance;system-application integrated policy;system power;application design constraints;application awareness;unified power management stack;power delivery constraints;power limiting;greater-than-expected power demand;HPC sites;system-level power management approach;site-level power requirements;application demands;job-level power management solution;application design patterns;site-level power constraints;system stability;application-aware power management approaches","",3.0,"",24.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"PALM: Progress- and Locality-Aware Adaptive Task Migration for Efficient Thread Packing","J. Park; S. Park; M. Han; W. Baek","Department of CSE, UNIST; Department of CSE, UNIST; Department of CSE, UNIST; Department of CSE and Graduate School of AI, UNIST","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",330,339,"Thread packing (TP) is an effective and widely-used technique to significantly improve the efficiency of parallel systems by dynamically controlling the number of cores allocated to multithreaded applications based on their requirements such as performance and energy efficiency. Despite the extensive prior works on TP, little work has been done to investigate and address its performance inefficiencies that arise across various parallel systems and applications with different characteristics. To bridge this gap, we investigate the performance inefficiencies of TP using a wide range of parallel applications and system configurations and identify their root causes. Guided by the in-depth performance characterization results, we propose PALM, progress- and locality-aware adaptive task migration for efficient TP. Through quantitative evaluation, we demonstrate that PALM achieves significantly higher performance and lower energy consumption than TP across various synchronization-intensive applications and system configurations, provides the performance and energy consumption comparable with the thread reduction technique, and considerably improves the efficiency of dynamic server consolidation and the performance under power capping.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460449","Progress;locality;adaptivity;task migration;thread packing","Energy consumption;Pathology;Distributed processing;Adaptive systems;Instruction sets;Dynamic scheduling;Control systems","energy consumption;multiprocessing systems;multi-threading;power aware computing","locality-aware adaptive task migration;PALM;energy consumption;synchronization-intensive applications;system configurations;thread reduction technique;efficient thread packing;parallel systems;multithreaded applications;energy efficiency;performance inefficiencies;parallel applications;in-depth performance characterization results","","","",29.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Performance Evaluation of Adaptive Routing on Dragonfly-based Production Systems","S. Chunduri; K. Harms; T. Groves; P. Mendygral; J. Zarins; M. Weiland; Y. Ghadar","Argonne National Laboratory; Argonne National Laboratory; Lawrence Berkeley National Laboratory; Hewlett Packard Enterprise; EPCC, The University of Edinburgh; Argonne National Laboratory; Argonne National Laboratory","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",340,349,"Performance of applications in production environments can be sensitive to network congestion. Cray Aries supports adaptively routing each network packet independently based on the load or congestion encountered as a packet traverses the network. Software can dictate different routing policies, adjusting between minimal and non-minimal bias, for each posted message. We have extensively evaluated the sensitivity of the routing bias selection on application performance as well as whole system performance in both production and controlled conditions. We show that the default routing bias used in Aries-based systems is often sub-optimal and that using a higher bias towards minimal routes will not only reduce the congestion effects on the application but also will decrease the overall congestion on the network. This routing scheme results in not only improved mean performance (by up to 12%) of most production applications but also reduced run-to-run variability. Our study prompted the two supercomputing facilities (ALCF and NERSC) to change the default routing mode on their Aries-based systems. We present the substantial improvement measured in the overall congestion management and interconnect performance in production after making this change.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00042","Argonne National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460477","","Performance evaluation;Production systems;Distributed processing;Adaptive systems;Sensitivity;Runtime;System performance","parallel machines;performance evaluation;telecommunication network routing","improved mean performance;routing scheme results;congestion effects;default routing bias;system performance;application performance;routing bias selection;nonminimal bias;routing policies;network packet;Cray Aries;network congestion;production environments;dragonfly-based production systems;adaptive routing;performance evaluation;congestion management;Aries-based systems","","","",23.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Cori: Dancing to the Right Beat of Periodic Data Movements over Hybrid Memory Systems","T. D. Doudali; D. Zahka; A. Gavrilovska","Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",350,359,"Emerging hybrid memory systems that comprise technologies such as Intel's Optane DC Persistent Memory, exhibit disparities in the access speeds and capacity ratios of their heterogeneous memory components. This breaks many assumptions and heuristics designed for traditional DRAM-only platforms. High application performance is feasible via dynamic data movement across memory units, which maximizes the capacity use of DRAM while ensuring efficient use of the aggregate system resources. Newly proposed solutions use performance models and machine intelligence to optimize which and how much data to move dynamically. However, the decision of when to move this data is based on empirical selection of time intervals, or left to the applications. Our experimental evaluation shows that failure to properly conFigure the data movement frequency can lead to 10%-100% performance degradation for a given data movement policy; yet, there is no established methodology on how to properly conFigure this value for a given workload, platform and policy. We propose Cori, a system-level tuning solution that identifies and extracts the necessary application-level data reuse information, and guides the selection of data movement frequency to deliver gains in application performance and system resource efficiency. Experimental evaluation shows that Cori configures data movement frequencies that provide application performance within 3% of the optimal one, and that it can achieve this up to 5 x more quickly than random or brute-force approaches. System-level validation of Cori on a platform with DRAM and Intel's Optane DC PMEM confirms its practicality and tuning efficiency.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00043","Facebook; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460495","Hybrid Memory;Optane Persistent Memory;Tuning;Data Movement Frequency;Page Scheduler;Data Tiering","Degradation;Frequency synthesizers;Distributed processing;Memory management;Random access memory;Dynamic scheduling;Software","buffer storage;DRAM chips;instruction sets;memory architecture;microprocessor chips;multiprocessing systems;optimisation;paged storage;reconfigurable architectures;storage management","data movement policy;data movement frequencies;performance models;aggregate system resources;memory units;dynamic data movement;high application performance;traditional DRAM-only;heterogeneous memory components;capacity ratios;Intel's Optane DC Persistent Memory;hybrid memory systems;periodic data movements;Intel's Optane DC PMEM;system-level validation;experimental evaluation;system resource efficiency;necessary application-level data;system-level tuning solution","",7.0,"",40.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Nowa: A Wait-Free Continuation-Stealing Concurrency Platform","F. Schmaus; N. Pfeiffer; T. Hönig; J. Nolte; W. Schröder-Preikschat","Erlangen-Nürnberg (FAU), Friedrich-Alexander-University, Erlangen, Germany; Erlangen-Nürnberg (FAU), Friedrich-Alexander-University, Erlangen, Germany; Ruhr University Bochum (RUB), Bochum, Germany; Cottbus-Senftenberg (BTU), Brandenburg University of Technology, Cottbus, Germany; Erlangen-Nürnberg (FAU), Friedrich-Alexander-University, Erlangen, Germany","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",360,371,"It is an ongoing challenge to efficiently use parallelism with today's multi- and many-core processors. Scalability becomes more crucial than ever with the rapidly growing number of processing elements in many-core systems that operate in data centres and embedded domains. Guaranteeing scalability is often ensured by using fully-strict fork/join concurrency, which is the prevalent approach used by concurrency platforms like Cilk. The runtime systems employed by those platforms typically resort to lock-based synchronisation due to the complex interactions of data structures within the runtime. However, locking limits scalability severely. With the availability of commercial off-the-shelf systems with hundreds of logical cores, this is becoming a problem for an increasing number of systems.This paper presents Nowa, a novel wait-free approach to arbitrate the plentiful concurrent strands managed by a concurrency platform's runtime system. The wait-free approach is enabled by exploiting inherent properties of fully-strict fork/join concurrency, and hence is potentially applicable for every continuation-stealing runtime system of a concurrency platform. We have implemented Nowa and compared it with existing runtime systems, including Cilk Plus, and Threading Building Blocks (TBB), which employ a lock-based approach. Our evaluation results show that the wait-free implementation increases the performance up to 1.64× compared to lock-based ones, on a system with 256 hardware threads. The performance increased by 1.17× on average, while no but one benchmark exhibited performance regression. Compared against OpenMP tasks using Clang's libomp, Nowa outperforms OpenMP by 8.68× on average.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00044","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460452","scheduling;concurrency platform;wait-free","Concurrent computing;Distributed processing;Runtime;Scalability;Instruction sets;Parallel processing;Performance gain","concurrency control;data structures;message passing;multiprocessing systems;multi-threading;parallel programming;shared memory systems;synchronisation","Nowa;wait-free continuation-stealing concurrency platform;many-core processors;rapidly growing number;processing elements;many-core systems;data centres;guaranteeing scalability;fully-strict fork;prevalent approach;runtime systems;data structures;locking limits scalability;off-the-shelf systems;logical cores;wait-free approach;plentiful concurrent strands;continuation-stealing runtime system;lock-based approach;wait-free implementation","",1.0,"",21.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Efficient Algorithms for Encrypted All-gather Operation","M. S. Lahijani; A. Naser; C. Wu; M. Gavahi; V. T. Hoang; Z. Wang; X. Yuan","Department of Computer Science, Florida State University, Tallahassee, Florida, USA; Department of Computer Science, Florida State University, Tallahassee, Florida, USA; Department of Computer Science, Florida State University, Tallahassee, Florida, USA; Department of Computer Science, Florida State University, Tallahassee, Florida, USA; Department of Computer Science, Florida State University, Tallahassee, Florida, USA; Department of Computer Science, Florida State University, Tallahassee, Florida, USA; Department of Computer Science, Florida State University, Tallahassee, Florida, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",372,381,"As more High-Performance Computing (HPC) applications that process sensitive data are moving to run on the public cloud, there is a need for the cloud infrastructure to provide privacy and integrity support. In this work, we investigate how to add encryption to all-gather to protect internode communication. This task is challenging since encryption is often more expensive than communication in contemporary HPC systems. We derive performance bounds for encrypted allgather, and develop new algorithms that meet the theoretical lower bounds. Our empirical evaluation on production systems demonstrates that the new algorithms achieve substantially better performance than the naive approach.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00045","National Science Foundation; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460464","Encrypted MPI_Allgather;Algorithm Design;Security","Measurement;Cloud computing;Production systems;Distributed processing;Data privacy;Clustering algorithms;Encryption","cloud computing;cryptography;data privacy;message passing;parallel processing","theoretical lower bounds;contemporary HPC systems;internode communication;encryption;integrity support;privacy;cloud infrastructure;public cloud;sensitive data;High-Performance Computing applications;encrypted","",1.0,"",30.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"CBNet: Minimizing Adjustments in Concurrent Demand-Aware Tree Networks","O. A. d. O. Souza; O. Goussevskaia; S. Schmid","Universidade Federal de Minas Gerais, Brazil; Universidade Federal de Minas Gerais, Brazil; University of Vienna, Austria","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",382,391,"This paper studies the design of demand-aware network topologies: networks that dynamically adapt themselves toward the demand they currently serve, in an online manner. While demand-aware networks may be significantly more efficient than demand-oblivious networks, frequent adjustments are still costly. Furthermore, a centralized controller of such networks may become a bottleneck.We present CBNet (Counting-Based self-adjusting Network), a demand-aware network that relies on a distributed control plane supporting concurrent adjustments, while significantly reducing the number of reconfigurations, compared to related work. CBNet comes with formal guarantees and is based on concepts of self-adjusting data structures. We evaluate CBNet analytically and empirically and we find that CBNet can effectively exploit locality structure in the traffic demand.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00046","European Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460465","Self-adjusting networks;decentralization;concurrency;online algorithms","Distributed processing;Network topology;Decentralized control;Data structures","computer networks;data structures;distributed control;telecommunication network routing;telecommunication network topology;telecommunication traffic;trees (mathematics)","CBNet;self-adjusting data structures;traffic demand;concurrent demand-aware tree networks;demand-aware network topologies;demand-oblivious networks;Counting-Based self-adjusting Network;concurrent adjustments","",1.0,"",35.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Scaling Sparse Matrix Multiplication on CPU-GPU Nodes","Y. Xia; P. Jiang; G. Agrawal; R. Ramnath","Computer Science and Engineering, The Ohio State University; Computer Science Department, University of Iowa; Computer and Cyber Sciences, Augusta University; Computer Science and Engineering, The Ohio State University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",392,401,"Multiplication of two sparse matrices (SpGEMM) is a popular kernel behind many numerical solvers, and also features in implementing many common graph algorithms. Though many recent research efforts have focused on implementing SpGEMM efficiently on a single GPU, none of the existing work has considered the case where the memory requirements exceed the size of GPU memory. Similarly, the use of the aggregate computing power of CPU and GPU has also not been addressed for those large matrices. In this paper, we present a framework for scaling SpGEMM computations for matrices that do not fit into GPU memory. We address how the computation and data can be partitioned across kernel executions on GPUs. An important emphasis in our work is overlapping data movement and computation. We achieve this by addressing many challenges, such as avoiding dynamic memory allocations, and re-scheduling data transfers with the computation of chunks. We extend our framework to make efficient use of both GPU and CPU, by developing an efficient work distribution strategy. Our evaluation on 9 large matrices shows that our out-of-core GPU implementation achieves 1.98-3.03X speedups over a state-of-the-art multi-core CPU implementation, our hybrid implementation further achieves speedups up to 3.74x, and that our design choices are directly contributing towards achieving this performance.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460469","","Distributed processing;Distribution strategy;Memory management;Graphics processing units;Data transfer;Dynamic scheduling;Sparse matrices","graph theory;graphics processing units;matrix multiplication;multiprocessing systems;parallel processing;sparse matrices","numerical solvers;popular kernel;sparse matrices;CPU-GPU nodes;sparse matrix multiplication;hybrid implementation;state-of-the-art multicore CPU implementation;out-of-core GPU implementation;efficient work distribution strategy;re-scheduling data transfers;dynamic memory allocations;data movement;kernel executions;SpGEMM computations;aggregate computing power;GPU memory;memory requirements;single GPU;SpGEMM efficiently;common graph algorithms","",2.0,"",37.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"zMesh: Exploring Application Characteristics to Improve Lossy Compression Ratio for Adaptive Mesh Refinement","H. Luo; J. Wang; Q. Liu; J. Chen; S. Klasky; N. Podhorszki","New Jersey Institute of Technology, Newark, NJ, USA; Rutgers University-Newark, Newark, NJ, USA; New Jersey Institute of Technology, Newark, NJ, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",402,411,"Scientific simulations on high-performance computing systems produce vast amounts of data that need to be stored and analyzed efficiently. Lossy compression significantly reduces the data volume by trading accuracy for performance. Despite the recent success of lossy compression, such as ZFP and SZ, the compression performance is still far from being able to keep up with the exponential growth of data. This paper aims to further take advantage of application characteristics, an area that is often under-explored, to improve the compression ratios of adaptive mesh refinement (AMR) - a widely used numerical solver that allows for an improved resolution in limited regions. We propose a level reordering technique zMesh to reduce the storage footprint of AMR applications. In particular, we group the data points that are mapped to the same or adjacent geometric coordinates such that the dataset is smoother and more compressible. Unlike the prior work where the compression performance is affected by the overhead of metadata, this work re-generates restore recipe using a chained tree structure, thus involving no extra storage overhead for compressed data, which substantially improves the compression ratios. The results demonstrate that zMesh can improve the smoothness of data by 67.9% and 71.3% for Z-ordering and Hilbert, respectively. Overall, zMesh improves the compression ratios by up to 16.5% and 133.7% for ZFP and SZ, respectively. Despite that zMesh involves additional compute overhead for tree and restore recipe construction, we show that the cost can be amortized as the number of quantities to be compressed increases.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00048","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460489","High-performance computing (HPC);data storage;lossy compression;adaptive mesh refinement (AMR)","Distributed processing;Analytical models;Computational modeling;Redundancy;Organizations;Metadata;Adaptive mesh refinement","data compression;meta data;trees (mathematics)","metadata;data points;AMR applications;level reordering technique zMesh;data volume;high-performance computing systems;adaptive mesh refinement;lossy compression ratio;application characteristics;compression ratios","",4.0,"",26.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Efficient parallel CP decomposition with pairwise perturbation and multi-sweep dimension tree","L. Ma; E. Solomonik","Department of Computer Science, University of Illinois, Urbana-Champaign; Department of Computer Science, University of Illinois, Urbana-Champaign","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",412,421,"The widely used alternating least squares (ALS) algorithm for the canonical polyadic (CP) tensor decomposition is dominated in cost by the matricized-tensor times Khatri-Rao product (MTTKRP) kernel. This kernel is necessary to set up the quadratic optimization subproblems. State-of-the-art parallel ALS implementations use dimension trees to avoid redundant computations across MTTKRPs within each ALS sweep. In this paper, we propose two new parallel algorithms to accelerate CP-ALS. We introduce the multi-sweep dimension tree (MSDT) algorithm, which requires the contraction between an order N input tensor and the first-contracted input matrix once every (N-1)/N sweeps. This algorithm reduces the leading order computational cost by a factor of 2(N-1)/N relative to the best previously known approach. In addition, we introduce a more communication-efficient approach to parallelizing an approximate CP-ALS algorithm, pairwise perturbation. This technique uses perturbative corrections to the subproblems rather than recomputing the contractions, and asymptotically accelerates ALS. Our benchmark results on 1024 processors on the Stampede2 supercomputer show that CP decomposition obtains a 1.25X speed-up from MSDT and a 1.94X speedup from pairwise perturbation compared to the state-of-the-art dimension-tree based CP-ALS implementations.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00049","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460504","tensor decomposition;CP decomposition;dimension tree;pairwise perturbation;communication avoiding algorithm","Tensors;Program processors;Perturbation methods;Approximation algorithms;Supercomputers;Computational efficiency;Parallel algorithms","mathematics computing;matrix algebra;matrix decomposition;optimisation;parallel algorithms;parallel machines;tensors;trees (mathematics)","CP tensor decomposition;Stampede2 supercomputer;parallel alternating least squares algorithm;parallel ALS algorithm;CP-ALS;parallel algorithm;quadratic optimization subproblems;matricized-tensor times Khatri-Rao product kernel;canonical polyadic tensor decomposition;parallel CP decomposition;perturbative corrections;pairwise perturbation;computational cost reduction;MSDT;multisweep dimension tree","",2.0,"",41.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"12 Ways to Fool the Masses with Irreproducible Results","L. Barba","George Washington University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",422,422,"Thirty years ago, David Bailey published a humorous piece in the Supercomputing Review magazine, listing 12 ways of presenting results to artificially boost performance claims. That was at a time when the debate was between Cray “two-oxen” machines versus parallel “thousand-chickens” systems, when parallel standards (like MPI) were still unavailable, and the Top500 list didn’t yet exist. In the years since, David and others updated the list of tricks a few times, notably in 2010–11 (when the marketing departments of Intel and Nvidia were really going at each other) Georg Hager in his blog and Scott Pakin in HPC Wire. Heterogeneity of computing systems has only escalated in the last decade, and many remiss reporting tactics continue unabated. Alas, two new ingredients have entered into the mix: wide adoption of machine learning techniques both in the science applications and systems research; and a swell of concern over reproducibility and replicability. My talk will be a new twist on the 12 ways to fool the masses, focusing on how researchers in computational science and high-performance computing miss the mark when conducting or reporting their results with poor reproducibility. By showcasing in a lighthearted manner a set of anti-patterns, I aim to encourage us to see the value and commit to adapting our practice to achieve more trustworthy scientific evidence with high-performance computing.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460485","","","","","","","","","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Consistent Lock-free Parallel Stochastic Gradient Descent for Fast and Stable Convergence","K. Bäckström; I. Walulya; M. Papatriantafilou; P. Tsigas","Dept. of Computer Science and Engineering, Chalmers University of Technology, Gothenburg, Sweden; Dept. of Computer Science and Engineering, Chalmers University of Technology, Gothenburg, Sweden; Dept. of Computer Science and Engineering, Chalmers University of Technology, Gothenburg, Sweden; Dept. of Computer Science and Engineering, Chalmers University of Technology, Gothenburg, Sweden","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",423,432,"Stochastic Gradient Descent (SGD) is an essential element in Machine Learning (ML) algorithms. Asynchronous shared-memory parallel SGD (AsyncSGD), including synchronization-free algorithms, e.g. HOGWILD!, have received interest in certain contexts, due to reduced overhead compared to synchronous parallelization. Despite that they induce staleness and inconsistency, they have shown speedup for problems satisfying smooth, strongly convex targets, and gradient sparsity. Recent works take important steps towards understanding the potential of parallel SGD for problems not conforming to these strong assumptions, in particular for deep learning (DL). There is however a gap in current literature in understanding when AsyncSGD algorithms are useful in practice, and in particular how mechanisms for synchronization and consistency play a role. We contribute with answering questions in this gap by studying a spectrum of parallel algorithmic implementations ofAsyncSGD, aiming to understand how shared-data synchronization influences the convergence properties in fundamental DL applications. We focus on the impact of consistency-preserving non-blocking synchronization in SGD convergence, and in sensitivity to hyper-parameter tuning. We propose Leashed-SGD, an extensible algorithmic framework of consistency-preserving implementations of AsyncSGD, employing lock-free synchronization, effectively balancing throughput and latency. Leashed-SGD features a natural contention-regulating mechanism, as well as dynamic memory management, allocating space only when needed. We argue analytically about the dynamics of the algorithms, memory consumption, the threads' progress over time, and the expected contention. We provide a comprehensive empirical evaluation, validating the analytical claims, benchmarking the proposed Leashed-SGD framework, and comparing to baselines for two prominent deep learning (DL) applications: multilayer perceptrons (MLP) and convolutional neural networks (CNN). We observe the crucial impact of contention, staleness and consistency and show how, thanks to the aforementioned properties, Leashed-SGD provides significant improvements in stability as well as wall-clock time to convergence (from 20-80% up to 4 x improvements) compared to the standard lock-based AsyncSGD algorithm and HOGWILD!, while reducing the overall memory footprint.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460457","artificial neural networks;parallel algorithms;lock-free synchronization;stochastic gradient descent","Deep learning;Training;Machine learning algorithms;Heuristic algorithms;Memory management;Benchmark testing;Synchronization","convolutional neural nets;deep learning (artificial intelligence);gradient methods;multilayer perceptrons;parallel algorithms;parallel programming;shared memory systems;stochastic processes;synchronisation","shared-data synchronization;lock-free synchronization;natural contention-regulating mechanism;dynamic memory management;memory consumption;Leashed-SGD framework;deep learning;lock-free parallel stochastic gradient descent;asynchronous shared-memory parallel SGD;machine learning;multilayer perceptrons;convolutional neural networks;parallel algorithmic implementations;AsyncSGD algorithms;gradient sparsity;synchronous parallelization;HOGWILD;synchronization-free algorithms;stable convergence","",3.0,"",40.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Redesigning Peridigm on SIMT Accelerators for High-performance Peridynamics Simulations","X. Li; H. Ye; J. Zhang","Computer Information Network Center, CAS, University of Chinese Academy of Sciences, Beijing, China; Computer Information Network Center, CAS, Beijing, China; Computer Information Network Center, CAS, Beijing, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",433,443,"Peridigm is one of the most frequently utilized Peridynamics (PD) simulation software for problems involving discontinuity, such as cracks and fragmentation. However, performing long-term and large-scale simulations is very time-consuming for Peridigm. To enhance the performance and scalability of Peridigm, we port and optimize Peridigm on the SIMT accelerators. Challenges are imposed on efficient Peridigm on the SIMT architecture by the complex calculations and massive memory access of PD simulations. In this study, a series of strategies and techniques are proposed to optimize the performance of Peridigm. We first adjust the algorithms of bond-based calculations to eliminate the data conflicts with minimized overhead in order to achieve parallel Peridigm on accelerators. Furthermore, we propose thread grouping and collaborative memory access strategies to decrease the overhead of data fetch from device memory. To improve the efficiency of calculations, we also refine the calculation instructions. Finally, we offer a transmission-computation overlapping strategy for reducing the overhead brought by the data transmissions and improving the scalability. The optimized Peridigm on 4 Nvidia Tesla V100 GPUs accelerates the basic parallel Peridigm on 4 V100 GPUs 10.24 times. Compared to the original Peridigm run on 8 Intel Xeon Gold 6248 CPUs (160 cores, 320 threads) and the optimized PD application run on 4 SW26010 processors (1,040 cores), our work on 4 V100 GPUs accelerates the simulation 9 times and 4 times respectively. As for large-scale simulations, because we don't have enough V100 GPUs, we run our work on noncommercial SIMT accelerators which have similar performance to the V100 of the PCIe version, with the example scales from 282,000 points to 36,096,000 points and the number of accelerators scales from 4 to 512, near-linear scalability is observed and the performance ultimately reaching 825.72 TFLOPS with 98.81% parallel efficiency","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00052","Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460502","Peridynamics;Peridigm;SIMT accelerator;high-performance simulation;parallel","Performance evaluation;Ports (computers);Scalability;Instruction sets;Memory management;Collaboration;Software","graphics processing units;microprocessor chips;multiprocessing systems;multi-threading","high-performance Peridynamics;frequently utilized Peridynamics simulation software;large-scale simulations;efficient Peridigm;massive memory access;PD simulations;collaborative memory access strategies;optimized Peridigm;basic parallel Peridigm;original Peridigm;optimized PD application;V100 GPUs;simulation 9 times;noncommercial SIMT accelerators;accelerators scales;Nvidia Tesla V100 GPUs accelerates;voltage 4.0 V;computer speed 825.72 TFLOPS","",3.0,"",21.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Designing High-Performance MPI Libraries with On-the-fly Compression for Modern GPU Clusters","Q. Zhou; C. Chu; N. S. Kumar; P. Kousha; S. M. Ghazimirsaeed; H. Subramoni; D. K. Panda","Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University; Department of Computer Science and Engineering, The Ohio State University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",444,453,"While the memory bandwidth of accelerators such as GPU has significantly improved over the last decade, the commodity networks such as Ethernet and InfiniBand are lagging in terms of raw throughput creating. Although there are significant research efforts on improving the large message data transfers for GPU-resident data, the inter-node communication remains the major performance bottleneck due to the data explosion created by the emerging High-Performance Computing (HPC) applications. On the other hand, the recent developments in GPU-based compression algorithms exemplify the potential of using high-performance message compression techniques to reduce the volume of data transferred thereby reducing the load on an already overloaded inter-node communication fabric. The existing GPU-based compression schemes are not designed for “on-the-fly” execution and lead to severe performance degradation when integrated into the communication libraries. In this paper, we take up this challenge and redesign the MVAPICH2 MPI library to enable high-performance, on-the-fly message compression for modern, dense GPU clusters. We also enhance existing implementations of lossless and lossy compression algorithms, MPC and ZFP, to provide high-performance, on-the-fly message compression and decompression. We demonstrate that our proposed designs can offer significant benefits at the microbenchmark and application-levels. The proposed design is able to provide up to 19% and 37% improvement in the GPU computing flops of AWP-ODC with the enhanced MPCOPT and ZFP-OPT schemes, respectively. Moreover, we gain up to 1.56x improvement in Dask throughput. To the best of our knowledge, this is the first work that leverages the GPU-based compression techniques to significantly improve the GPU communication performance for various MPI primitives, MPI-based data science, and HPC applications.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460453","GPU;Compression;GPU-Aware MPI;HPC;Dask","Liquids;Heuristic algorithms;Graphics processing units;Data science;Throughput;Libraries;Real-time systems","application program interfaces;data compression;graphics processing units;local area networks;message passing;parallel processing","data explosion;emerging High-Performance Computing;GPU-based compression algorithms;high-performance message compression techniques;overloaded inter-node communication fabric;existing GPU-based compression schemes;severe performance degradation;communication libraries;MVAPICH2 MPI library;modern GPU clusters;dense GPU clusters;lossless compression algorithms;lossy compression algorithms;GPU computing flops;1.56x improvement;GPU-based compression techniques;GPU communication performance;MPI-based data science;HPC applications;designing High-Performance MPI libraries;On-the-fly Compression;commodity networks;raw throughput creating;significant research efforts;message data transfers;GPU-resident data","",6.0,"",36.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"xBGAS: A Global Address Space Extension on RISC-V for High Performance Computing","X. Wang; J. D. Leidel; B. Williams; A. Ehret; M. Mark; M. A. Kinsy; Y. Chen","Texas Tech University; Tactical Computing Laboratories; Texas Tech University; Texas A&M University; Texas A&M University; Texas A&M University; Texas Tech University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",454,463,"The tremendous expansion of data volume has driven the transition from monolithic architectures towards systems integrated with discrete and distributed subcomponents in modern scalable high performance computing (HPC) systems. As such, multi-layered software infrastructures have become essential to bridge the gap between heterogeneous commodity devices. However, operations across synthesized components with divergent interfaces inevitably lead to redundant software footprints and undesired latency. Therefore, a scalable and unified computing platform, capable of supporting efficient interactions between individual components, is desirable for largescale data-intensive applications. In this work, we introduce the Extended Base Global Address Space, or xBGAS, microarchitecture extension to the RISC-V instruction set architecture (ISA) for scalable high performance computing. The xBGAS extension provides native ISA-level support for direct accesses to remote shared memory by mapping remote data objects into a system's extended address space. We perform both software and hardware evaluations of the xBGAS design. The results show that xBGAS reduces instruction count generated by interprocess communication by 69.26% on average. Overall, xBGAS achieves an average performance gain of 21.96% (up to 37.29%) across the tested workloads.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460481","RISC V;Data intensive Computing;OpenSHMEM;Extended Global Address Space;Remote Memory Access","Microarchitecture;Runtime library;High performance computing;Instruction sets;Computer architecture;Programming;Performance gain","integrated circuit design;microprocessor chips;parallel architectures;parallel programming;reduced instruction set computing","monolithic architectures;discrete distributed subcomponents;modern scalable high performance computing systems;multilayered software infrastructures;heterogeneous commodity devices;redundant software footprints;scalable computing platform;unified computing platform;largescale data-intensive applications;Extended Base Global Address Space;microarchitecture extension;RISC-V instruction set architecture;xBGAS extension;native ISA-level support;remote data objects;xBGAS design;average performance gain;tremendous expansion;data volume;global address space extension","",4.0,"",45.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"ARBALEST: Dynamic Detection of Data Mapping Issues in Heterogeneous OpenMP Applications","L. Yu; J. Protze; O. Hernandez; V. Sarkar","College of Computing, Georgia Institute of Technology, Atlanta, USA; IT Center, RWTH Aachen University, Aachen, Germany; Computer Science Research Oak Ridge National Laboratory, Oak Ridge, USA; College of Computing, Georgia Institute of Technology, Atlanta, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",464,474,"From OpenMP 4.0 onwards, programmers can offload code regions to accelerators by using the target offloading feature. However, incorrect usage of target offloading constructs may incur data mapping issues. A data mapping issue occurs when the host fails to observe updates on the accelerator or vice versa. It may further lead to multiple memory issues such as use of uninitialized memory, use of stale data, and data race. To the best of our knowledge, currently there is no prior work on dynamic detection of data mapping issues in heterogeneous OpenMP applications.In this paper, we identify possible root causes of data mapping issues in OpenMP's standard memory model and the unified memory model. We find that data mapping issues primarily result from incorrect settings of map and nowait clauses in target offloading constructs. Further, the novel unified memory model introduced in OpenMP 5.0 cannot avoid the occurrence of data mapping issues. To mitigate the difficulty of detecting data mapping issues, we propose ARBALEST, an on-the-fly data mapping issue detector for OpenMP applications. For each variable mapped to the accelerator, ARBALEST's detection algorithm leverages a state machine to track the last write's visibility. ARBALEST requires constant storage space for each memory location and takes amortized constant time per memory access. To demonstrate ARBALEST's effectiveness, an experimental comparison with four other dynamic analysis tools (Valgrind, Archer, AddressSanitizer, MemorySanitizer) has been carried out on a number of open-source benchmark suites. The evaluation results show that ARBALEST delivers demonstrably better precision than the other four tools, and its execution time overhead is comparable to that of state-of-the-art dynamic analysis tools.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00055","Battelle; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460498","Dynamic Analysis;Concurrency Bug Detection;Data Mapping Issue;OpenMP;Accelerator","Distributed processing;Memory management;Computer bugs;Distributed databases;Detectors;Tools;Data models","benchmark testing;message passing;multiprocessing systems;parallel programming","open-source benchmark suites;state machine;heterogeneous OpenMP applications;data mapping issues;ARBALEST;on-the-fly data mapping issue detector","","","",34.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Spray: Sparse Reductions of Arrays in OPENMP","J. Hückelheim; J. Doerfert","Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Argonne Leadership Computing Facility, Argonne National Laboratory, Lemont, IL, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",475,484,"We present SPRAY, an open-source header-only C++ library for sparse reductions of arrays. SPRAY is meant for applications in which a large array is collaboratively updated by multiple threads using an associative and commutative operation such as +=. Especially when each thread accesses only parts of the array, SPRAY can perform significantly better than OPENMP's built-in reduction clause or atomic updates, while also using less memory than the former. SPRAY provides both an easy-to-use interface that can serve as a drop-in replacement for OPENMP reductions and a selection of reducer objects that accumulate the final result in different thread-safe ways. We demonstrate SPRAY through multiple test cases including the LULESH shock hydrodynamics code and a transpose-matrix-vector multiplication for sparse matrices stored in CSR format. SPRAY reductions outperform built-in OPENMP reductions consistently, in some cases improving run time and memory overhead by 20X, and even beating domain-specific approaches such as Intel MKL by over 2X in some cases. Furthermore, SPRAY reductions have a minimal impact on the code base, requiring only a few lines of source code changes. Once in place, SPRAY reduction schemes can be switched easily, allowing performance portability and tuning opportunities by separating performance-critical implementation details from application code.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00056","U.S. Department of Energy; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460492","OpenMP;Reductions;Performance Portability","Distributed processing;Instruction sets;Electric shock;Memory management;Switches;Hydrodynamics;Libraries","C++ language;matrix multiplication;multi-threading;parallel programming;public domain software;software libraries;sparse matrices;vectors","open-source header-only C++ library;multiple threads;reduction clause;OPENMP reductions;SPRAY reduction schemes","",3.0,"",22.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Code Generation for Room Acoustics Simulations with Complex Boundary Conditions","L. Stoltzfus; B. Hamilton; M. Steuwer; L. Li; C. Dubach","University of Edinburgh, United Kingdom; University of Edinburgh, United Kingdom; University of Edinburgh, United Kingdom; University of Edinburgh, United Kingdom; McGill University, Canada","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",485,496,"The software and hardware landscape of high performance computing is expanding faster than computational scientists can take advantage of new frameworks and platforms. In an ideal world, simulation codes would be written once in a high-level manner and achieve high-performance anywhere, but the reality is more complicated. Currently, high-level solutions lack support for sophisticated physical models across different parallel backends. Existing solutions with appropriate support are low-level and, therefore, tied to a specific hardware target. We present an approach that tackles this problem with a modularized separation of concerns: a middle layer separates the management of generating low-level optimized code from a high-level programmable layer. In this paper, we describe how our contributions to this hardware-agnostic, middle-layer language provide functionality for complex room acoustics simulations, a type of Finite Difference Time Domain (FDTD) simulation using stencils which is representative of many other 3D wave models. We show that we are able to develop performance-portable codes for these types of models which leads to performance on par with tuned hand-written implementations. Furthermore, we show how this approach is used to develop both host and device side code for multi-kernel applications, as is required for room acoustics simulations with complex boundaries.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00057","Engineering and Physical Sciences Research Council; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460499","compilers;stencils;programming languages","Performance evaluation;Solid modeling;Computational modeling;Graphics processing units;Writing;Boundary conditions;Acoustics","architectural acoustics;finite difference time-domain analysis;hardware-software codesign;high level languages;multiprocessing systems;optimisation;program compilers;software portability;solid modelling","high performance computing;computational scientists;sophisticated physical models;parallel backends;low-level optimized code;high-level programmable layer;hardware-agnostic;middle-layer language;complex room acoustics simulations;finite difference time domain simulation;3D wave models;performance-portable codes;device side code;complex boundaries;code generation;complex boundary conditions","","","",42.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Temporal blocking of finite-difference stencil operators with sparse “off-the-grid” sources","G. Bisbas; F. Luporini; M. Louboutin; R. Nelson; G. J. Gorman; P. H. J. Kelly","Imperial College London, London, UK; Devito Codes, London, UK; Georgia Institute of Technology, Atlanta, GA; Imperial College London, London, UK; Imperial College London, London, UK; Imperial College London, London, UK","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",497,506,"Stencil kernels dominate a range of scientific applications, including seismic and medical imaging, image processing, and neural networks. Temporal blocking is a performance optimization that aims to reduce the required memory bandwidth of stencil computations by re-using data from the cache for multiple time steps. It has already been shown to be beneficial for this class of algorithms. However, applying temporal blocking to practical applications' stencils remains challenging. These computations often consist of sparsely located operators not aligned with the computational grid (“off-the-grid”). Our work is motivated by modelling problems in which source injections result in wavefields that must then be measured at receivers by interpolation from the grided wavefield. The resulting data dependencies make the adoption of temporal blocking much more challenging. We propose a methodology to inspect these data dependencies and reorder the computation, leading to performance gains in stencil codes where temporal blocking has not been applicable. We implement this novel scheme in the Devito domain-specific compiler toolchain. Devito implements a domain-specific language embedded in Python to generate optimized partial differential equation solvers using the finite-difference method from high-level symbolic problem definitions. We evaluate our scheme using isotropic acoustic, anisotropic acoustic, and isotropic elastic wave propagators of industrial significance. After auto-tuning, performance evaluation shows that this enables substantial performance improvement through temporal blocking over highly-optimized vectorized spatially-blocked code of up to 1.6x.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00058","Engineering and Physical Sciences Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460483","temporal blocking;stencil computations;code generation;partial differential equations;seismic imaging;domain-specific languages;wave-propagation","Performance evaluation;Microarchitecture;Partial differential equations;Neural networks;Receivers;Performance gain;Acoustics","elastic waves;finite difference methods;interpolation;optimisation;parallel processing;partial differential equations;program compilers","temporal blocking;finite-difference stencil operators;off-the-grid sources;stencil kernels;scientific applications;seismic imaging;medical imaging;stencil computations;practical applications;sparsely located operators;computational grid;data dependencies;stencil codes;Devito domain-specific compiler toolchain;isotropic acoustic wave propagators;anisotropic acoustic wave propagators;isotropic elastic wave propagators;interpolation","","","",65.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Accelerating non-power-of-2 size Fourier transforms with GPU Tensor Cores","L. Pisha; Ł. Ligowski","NVIDIA Corporation, Santa Clara, CA, USA; NVIDIA Corporation, Santa Clara, CA, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",507,516,"Fourier transforms whose sizes are powers of two or have only small prime factors have been extensively studied, and optimized implementations are typically memory-bound. However, handling arbitrary transform sizes-which may be prime or have large prime factors-is difficult. Direct discrete Fourier transform (DFT) implementations involve extra computation, while fast Fourier transform (FFT)-style factorized decompositions introduce additional overheads in register use, multiprocessor occupancy, and memory traffic. Tensor Cores are hardware units included in modern GPUs which perform matrix multiply-adds at a much higher throughput than normal GPU floating-point instructions. Because of their higher throughput and better uniformity across sizes, DFT/FFT implementations using Tensor Cores can surpass the performance of existing DFT/FFT implementations for difficult sizes. We present key insights in this approach, including complex number representation, efficient mapping of odd sizes to Tensor Cores (whose dimensions are all powers of 2), and adding a size 2 or size 4 epilogue transform at very low cost. Furthermore, we describe a method for emulating FP32 precision while using lower-precision Tensor Cores to accelerate the computation. For large batch sizes, our fastest Tensor Core implementation per size is at least 10% faster than the state-of-the-art cuFFT library in 49% of supported sizes for FP64 (double) precision and 42% of supported sizes for FP32 precision. The numerical accuracy of the results matches that of cuFFT for FP64 and is degraded by only about 0.3 bits on average for emulated FP32. To our knowledge, this is the first application of Tensor Cores to FFT computation which meets the accuracy and exceeds the speed of the state of the art.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460474","Discrete Fourier transforms;fast Fourier transforms;mixed-precision arithmetic;emergent floating-point formats;Tensor Cores;accelerated matrix multiply hardware;parallel algorithms","Distributed processing;Tensors;Fast Fourier transforms;Discrete Fourier transforms;Graphics processing units;Throughput;Libraries","discrete Fourier transforms;fast Fourier transforms;floating point arithmetic;graphics processing units;matrix algebra","GPU tensor cores;prime factors;memory-bound;normal GPU floating-point instructions;FP32 precision;batch sizes;FFT computation;accelerating nonpower-of-2 size Fourier transforms;direct discrete Fourier transform;hardware units;matrix multiply-adds;DFT;complex number representation;odd size mapping;word length 0.3 bit","",2.0,"",17.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Parallel String Graph Construction and Transitive Reduction for De Novo Genome Assembly","G. Guidi; O. Selvitopi; M. Ellis; L. Oliker; K. Yelick; A. Buluç","Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",517,526,"One of the most computationally intensive tasks in computational biology is de novo genome assembly, the decoding of the sequence of an unknown genome from redundant and erroneous short sequences. A common assembly paradigm identifies overlapping sequences, simplifies their layout, and creates consensus. Despite many algorithms developed in the literature, the efficient assembly of large genomes is still an open problem. In this work, we introduce new distributed-memory parallel algorithms for overlap detection and layout simplification steps of de novo genome assembly, and implement them in the diBELLA 2D pipeline. Our distributed memory algorithms for both overlap detection and layout simplification are based on linear-algebra operations over semirings using 2D distributed sparse matrices. Our layout step consists of performing a transitive reduction from the overlap graph to a string graph. We provide a detailed communication analysis of the main stages of our new algorithms. diBELLA 2D achieves near linear scaling with over 80% parallel efficiency for the human genome, reducing the runtime for overlap detection by 1.2-1.3× for the human genome and 1.5-1.9× for C.elegans compared to the state-of-the-art. Our transitive reduction algorithm outperforms an existing distributed-memory implementation by 10.5-13.3× for the human genome and 18-29× for the C. elegans. Our work paves the way for efficient de novo assembly of large genomes using long reads in distributed memory.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00060","Advanced Scientific Computing Research; Office of Science; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460463","","Concurrent computing;Runtime;Program processors;Layout;Pipelines;Memory management;Genomics","biology computing;distributed memory systems;genomics;graph theory;microorganisms;parallel algorithms;sparse matrices","C.elegans;parallel efficiency;linear scaling;communication analysis;over semirings;linear-algebra operations;diBELLA 2D pipeline;sequence decoding;computational biology;layout simplification;distributed-memory parallel algorithms;parallel string graph construction;transitive reduction;human genome;overlap graph;2D distributed sparse matrices;overlap detection;de novo genome assembly","",3.0,"",36.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Distributed-Memory k-mer Counting on GPUs","I. Nisa; P. Pandey; M. Ellis; L. Oliker; A. Buluç; K. Yelick","Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley; Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",527,536,"A fundamental step in many bioinformatics computations is to count the frequency of fixed-length sequences, called k-mers, a problem that has received considerable attention as an important target for shared memory parallelization. With datasets growing at an exponential rate, distributed memory parallelization is becoming increasingly critical. Existing distributed memory k-mer counters do not take advantage of GPUs for accelerating computations. Additionally, they do not employ domain-specific optimizations to reduce communication volume in a distributed environment. In this paper, we present the first GPU-accelerated distributed-memory parallel k-mer counter. We evaluate the communication volume as the major bottleneck in scaling k-mer counting to multiple GPU-equipped compute nodes and implement a supermer-based optimization to reduce the communication volume and to enhance scalability. Our empirical analysis examines the balance of communication to computation on a state-of-the-art system, the Summit supercomputer at Oak Ridge National Lab. Results show overall speedups of up to two orders of magnitude with GPU optimization over CPU-based k mer counters. Furthermore, we show an additional 1.5× speedup using the supermer-based communication optimization.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00061","Advanced Scientific Computing Research; Office of Science; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460480","GPU;k-mer counter;distributed memory","Proteins;Scalability;Graphics processing units;Genomics;Tools;Supercomputers;Partitioning algorithms","bioinformatics;graphics processing units;optimisation;parallel processing","supermer-based communication optimization;GPU optimization;supermer-based optimization;multiple GPU-equipped compute nodes;parallel k-mer counter;GPU-accelerated distributed-memory;distributed environment;communication volume;domain-specific optimizations;distributed memory k-mer counters;distributed memory parallelization;exponential rate;shared memory parallelization;important target;called k-mers;fixed-length sequences;bioinformatics computations;GPUs;distributed-memory k-mer counting","",1.0,"",33.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Distributed-memory multi-GPU block-sparse tensor contraction for electronic structure","T. Herault; Y. Robert; G. Bosilca; R. J. Harrison; C. A. Lewis; E. F. Valeev; J. J. Dongarra","ICL, University of Tennessee, TN, USA; ENS, Lyon, France; IACS, Stony Brook University, NY, USA; IACS, Stony Brook University, NY, USA; Sandia Ntl. Lab., CA, USA; Dept. of Chemistry, Virignia Tech, VA, USA; ICL, University of Tennessee, TN, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",537,546,"Many domains of scientific simulation (chemistry, condensed matter physics, data science) increasingly eschew dense tensors for block-sparse tensors, sometimes with additional structure (recursive hierarchy, rank sparsity, etc.). Distributed-memory parallel computation with block-sparse tensorial data is paramount to minimize the time-to-solution (e.g., to study dynamical problems or for real-time analysis) and to accommodate problems of realistic size that are too large to fit into the host/device memory of a single node equipped with accelerators. Unfortunately, computation with such irregular data structures is a poor match to the dominant imperative, bulk-synchronous parallel programming model. In this paper, we focus on the critical element of block-sparse tensor algebra, namely binary tensor contraction, and report on an efficient and scalable implementation using the task-focused PaRSEC runtime. High performance of the block-sparse tensor contraction on the Summit supercomputer is demonstrated for synthetic data as well as for real data involved in electronic structure simulations of unprecedented size.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460455","electronic structure;tensor contraction;block-sparse matrix multiplication;distributed memory;multi-GPU nodes;PaRSEC","Tensors;Runtime;Computational modeling;Tools;Data models;Supercomputers;Real-time systems","distributed memory systems;graphics processing units;parallel programming;tensors","dense tensors;distributed-memory parallel computation;block-sparse tensorial data;irregular data structures;block-sparse tensor algebra;binary tensor contraction;electronic structure simulations;distributed-memory multiGPU block-sparse tensor contraction","",1.0,"",32.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Adaptive Spatially Aware I/O for Multiresolution Particle Data Layouts","W. Usher; X. Huang; S. Petruzza; S. Kumar; S. R. Slattery; S. T. Reeve; F. Wang; C. R. Johnson; V. Pascucci","SCI Institute, University of Utah.; SCI Institute, University of Utah.; Utah State University.; University of Alabama, Birmingham.; Oak Ridge National Laboratory.; Lawrence Livermore National Laboratory.; SCI Institute, University of Utah.; SCI Institute, University of Utah.; SCI Institute, University of Utah.","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",547,556,"Large-scale simulations on nonuniform particle distributions that evolve over time are widely used in cosmology, molecular dynamics, and engineering. Such data are often saved in an unstructured format that neither preserves spatial locality nor provides metadata for accelerating spatial or attribute subset queries, leading to poor performance of visualization tasks. Furthermore, the parallel I/O strategy used typically writes a file per process or a single shared file, neither of which is portable or scalable across different HPC systems. We present a portable technique for scalable, spatially aware adaptive aggregation that preserves spatial locality in the output. We evaluate our approach on two supercomputers, Stampede2 and Summit, and demonstrate that it outperforms prior approaches at scale, achieving up to 2.5 x faster writes and reads for nonuniform distributions. Furthermore, the layout written by our method is directly suitable for visual analytics, supporting low-latency reads and attribute-based filtering with little overhead.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00063","UT-Battelle; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460496","Parallel I/O;Load Balancing","Visual analytics;Layout;Data visualization;Writing;Supercomputers;Reproducibility of results;Task analysis","data visualisation;filtering theory;parallel processing","adaptive spatially aware;multiresolution particle data;large-scale simulations;nonuniform particle distributions;molecular dynamics;unstructured format;spatial locality;subset queries;visualization tasks;single shared file;HPC systems;portable technique;aware adaptive aggregation;nonuniform distributions","",2.0,"",43.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Interpreting Write Performance of Supercomputer I/O Systems with Regression Models","B. Xie; Z. Tan; P. Carns; J. Chase; K. Harms; J. Lofstead; S. Oral; S. S. Vazhkudai; F. Wang","Oak Ridge National Laboratory; Carnegie Mellon University; Argonne National Laboratory; Duke University; Argonne National Laboratory; Sandia National Laboratories; Oak Ridge National Laboratory; Oak Ridge National Laboratory; Oak Ridge National Laboratory","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",557,566,"This work seeks to advance the state of the art in HPC I/O performance analysis and interpretation. In particular, we demonstrate effective techniques to: (1) model output performance in the presence of I/O interference from production loads; (2) build features from write patterns and key parameters of the system architecture and configurations; (3) employ suitable machine learning algorithms to improve model accuracy. We train models with five popular regression algorithms and conduct experiments on two distinct production HPC platforms. We find that the lasso and random forest models predict output performance with high accuracy on both of the target systems. We also explore use of the models to guide adaptation in I/O middleware systems, and show potential for improvements of at least 15% from model-guided adaptation on 70% of samples, and improvements up to 10 x on some samples for both of the target systems.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00064","U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research; Office of Science; National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460466","high performance computing;I/O performance;machine learning","Training;Adaptation models;Machine learning algorithms;Computational modeling;Systems architecture;Production;Predictive models","learning (artificial intelligence);middleware;regression analysis","middleware systems;model-guided adaptation;target systems;regression models;performance analysis;production loads;system architecture;model accuracy;conduct experiments;distinct production HPC;random forest models","",3.0,"",26.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Finer-LRU: A Scalable Page Management Scheme for HPC Manycore Architectures","J. Bang; C. Kim; S. Kim; Q. Chen; C. Lee; E. -K. Byun; J. Lee; H. Eom","Department of Computer Science and Engineering, Seoul National University; Department of Computer Science and Engineering, Seoul National University; Department of Computer Science and Engineering, Seoul National University; Department of Computer Science and Engineering, Seoul National University; Department of Electronics and Information Engineering, Korea Aerospace University; Division of National Supercomputing, Korea Institute of Science and Technology Information; Department of Electronics and Information Engineering, Korea Aerospace University; Department of Computer Science and Engineering, Seoul National University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",567,576,"In HPC systems, the increasing need for a higher level of concurrency has led to packing more cores within a single chip. However, since multiple processes share memory space, the frequent access to resources in critical sections where only atomic operation has to be executed can result in poor performance. In this paper, we focus on reducing lock contention on the memory management system of an HPC manycore architecture. One of the critical sections causing severe lock contention in the I/O path is in the page management system, which uses multiple Least Recently Used (LRU) lists with a single lock instance. To solve this problem, we propose a Finer-LRU scheme, which optimizes the page reclamation process by splitting LRU lists into multiple sub-lists, each having its own lock instance. Our evaluation result shows that the Finer-LRU scheme can improve sequential write throughput by 57.03% and reduce latency by 98.94% compared to the baseline Linux kernel version 5.2.8 in the Intel Knights Landing (KNL) architecture.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00065","Korea Institute of Science and Technology Information; National Research Foundation of Korea; National Research Foundation; K2; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460503","High performance computing;Manycore architecture;Page reclamation process;Fine-grained lock","Concurrent computing;Distributed processing;Linux;Scalability;Instruction sets;Memory management;Data structures","cache storage;input-output programs;Linux;multiprocessing systems;operating system kernels;shared memory systems;storage management","HPC manycore architecture;critical sections;severe lock contention;page management system;single lock instance;Finer-LRU scheme;page reclamation process;splitting LRU lists;multiple sub-lists;Intel Knights Landing architecture;scalable page management scheme;HPC systems;single chip;multiple processes share memory space;frequent access;atomic operation;memory management system","",2.0,"",34.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Arbitration Policies for On-Demand User-Level I/O Forwarding on HPC Platforms","J. L. Bez; A. Miranda; R. Nou; F. Z. Boito; T. Cortes; P. Navaux","Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil; Barcelona Supercomputing Center (BSC), Barcelona, Spain; Barcelona Supercomputing Center (BSC), Barcelona, Spain; LaBRI, University of Bordeaux, Inria, CNRS, Bordeaux-INP, Bordeaux, France; Polytechnic University of Catalonia, Barcelona, Spain; Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",577,586,"I/O forwarding is a well-established and widely-adopted technique in HPC to reduce contention in the access to storage servers and transparently improve I/O performance. Rather than having applications directly accessing the shared parallel file system, the forwarding technique defines a set of I/O nodes responsible for receiving application requests and forwarding them to the file system, thus reshaping the flow of requests. The typical approach is to statically assign I/O nodes to applications depending on the number of compute nodes they use, which is not always necessarily related to their I/O requirements. Thus, this approach leads to inefficient usage of these resources. This paper investigates arbitration policies based on the applications I/O demands, represented by their access patterns. We propose a policy based on the Multiple-Choice Knapsack problem that seeks to maximize global bandwidth by giving more I/O nodes to applications that will benefit the most. Furthermore, we propose a user-level I/O forwarding solution as an on-demand service capable of applying different allocation policies at runtime for machines where this layer is not present. We demonstrate our approach's applicability through extensive experimentation and show it can transparently improve global I/O bandwidth by up to 85% in a live setup compared to the default static policy.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00066","Conselho Nacional de Desenvolvimento Científico e Tecnológico; Ministry of Economy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460487","I/O forwarding;allocation policy;MCKP","Distributed processing;Runtime;File systems;Bandwidth;Production;Dynamic scheduling;Supercomputers","knapsack problems;parallel processing;resource allocation;storage management;telecommunication traffic","arbitration policies;On-demand User-level;HPC platforms;contention;storage servers;shared parallel file system;forwarding technique;application requests;compute nodes;access patterns;Multiple-Choice Knapsack problem;global bandwidth;forwarding solution;on-demand service;allocation policies;default static policy","",1.0,"",22.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"A Hybrid Scheduling Scheme for Parallel Loops","A. Handleman; A. G. Rattew; I. -T. A. Lee; T. B. Schardl","Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis; Massachusetts Institute of Technology","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",587,598,"Parallel loops are commonly used parallel constructs to parallelize high-performance scientific applications. In the paradigm of task parallelism, the parallel loop construct is used to express the logical parallelism of the loop, indicating that the iterations in a loop are logically in parallel and let an underlying runtime scheduler determines how to best map the parallel iterations onto available processing cores. Researchers have investigated multiple scheduling schemes for scheduling parallel loops, with the static partitioning and dynamic partitioning being most prevalent. Static partitioning obtains low scheduling overhead while potentially retaining locality benefit in iterative applications that perform a sequence of parallel loops that access the same set of data repeatedly. But static partitioning may perform poorly relatively to dynamic partitioning if the loop iterations contain unbalanced workloads or if the cores can arrive at the loops in different times. We propose a hybrid scheduling scheme, which first schedules loops using static partitioning but then employs dynamic partitioning when load balancing is necessary. Moreover, the work distribution employs a claiming heuristic that allows a core to check for partitions to work on in a semi-deterministic fashion, allowing the scheduling to better retain data locality in the case of iterative applications. Unlike prior work that optimizes for iterative applications, our scheme does not require programmer annotations and can provide provably efficient execution time. In this paper, we discuss the hybrid scheme, prove its correctness, and analyze its scheduling bound. We have also implemented the proposed scheme in a Cilk-based work-stealing platform and experimentally verified that the scheme load balances well and can retain locality for such iterative applications.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00067","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460531","static partitioning;dyanmic partitioning;work stealing;work;span","Schedules;Distributed processing;Runtime;Annotations;Scalability;Parallel processing;Dynamic scheduling","iterative methods;multiprocessing systems;multi-threading;parallel programming;processor scheduling;program control structures;resource allocation;scheduling","static partitioning;dynamic partitioning;iterative applications;hybrid scheduling scheme;parallel loop;task parallelism;logical parallelism;runtime scheduler;parallel iterations;multiple scheduling schemes;scheduling parallel loops;loop iterations","",1.0,"",39.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"EAGLE: Expedited Device Placement with Automatic Grouping for Large Models","H. Lan; L. Chen; B. Li","University of Toronto; University of Louisiana at Lafayette; University of Toronto","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",599,608,"Advanced deep neural networks with large sizes are usually trained on a mixture of devices, including multiple CPUs and GPUs. The model training speed and efficiency are drastically impacted by the placement of operations on devices. To identify the optimal device placement, the state-of-the-art method is based on reinforcement learning with a hierarchical model, which partitions the operations into groups and then assigns each group to specific devices. However, due to the additional dimension of grouping decisions coupled with the placement, the reinforcement learning efficiency is greatly reduced. With modern neural networks growing in size and complexity, the issue of low efficiency and high cost in device placement is further aggravated. In this paper, we propose our design of EAGLE (Expedited Automatic Grouping for Large modEls), which integrates automatic grouping into reinforcement learning-based placement in an optimal way, to achieve the best possible training time performance for very large models. An extra RNN is introduced to transform parameters of the grouper into inputs of the placer, linking the originally separated parts together. Further optimizations have also been made in the network inputs. We have deployed and extensively evaluated EAGLE on InceptionV3, GNMT and BERT benchmarks. Compared with the state-of-the-art, the performance achieved by our design, measured by the per-step time with the resulted placement, is 2.7% and 18.7% better for GNMT and BERT, respectively. For Inception-V3, our design achieves the fastest speed in discovering the optimal placement.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00068","Huawei Technologies; Louisiana Board of Regents; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460520","device placement;reinforcement learning;neural networks","Training;Performance evaluation;Computational modeling;Neural networks;Bit error rate;Reinforcement learning;Transforms","deep learning (artificial intelligence);recurrent neural nets","possible training time performance;network inputs;EAGLE;resulted placement;optimal placement;automatic grouping;advanced deep neural networks;optimal device placement;hierarchical model;grouping decisions;reinforcement learning efficiency;modern neural networks;reinforcement learning-based placement","",3.0,"",28.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"BiPS: Hotness-aware Bi-tier Parameter Synchronization for Recommendation Models","Q. Zheng; Q. Chen; K. Bai; H. Guo; Y. Gao; X. He; M. Guo","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Huawei Technologies Ltd, Shenzhen, China; Huawei Technologies Ltd, Shenzhen, China; Huawei Technologies Ltd, Shenzhen, China; Shanghai Jiao Tong University, Shanghai, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",609,618,"While current deep learning frameworks are mainly optimized for dense-accessed models, they show low throughput and poor scalability in training sparse-accessed recommendation models. Our investigation shows that the poor performance is due to the parameter synchronization bottleneck. We therefore propose BiPS, a bi-tier parameter synchronization system that alleviates the parameter update and the sparse-accessed parameters communication bottleneck. BiPS includes a bi-tier parameter server that accelerates the traditional CPU-based parameter update process, a hotness-aware parameter placement and communication policy to balance the workloads between CPU and GPU and optimize the communication of sparse-accessed parameters. BiPS overlaps the worker computation with the synchronization stage to enable parameter updates in advance. We implement BiPS and incorporate it into mainstream DL frameworks including TensorFlow, MXNet, and PyTorch. The experimental results based on various deep learning frameworks show that BiPS greatly speeds up the training of recommenders (5 - 9$\times$) as the model scale increases, without degrading the accuracy.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460524","","Training;Deep learning;Distributed processing;Scalability;Graphics processing units;Computer architecture;Throughput","deep learning (artificial intelligence);graphics processing units;recommender systems;resource allocation;synchronisation","BiPS;deep learning;dense-accessed models;parameter synchronization bottleneck;sparse-accessed parameters communication bottleneck;bi-tier parameter server;hotness-aware parameter placement;recommenders;hotness-aware bi-tier parameter synchronization;CPU-based parameter update;sparse-accessed recommendation;recommendation models;communication policy;GPU;worker computation;TensorFlow;MXNet;PyTorch;workload balance","","","",35.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"DSXplore: Optimizing Convolutional Neural Networks via Sliding-Channel Convolutions","Y. Wang; B. Feng; Y. Ding","Department of Computer Science, University of California, Santa Barbara; Department of Computer Science, University of California, Santa Barbara; Department of Computer Science, University of California, Santa Barbara","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",619,628,"As the key advancement of the convolutional neural networks (CNNs), depthwise separable convolutions (DSCs) are becoming one of the most popular techniques to reduce the computations and parameters size of CNNs meanwhile maintaining the model accuracy. It also brings profound impact to improve the applicability of the compute- and memory-intensive CNNs to a broad range of applications, such as mobile devices, which are generally short of computation power and memory. However, previous research in DSCs are largely focusing on compositing the limited existing DSC designs, thus, missing the opportunities to explore more potential designs that can achieve better accuracy and higher computation/parameter reduction. Besides, the off-the-shelf convolution implementations offer limited computing schemes, therefore, lacking support for DSCs with different convolution patterns.To this end, we introduce, DSXplore, the first optimized design for exploring DSCs on CNNs. Specifically, at the algorithm level, DSXplore incorporates a novel factorized kernel-sliding-channel convolution (SCC), featured with input-channel overlapping to balance the accuracy performance and the reduction of computation and memory cost. SCC also offers enormous space for design exploration by introducing adjustable kernel parameters. Further, at the implementation level, we carry out an optimized GPU-implementation tailored for SCC by leveraging several key techniques, such as the input-centric backward design and the channel-cyclic optimization. Intensive experiments on different datasets across mainstream CNNs show the advantages of DSXplore in balancing accuracy and computation/parameter reduction over the standard convolution and the existing DSCs.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00070","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460557","Deep Learning;Convolutional Neural Networks;GPU","Technological innovation;Focusing;Mobile handsets;Space exploration;Classification algorithms;Convolutional neural networks;Kernel","convolutional neural nets;power aware computing;storage management","input-channel overlapping;memory cost reduction;GPU;CNN optimization;DSXplore;standard convolution;convolutional neural network optimization;sliding-channel convolutions;depthwise separable convolutions;computation power reduction;DSC;off-the-shelf convolution;factorized kernel-sliding-channel convolution;factorized kernel SCC","",3.0,"",22.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"SUPER: SUb-Graph Parallelism for TransformERs","A. Jain; T. Moon; T. Benson; H. Subramoni; S. A. Jacobs; D. K. Panda; B. V. Essen","Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, California, USA; Global Security Computing Applications Division, Lawrence Livermore National Laboratory, Livermore, California, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, California, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, California, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, Ohio, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, California, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",629,638,"Transformer models have revolutionized the field of Natural Language Processing (NLP) and they achieve state-of-the-art performance in applications like machine translation, question answering, regression, and summarization. However, training Transformers is challenging because of their large memory and compute requirements. The literature contains several approaches to parallelize training, like layer parallelism and pipeline parallelism, but they are optimized to benefit out-of-core models and they don’t exploit the inherent parallelism in Transformer models. Other work uses model parallelism to achieve weak scaling by increasing the model size. In this paper, we propose sub-graph parallelism that provides a significant performance improvement over pure data parallelism with a fixed number of resources, and as an additional technique for strong- and weak-scaling without increasing model capacity. Our technique accelerates the training of Transformer models and we generalize the concept to any neural network with multiple branches. We optimize the communication for sub-graph parallelism and combine it with data parallelism to scale performance up to 1024 GPUs. To decrease communication overheads, we propose a topology-aware scheme that limits inter-node communication. Finally, we empirically compare sub-graph parallelism with pure data parallelism and demonstrate its performance benefits in end-to-end training.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460532","scalable deep learning;transformers;sub-graph aparallelism;algorithms;deep learning;distributed training","Training;Machine learning algorithms;Computational modeling;Neural networks;Pipelines;Memory management;Parallel processing","graph theory;language translation;learning (artificial intelligence);natural language processing;neural nets;parallel architectures;parallel processing;telecommunication network topology","sub-graph parallelism;Transformer models;training Transformers;layer parallelism;pipeline parallelism;out-of-core models;inherent parallelism;model parallelism;pure data parallelism;model capacity","",4.0,"",27.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Scalable Epidemiological Workflows to Support COVID-19 Planning and Response","D. Machi; P. Bhattacharya; S. Hoops; J. Chen; H. Mortveit; S. Venkatramanan; B. Lewis; M. Wilson; A. Fadikar; T. Maiden; C. L. Barrett; M. V. Marathe","Biocomplexity Institute and Initiative, University of Virginia; Biocomplexity Institute and Initiative, University of Virginia; Biocomplexity Institute and Initiative, University of Virginia; Biocomplexity Institute and Initiative, University of Virginia; Department of Engineering Systems and Environment, University of Virginia; Biocomplexity Institute and Initiative, University of Virginia; Biocomplexity Institute and Initiative, University of Virginia; Biocomplexity Institute and Initiative, University of Virginia; Argonne National Laboratory; Pittsburgh Supercomputing Center; Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",639,650,"The COVID-19 global outbreak represents the most significant epidemic event since the 1918 influenza pandemic. Simulations have played a crucial role in supporting COVID-19 planning and response efforts. Developing scalable workflows to provide policymakers quick responses to important questions pertaining to logistics, resource allocation, epidemic forecasts and intervention analysis remains a challenging computational problem. In this work, we present scalable high performance computing-enabled workflows for COVID-19 pandemic planning and response. The scalability of our methodology allows us to run fine-grained simulations daily, and to generate county-level forecasts and other counterfactual analysis for each of the 50 states (and DC), 3140 counties across the USA. Our workflows use a hybrid cloud/cluster system utilizing a combination of local and remote cluster computing facilities, and using over 20,000 CPU cores running for 6-9 hours every day to meet this objective. Our state (Virginia), state hospital network, our university, the DOD and the CDC use our models to guide their COVID-19 planning and response efforts. We began executing these pipelines March 25, 2020, and have delivered and briefed weekly updates to these stakeholders for over 30 weeks without interruption.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00072","National Institutes of Health; Centers for Disease Control and Prevention; University of Virginia; Google; Defense Threat Reduction Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460555","COVID-19;Epidemic Modeling;HPC Workflow Development","COVID-19;Analytical models;Pandemics;Computational modeling;Surveillance;Scalability;Tools","cloud computing;data visualisation;diseases;emergency services;health care;hospitals;medical computing;medical information systems;resource allocation","remote cluster computing facilities;local cluster computing facilities;scalable high performance computing-enabled workflows;epidemic forecasts;policymakers quick responses;scalable workflows;response efforts;1918 influenza pandemic;significant epidemic event;COVID-19 global outbreak;scalable epidemiological workflows;time 6.0 hour to 9.0 hour","",4.0,"",38.0,"USGov","28 Jun 2021","","","IEEE","IEEE Conferences"
"Facilitating Data Discovery for Large-scale Science Facilities using Knowledge Networks","Y. Qin; I. Rodero; M. Parashar","Rutgers Discovery Informatics Institute, Rutgers University, New Brunswick, New Jersey, USA; Rutgers Discovery Informatics Institute, Rutgers University, New Brunswick, New Jersey, USA; Scientific Computing Imaging Institute, University of Utah, Salt Lake City, Utah, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",651,660,"Large-scale multiuser scientific facilities, such as geographically distributed observatories, remote instruments, and experimental platforms, represent some of the largest national investments and can enable dramatic advances across many areas of science. Recent examples of such advances include the detection of gravitational waves and the imaging of a black hole's event horizon. However, as the number of such facilities and their users grow, along with the complexity, diversity, and volumes of their data products, finding and accessing relevant data is becoming increasingly challenging, limiting the potential impact of facilities. These challenges are further amplified as scientists and application workflows increasingly try to integrate facilities' data from diverse domains. In this paper, we leverage concepts underlying recommender systems, which are extremely effective in e-commerce, to address these data-discovery and data-access challenges for large-scale distributed scientific facilities. We first analyze data from facilities and identify and model user-query patterns in terms of facility location and spatial localities, domain-specific data models, and user associations. We then use this analysis to generate a knowledge graph and develop the collaborative knowledge-aware graph attention network (CKAT) recommendation model, which leverages graph neural networks (GNNs) to explicitly encode the collaborative signals through propagation and combine them with knowledge associations. Moreover, we integrate a knowledge-aware neural attention mechanism to enable the CKAT to pay more attention to key information while reducing irrelevant noise, thereby increasing the accuracy of the recommendations. We apply the proposed model on two real-world facility datasets and empirically demonstrate that the CKAT can effectively facilitate data discovery, significantly outperforming several compelling state-of-the-art baseline models.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00073","National Aeronautics and Space Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460512","Large scale facility;Observatory;Knowledge networks;Recommender system;Data discovery","Knowledge engineering;Analytical models;Observatories;Limiting;Instruments;Collaboration;Data models","data analysis;data models;electronic commerce;graph theory;groupware;neural nets;query processing;recommender systems","data-access challenges;large-scale distributed scientific facilities;user-query patterns;facility location;domain-specific data models;user associations;collaborative knowledge-aware graph attention network;graph neural networks;knowledge associations;knowledge-aware neural attention mechanism;data discovery;knowledge networks;large-scale multiuser scientific facilities;geographically distributed observatories;black hole;data products;CKAT recommendation model;e-commerce","",1.0,"",42.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Optimal Task Assignment for Heterogeneous Federated Learning Devices","L. Lima Pilla","Laboratoire de Recherche en Informatique (LRI), Univ. Paris-Saclay, CNRS, Orsay, France","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",661,670,"Federated Learning provides new opportunities for training machine learning models while respecting data privacy. This technique is based on heterogeneous devices that work together to iteratively train a model while never sharing their own data. Given the synchronous nature of this training, the performance of Federated Learning systems is dictated by the slowest devices, also known as stragglers. In this paper, we investigate the problem of minimizing the duration of Federated Learning rounds by controlling how much data each device uses for training. We formulate this as a makespan minimization problem with identical, independent, and atomic tasks that have to be assigned to heterogeneous resources with non-decreasing cost functions, while also respecting lower and upper limits of tasks per resource. Based on this formulation, we propose a polynomial-time algorithm named OLAR and prove that it provides optimal schedules. We evaluate OLAR in an extensive series of experiments using simulation that includes comparisons to other algorithms from the state of the art, and new extensions to them. Our results indicate that OLAR provides optimal solutions with a small execution time. They also show that the presence of lower and upper limits of tasks per resource erase any benefits that suboptimal heuristics could provide in terms of algorithm execution time.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460535","Task Assignment;Scheduling;Federated Learning;Makespan Minimization;Proof of Optimality;Simulation","Training;Performance evaluation;Distributed processing;Heuristic algorithms;Optimal scheduling;Machine learning;Collaborative work","computational complexity;data privacy;learning (artificial intelligence);minimisation;scheduling","optimal task assignment;heterogeneous federated learning devices;machine learning;data privacy;heterogeneous devices;federated learning systems;makespan minimization;identical tasks;independent tasks;atomic tasks;heterogeneous resources;polynomial-time algorithm;OLAR;optimal schedules;optimal solutions;federated learning rounds;stragglers;nondecreasing cost functions;suboptimal heuristics;algorithm execution time","",6.0,"",21.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Detecting Malicious Model Updates from Federated Learning on Conditional Variational Autoencoder","Z. Gu; Y. Yang","College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",671,680,"In federated learning, the central server combines local model updates from the clients in the network to create an aggregated model. To protect clients' privacy, the server is designed to have no visibility into how these updates are generated. The nature of federated learning makes detecting and defending against malicious model updates a challenging task. Unlike existing works that struggle to defend against Byzantine clients, the paper considers defending against targeted model poisoning attack in the federated learning setting. The adversary aims to reduce the model performance on targeted subtasks while maintaining the main task's performance. This paper proposes Fedcvae, a robust and unsupervised federated learning framework where the central server uses conditional variational autoencoder to detect and exclude malicious model updates. Since the reconstruction error of malicious updates is much larger than that of benign ones, it can be used as an anomaly score. We formulate a dynamic threshold of reconstruction error to differentiate malicious updates from normal ones based on this idea. Fedcvae is tested with extensive experiments on IID and non-IID federated benchmarks, showing a competitive performance over existing aggregation methods under Byzantine attack and targeted model poisoning attack.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460523","federated learning;anomaly detection","Privacy;Distributed processing;Benchmark testing;Collaborative work;Servers;Task analysis;Anomaly detection","security of data;unsupervised learning","malicious model updates detection;differentiate malicious updates;conditional variational autoencoder;central server;unsupervised federated learning framework;robust learning framework;targeted model poisoning attack;Byzantine clients;aggregated model;local model updates","",6.0,"",35.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Is Asymptotic Cost Analysis Useful in Developing Practical Parallel Algorithms","G. Blelloch","Carnegie Mellon University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",681,681,"Summary form only given, as follows. Asymptotic analysis of runtime, and space, has been the cornerstone in the development of practical sequential algorithms. The analysis is not meant to predict runtimes on any particular machine, but rather to guide algorithm designers and implementors in the right direction, and let them better understand how algorithms might scale. Quicksort, Dijkstra's algorithm, dynamic programming, depth first search, for example, are fast in theory and in practice–-and scale pretty much as the theory predicts. All CS undergrads learn about these algorithms and techniques, and they are broadly implemented in many widely used libraries and applications. Over the years models have been extended to include the benefits of locality, allowing for a refined asymptotic analysis when needed. Unfortunately the jury is still out on the role of asymptotic analysis in the practice of parallel algorithms. There is no lack of theoretical work on analyzing the cost of parallel algorithms, with such work dating back almost fifty years, but these ideas have not been widely adopted in practice. There are various possible reasons, including inaccurate cost models, giant constants in the big-O, no adequate programming languages, lack of education on the topic, or perhaps parallel machines are just too complicated to model theoretically and we should just give up. In this talk I will describe how simple parallel models can be useful for developing practical parallel algorithms, and many of the theoretical ideas in parallel algorithms are useful, at least in the context of shared-memory multicore machines. I will cover work on developing practically efficient algorithms for a wide variety of applications of graphs, trees, computational geometry, and strings. As with sequential algorithms, the basic model does not account for locality, but I will discuss some simple ways to extend it.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460506","","Parallel algorithms;Prediction algorithms;Heuristic algorithms;Computer languages;Runtime;Computational modeling;Programming profession","computational complexity;computational geometry;dynamic programming;graph theory;parallel algorithms;shared memory systems","refined asymptotic analysis;widely used libraries;depth first search;Dijkstra's algorithm;algorithm designers;runtime;practical sequential algorithms;asymptotic cost analysis;practically efficient algorithms;practical parallel algorithms;simple parallel models;parallel machines;inaccurate cost models","","","","","IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"From Parallelization to Customization – Challenges and Opportunities","J. Cong","UCLA Computer Science Department, Center for Domain-Specific Computing (CDSC)","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",682,682,"With large-scale deployment of FPGAs in both private and public clouds in the past a few years, customizable computing is transitioning from advanced research into mainstream computing. In this talk, I shall first showcase a few big data and machine learning applications that benefit significantly from customization. Next, I shall discuss the challenges of FPGA programming for the efficient accelerator designs, which presents a significant barrier to many software programmers, despite the recent advances in high-level synthesis. Then, I shall highlight our recent progress on automated compilation for customized archictectures, such as systolic arrays, stencils, and more general CPPs (composable parallel and pipelined) architectures. I shall also present our ongoing work on HeteroCL, a highly productive multi-paradigm programming framework targeting accelerator-rich heterogeneous architectures, and is being used as a focal point to integrate various optimizaiton techniques and support high-level domain-specific languages (DSL) such as Halide and Pytorch. Our goal is to “demacratize customizable computing” so that most (if not all) software programmers can design optimized accelerators on FPGAs.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460515","","Distributed processing;Cloud computing;Computer architecture;Machine learning;Programming;Big Data;Software","Big Data;field programmable gate arrays;parallel programming;program compilers;specification languages","high-level domain-specific languages;customizable computing;software programmers;optimized accelerators;large-scale deployment;private clouds;public clouds;big data;machine learning applications;FPGA programming;efficient accelerator designs;significant barrier;high-level synthesis;automated compilation;customized archictectures;systolic arrays;general CPPs;composable parallel;highly productive multiparadigm programming framework;heterogeneous architectures","",1.0,"",0.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"High Performance Streaming Tensor Decomposition","Y. Soh; P. Flick; X. Liu; S. Smith; F. Checconi; F. Petrini; J. Choi","Computer & Info. Science, University of Oregon, Eugene, OR, USA; Google, Facebook, Microsoft; Google, Facebook, Microsoft; Google, Facebook, Microsoft; Parallel Computing Lab, Intel, Santa Clara, CA, USA; Parallel Computing Lab, Intel, Santa Clara, CA, USA; Computer & Info. Science, University of Oregon, Eugene, OR, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",683,692,"We present a new algorithm for computing tensor decomposition on streaming data that achieves up to 102× speedup over the state-of-the-art CP-stream algorithm through lower computational complexity and performance optimization. For each streaming time slice, our algorithm partitions the factor matrix rows into those with and without updates and keeps them in Gram matrix form to significantly reduce the required computation. We also improve the scalability and performance of the matricized tensor times Khatri-Rao product (MTTKRP) kernel, a key performance bottleneck in many tensor decomposition algorithms, by reducing the synchronization overhead through the combined use of mutex locks and thread-local memory. For problems with constraints (e.g., non-negativity), we apply data blocking and operation fusion to the alternating direction method of multiplier (ADMM) kernel in the constrained CP-stream algorithm. By combining this ADMM optimization with the aforementioned MTTKRP optimization, our improved algorithm achieves up to 47× speedup over the original. We evaluate the performance and scalability of our new algorithm and optimization techniques using a 56-core quad-socket Intel Xeon system on four representative real-world tensors.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460519","tensor decomposition;streaming;high performance;algorithm","Distributed processing;Tensors;Scalability;Instruction sets;Convex functions;Partitioning algorithms;Matrix decomposition","computational complexity;convex programming;matrix decomposition;matrix multiplication;microprocessor chips;multiprocessing systems;tensors","algorithm partitions;factor matrix rows;Gram matrix form;matricized tensor times Khatri-Rao product kernel;key performance bottleneck;tensor decomposition algorithms;ADMM optimization;optimization techniques;real-world tensors;high performance streaming tensor decomposition;performance optimization;streaming time slice;MTTKRP optimization;CP-stream algorithm;alternating direction method of multiplier kernel","","","",20.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Plex: Scaling Parallel Lexing with Backtrack-Free Prescanning","L. Li; S. Sato; Q. Liu; K. Taura","The University of Tokyo, Tokyo, Japan; The University of Tokyo, Tokyo, Japan; The University of Tokyo; The University of Tokyo, Tokyo, Japan","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",693,702,"Lexical analysis, which converts input text into a list of tokens, plays an important role in many applications, including compilation and data extraction from texts. To recognize token patterns, a lexer incorporates a sequential computation model - automaton as its basic building component. As such, it is considered difficult to parallelize due to the inherent data dependency. Much work has been done to accelerate lexical analysis through parallel techniques. Unfortunately, existing attempts mainly rely on language-specific remedies for input segmentation, which makes it not only tricky for language extension, but also challenging for automatic lexer generation. This paper presents Plex - an automated tool for generating parallel lexers from user-defined grammars. To overcome the inherent sequentiality, Plex applies a fast prescanning phase to collect context information prior to scanning. To reduce the overheads brought by prescanning, Plex adopts a special automaton, which is derived from that of the scanner, to avoid backtracking behavior and exploits data-parallel techniques. The evaluation under several languages shows that the prescanning overhead is small, and consequently Plex is scalable and achieves 9.8-11.5X speedups using 18 threads.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460518","Lexical Analysis;Finite Automaton;Parallelism","Context;Backtracking;Distributed processing;Instruction sets;Automata;Tools;Parallel processing","data handling;grammars;parallel processing","data-parallel techniques;special automaton;user-defined grammars;parallel lexers;automated tool;automatic lexer generation;language extension;language-specific remedies;data dependency;sequential computation model;token patterns;data extraction;lexical analysis;backtrack-free prescanning;scaling parallel lexing;Plex","","","",35.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Speculative Parallel Reverse Cuthill-McKee Reordering on Multi- and Many-core Architectures","D. Mlakar; M. Winter; M. Parger; M. Steinberger","Graz University of Technology, Austria; Graz University of Technology, Austria; Graz University of Technology, Austria; Graz University of Technology, Austria","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",703,713,"Bandwidth reduction of sparse matrices is used to reduce fill-in of linear solvers and to increase performance of other sparse matrix operations, e.g., sparse matrix vector multiplication in iterative solvers. To compute a bandwidth reducing permutation, Reverse Cuthill-McKee (RCM) reordering is often applied, which is challenging to parallelize, as its core is inherently serial. As many-core architectures, like the GPU, offer subpar single-threading performance and are typically only connected to high-performance CPU cores via a slow memory bus, neither computing RCM on the GPU nor moving the data to the CPU are viable options. Nevertheless, reordering matrices, potentially multiple times in-between operations, might be essential for high throughput. Still, to the best of our knowledge, we are the first to propose an RCM implementation that can execute on multicore CPUs and many-core GPUs alike, moving the computation to the data rather than vice versa.Our algorithm parallelizes RCM into mostly independent batches of nodes. For every batch, a single CPU-thread/a GPU thread-block speculatively discovers child nodes and sorts them according to the RCM algorithm. Before writing their permutation, we re-evaluate the discovery and build new batches. To increase parallelism and reduce dependencies, we create a signaling chain along successive batches and introduce early signaling conditions. In combination with a parallel work queue, new batches are started in order and the resulting RCM permutation is identical to the ground-truth single-threaded algorithm.We propose the first RCM implementation that runs on the GPU. It achieves several orders of magnitude speed-up over NVIDIA's single-threaded cuSolver RCM implementation and is significantly faster than previous parallel CPU approaches. Our results are especially significant for many-core architectures, as it is now possible to include RCM reordering into sequences of sparse matrix operations without major performance loss.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460553","many-core;multicore;CPU;GPU;Reverse Cuthill-McKee;scheduling;work distribution","Runtime;Multicore processing;Graphics processing units;Computer architecture;Bandwidth;Parallel processing;Writing","graphics processing units;iterative methods;matrix algebra;matrix multiplication;multiprocessing systems;multi-threading;parallel algorithms;parallel architectures;parallel programming;sparse matrices","speculative parallel reverse Cuthill-McKee;many-core architectures;bandwidth reduction;sparse matrices;linear solvers;sparse matrix operations;sparse matrix vector multiplication;iterative solvers;bandwidth reducing permutation;Cuthill-McKee reordering;offer;single-threading performance;high-performance CPU;slow memory bus;reordering matrices;potentially multiple times;many-core GPUs;independent batches;single CPU-thread;GPU thread-block;RCM algorithm;parallelism;successive batches;parallel work queue;resulting RCM permutation;ground-truth single-threaded algorithm;NVIDIA's single-threaded cuSolver RCM implementation;previous parallel CPU approaches;RCM reordering","","","",35.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Jigsaw: A Slice-and-Dice Approach to Non-uniform FFT Acceleration for MRI Image Reconstruction","B. L. West; J. A. Fessler; T. F. Wenisch","University of Michigan; University of Michigan; University of Michigan","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",714,723,"The Fast Fourier Transform (FFT) is a fundamental algorithm in signal processing; significant efforts have been made to improve its performance using software optimizations and specialized hardware accelerators. Computational imaging modalities, such as MRI, often rely on the Non-uniform Fast Fourier Transform (NuFFT), a variant of the FFT for processing data acquired from non-uniform sampling patterns. The most time-consuming step of the NuFFT algorithm is “gridding;” wherein non-uniform samples are interpolated to allow a uniform FFT to be computed over the data. Each non-uniform sample affects a window of non-contiguous memory locations, resulting in poor cache and memory bandwidth utilization. As a result, gridding can account for more than 99.6% of the NuFFT computation time, while the FFT requires less than 0.4%. We present Slice-and-Dice, a novel approach to the NuFFT's gridding step that eliminates the presorting operations required by prior methods and maps more efficiently to hardware. Our GPU implementation achieves gridding speedups of over 250× and 16× vs prior state-of-the-art CPU and GPU implementations, respectively. We achieve further speedup and energy efficiency gains by implementing Slice-and-Dice in hardware with JIGSAW, a streaming hardware accelerator for non-uniform data gridding. JIGSAW uses stall-free fixed-point pipelines to process M non-uniform samples in approximately M cycles, irrespective of sampling pattern-yielding speedups of over 1500× the CPU baseline and 36× the state-of-the-art GPU implementation, consuming $\sim 200\mathrm{m}\mathrm{W}$ power and $\sim 12\mathrm{m}\mathrm{m}^{2}$ area in 16 nm technology. Slice-and-Dice GPU and JIGSAW ASIC implementations achieve unprecedented end-to-end NuFFT speedups of 8× and 36× compared to the state-of-the-art GPU implementation, respectively.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460544","Fast Fourier Transforms;Signal Processing;Parallel Computing;Hardware Accelerators","Runtime;Power demand;Fast Fourier transforms;Magnetic resonance imaging;Software algorithms;Graphics processing units;Signal processing algorithms","application specific integrated circuits;biomedical MRI;computer graphic equipment;coprocessors;fast Fourier transforms;image reconstruction;interpolation;medical image processing","nonuniform FFT acceleration;nonuniform data gridding;streaming hardware accelerator;gridding speedups;NuFFT computation time;memory bandwidth utilization;noncontiguous memory locations;nonuniform sample;NuFFT algorithm;nonuniform sampling patterns;nonuniform fast Fourier transform;computational imaging modalities;specialized hardware accelerators;signal processing;MRI image reconstruction;JIGSAW ASIC implementations;GPU implementation","","","",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Rank Position Forecasting in Car Racing","B. Peng; J. Li; S. Akkas; T. Araki; O. Yoshiyuki; J. Qiu","Indiana University; Indiana University; Indiana University; NEC Corporation, Japan; NEC Corporation, Japan; Indiana University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",724,733,"Rank position forecasting in car racing is a challenging problem when using a Deep Learning-based model over time-series data. It is featured with highly complex global dependency among the racing cars, with uncertainty resulted from existing and external factors; and it is also a problem with data scarcity. Existing methods, including statistical models, machine learning regression models, and several state-of-the-art deep forecasting models all perform not well on this problem. By an elaborate analysis of pit stop events, we find it critical to decompose the cause-and-effect relationship and model the rank position and pit stop events separately. In choosing a sub-model from different neural network models, we find the model with weak assumptions on the global dependency structure performs the best. Based on these observations, we propose RankNet, a combination of the encoder-decoder network and a separate Multilayer Perception network that is capable of delivering probabilistic forecasting to model the pit stop events and rank position in car racing. Further with the help of feature optimizations, RankNet demonstrates a significant performance improvement, where MAE improves 19% in two laps forecasting task and 7% in the stint forecasting task over the best baseline and is also more stable when adapting to unseen new data. Details of the model optimizations and performance profiling are presented. It is promising to provide useful interactions of neural networks in forecasting racing cars and shine a light on solutions to similar challenging issues in general forecasting problems.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460538","","Deep learning;Uncertainty;Neural networks;Predictive models;Probabilistic logic;Data models;Automobiles","deep learning (artificial intelligence);forecasting theory;multilayer perceptrons;neural nets;probability;regression analysis","RankNet;MAE;cause-and-effect relationship;external factor;pit stop events;general forecasting problems;forecasting racing cars;neural networks;model optimizations;unseen new data;stint forecasting task;probabilistic forecasting;separate Multilayer Perception network;encoder-decoder network;global dependency structure;neural network models;state-of-the-art deep forecasting models;machine learning regression models;statistical models;data scarcity;existing factors;highly complex global dependency;time-series data;Deep Learning-based model;car racing;rank position forecasting","",1.0,"",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Towards Practical Cloud Offloading for Low-cost Ground Vehicle Workloads","Y. Xu; T. Zhang; J. Han; S. Wang; Y. Bao","Peng Cheng Laboratory; Nanyang Technological University; Peng Cheng Laboratory; Peng Cheng Laboratory; Peng Cheng Laboratory","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",734,745,"Low-cost Ground Vehicles (LGVs) have been widely adopted to conduct various tasks in our daily life. However, the limited on-board battery capacity and computation resources prevent LGVs from taking more complex and intelligent workloads. A promising approach is to offload the computation from local LGVs to remote servers. However, current cloud-robotic research and platforms are still at a very early stage. Compared to other systems and devices, optimizing LGV workload offloading faces more challenges, such as the uncertainty of environments and the mobility feature of devices.In this paper, we explore the opportunities of optimizing cloud offloading of LGV workloads from the perspectives of performance, energy efficiency and network robustness. We first build an analytical model to reveal the computation role and impact of each function in LGV workloads. Then we propose several optimization strategies (fine-grained migration, cloud acceleration, real-time monitoring and adjustment) to accelerate workload computation, reduce on-board energy consumption, and increase the network robustness. We implement an end-to-end cloud-robotic framework with such strategies to achieve dynamic and adaptive offloading. Evaluations on physical LGVs show that our strategies can significantly reduce the total energy consumption by 2.12× and mission completion time by 2.53×, and maintain strong robust ness under poor network quality.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00083","Research and Development; National Natural Science Foundation of China; Chinese Academy of Sciences; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460507","performance analysis;adaptive offloading;cloud acceleration;network robustness","Energy consumption;Analytical models;Uncertainty;Robustness;Land vehicles;Energy efficiency;Servers","cloud computing;mobile computing;mobile robots","end-to-end cloud-robotic framework;dynamic offloading;adaptive offloading;physical LGVs;towards practical cloud offloading;low-cost Ground vehicle;Low-cost Ground Vehicles;on-board battery capacity;complex workloads;intelligent workloads;local LGVs;current cloud-robotic research;LGV workload offloading;optimizing cloud offloading;LGV workloads;network robustness;computation role;optimization strategies;cloud acceleration;workload computation;on-board energy consumption","","","",69.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Towards Internet-Scale Convolutional Root-Cause Analysis with DIAGNET","L. Bonniot; C. Neumann; F. Taïani","Inria, CNRS, IRISA, Univ Rennes; InterDigital; Inria, CNRS, IRISA, Univ Rennes","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",746,755,"Diagnosing problems in Internet-scale services remains particularly difficult and costly for both content providers and ISPs. Because the Internet is decentralized, the cause of such problems might lie anywhere between a user's device and the datacenters hosting the service. Further, the set of possible problems and causes is not known in advance, making it impossible in practice to train a classifier with all combinations of problems, causes and locations.In this paper, we explore how machine learning techniques can be used for Internet-scale root cause analysis based on measurements taken from end-user devices. Using convolutional neural networks, we show how to build generic models that (i) are agnostic to the underlying network topology, (ii) do not require to define the full set of possible causes during training, and (iii) can be quickly adapted to diagnose new services. We evaluate our proposal, DIAGNET, on a geodistributed multi-cloud deployment of online services, using a combination of fault injection and emulated clients running within automated browsers. Our experiments demonstrate the promising capabilities of our technique, delivering a recall of 73.9%, including on causes that were unknown at training time.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460522","","Training;Knowledge engineering;Root cause analysis;Distributed processing;Network topology;Logic gates;Proposals","cloud computing;computer centres;convolutional neural nets;Internet;learning (artificial intelligence);online front-ends","DIAGNET;Internet-scale services;content providers;possible problems;Internet-scale root cause analysis;end-user devices;convolutional neural networks;underlying network topology;online services;internet-scale convolutional root-cause analysis;ISP;datacenters;machine learning techniques;network topology;geodistributed multicloud deployment;fault injection;automated browsers","","","",47.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Astra: Autonomous Serverless Analytics with Cost-Efficiency and QoS-Awareness","J. Jarachanthan; L. Chen; F. Xu; B. Li","University of Louisiana at Lafayette; University of Louisiana at Lafayette; East China Normal University; Hong Kong University of Science and Technology","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",756,765,"With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter with the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of Astra, which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. Astra relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain optimal job execution. We deploy Astra in the AWS Lambda platform and conduct real-world experiments over three representative benchmarks with different scales. Results demonstrate that Astra can achieve the optimal execution decision for serverless analytics, by improving the performance of 21% to 60% under a given budget constraint, and resulting in a cost reduction of 20% to 80% without violating performance requirement, when compared with three baseline configuration algorithms.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00085","Louisiana Board of Regents; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460548","Cloud computing;serverless computing;resource provisioning;modeling;optimization","Distributed processing;Computational modeling;Quality of service;Parallel processing;Benchmark testing;Graph theory;Optimization","cloud computing;cost reduction;graph theory;optimisation;quality of service;resource allocation","optimal execution decision;cost reduction;performance requirement;baseline configuration algorithms;Astra;autonomous serverless analytics;cost-efficiency;QoS-awareness;code deployment;one-click upload;lightweight execution;serverless computing;adapting data-intensive analytics applications;serverless context;provisioning resources;configuration space;serverless analytics jobs;autonomous manner;account flexibly-specified user requirements;user-specific requirements;optimal job execution;conduct real-world experiments","",2.0,"",32.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Max-Stretch Minimization on an Edge-Cloud Platform","A. Benoit; R. Elghazi; Y. Robert","LIP, ENS Lyon, France; LIP, ENS Lyon, France & FEMTO Univ. Franche Comté, Besançon, France; LIP, ENS Lyon, France & UT Knoxville, TN, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",766,775,"We consider the problem of scheduling independent jobs that are generated by processing units at the edge of the network. These jobs can either be executed locally, or sent to a centralized cloud platform that can execute them at greater speed. Such edge-generated jobs may come from various applications, such as e-health, disaster recovery, autonomous vehicles or flying drones. The problem is to decide where and when to schedule each job, with the objective to minimize the maximum stretch incurred by any job. The stretch of a job is the ratio of the time spent by that job in the system, divided by the minimum time it could have taken if the job was alone in the system. We formalize the problem and explain the differences with other models that can be found in the literature. We prove that minimizing the max-stretch is NP-complete, even in the simpler instance with no release dates (all jobs are known in advance). This result comes from the proof that minimizing the max-stretch with homogeneous processors and without release dates is NP-complete, a complexity problem that was left open before this work. We design several algorithms to propose efficient solutions to the general problem, and we conduct simulations based on real platform parameters to evaluate the performance of these algorithms.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460516","edge cloud platform;max strech;NP completeness;heuristic algorithms","Schedules;Program processors;Processor scheduling;Computational modeling;Heuristic algorithms;Minimization;Filling","cloud computing;computational complexity;processor scheduling","max-stretch minimization;edge-cloud platform;centralized cloud platform;edge-generated jobs;ndependent job scheduling","",2.0,"",35.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Decentralized Low-Latency Task Scheduling for Ad-Hoc Computing","J. Edinger; M. Breitbach; N. Gabrisch; D. Schäfer; C. Becker; A. Rizk","University of Hamburg, Germany; University of Mannheim, Germany; University of Mannheim, Germany; University of Mannheim, Germany; University of Mannheim, Germany; University of Duisburg-Essen, Germany","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",776,785,"End users can mutually share their computing resources in ad-hoc computing environments with code offloading. This augments the computational power of resource-constrained mobile devices and enables interactive user-facing applications that would otherwise exceed single device capabilities. However, ad-hoc computing comes along with new challenges such as heterogeneity and unreliability of devices. Resource consumers have to make task scheduling decisions without relying on a centralized scheduler to facilitate sub-second response times in environments with communication latencies that are in the order of the task execution times. In this paper, we present a decentralized low-latency task scheduling approach that minimizes job execution times in heterogeneous ad-hoc environments. We propose two decentralized task scheduling algorithms that select powerful computing resources for parallel task execution while avoiding delays that arise from congested devices. We provide an analytical model of the performance of these algorithms before conducting an extensive evaluation based on real-world applications and a realistic computing infrastructure. Our results show that decentralized scheduling can dynamically adapt to varying system load and outperform a central scheduler in both task and job execution times, which enables low-latency task offloading in ad-hoc environments.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460513","distributed computing;task offloading;task scheduling;low latency","Performance evaluation;Schedules;Processor scheduling;Scheduling algorithms;Heuristic algorithms;Probability;Mobile handsets","ad hoc networks;mobile computing;parallel processing;resource allocation;scheduling","decentralized low-latency task scheduling;ad-hoc computing environments;code offloading;resource-constrained mobile devices;interactive user-facing applications;centralized scheduler;communication latencies;job execution times;parallel task execution;low-latency task offloading","",5.0,"",54.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Lightweight Function Monitors for Fine-Grained Management in Large Scale Python Applications","T. Shaffer; Z. Li; B. Tovar; Y. Babuji; T. Dasso; Z. Surma; K. Chard; I. Foster; D. Thain","University of Notre Dame; University of Chicago; University of Notre Dame; University of Chicago; University of Notre Dame; University of Notre Dame; Argonne National Laboratory; Argonne National Laboratory; University of Notre Dame","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",786,796,"Python has become a widely used programming language for research, not only for small one-off analyses, but also for complex application pipelines running at supercomputer-scale. Modern parallel programming frameworks for Python present users with a more granular unit of management than traditional Unix processes and batch submissions: the Python function. We review the challenges involved in running native Python functions at scale, and present techniques for dynamically determining a minimal set of dependencies and for assembling a lightweight function monitor (LFM) that captures the software environment and manages resources at the granularity of single functions. We evaluate these techniques in a range of environments, from campus cluster to supercomputer, and show that our advanced dependency management planning and dynamic resource management methods provide superior performance and utilization relative to coarser-grained management approaches, achieving several-fold decrease in execution time for several large Python applications.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00088","U.S. Department of Energy; U.S. Department of Energy; Office of Science; Workforce Development for Teachers and Scientists; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460530","","Parallel programming;Pipelines;Tools;Software;Supercomputers;Planning;Resource management","parallel programming;Python;resource allocation;Unix","large scale Python applications;coarser-grained management approaches;dynamic resource management methods;advanced dependency management planning;single functions;native Python functions;batch submissions;traditional Unix processes;modern parallel programming frameworks;supercomputer-scale;complex application pipelines;programming language;fine-grained management;lightweight function monitor","",3.0,"",47.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"AlphaR: Learning-Powered Resource Management for Irregular, Dynamic Microservice Graph","X. Hou; C. Li; J. Liu; L. Zhang; S. Ren; J. Leng; Q. Chen; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Electrical and Computer Engineering, University of California, Riverside; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",797,806,"The microservice architecture is a hot trend which proposes to transform the traditional monolith application into massive dynamic and irregular small services. To boost the overall throughput and ensure the guaranteed latency, it is desirable to process massive service requests in parallel with efficient resource sharing in data centers. However, the disaggregation nature of microservice unavoidably upscales the design space of resource management and increases its complexity. In this paper, we propose AlphaR, a learning-powered resource management system tailored to the microservice environment. The basic idea of AlphaR is to generate microservice-specific resource management policies for improving efficiency. Specifically, we take the first step to use bipartite graph as a convenient abstraction for application built with microservices. Based on this, we devise a bipartite feature inference approach named Bi-GNN to extract the temporal characteristics of microservices. Furthermore, we implement a policy network to select appropriate resource allocation choices for maximizing the performance in resource-constrained data centers. AlphaR can improve the mean and p95 response time by up to 80% and 77.5% respectively compared with conventional schemes.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00089","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460554","","Training;Data centers;Transforms;Throughput;Feature extraction;Dynamic scheduling;Market research","computer centres;graph theory;learning (artificial intelligence);mathematics computing;neural nets;resource allocation","learning-powered resource management system;microservice environment;AlphaR;microservice-specific resource management policies;bipartite graph;bipartite feature inference approach;appropriate resource allocation choices;resource-constrained data centers;irregular microservice graph;dynamic microservice graph;microservice architecture;massive dynamic services;irregular small services;massive service requests;design space","",6.0,"",45.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Deep Reinforcement Agent for Scheduling in HPC","Y. Fan; Z. Lan; T. Childers; P. Rich; W. Allcock; M. E. Papka","Illinois Institute of Technology, Chicago, IL; Illinois Institute of Technology, Chicago, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Northern Illinois University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",807,816,"Cluster scheduler is crucial in high-performance computing (HPC). It determines when and which user jobs should be allocated to available system resources. Existing cluster scheduling heuristics are developed by human experts based on their experience with specific HPC systems and workloads. However, the increasing complexity of computing systems and the highly dynamic nature of application workloads have placed tremendous burden on manually designed and tuned scheduling heuristics. More aggressive optimization and automation are needed for cluster scheduling in HPC. In this work, we present an automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for Scheduling) by leveraging deep reinforcement learning. DRAS is built on a novel, hierarchical neural network incorporating special HPC scheduling features such as resource reservation and backfilling. A unique training strategy is presented to enable DRAS to rapidly learn the target environment. Once being provided a specific scheduling objective given by system manager, DRAS automatically learns to improve its policy through interaction with the scheduling environment and dynamically adjusts its policy as workload changes. The experiments with different production workloads demonstrate that DRAS outperforms the existing heuristic and optimization approaches by up to 45%.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00090","National Science Foundation; Office of Science; National Energy Research Scientific Computing Center; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460549","cluster scheduling;high-performance computing;deep reinforcement learning;job starvation;backfilling;resource reservation","Training;Distributed processing;Processor scheduling;Neural networks;Reinforcement learning;Production;Grasping","deep learning (artificial intelligence);neural nets;parallel processing;power aware computing;processor scheduling","specific HPC systems;highly dynamic nature;application workloads;manually designed tuned scheduling heuristics;aggressive optimization;automated HPC scheduling agent;DRAS;deep reinforcement agent;deep reinforcement learning;special HPC scheduling features;resource reservation;backfilling;specific scheduling objective;system manager;scheduling environment;workload changes;production workloads;existing heuristic optimization approaches;cluster scheduler;high-performance computing;user jobs;available system resources;cluster scheduling heuristics","",9.0,"",34.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"F-Write: Fast RDMA-supported Writes in Erasure-coded In-memory Clusters","B. Xu; J. Huang; Q. Cao; X. Qin; P. Xie","Huazhong University of Sci.& Tech, Wuhan, Hubei, China; Huazhong University of Sci.& Tech, Wuhan, Hubei, China; Huazhong University of Sci.& Tech, Wuhan, Hubei, China; Auburn University, Auburn, AL, USA; Qinghai Normal University, Xining, Qinghai, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",817,826,"To satisfy high reliability accompanied by space efficiency requirements, erasure coding is elected to substitute replication as a redundancy mechanism of in-memory clusters. More often than not, the use of erasure coding is limited to read-intensive applications, in which data in erasure-coded clusters are rarely updated. The essential rationale is that update penalty incurred by parity-synchronizations makes long write latency compared with read latency counterpart.In this paper, we propose F-Write: a fast RDMA-supported write optimization scheme for erasure-coded in-memory clusters. It entails two distinct features: 1) an extended version of consistency protocol called Fast2PC is created. It directly modifies remote memory regions using one-sided WRITE verb provided by RDMA to implement the operations of transaction log, and records multiple transactions in the log to submit together, effectively curtailing network latency; 2) a speculative update approach is given to substitute immediate update. Aggregated undo transactions are handled speculatively to synchronize parity blocks in the background when parity blocks are needed. For multiple writes to an identical data block at different times-tamps, only the original and the latest data blocks are involved in calculating parity blocks, thus mitigating encoding latency. Experimental results indicate that F-Write has lower latency, higher throughput compared to the candidate write schemes. Moreover, the impact on recovery time is negligible. Specifically, under the update-intensive workloads, F-Write cuts down write latency by more than 61%, thereby boosting system throughput by a factor of at least 2.6x.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00091","National Science Foundation; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460540","In-memory cluster;Erasure encoding;Remote direct memory access;Write optimization","Distributed processing;Protocols;Redundancy;Throughput;Boosting;Encoding;Synchronization","cache storage;computational complexity;flash memories;optimisation;protocols;redundancy;security of data;transaction processing","Fast2PC;remote memory regions;WRITE verb;speculative update approach;identical data block;calculating parity blocks;update-intensive workloads;F-Write cuts;Fast RDMA-supported writes;erasure-coded in-memory clusters;erasure coding;read-intensive applications;erasure-coded clusters;update penalty","","","",29.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Argus: Efficient Job Scheduling in RDMA-assisted Big Data Processing","S. Wu; H. Chen; Y. Wang; H. Jin","National Engineering Research Center for Big Data Technology and System Cluster and Grid Computing Lab Services Computing Technology and System Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Cluster and Grid Computing Lab Services Computing Technology and System Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Cluster and Grid Computing Lab Services Computing Technology and System Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System Cluster and Grid Computing Lab Services Computing Technology and System Lab School of Computer Science and Technology, Huazhong University of Science and Technology, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",827,836,"Efficient job scheduling is an important and challenging issue in big data processing systems. Traditional designs commonly give priority to data locality during scheduling and follow a network-optimized principle to avoid costly data moving across the network. The emergence of the high-performance Remote Direct Memory Access (RDMA) network brings new opportunities for big data processing systems. However, the existing RDMA-assisted designs ignore the dependency among stages during scheduling and this can result in unsatisfied system efficiency. In this work, we propose Argus, a novel RDMA-assisted job scheduler which achieves high resource utilization by fully exploiting the structure feature of stage dependency. Argus prioritizes the stages whose completion can enable more schedulable stages. We implement Argus on top of RDMA-Spark, and conduct comprehensive experiments to evaluate the performance using large-scale traces collected from real-world systems. Results show that compared to state-of-the-art designs, Argus reduces the job completion time and makespan by 38% and 31%, respectively.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00092","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460510","Remote direct memory access;big data processing;job scheduling","Distributed processing;Processor scheduling;Distributed databases;Big Data;Resource management","Big Data;cluster computing;parallel processing;scheduling","high-performance remote direct memory access network;job completion;RDMA-Spark;Argus;unsatisfied system efficiency;network-optimized principle;data locality;RDMA-assisted big data processing;job scheduling","",1.0,"",38.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Scaling Out a Combinatorial Algorithm for Discovering Carcinogenic Gene Combinations to Thousands of GPUs","S. Dash; Q. Al-Hajri; W. -c. Feng; H. R. Garner; R. Anandakrishnan","National Center for Computational Sciences, Oak Ridge National Laboratory, Oak Ridge, TN; Computer and Electrical Engineering, Virginia Tech, Blacksburg, VA; Computer Science, Virginia Tech, Blacksburg, VA; Biomedical Sciences, Edward Via College of Osteopathic Medicine, Spartanburg, SC; Biomedical Sciences, Edward Via College of Osteopathic Medicine, Blacksburg, VA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",837,846,"Cancer is a leading cause of death in the US, second only to heart disease. It is primarily a result of a combination of an estimated two-nine genetic mutations (multi-hit combinations). Although a body of research has identified hundreds of cancer-causing genetic mutations, we don't know the specific combination of mutations responsible for specific instances of cancer for most cancer types. An approximate algorithm for solving the weighted set cover problem was previously adapted to identify combinations of genes with mutations that may be responsible for individual instances of cancer. However, the algorithm's computational requirement scales exponentially with the number of genes, making it impractical for identifying more than three-hit combinations, even after the algorithm was parallelized and scaled up to a V100 GPU. Since most cancers have been estimated to require more than three hits, we scaled out the algorithm to identify combinations of four or more hits using 1000 nodes (6000 V100 GPUs with ≈ 48×106 processing cores) on the Summit supercomputer at Oak Ridge National Laboratory. Efficiently scaling out the algorithm required a series of algorithmic innovations and optimizations for balancing an exponentially divergent workload across processors and for minimizing memory latency and inter-node communication. We achieved an average strong scaling efficiency of 90.14% (80.96%-97.96% for 200 to 1000 nodes), compared to a 100 node run, with 84.18% scaling efficiency for 1000 nodes. With experimental validation, the multi-hit combinations identified here could provide further insight into the etiology of different cancer subtypes and provide a rational basis for targeted combination therapy.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00093","Oak Ridge National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460537","Cancer genomics;Parallel computing;GPU;Set Cover algorithm","Heart;Technological innovation;Genetic mutations;Memory management;Graphics processing units;Medical treatment;Approximation algorithms","cancer;cardiology;combinatorial mathematics;diseases;genetics;medical computing;optimisation;patient treatment","approximate algorithm;weighted set cover problem;V100 GPU;optimizations;combination therapy;combinatorial algorithm;carcinogenic gene combinations;heart disease;cancer-causing genetic mutations","",1.0,"",29.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"A Multi-GPU Design for Large Size Cryo-EM 3D Reconstruction","Z. Wang; X. Wan; Z. Liu; Q. Fan; F. Zhang; G. Tan","University of Chinese Academy of Sciences, Beijing, China; High Performance Computer Research Center, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; High Performance Computer Research Center, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Huazhong University of Science and Technology, Wuhan, China; High Performance Computer Research Center, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; High Performance Computer Research Center, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",847,858,"Three-dimensional (3D) reconstruction of cryo-electron microscopy (cryo-EM) is a powerful method to determine the structures of macromolecules at near-atomic resolution. Recently, larger size with finer resolution 2D images has been collected, which can improve the reconstruction resolution. However, large size data incurs high computation and huge memory overhead. Current implementations fail to perform the complete reconstruction workflow on a multi-GPU cluster for large size data. Because of no effective parallel method for 3D convolution and the huge memory demanding, large size data can not be efficiently reconstructed, which impede the resolution improving 3D reconstruction. To enable cryo-EM 3D reconstruction with large size data on multi-GPU, in this work, we propose a new parallel framework called OML-Relion. In OML-Relion, we first adopt a stride based Fourier transform and eliminate data dependence to parallelize the 3D convolution on multi-GPU. Considering the input size varying in each iteration, we next use an auto-tuning model to optimize 3D convolution performance. Finally, guaranteeing the whole reconstruction on a multi-GPU cluster for large size data, we design a novel lossless data compression algorithm to reduce memory overhead on each GPU further. The experiment shows that OML-Relion can efficiently handle large size cryo-EM 3D reconstruction on multi-GPU. The reconstruction module, including 3D convolution operation, achieves 225-330x times speedup for 200-800 pixel size particles. The compression algorithm significantly reduces memory overhead approaching 70%. Moreover, the whole workflow with OMLRelion can achieve 54-65x speedup compared with Relion using two large size datasets.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00094","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460508","cryo-EM;3D reconstruction;multi-GPU;memory optimization","Solid modeling;Three-dimensional displays;Image resolution;Convolution;Graphics processing units;Data compression;Software","convolution;data compression;electron microscopy;graphics processing units;image reconstruction;image resolution;parallel processing","near-atomic resolution;large size cryo-EM 3D reconstruction;3D convolution performance;OML-Relion;multiGPU cluster;reconstruction workflow;reconstruction resolution;finer resolution 2D images;cryo-electron microscopy;three-dimensional reconstruction","","","",38.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Accelerating Multigrid-based Hierarchical Scientific Data Refactoring on GPUs","J. Chen; L. Wan; X. Liang; B. Whitney; Q. Liu; D. Pugmire; N. Thompson; J. Y. Choi; M. Wolf; T. Munson; I. Foster; S. Klasky","Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Missouri University of Science and Technology, Rolla, MO, UAS; Oak Ridge National Laboratory, Oak Ridge, TN, USA; New Jersey Institute of Technology, Newark, NJ, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Argonne National Laboratory, Lemont, IL, USA; University of Chicago, Chicago, IL, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",859,868,"Rapid growth in scientific data and a widening gap between computational speed and I/O bandwidth make it increasingly infeasible to store and share all data produced by scientific simulations. Instead, we need methods for reducing data volumes: ideally, methods that can scale data volumes adaptively so as to enable negotiation of performance and fidelity tradeoffs in different situations. Multigrid-based hierarchical data representations hold promise as a solution to this problem, allowing for flexible conversion between different fidelities so that, for example, data can be created at high fidelity and then transferred or stored at lower fidelity via logically simple and mathematically sound operations. However, the effective use of such representations has been hindered until now by the relatively high costs of creating, accessing, reducing, and otherwise operating on such representations. We describe here highly optimized data refactoring kernels for GPU accelerators that enable efficient creation and manipulation of data in multigrid-based hierarchical forms. We demonstrate that our optimized design can achieve up to 250 TB/s aggregated data refactoring throughput-83% of theoretical peak-on 1024 nodes of the Summit supercomputer. We showcase our optimized design by applying it to a large-scale scientific visualization workflow and the MGARD lossy compression software.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00095","Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460526","Multigrid;Data refactoring;GPU","Distributed processing;Computational modeling;Graphics processing units;Data visualization;Distributed databases;Throughput;Supercomputers","data compression;data structures;data visualisation;graphics processing units;parallel processing;software maintenance","scientific simulations;computational speed;hierarchical scientific data refactoring;large-scale scientific visualization workflow;multigrid-based hierarchical forms;GPU accelerators;highly optimized data;mathematically sound operations;logically simple operations;multigrid-based hierarchical data representations;fidelity tradeoffs;data volumes;byte rate 250.0 TByte/s","",3.0,"",36.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Extremely Fast and Energy Efficient One-way Wave Equation Migration on GPU-based heterogeneous architecture","L. Qu; L. Lucido; M. Bonnasse-Gahot; P. Vezolle; D. Klahr","Total, Pau, France; Eolen, Pau, France; Total, Pau, France; IBM, Montpellier, France; Total, Houston, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",869,880,"One-way Wave Equation Migration (OWEM) is a classic seismic imaging method offering a good trade-off between quality and compute cost in most geological cases. In recent years, GPU-based heterogeneous architecture has gained popularity for seismic imaging. In this paper, we present a generic design for asynchronous processing and data management. By applying this design, we present an efficient GPU implementation of OWEM combining OpenACC and CUDA. Our approach improves upon classic designs by exploring asynchronous compute and data transfer between CPU and GPU using high-speed NVLink, completely masking the cost of MPI communications and I/O. Using 3, 01S GPUs, our fine-tuned OWEM can process 11, 172 seismic shots in less than 75 minutes. By tuning CPU and GPU clock frequencies, we achieve around 30% energy saving with only 4% loss of performance on PANGEA III supercomputer. We believe our design combined with the energy-aware tuning will be beneficial to many GPU applications.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460534","seismic imaging;GPU accelerated computing;parallel computing;heterogeneous architecture;energy-aware HPC","Performance evaluation;Propagation;Memory management;Graphics processing units;Imaging;Data transfer;Data structures","application program interfaces;geophysical techniques;graphics processing units;message passing;parallel architectures;seismology","GPU applications;energy-aware tuning;172 seismic shots;fine-tuned OWEM;data transfer;asynchronous compute;classic designs;efficient GPU implementation;data management;asynchronous processing;generic design;compute cost;classic seismic imaging method;One-way Wave Equation Migration;GPU-based heterogeneous architecture;time 75.0 min","","","",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Revisiting Huffman Coding: Toward Extreme Performance on Modern GPU Architectures","J. Tian; C. Rivera; S. Di; J. Chen; X. Liang; D. Tao; F. Cappello","School of Electrical Engineering and Computer Science, Washington State University, WA, USA; Department of Computer Science, The University of Alabama, AL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, IL, USA; Oak Ridge National Laboratory, TN, USA; Oak Ridge National Laboratory, TN, USA; School of EECS, Washington State University, Pullman, WA, USA; University of Illinois at Urbana-Champaign, IL, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",881,891,"Today's high-performance computing (HPC) applications are producing vast volumes of data, which are challenging to store and transfer efficiently during the execution, such that data compression is becoming a critical technique to mitigate the storage burden and data movement cost. Huffman coding is arguably the most efficient Entropy coding algorithm in information theory, such that it could be found as a fundamental step in many modern compression algorithms such as DEFLATE. On the other hand, today's HPC applications are more and more relying on the accelerators such as GPU on supercomputers, while Huffman encoding suffers from low throughput on GPUs, resulting in a significant bottleneck in the entire data processing. In this paper, we propose and implement an efficient Huffman encoding approach based on modern GPU architectures, which addresses two key challenges: (1) how to parallelize the entire Huffman encoding algorithm, including codebook construction, and (2) how to fully utilize the high memory-bandwidth feature of modern GPU architectures. The detailed contribution is fourfold. (1) We develop an efficient parallel codebook construction on GPUs that scales effectively with the number of input symbols. (2) We propose a novel reduction based encoding scheme that can efficiently merge the codewords on GPUs. (3) We optimize the overall GPU performance by leveraging the state-of-the-art CUDA APIs such as Cooperative Groups. (4) We evaluate our Huffman encoder thoroughly using six real-world application datasets on two advanced GPUs and compare with our implemented multithreaded Huffman encoder. Experiments show that our solution can improve the encoding throughput by up to 5.0× and 6.8× on NVIDIA RTX 5000 and V100, respectively, over the state-of-the-art GPU Huffman encoder, and by up to 3.3× over the multithread encoder on two 28-core Xeon Platinum 8280 CPUs.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00097","U.S. Department of Energy; Office of Science; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460511","Compression;Huffman Coding;GPU;CUDA;Performance","Distributed processing;Platinum;Graphics processing units;Computer architecture;Throughput;Data processing;Supercomputers","application program interfaces;data compression;encoding;entropy codes;graphics processing units;Huffman codes;image coding;multi-threading;parallel architectures;parallel processing","entire data processing;HPC applications;modern compression algorithms;efficient Entropy coding algorithm;data movement cost;storage burden;data compression;high-performance computing applications;toward extreme performance;Huffman coding;multithread encoder;state-of-the-art GPU Huffman encoder;encoding throughput;implemented multithreaded Huffman encoder;GPU performance;encoding scheme;efficient parallel codebook construction;high memory-bandwidth feature;entire Huffman encoding algorithm;modern GPU architectures;efficient Huffman encoding approach","",7.0,"",47.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Rack-Scaling: An efficient rack-based redistribution method to accelerate the scaling of cloud disk arrays","Z. Lin; H. Guo; C. Wu; J. Li; G. Xue; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Sichuan Research Institute, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",892,901,"In cloud storage systems, disk arrays are widely used because of their high reliability and low monetary cost. Due to the burst of I/O in sprinting computing scenarios (i.e. online retailer services on Black Friday or Cyber Monday), large scale cloud storage systems such as AWS S3 and GFS need to afford 10XI/O workloads. Therefore, rack level scaling for cloud disk arrays becomes urgent for sprinting services. Although several existing methods, such as Round-Robin(RR) and Scale-RS, are proposed to accelerate the scaling processes, the efficiencies of these approaches are limited. It is because that the cross-rack data migrations are ill-considered in their designs. To address the above problem, in this paper, we propose Rack-Scaling, a novel data redistribution method to accelerate rack level scaling process in cloud storage systems. The basic idea of Rack-Scaling is migrating appropriate data blocks within and among racks to achieve a uniform data distribution while minimizing the cross-rack migration, which costs more than intra-rack migration. We conduct simulations via Disksim and we also implement Rack-Scaling on Hadoop to demonstrate the effectiveness of Rack-Scaling. The results show that, compared to typical methods such as Round-Robin (RR), Semi-RR, Scale-RS and BDR, Rack-Scaling reduces the number of I/O operations and the data amount of cross-rack transmission by up to 90.4% and 99.9%, respectively, and speeds up the scaling by up to 8.77X.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00098","Natural Science Foundation of Shanghai; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460514","Disk Arrays;RAID;Cloud Storage;Scalability;Reliability;Data Migration","Cloud computing;Distributed processing;Data models;Arrays;Reliability","cloud computing;data handling;input-output programs;parallel processing;storage management","Round-Robin;Hadoop;I/O operations;sprinting computing scenarios;rack-based redistribution method;cloud disk arrays;Scale-RS;intra-rack migration;cloud storage systems;rack level scaling process;Rack-Scaling;cross-rack data migrations","",2.0,"",43.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Optimizing Performance for Open-Channel SSDs in Cloud Storage System","X. Zhang; F. Zhu; S. Li; K. Wang; W. Xu; D. Xu","Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",902,911,"In large-scale cloud storage systems, Solid-State Drive (SSD) has been broadly used as the mainstream storage device because it has the advantages of low access latency and high throughput. However, conventional SSD is a black-box system to host softwares, thus failing to fully exploit the benefits of NAND flash and provide high quality of service (QoS). On the other hand, Open-Channel SSD (OCSSD) which exposes its internal information to the host software, has the potential to solve this problem. However, existing OCSSD fails to achieve anticipated performance under heavy workloads. To this end, we propose an advanced OCSSD-based driver developed with the novel data placement policy, redefined garbage collection (GC) with copyback technique, efficient prefetch read scheme, and fast live upgrade method. Our work describes the consistent efforts to pursue high performance and QoS in OCSSDs with different approaches. The evaluation results show that our novel Open-Channel SSD is able to provide high I/O throughputs and predictable I/O latencies. For example, our Open-Channel SSD can improve I/O throughputs by 103% and reduce the 99th percentile latency by 62.9% on average compared with the state-of-the-art NVMe SSDs.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460536","Open-Channel SSD;High Performance;Cloud Storage","Performance evaluation;Degradation;Cloud computing;Distributed processing;Prefetching;Quality of service;Throughput","cloud computing;driver circuits;flash memories;NAND circuits;quality of service;solid state drives","QoS;host software;advanced OCSSD-based driver;open-channel SSD;large-scale cloud storage systems;solid-state drive;mainstream storage device;black-box system;NAND flash;NVMe SSD;prefetch read scheme;data placement policy;redefined garbage collection;copyback technique;fast live upgrade method","","","",27.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"AuTraScale: An Automated and Transfer Learning Solution for Streaming System Auto-Scaling","L. Zhang; W. Zheng; C. Li; Y. Shen; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",912,921,"The complexity and variability of streaming data have brought a great challenge to the elasticity of the data processing systems. Streaming systems, such as Flink and Storm, need to adapt to the changes of workload with auto-scaling to meet the QoS requirements while saving resources. However, the accuracy of classical models (such as a queueing model) for QoS prediction decreases with the increase of the complexity and variability of streaming data and the resource interference. On the other hand, the indirect metrics used to optimize QoS may not accurately guide resource adjustment. Those problems can easily lead to waste of resources or QoS violation in practice. To solve the above problems, we propose AuTraScale, an automated and transfer learning auto-scaling solution, to determine the appropriate parallelism and resource allocation that meet the latency and throughput targets. AuTraScale uses Bayesian optimization to adapt to the complex relationship between resources and QoS, minimizing the impact of resource interference on the prediction accuracy, and a new metric that measures the performance of operators for accurate optimization. Even when the input data rate changes, it can quickly adjust the parallelism of each operator in response, with a transfer learning algorithm. We have implemented and evaluated AuTraScale on a Flink platform. The experimental results show that, compared with the state-of-the-art method like DRS and DS2, AuTraScale can reduce 66.6% and 36.7% resource consumption respectively in the scale-down and scale-up scenarios while ensuring QoS requirements, and save 13.5% resource on average when the input data rate changes.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460552","Streaming system;Auto-scaling;Bayesian optimization","Measurement;Adaptation models;Transfer learning;Quality of service;Interference;Parallel processing;Predictive models","Bayes methods;learning (artificial intelligence);media streaming;optimisation;quality of service;resource allocation","automated transfer learning solution;system auto-scaling;data processing systems;streaming systems;classical models;queueing model;QoS prediction;resource interference;indirect metrics;resource adjustment;QoS violation;AuTraScale;automated transfer learning auto-scaling solution;appropriate parallelism;resource allocation;throughput targets;Bayesian optimization;complex relationship;prediction accuracy;input data rate changes;transfer learning algorithm;Flink platform","",2.0,"",25.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"SNOW Revisited: Understanding When Ideal READ Transactions Are Possible","K. M. Konwar; W. Lloyd; H. Lu; N. Lynch","RLE, MIT, Cambridge, MA; Computer Science, Princeton University, Princeton, NJ, USA; Computer Science, Princeton University, Princeton, NJ, USA; CSAIL, MIT, Cambridge, MA, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",922,931,"READ transactions that read data distributed across servers dominate the workloads of real-world distributed storage systems. The SNOW Theorem [13] stated that ideal READ transactions that have optimal latency and the strongest guarantees-i.e., “SNOW” READ transactions-are impossible in one specific setting that requires three or more clients: at least two readers and one writer. However, it left many open questions. We close all of these open questions with new impossibility results and new algorithms. First, we prove rigorously the result from [13] saying that it is impossible to have a READ transactions system that satisfies SNOW properties with three or more clients. The insight we gained from this proof led to teasing out the implicit assumptions that are required to state the results and also, resolving the open question regarding the possibility of SNOW with two clients. We show that it is possible to design an algorithm, where SNOW is possible in a multi-writer, single-reader (MWSR) setting when a client can send messages to other clients; on the other hand, we prove it is impossible to implement SNOW in a multi-writer, single-reader (MWSR) setting-which is more general than the two-client setting-when client-to-client communication is disallowed. We also correct the previous claim in [13] that incorrectly identified one existing system, Eiger [12], as supporting the strongest guarantees (SW) and whose read-only transactions had bounded latency. Thus, there were no previous algorithms that provided the strongest guarantees and had bounded latency. Finally, we introduce the first two algorithms to provide the strongest guarantees with bounded latency.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460529","distributed transactions;strict-serializability","Distributed processing;Snow;Distributed databases;Servers;IEEE transactions","distributed databases;Internet;software fault tolerance;storage management","read-only transactions;SNOW revisited;ideal READ transactions;real-world distributed storage systems;SNOW Theorem;SNOW READ transactions;READ transactions system;SNOW properties;multiwriter single-reader setting-which","","","",16.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"QoS-Aware and Resource Efficient Microservice Deployment in Cloud-Edge Continuum","K. Fu; W. Zhang; Q. Chen; D. Zeng; X. Peng; W. Zheng; M. Guo","Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; China University of Geosciences; Fudan University; Shanghai Jiao Tong University; Shanghai Jiao Tong University","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",932,941,"User-facing services are now evolving towards the microservice architecture where a service is built by connecting multiple microservice stages. While an entire service is heavy, the microservice architecture shows the opportunity to only offload some microservice stages to the edge devices that are close to the end users. However, emerging techniques often result in the violation of Quality-of-Service (QoS) of microservice-based services in cloud-edge continuum, as they do not consider the communication overhead or the resource contention between microservices.We propose Nautilus, a runtime system that effectively deploys microservice-based user-facing services in cloud-edge continuum. It ensures the QoS of microservice-based user-facing services while minimizing the required computational resources. Nautilus is comprised of a communication-aware microservice mapper, a contention-aware resource manager and a load-aware microservice scheduler. The mapper divides the microservice graph into multiple partitions based on the communication overhead and maps the partitions to the nodes. On each node, the resource manager determines the optimal resource allocation for its microservices based on reinforcement learning that may capture the complex contention behaviors. The microservice scheduler monitors the QoS of the entire service, and migrates microservices from busy nodes to idle ones at runtime. Our experimental results show that Nautilus reduces the computational resource usage by 23.9% and the network bandwidth usage by 53.4%, while achieving the required 99%-ile latency.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460542","","Distributed processing;Runtime;Quality of service;Computer architecture;Reinforcement learning;Bandwidth;Resource management","cloud computing;optimisation;quality of service;resource allocation;scheduling","communication overhead;resource contention;microservice-based user-facing services;cloud-edge continuum;communication-aware microservice mapper;contention-aware resource manager;load-aware microservice scheduler;microservice graph;QoS-aware;resource efficient microservice deployment;microservice architecture;multiple microservice stages;Quality-of-Service;microservice-based services","",12.0,"",38.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Byzantine Dispersion on Graphs","A. R. Molla; K. Mondal; W. K. Moses","Computer & Communication Sciences, Indian Statistical Institute, Kolkata, India; Dept. of Mathematics, Indian Institute of Technology Ropar, Ropar, India; Dept. of Computer Science, University of Houston, Houston, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",942,951,"This paper considers the problem of Byzantine dispersion and extends previous work along several parameters. The problem of Byzantine dispersion asks: given n robots, up to f of which are Byzantine, initially placed arbitrarily on an n node anonymous graph, design a terminating algorithm to be run by the robots such that they eventually reach a configuration where each node has at most one non-Byzantine robot on it. Previous work solved this problem for rings and tolerated up to n - 1 Byzantine robots. In this paper, we investigate the problem on more general graphs. We first develop an algorithm that tolerates up to n - 1 Byzantine robots and works for a more general class of graphs. We then develop an algorithm that works for any graph but tolerates a lesser number of Byzantine robots. We subsequently turn our focus to the strength of the Byzantine robots. Previous work considers only “weak” Byzantine robots that cannot fake their IDs. We develop an algorithm that solves the problem when Byzantine robots are not weak and can fake IDs. Finally, we study the situation where the number of the robots is not n but some k. We show that in such a scenario, the number of Byzantine robots that can be tolerated is severely restricted. Specifically, we show that it is impossible to deterministically solve Byzantine dispersion when ⌈k/n⌉ > ⌈(k - f)/n⌉.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00103","Israel Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460521","Dispersion;Mobile robots;Distributed algorithms;Byzantine faults;Faulty robots;General graphs","Distributed processing;Robots;Dispersion","computational complexity;graph theory","n node anonymous graph;n-1 Byzantine robots;weak Byzantine robots;nonByzantine robot;Byzantine dispersion","",4.0,"",44.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Byzantine Agreement with Unknown Participants and Failures","P. Khanchandani; R. Wattenhofer","ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",952,961,"A set of mutually distrusting participants that want to agree on a common opinion must solve an instance of a Byzantine agreement problem. These problems have been extensively studied in the literature. However, most of the existing solutions assume that the participants are aware of n — the total number of participants in the system — and f — an upper bound on the number of Byzantine participants. In this paper, we show that most of the fundamental agreement problems can be solved without affecting resiliency even if the participants do not know the values of(possibly changing) n and f. Specifically, we consider a synchronous system where the participants have unique but not necessarily consecutive identifiers, and give Byzantine agreement algorithms for reliable broadcast, approximate agreement, rotor-coordinator, early terminating consensus and total ordering in static and dynamic systems, all with the optimal resiliency of n>3f. Moreover, we show that some synchrony is necessary as an agreement with probabilistic termination is impossible in a semi-synchronous or asynchronous system if the participants are unaware of n and f.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460539","","Distributed processing;Upper bound;Heuristic algorithms;Probabilistic logic;Approximation algorithms;Reliability;Dynamical systems","","","",1.0,"",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"QPR: Quantizing PageRank with Coherent Shared Memory Accelerators","A. T. Mughrabi; M. Ibrahim; G. T. Byrd","Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, North Carolina; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, North Carolina; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, North Carolina","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",962,972,"Graph algorithms often require fine-grained, random access across substantially large data structures. Previous work on FPGA-based acceleration has required significant preprocessing and restructuring to transform the memory access patterns into a streaming format that is more friendly to of fchip hardware. However, the emergence of cache-coherent shared memory interfaces, such as CAPI, allows designers to more easily work with the natural in-memory organization of the data. This paper introduces a vertex-centric shared-memory accelerator for the PageRank algorithm, optimized for high performance while effectively using coherent caching on the FPGA hardware. The proposed design achieves up to 14.9x speedups by selectively caching graph data for the accelerator while taking into account locality and reuse, compared to naively using the shared address space access and DRAM only. We also introduce PageRank Quantization, an innovative technique to represent page-ranks with 32-bit quantized fixed-point values. This approach is up to 1.5x faster than 64-bit fixed-point while keeping precision within a tolerable error margin. As a result, we maintain both the hardware scalability of fixed-point representation and the cache performance of 32-bit floating-point.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460509","Graphs;PageRank;CAPI;FPGA","Distributed processing;Quantization (signal);Scalability;Memory management;Random access memory;Transforms;Organizations","cache storage;data structures;field programmable gate arrays;graph theory;quantisation (signal);shared memory systems","FPGA hardware;graph data;shared address space access;PageRank Quantization;64-bit fixed-point;hardware scalability;cache performance;quantizing PageRank;coherent shared memory accelerators;graph algorithms;fine-grained access;random access;FPGA-based acceleration;significant preprocessing;memory access patterns;streaming format;fchip hardware;cache-coherent shared memory interfaces;in-memory organization;shared-memory accelerator;PageRank algorithm;coherent caching","",1.0,"",58.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Distributed Training of Embeddings using Graph Analytics","G. Gill; R. Dathathri; S. Maleki; M. Musuvathi; T. Mytkowicz; O. Saarikivi","Katana Graph Inc., Austin, TX, USA; Katana Graph Inc., Austin, TX, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",973,983,"Many applications today, such as natural language processing, network and code analysis, rely on semantically embedding objects into low-dimensional fixed-length vectors. Such embeddings naturally provide a way to perform useful downstream tasks, such as identifying relations among objects and predicting objects for a given context. Unfortunately, training accurate embeddings is usually computationally intensive and requires processing large amounts of data. This paper presents a distributed training framework for a class of applications that use Skip-gram-like models to generate embeddings. We call this class Any2Vec and it includes Word2Vec (Gensim), and Vertex2Vec (DeepWalk and Node2Vec) among others. We first formulate Any2Vec training algorithm as a graph application. We then adapt the state-of-the-art distributed graph analytics framework, D-Galois, to support dynamic graph generation and re-partitioning, and incorporate novel communication optimizations. We show that on a cluster of 3248-core hosts our framework GraphAny2Vec matches the accuracy of the state-of-the-art shared-memory implementations of Word2Vec and Vertex2Vec, and gives geo-mean speedups of 12 x and 5 x respectively. Furthermore, GraphAny2Vec is on average 2 x faster than DMTK, the state-of-the-art distributed Word2Vec implementation, on 32 hosts while yielding much better accuracy.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460533","Machine Learning;Graph Analytics;Distributed Computing","Training;Distributed processing;Machine learning algorithms;Heuristic algorithms;Computational modeling;Clustering algorithms;Natural language processing","graph theory;natural language processing;shared memory systems;unsupervised learning;vectors","code analysis;fixed-length vectors;distributed training framework;Skip-gram-like models;Vertex2Vec;Node2Vec;Any2Vec training algorithm;graph application;distributed graph analytics framework;dynamic graph generation;shared-memory implementations;Word2Vec implementation;natural language processing;GraphAny2Vec framework","",1.0,"",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Multiplicative Weights Algorithms for Parallel Automated Software Repair","J. Renzullo; W. Weimer; S. Forrest","Arizona State University Tempe, Arizona, USA; University of Michigan Ann Arbor, Michigan, USA; Arizona State University Tempe, Arizona, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",984,993,"Multiplicative Weights Update (MWU) algorithms are a form of online learning that is applied to multi-armed bandit problems. Such problems involve allocating a fixed number of trials among multiple options to maximize cumulative payoff. MWU is a popular and effective method for dynamically balancing the trade-off between exploring the value of new options and exploiting the information already gained. However, no clear strategy exists to help practitioners choose which of the several algorithmic designs within this family to deploy. In this paper, three variants of parallel MWU algorithms are considered: Two parallel variants that rely on global memory, and one variant that uses distributed memory. The three variants are first analyzed theoretically, and then their effectiveness is assessed empirically on the task of estimating distributions in the context of stochastic search for repairs to bugs in software. Earlier work on APR suffers from various inefficiencies, and the paper shows how to decompose the problem into two stages: one that is embarrassingly parallel and one that is amenable to MWU. We then model the cost of each MWU variant and derive the conditions under which it is likely to be preferred in practice. We find that all three MWU algorithms achieve accuracy above 90% but that there are significant differences in runtime and total cost. When 90% accuracy is sufficient and evaluating options is expensive, such as in our use case, we find that the algorithm that uses global memory and has high communication cost outperforms the other two. We analyze the reasons for this surprising result.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460525","Automated Program Repair;Online Learning;Multiplicative Weights Update","Distributed processing;Runtime;Heuristic algorithms;Software algorithms;Computer bugs;Stochastic processes;Maintenance engineering","learning (artificial intelligence);parallel processing;program debugging;search problems;software maintenance;stochastic processes","stochastic search;software bugs;parallel MWU algorithms;algorithmic designs;cumulative payoff;multiple options;multiarmed bandit problems;online learning;Multiplicative Weights Update algorithms;parallel automated software repair;multiplicative weights algorithms;MWU variant;global memory;parallel variants","","","",42.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"An In-Depth Analysis of Distributed Training of Deep Neural Networks","Y. Ko; K. Choi; J. Seo; S. -W. Kim","Dept. Computer Software, Hanyang University, Seoul, Korea; Dept. Computer Software, Hanyang University, Seoul, Korea; Dept. Computer Software, Hanyang University, Seoul, Korea; Dept. Computer Software, Hanyang University, Seoul, Korea","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",994,1003,"As the popularity of deep learning in industry rapidly grows, efficient training of deep neural networks (DNNs) becomes important. To train a DNN with a large amount of data, distributed training with data parallelism has been widely adopted. However, the communication overhead limits the scalability of distributed training. To reduce the overhead, a number of distributed training algorithms have been proposed. The model accuracy and training performance of those algorithms can be different depending on various factors such as cluster settings, training models/datasets, and optimization techniques applied. In order for someone to adopt a distributed training algorithm appropriate for her/his situation, it is required for her/him to fully understand the model accuracy and training performance of these algorithms in various settings. Toward this end, this paper reviews and evaluates seven popular distributed training algorithms (BSP, ASP, SSP, EASGD, AR-SGD, GoSGD, and AD-PSGD) in terms of the model accuracy and training performance in various settings. Specifically, we evaluate those algorithms for two CNN models, in different cluster settings, and with three well-known optimization techniques. Through extensive evaluation and analysis, we made several interesting discoveries. For example, we found out that some distributed training algorithms (SSP, EASGD, and GoSGD) have highly negative impact on the model accuracy because they adopt intermittent and asymmetric communication to improve training performance; the communication overhead of some centralized algorithms (ASP and SSP) is much higher than we expected in a cluster setting with limited network bandwidth because of the PS bottleneck problem. These findings, and many more in the paper, can guide the adoption of proper distributed training algorithms in industry; our findings can be useful in academia as well for designing new distributed training algorithms.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00108","Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460556","deep learning;distributed training algorithm","Training;Industries;Backpropagation;Scalability;Neural networks;Clustering algorithms;Distributed databases","deep learning (artificial intelligence);distributed processing;neural nets","deep learning;CNN models;AD-PSGD;GoSGD;AR-SGD;EASGD;SSP;ASP;BSP;distributed training algorithms;training performance;model accuracy;communication overhead;deep neural networks","",6.0,"",27.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Automatic Graph Partitioning for Very Large-scale Deep Learning","M. Tanaka; K. Taura; T. Hanawa; K. Torisawa","Data-driven Intelligent System Research Center (DIRECT), Universal Communication Research Institute National Institute of Information and Communications Technology (NICT) 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan; Department of Information and Communication Engineering, Graduate School of Information Science and Technology, University of Tokyo 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan; Information Technology Center, University of Tokyo 5-1-5 Kashiwanoha, Kashiwa-shi, Chiba, Japan; Data-driven Intelligent System Research Center (DIRECT), Universal Communication Research Institute National Institute of Information and Communications Technology (NICT) 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1004,1013,"This work proposes RaNNC (Rapid Neural Network Connector) as middleware for automatic hybrid parallelism. In recent deep learning research, as exemplified by T5 and GPT-3, the size of neural network models continues to grow. Since such models do not fit into the memory of accelerator devices, they need to be partitioned by model parallelism techniques. Moreover, to accelerate training for huge training data, we need a combination of model and data parallelisms, i.e., hybrid parallelism. Given a model description for PyTorch without any specification for model parallelism, RaNNC automatically partitions the model into a set of subcomponents so that (1) each subcomponent fits a device memory and (2) a high training throughput for pipeline parallelism is achieved by balancing the computation times of the subcomponents. Since the search space for partitioning models can be extremely large, RaNNC partitions a model through the following three phases. First, it identifies atomic subcomponents using simple heuristic rules. Next it groups them into coarser-grained blocks while balancing their computation times. Finally, it uses a novel dynamic programming-based algorithm to efficiently search for combinations of blocks to determine the final partitions. In our experiments, we compared RaNNC with two popular frameworks, Megatron-LM (hybrid parallelism) and GPipe (originally proposed for model parallelism, but a version allowing hybrid parallelism also exists), for training models with increasingly greater numbers of parameters. In the pre-training of enlarged BERT models, RaNNC successfully trained models five times larger than those Megatron-LM could, and RaNNC's training throughputs were comparable to Megatron-LM's when pre-training the same models. RaNNC also achieved better training throughputs than GPipe on both the enlarged BERT model pre-training (GPipe with hybrid parallelism) and the enlarged ResNet models (GPipe with model parallelism) in all of the settings we tried. These results are remarkable, since RaNNC automatically partitions models without any modification to their descriptions; Megatron-LM and GPipe require users to manually rewrite the models' descriptions.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460551","","Training;Deep learning;Computational modeling;Heuristic algorithms;Bit error rate;Pipelines;Neural networks","computational complexity;deep learning (artificial intelligence);dynamic programming;graph theory;metaheuristics;middleware;natural language processing;neural nets;parallel processing;search problems","very large-scale deep learning;automatic hybrid parallelism;data parallelisms;pipeline parallelism;computational times;RaNNC partitions;Megatron-LM;automatic graph partitioning;rapid neural network connector;enlarged BERT model;enlarged ResNet model;PyTorch;heuristic rules;search space;dynamic programming;GPipe;middleware","",6.0,"",29.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Extending Sparse Tensor Accelerators to Support Multiple Compression Formats","E. Qin; G. Jeong; W. Won; S. -C. Kao; H. Kwon; S. Srinivasan; D. Das; G. E. Moon; S. Rajamanickam; T. Krishna","Georgia Tech, Atlanta, USA; Georgia Tech, Atlanta, USA; Georgia Tech, Atlanta, USA; Georgia Tech, Atlanta, USA; Georgia Tech, Atlanta, USA; Intel Labs, Bangalore, India; Intel Labs, Bangalore, India; Korea Aerospace University, Goyang, Republic of Korea; Sandia National Laboratories, Albuquerque, USA; Georgia Tech, Atlanta, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1014,1024,"Sparsity, which occurs in both scientific applications and Deep Learning (DL) models, has been a key target of optimization within recent ASIC accelerators due to the potential memory and compute savings. These applications use data stored in a variety of compression formats. We demonstrate that both the compactness of different compression formats and the compute efficiency of the algorithms enabled by them vary across tensor dimensions and amount of sparsity. Since DL and scientific workloads span across all sparsity regions, there can be numerous format combinations for optimizing memory and compute efficiency. Unfortunately, many proposed accelerators operate on one or two fixed format combinations. This work proposes hardware extensions to accelerators for supporting numerous format combinations seamlessly and demonstrates ~ 4 x speedup over performing format conversions in software.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00110","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460546","","Deep learning;Distributed processing;Tensors;Computational modeling;Memory management;Hardware;Software","application specific integrated circuits;data compression;deep learning (artificial intelligence);tensors","sparse tensor accelerators;multiple compression formats;scientific applications;deep learning models;ASIC accelerators;scientific workloads;sparsity regions;format conversions","",2.0,"",39.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Pase: Parallelization Strategies for Efficient DNN Training","V. Elango","Baidu Research, Sunnyvale, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1025,1034,"Training a deep neural network (DNN) requires substantial computational and memory requirements. It is common to use multiple devices to train a DNN to reduce the overall training time. There are several choices to parallelize each layer in a DNN. Exhaustively searching this list to find an optimal parallelization strategy is prohibitively time consuming and impractical. The standard practice is to use data parallelism because of its simplicity. However, data parallelism is often suboptimal, and suffers from poor performance and high memory requirement. Expert-designed strategies have been proposed on a case-by-case basis using domain specific knowledge. These expert-designed strategies do not generalize well to DNNs other than the ones for which they were designed, and are not always necessarily the best choice. In this paper, we propose an approach to automatically find efficient parallelization strategies for DNNs from their computation graphs. We present an efficient algorithm to compute these strategies within a reasonable time in practice. We evaluate the effectiveness of our approach on various DNNs. We also compare the performance of the strategies identified by our approach against data parallelism, expert-designed strategies, and the state-of-the-art approaches. Our results show that the strategies found using our approach outperform the baseline data parallelism strategy in all the cases. In addition, our strategies achieve better performance than the expert-designed strategies and the state-of-the-art approaches.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460527","Machine learning;neural nets;parallelism;automatic parallelization;dynamic programming;optimization","Training;Deep learning;Distributed processing;Memory management;Neural networks;Parallel processing;Computational efficiency","deep learning (artificial intelligence);parallel algorithms","expert-designed strategies;efficient parallelization strategies;baseline data parallelism strategy;efficient DNN training;substantial computational memory requirements;training time;optimal parallelization strategy;high memory requirement","","","",26.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Efficient Video Captioning on Heterogeneous System Architectures","H. -R. Huang; D. -Y. Hong; J. -J. Wu; P. Liu; W. -C. Hsu","Academia Sinica, Institute of Information Science, Taipei, Taiwan; Academia Sinica, Institute of Information Science, Taipei, Taiwan; Academia Sinica, Institute of Information Science, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1035,1045,"Video captioning is the core technology to drive the development of many important multidisciplinary applications, such as AI-assisted medical diagnosis, storytelling through videos, video question answering, lip-reading, just to name a few. Video captioning employs a hybrid CNN+RNN neural network model to translate video scenes into natural language descriptions. For deep learning inference, a typical approach is running both the CNN and the RNN on a GPU. Such a GPU-only approach often suffers long inference time due to underutilization of the computing power offered by the CPU+GPU heterogeneous system architecture, which is a common architecture in modern computers.This work is an early effort to tackle the performance issue of performing deep learning inference using a hybrid CNN+RNN model on a heterogeneous system with a CPU and a GPU. This is a challenging task because of (1) CNN and RNN exhibit very different computing behaviors. This raises the question of how to split the two models into computing tasks and properly assign the tasks to the CPU and the GPU to minimize the inference time for a video frame, and (2) Data dependency exists between the CNN and the RNN within a video frame, as well as between the adjacent RNNs across two video frames. These data dependencies prohibit full parallelization of the hybrid model. To solve these two problems, we propose two optimizations: a fine-grained scheduling scheme for mapping computation and devices within a video frame, and a pipeline scheduling scheme to exploit maximum parallelism between the execution of the video frames. To facilitate our optimizations, we also develop an accurate regression-based cost model to predict the computation time of CNN/RNN operations and the communication time for moving data between CPU and GPU. Experimental results show that our optimization improves the performance of video captioning by up to 3.24× on the CPU+GPU system, compared with the GPU-only execution.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00112","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460545","Video captioning;heterogeneous system architectures;model scheduling;dynamic programming;pipelining","Deep learning;Processor scheduling;Computational modeling;Pipelines;Graphics processing units;Systems architecture;Predictive models","convolutional neural nets;deep learning (artificial intelligence);inference mechanisms;natural language processing;parallel processing;pipeline processing;recurrent neural nets;regression analysis;scheduling;video signal processing","video captioning;heterogeneous system architectures;video question answering;hybrid CNN+RNN neural network model;video scenes;deep learning inference;CPU+GPU heterogeneous system architecture;video frame;data dependency;pipeline scheduling;parallelization;regression-based cost model;natural language descriptions","",1.0,"",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"SRNoC: A Statically-Scheduled Circuit-Switched Superconducting Race Logic NoC","G. Michelogiannakis; D. Lyles; P. Gonzalez-Guerrero; M. Bautista; D. Vasudevan; A. Butko","Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1046,1055,"Temporal encoding has been shown to be a natural fit for single flux quantum (SFQ) superconducting computing since SFQ already encodes information with the presence or absence of voltage pulses. However, past work in SFQ has focused on binary-encoded networks on chip (NoCs). In this paper, we propose superconducting rotary NoC (SRNoC), a NoC where both data and control paths operate in the temporal domain following the race logic (RL) convention. Therefore, SFQ chips with temporal compute or memory can use SRNoC to avoid converting between the temporal and binary domains that would result from using a binary-encoded NoC. Using RL also enables SRNoC to be area-efficient, mitigating SFQ technology's low device density. SRNoC treats pulses as independent packets and delivers them to outputs without changing their value, i.e. preserving the RL convention. SRNoC operates on a fixed, rotating connection schedule between inputs and outputs. In each connection window, multiple pulses (packets) can be transmitted sequentially. SRNoC provides 13.1x higher throughput per port per Josephson junction (JJ) compared to the best-performing of three demonstrated NoCs.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00113","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460541","RSFQ;Race logic;NoC;circuit switching;rotary","Performance evaluation;Schedules;Distributed processing;Quantum computing;Superconducting logic circuits;Throughput;Josephson junctions","encoding;Josephson effect;network-on-chip;superconducting logic circuits","temporal encoding;single flux quantum superconducting computing;binary-encoded networks;rotary NoC;SRNoC;race logic convention;SFQ chips;temporal compute;temporal domains;binary domains;statically-scheduled circuit-switched superconducting race logic NoC","",4.0,"",20.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Matrix Engines for High Performance Computing: A Paragon of Performance or Grasping at Straws?","J. Domke; E. Vatai; A. Drozd; P. ChenT; Y. Oyama; L. Zhang; S. Salaria; D. Mukunoki; A. Podobas; M. WahibT; S. Matsuoka","Tokyo Institute of Technology, Japan; Tokyo Institute of Technology, Japan; Tokyo Institute of Technology, Japan; National Institute of Advanced Industrial Science and Technology, Japan; Tokyo Institute of Technology, Japan; Tokyo Institute of Technology, Japan; Tokyo Institute of Technology, Japan; RIKEN CCS, Japan; KTH Royal Institute of Technology, Stockholm, Sweden; National Institute of Advanced Industrial Science and Technology, Japan; Tokyo Institute of Technology, Japan","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1056,1065,"Matrix engines or units, in different forms and affinities, are becoming a reality in modern processors; CPUs and otherwise. The current and dominant algorithmic approach to Deep Learning merits the commercial investments in these units, and deduced from the No. 1 benchmark in supercomputing, namely High Performance Linpack, one would expect an awakened enthusiasm by the HPC community, too. Hence, our goal is to identify the practical added benefits for HPC and machine learning applications by having access to matrix engines. For this purpose, we perform an in-depth survey of software stacks, proxy applications and benchmarks, and historical batch job records. We provide a cost-benefit analysis of matrix engines, both asymptotically and in conjunction with state-of-the-art processors. While our empirical data will temper the enthusiasm, we also outline opportunities to “misuse” these dense matrix-multiplication engines if they come for free.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00114","Japan Society for the Promotion of Science; New Energy and Industrial Technology Development Organization; RWB; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460517","","Deep learning;Program processors;Tensors;Machine learning algorithms;Benchmark testing;Throughput;Supercomputers","benchmark testing;cost-benefit analysis;learning (artificial intelligence);matrix multiplication;parallel machines;parallel processing;performance evaluation;power aware computing","High Performance Linpack;Deep Learning merits;dominant algorithmic approach;High Performance computing;dense matrix-multiplication engines;proxy applications;machine learning applications","",8.0,"",66.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"Performance Analysis of Scientific Computing Workloads on General Purpose TEEs","A. Akram; A. Giannakou; V. Akella; J. Lowe-Power; S. Peisert","UC Davis; LBNL; UC Davis; UC Davis; LBNL & UC Davis","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1066,1076,"Scientific computing sometimes involves computation on sensitive data. Depending on the data and the execution environment, the HPC (high-performance computing) user or data provider may require confidentiality and/or integrity guarantees. To study the applicability of hardware-based trusted execution environments (TEEs) to enable secure scientific computing, we deeply analyze the performance impact of general purpose TEEs, AMD SEV, and Intel SGX, for diverse HPC benchmarks including traditional scientific computing, machine learning, graph analytics, and emerging scientific computing workloads. We observe three main findings: 1) SEV requires careful memory placement on large scale NUMA machines (1×-3.4× slowdown without and 1×-1.15× slowdown with NUMA aware placement), 2) virtualization-a prerequisite for SEV- results in performance degradation for workloads with irregular memory accesses and large working sets (1×-4× slowdown compared to native execution for graph applications) and 3) SGX is inappropriate for HPC given its limited secure memory size and inflexible programming model (1.2×-126× slowdown over unsecure execution). Finally, we discuss forthcoming new TEE designs and their potential impact on scientific computing.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00115","Office of Science; Advanced Scientific Computing Research; U.S. Department of Energy; National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460547","HPC;Hardware Security;TEE;SGX;SEV","Degradation;Technological innovation;Scientific computing;Prefetching;Memory management;Machine learning;Hardware","authorisation;graph theory;multiprocessing systems;parallel processing;performance evaluation;security of data;shared memory systems;storage management;trusted computing","large scale NUMA machines;TEE designs;secure scientific computing;high-performance computing;HPC user;execution environment;sensitive data;general purpose TEEs;performance analysis;performance degradation;scientific computing workloads;diverse HPC","",7.0,"",46.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"High-Performance Spectral Element Methods on Field-Programmable Gate Arrays : Implementation, Evaluation, and Future Projection","M. Karp; A. Podobas; N. Jansson; T. Kenter; C. Plessl; P. Schlatter; S. Markidis","KTH Royal Institute of Technology, Stockholm, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden; Paderborn University, Paderborn, Germany; Paderborn University, Paderborn, Germany; KTH Royal Institute of Technology, Stockholm, Sweden; KTH Royal Institute of Technology, Stockholm, Sweden","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1077,1086,"Improvements in computer systems have historically relied on two well-known observations: Moore's law and Dennard's scaling. Today, both these observations are ending, forcing computer users, researchers, and practitioners to abandon the general-purpose architectures' comforts in favor of emerging post-Moore systems. Among the most salient of these post-Moore systems is the Field-Programmable Gate Array (FPGA), which strikes a convenient balance between complexity and performance. In this paper, we study modern FPGAs' applicability in accelerating the Spectral Element Method (SEM) core to many computational fluid dynamics (CFD) applications. We design a custom SEM hardware accelerator operating in double-precision that we empirically evaluate on the latest Stratix 10 GX-series FPGAs and position its performance (and power-efficiency) against state-of-the-art systems such as ARM ThunderX2, NVIDIA Pascal/Volta/Ampere Teslaseries cards, and general-purpose manycore CPUs. Finally, we develop a performance model for our SEM-accelerator, which we use to project future FPGAs' performance and role to accelerate CFD applications, ultimately answering the question: what characteristics would a perfect FPGA for CFD applications have?","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460528","FPGA;CFD;HLS;NVIDIA;ARM;Intel;Nek5000","Distributed processing;Numerical analysis;Computational fluid dynamics;Graphics processing units;Hardware;Complexity theory;Field programmable gate arrays","field programmable gate arrays;multiprocessing systems;power aware computing","post-Moore systems;general-purpose architectures;computer users;Dennard's scaling;Moore's law;computer systems;future projection;Field-Programmable Gate arrays;high-performance Spectral Element methods;CFD applications;SEM-accelerator;performance model;general-purpose manycore CPUs;latest Stratix 10 GX-series FPGAs;custom SEM hardware accelerator operating;computational fluid dynamics applications;Spectral Element Method core","",6.0,"",49.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"High-Level FPGA Accelerator Design for Structured-Mesh-Based Explicit Numerical Solvers","K. Kamalakkannan; G. R. Mudalige; I. Z. Reguly; S. A. Fahmy","Dept. of Computer Science, University of Warwick, UK; Dept. of Computer Science, University of Warwick, UK; Faculty of Information Technology & Bionics, Pazmany Peter Catholic University, Hungary; King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia","2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","28 Jun 2021",2021,"","",1087,1096,"This paper presents a workflow for synthesizing near-optimal FPGA implementations of structured-mesh based stencil applications for explicit solvers. It leverages key characteristics of the application class and its computation-communication pattern and the architectural capabilities of the FPGA to accelerate solvers for high-performance computing applications. Key new features of the workflow are (1) the unification of standard state-of-the-art techniques with a number of high-gain optimizations such as batching and spatial blocking/tiling, motivated by increasing throughput for real-world workloads and (2) the development and use of a predictive analytical model to explore the design space, and obtain resource and performance estimates. Three representative applications are implemented using the design workflow on a Xilinx Alveo U280 FPGA, demonstrating near-optimal performance and over 85% predictive model accuracy. These are compared with equivalent highly-optimized implementations of the same applications on modern HPC-grade GPUs (Nvidia V100), analyzing time to solution, bandwidth, and energy consumption. Performance results indicate comparable runtimes with the V100 GPU, with over 2× energy savings for the largest non-trivial application on the FPGA. Our investigation shows the challenges of achieving high performance on current generation FPGAs compared to traditional architectures. We discuss determinants for a given stencil code to be amenable to FPGA implementation, providing insights into the feasibility and profitability of a design and its resulting performance.","1530-2075","978-1-6654-4066-0","10.1109/IPDPS49936.2021.00117","Royal Society; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460543","FPGAs;Stencil Applications;Explicit solvers","Runtime;Graphics processing units;Estimation;Predictive models;Numerical models;Space exploration;Field programmable gate arrays","field programmable gate arrays;logic design;mesh generation;numerical analysis","V100 GPU;Xilinx Alveo U280 FPG;architectural capabilities;computation-communication pattern;explicit solvers;structured-mesh based stencil applications;near-optimal FPGA implementations;structured-mesh-based explicit numerical solvers;high-level FPGA accelerator design;stencil code;energy savings;modern HPC-grade GPUs;predictive model accuracy;near-optimal performance;Xilinx Alveo U280 FPGA;design workflow;representative applications;performance estimates;design space;predictive analytical model;real-world workloads;high-gain optimizations;high-performance computing applications","",2.0,"",31.0,"IEEE","28 Jun 2021","","","IEEE","IEEE Conferences"
"vPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training","S. Zhao; F. Li; X. Chen; X. Guan; J. Jiang; D. Huang; Y. Qing; S. Wang; P. Wang; G. Zhang; C. Li; P. Luo; H. Cui","Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; 2012 Labs, Theory Lab, Huawei Technoloies, Co. Ltd, Shenzhen, China; 2012 Labs, Theory Lab, Huawei Technoloies, Co. Ltd, Shenzhen, China; 2012 Labs, Theory Lab, Huawei Technoloies, Co. Ltd, Shenzhen, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","9 Aug 2021",2022,33.0,3.0,489,506,"The increasing computational complexity of DNNs achieved unprecedented successes in various areas such as machine vision and natural language processing (NLP), e.g., the recent advanced Transformer has billions of parameters. However, as large-scale DNNs significantly exceed GPU’s physical memory limit, they cannot be trained by conventional methods such as data parallelism. Pipeline parallelism that partitions a large DNN into small subnets and trains them on different GPUs is a plausible solution. Unfortunately, the layer partitioning and memory management in existing pipeline parallel systems are fixed during training, making them easily impeded by out-of-memory errors and the GPU under-utilization. These drawbacks amplify when performing neural architecture search (NAS) such as the evolved Transformer, where different network architectures of Transformer needed to be trained repeatedly. vPipe is the first system that transparently provides dynamic layer partitioning and memory management for pipeline parallelism. vPipe has two unique contributions, including (1) an online algorithm for searching a near-optimal layer partitioning and memory management plan, and (2) a live layer migration protocol for re-balancing the layer distribution across a training pipeline. vPipe improved the training throughput of two notable baselines (Pipedream and GPipe) by 61.4-463.4 percent and 24.8-291.3 percent on various large DNNs and training settings.","1558-2183","","10.1109/TPDS.2021.3094364","Huawei Innovation Research Program(grant numbers:HK RGC ECS 27200916,HK RGC GRF 17207117,17202318,27208720); Croucher Innovation Award; National Natural Science Foundation of China(grant numbers:61802358); USTC Research Funds of Double First-Class Initiative(grant numbers:YD2150002006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9472938","Machine learning;distributed systems;distributed artificial intelligence;pipeline;parallel systems;memory management","Pipelines;Training;Graphics processing units;Throughput;Memory management;Parallel processing;Tensors","deep learning (artificial intelligence);neural net architecture;parallel processing;pipeline processing;storage management","pipeline parallelism;neural architecture search;Transformer;vPipe;near-optimal layer partitioning;memory management plan;live layer migration protocol;layer distribution;virtualized acceleration system;dynamic layer partitioning;parallel DNN training","",3.0,"",62.0,"CCBYNCND","2 Jul 2021","","","IEEE","IEEE Journals"
"Optimal Checkpointing Strategies for Iterative Applications","Y. Du; L. Marchal; G. Pallez; Y. Robert","ENS Lyon, LIP, Lyon, France; CNRS & Inria, LIP, École Normale Supérieure de Lyon, Lyon, France; Inria & Université de Bordeaux, Talence, France; University of Tennessee Knoxville, Knoxville, TN, USA","IEEE Transactions on Parallel and Distributed Systems","5 Aug 2021",2022,33.0,3.0,507,522,"This work provides an optimal checkpointing strategy to protect iterative applications from fail-stop errors. We consider a general framework, where the application repeats the same execution pattern by executing consecutive iterations, and where each iteration is composed of several tasks. These tasks have different execution lengths and different checkpoint costs. Assume that there are n tasks and that task ai, where 0 ≤ i <; n, has execution time ti and checkpoint cost ci. A naive strategy would checkpoint after each task. Another naive strategy would checkpoint at the end of each iteration. A strategy inspired by the Young/Daly formula would work for √{2 μcave } seconds, where μ is the application MTBF and cave is the average checkpoint time, and checkpoint at the end of the current task (and repeat). Another strategy, also inspired by the Young/Daly formula, would select the task amin with smallest checkpoint cost cmin and would checkpoint after every pth instance of that task, leading to a checkpointing period p T, where T = Σi=0n-1 ai is the time per iteration. One would choose the period so that p T ≈ √{2 μcmin} to obey the Young/Daly formula. All these naive and Young/Daly strategies are suboptimal. Our main contribution is to show that the optimal checkpoint strategy is globally periodic, and to design a dynamic programming algorithm that computes the optimal checkpointing pattern. This pattern may well checkpoint many different tasks, and this across many different iterations. We show through simulations, both from synthetic and real-life application scenarios, that the optimal strategy outperforms the naive and Young/Daly strategies.","1558-2183","","10.1109/TPDS.2021.3099440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495174","Iterative application;checkpoint strategy;fail-stop error;resilience","Task analysis;Checkpointing;Iterative methods;Heuristic algorithms;Fault tolerant systems;Fault tolerance;Dynamic programming","checkpointing;dynamic programming;iterative methods;naive Bayes methods;optimisation","optimal checkpointing strategy;iterative applications;execution pattern;naive strategy;MTBF;average checkpoint time;checkpoint costs;dynamic programming algorithm;Young/Daly strategies","",2.0,"",49.0,"IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Work-Stealing Prefix Scan: Addressing Load Imbalance in Large-Scale Image Registration","M. Copik; T. Grosser; T. Hoefler; P. Bientinesi; B. Berkels","Department of Computer Science, ETH Zurich, Zürich, Switzerland; School of Informatics, University of Edinburgh, Edinburgh, U.K; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computing Science, Umeå University, Umeå, Sweden; AICES Graduate School, Institute for Geometry and Practical Mathematics, RWTH Aachen University, Aachen, Germany","IEEE Transactions on Parallel and Distributed Systems","27 Jul 2021",2022,33.0,3.0,523,535,"Parallelism patterns (e.g., map or reduce) have proven to be effective tools for parallelizing high-performance applications. In this article, we study the recursive registration of a series of electron microscopy images - a time consuming and imbalanced computation necessary for nano-scale microscopy analysis. We show that by translating the image registration into a specific instance of the prefix scan, we can convert this seemingly sequential problem into a parallel computation that scales to over thousand of cores. We analyze a variety of scan algorithms that behave similarly for common low-compute operators and propose a novel work-stealing procedure for a hierarchical prefix scan. Our evaluation shows that by identifying a suitable and well-optimized prefix scan algorithm, we reduce time-to-solution on a series of 4,096 images spanning ten seconds of microscopy acquisition from over 10 hours to less than 3 minutes (using 1024 Intel Haswell cores), enabling derivation of material properties at nanoscale for long microscopy image series.","1558-2183","","10.1109/TPDS.2021.3095230","Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung(grant numbers:PZ00P2168016); Excellence Initiative of German Federal and State Governments(grant numbers:GSC 111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477174","Prefix sum;parallel algorithms;work stealing;load balancing;image registration","Microscopy;Image registration;Heuristic algorithms;Scanning electron microscopy;Load management;Standards;Parallel algorithms","image registration;parallel processing;resource allocation","work-stealing prefix scan;load imbalance;image registration;parallelism patterns;high-performance applications;recursive registration;nanoscale microscopy analysis;parallel computation;hierarchical prefix scan;microscopy acquisition;electron microscopy images","",3.0,"",34.0,"IEEE","7 Jul 2021","","","IEEE","IEEE Journals"
"Decentralized Edge Intelligence: A Dynamic Resource Allocation Framework for Hierarchical Federated Learning","W. Y. B. Lim; J. S. Ng; Z. Xiong; J. Jin; Y. Zhang; D. Niyato; C. Leung; C. Miao","Alibaba Group and Alibaba-NTU Joint Research Institute (JRI), Nanyang Technological University (NTU), Singapore, Singapore; Alibaba Group and Alibaba-NTU Joint Research Institute (JRI), Nanyang Technological University (NTU), Singapore, Singapore; Information Systems Technology and Design (ISTD) Pillar, Singapore University of Technology and Design, Singapore, Singapore; TuSimple, Beijing 100016, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore, Singapore; Department of Electrical and Computer Engineering, The University of British Columbia, Vancouver, BC, Canada; LILY, Alibaba-NTU JRI, Singapore, Singapore","IEEE Transactions on Parallel and Distributed Systems","16 Aug 2021",2022,33.0,3.0,536,550,"To enable the large scale and efficient deployment of Artificial Intelligence (AI), the confluence of AI and Edge Computing has given rise to Edge Intelligence, which leverages on the computation and communication capabilities of end devices and edge servers to process data closer to where it is produced. One of the enabling technologies of Edge Intelligence is the privacy preserving machine learning paradigm known as Federated Learning (FL), which enables data owners to conduct model training without having to transmit their raw data to third-party servers. However, the FL network is envisioned to involve thousands of heterogeneous distributed devices. As a result, communication inefficiency remains a key bottleneck. To reduce node failures and device dropouts, the Hierarchical Federated Learning (HFL) framework has been proposed whereby cluster heads are designated to support the data owners through intermediate model aggregation. This decentralized learning approach reduces the reliance on a central controller, e.g., the model owner. However, the issues of resource allocation and incentive design are not well-studied in the HFL framework. In this article, we consider a two-level resource allocation and incentive mechanism design problem. In the lower level, the cluster heads offer rewards in exchange for the data owners' participation, and the data owners are free to choose which cluster to join. Specifically, we apply the evolutionary game theory to model the dynamics of the cluster selection process. In the upper level, each cluster head can choose to serve a model owner, whereas the model owners have to compete amongst each other for the services of the cluster heads. As such, we propose a deep learning based auction mechanism to derive the valuation of each cluster head's services. The performance evaluation shows the uniqueness and stability of our proposed evolutionary game, as well as the revenue maximizing properties of the deep learning based auction.","1558-2183","","10.1109/TPDS.2021.3096076","Alibaba Group; Alibaba-NTU Singapore Joint Research Institute; National Research Foundation Singapore(grant numbers:AISG2-RP-2020-019,AISG-GC-2019-003); WASP/NTU)(grant numbers:M4082187 (4080)); Ministry of Education - Singapore(grant numbers:Tier 1 (RG16/20)); National Natural Science Foundation of China(grant numbers:62071343); Singapore University of Technology and Design(grant numbers:SRG-ISTD-2021-165); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9479786","Federated learning;edge intelligence;resource allocation;evolutionary game;auction","Training;Computational modeling;Resource management;Magnetic heads;Data models;Games;Servers","data privacy;evolutionary computation;game theory;learning (artificial intelligence);optimisation;resource allocation;telecommunication network reliability","privacy preserving machine learning paradigm;data owners;model training;raw data;third-party servers;FL network;heterogeneous distributed devices;communication inefficiency;device dropouts;Hierarchical Federated Learning framework;cluster head;intermediate model aggregation;model owner;incentive design;HFL framework;two-level resource allocation;incentive mechanism design problem;cluster selection process;deep learning based auction mechanism;decentralized Edge Intelligence;dynamic resource allocation framework;Artificial Intelligence;AI;Edge Computing;communication capabilities;edge servers;data closer","",46.0,"",40.0,"IEEE","9 Jul 2021","","","IEEE","IEEE Journals"
"cuNH: Efficient GPU Implementations of Post-Quantum KEM NewHope","Y. Gao; J. Xu; H. Wang","NUS-Singtel Cyber Security Research and Development Laboratory, National University of Singapore, Singapore; NUS-Singtel Cyber Security Research and Development Laboratory, National University of Singapore, Singapore; NUS-Singtel Cyber Security Research and Development Laboratory, National University of Singapore, Singapore","IEEE Transactions on Parallel and Distributed Systems","5 Aug 2021",2022,33.0,3.0,551,568,"Post-quantum cryptography was proposed in the past years due to the foreseeable emergence of quantum computers that are able to break the conventional public key cryptosystems at acceptable costs. However, post-quantum schemes are usually less efficient than conventional ones, which makes them less practical in scenarios with limited resources or high concurrency. Server-side applications always feature multiple users, therefore requiring efficient execution of batch tasks. GPU is intrinsically well-suited to batch tasks owing to its SIMD/SIMT execution fashion, so it naturally helps to achieve high performance. However, a naive GPU-based implementation cannot make the best use of hardware resources of the GPU regardless of task loads. In this article, we propose SIMD parallelization paradigms for fine-grained GPU implementations and then apply them to a post-quantum key encapsulation algorithm called NewHope, where we carefully design every module, especially NTT and inverse NTT, to fit into the SIMD parallelization paradigms. In addition, we employ multi-streaming to improve performance in user's perspective. Finally, our evaluations are made on two testbeds with GPU accelerators NVIDIA GeForce MX150 and GeForce GTX 1650, respectively. The experimental results show that the fine-grained implementations save up to 98 percent latency at low task loads, and their throughputs increase by up to 86 percent at high task loads, when compared with the naive ones in kernel's perspective, and the multi-streaming implementations greatly reduce the latency overhead percentage at high task loads by up to 86 percent, when compared with the fine-grained implementation in user's perspective. Moreover, our fine-grained implementation and multi-streaming implementation are respectively 51.5 and 45.5 percent faster than Gupta et al.'s implementations when compared with it under reasonable assumptions. Furthermore, as lattice-based post-quantum schemes have similar operations, our proposal also easily applies to other lattice-based post-quantum schemes.","1558-2183","","10.1109/TPDS.2021.3097277","National Research Foundation Singapore; Singapore Telecommunications Limited; National Natural Science Foundation of China(grant numbers:61632020,U1936209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9485052","NewHope;number-theoretic transform (NTT);post-quantum cryptography (PQC);ring-LWE;CUDA-enabled GPU","Graphics processing units;Task analysis;Parallel processing;Cryptography;Kernel;Instruction sets;Hardware","computer graphic equipment;coprocessors;graphics processing units;parallel architectures;parallel processing;public key cryptography;quantum cryptography","GeForce GTX 1650;lattice-based post-quantum schemes;multistreaming implementation;high task loads;low task loads;GPU accelerators NVIDIA GeForce MX150;user;post-quantum key encapsulation algorithm;fine-grained GPU implementations;SIMD parallelization paradigms;hardware resources;naive GPU-based implementation;batch tasks;multiple users;high concurrency;conventional public key cryptosystems;quantum computers;post-quantum cryptography;post-quantum KEM NewHope;efficient GPU implementations;efficiency 86.0 percent;efficiency 45.5 percent;efficiency 98.0 percent","",5.0,"",26.0,"IEEE","14 Jul 2021","","","IEEE","IEEE Journals"
"Online Pricing and Trading of Private Data in Correlated Queries","H. Cai; F. Ye; Y. Yang; Y. Zhu; J. Li; F. Xiao","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical and Computer Engineering, Stony Brook University, New York, NY, USA; Department of Electrical and Computer Engineering, Stony Brook University, New York, NY, USA; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China","IEEE Transactions on Parallel and Distributed Systems","6 Aug 2021",2022,33.0,3.0,569,585,"With the commoditization of private data, data trading in consideration of user privacy protection has become a fascinating research topic. The trading for private web browsing histories brings huge economic value to data consumers when leveraged by targeted advertising. And the online pricing of these private data further helps achieve more realistic data trading. In this paper, we study the trading and pricing of multiple correlated queries on private web browsing history data at the same time. We propose CTRADE, which is a novel online data CommodiTization fRamework for trAding multiple correlateD queriEs over private data. CTRADE first devises a modified matrix mechanism to perturb query answers. It especially quantifies privacy loss under the relaxation of classical differential privacy and a newly devised mechanism with relaxed matrix sensitivity, and further compensates data owners for their diverse privacy losses in a satisfying manner. CTRADE then proposes an ellipsoid-based query pricing mechanism according to a given linear market value model, which exploits the features of the ellipsoid to explore and exploit the close-optimal dynamic price at each round. In particular, the proposed mechanism produces a low cumulative regret, which is quadratic in the dimension of the feature vector and logarithmic in the number of total rounds. Through real-data based experiments, our analysis and evaluation results demonstrate that CTRADE balances total error and privacy preferences well within acceptable running time, indeed produces a convergent cumulative regret with more rounds, and also achieves all desired economic properties of budget balance, individual rationality, and truthfulness.","1558-2183","","10.1109/TPDS.2021.3095238","2030 National Key AI Program of China(grant numbers:2018AAA0100503,2018AAA0100500); National Natural Science Foundation of China(grant numbers:61772341,61472254,61772338,61672240); Science and Technology Commission of Shanghai Municipality(grant numbers:18511103002,19510760500,19511101500); Program for Changjiang Young Scholars in University of China; Program for China Top Young Talents; Program for Shanghai Top Young Talents; SJTU Global Strategic Partnership Fund 2019 SJTU-HKUST; NUPTSF(grant numbers:NY220134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477119","Data trading;data pricing;web browsing history;data privacy","Pricing;Privacy;Distributed databases;History;Economics;Correlation;Time complexity","data privacy;pricing;query processing","online pricing;private web browsing histories;realistic data trading;CTRADE;compensates data owners;query pricing mechanism;real-data based experiments;online data commoditization framework;multiple correlated queries;convergent cumulative regret;relaxed matrix sensitivity","",4.0,"",43.0,"Crown","7 Jul 2021","","","IEEE","IEEE Journals"
"Resilient Real-Valued Consensus in Spite of Mobile Malicious Agents on Directed Graphs","Y. Wang; H. Ishii; F. Bonnet; X. Défago","Division of Decision and Control Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan; Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan; Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2021",2022,33.0,3.0,586,603,"This article addresses novel real-valued consensus problems in the presence of malicious adversaries that can move within the network and induce faulty behaviors in the attacked agents. By adopting several mobile adversary models from the computer science literature, we develop protocols which can mitigate the influence of such malicious agents. The algorithms follow the class of mean subsequence reduced (MSR) algorithms, under which agents ignore the suspicious values received from neighbors during their state updates. Different from the static adversary models, even after the adversaries move away, the infected agents may remain faulty in their values, whose effects must be taken into account. We develop conditions on the network structures for both the complete and non-complete directed graph cases, under which the proposed algorithms are guaranteed to attain resilient consensus. The tolerance bound for network conditions becomes more strict as the adversaries are allowed to have more power. Extensive simulations are carried out over random graphs to verify the effectiveness of our approach when the information of the adversarial agents in terms of their models and numbers is unknown to the agents.","1558-2183","","10.1109/TPDS.2021.3096074","JST CREST(grant numbers:JPMJCR15K3); JST-Mirai Program(grant numbers:18077648); Scientific Research(grant numbers:18H01460); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9479723","Fault-tolerant distributed algorithms;multi-agent systems;resilient real-valued consensus;mobile adversary agents;malicious agents;network connectivity","Protocols;Directed graphs;Computer science;Computational modeling;Voltage measurement;Timing;Security","computational complexity;directed graphs;mobile computing;multi-agent systems;telecommunication security","static adversary models;infected agents;network structures;resilient consensus;network conditions;random graphs;adversarial agents;resilient real-valued consensus;mobile malicious agents;directed graphs;consensus problems;malicious adversaries;faulty behaviors;attacked agents;mobile adversary models;computer science literature;mean subsequence;suspicious values;state updates","",5.0,"",40.0,"IEEE","9 Jul 2021","","","IEEE","IEEE Journals"
"Fast and Portable Concurrent FIFO Queues With Deterministic Memory Reclamation","O. Giersch; J. Nolte","Brandenburg University of Technology (BTU) Cottbus-Senftenberg, Cottbus, Germany; Brandenburg University of Technology (BTU) Cottbus-Senftenberg, Cottbus, Germany","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2021",2022,33.0,3.0,604,616,"In this article we present an algorithm for a high performance, unbounded, portable, multi-producer/multi-consumer, lock-free FIFO (first-in first-out) queue. Aside from its competitive performance on current hardware, it is further characterized by its integrated memory reclamation mechanism, which is able to reliably and deterministically de-allocate nodes as soon as the final operation with a reference has concluded, similar to reference counting. This differentiates our approach from most other lock-free data structures, which usually require external (generic) memory reclamation or garbage collection mechanisms such as hazard pointers. Our deterministic memory reclamation mechanism completely prevents the build up of memory awaiting reclamation and is hence very memory efficient, yet it does not introduce any substantial performance overhead. By utilizing concrete knowledge about the internal structure and access patterns of our queue, we are able to construct and constrain the reclamation mechanism in such a way that keeps the overhead for memory management almost entirely out of the common fast path. The presented algorithm is portable to all modern 64-bit processor architectures, as it only relies on the commonly available and lock-free atomic synchronization primitives compare-and-swap and fetch-and-add.","1558-2183","","10.1109/TPDS.2021.3097901","Bundesministerium für Bildung und Forschung(grant numbers:01IS18072); German Science Foundation(grant numbers:625/7-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490347","Concurrent algorithms;lock-free/non-blocking data structures;memory reclamation;first in-first out (FIFO) queues;shared memory","Memory management;Instruction sets;Hazards;Indexes;Runtime;Arrays;Synchronization","concurrency control;data structures;storage management;synchronisation","lock-free FIFO;reference counting;lock-free data structures;external memory reclamation;garbage collection;access patterns;memory management;deterministic memory reclamation;integrated memory reclamation;concurrent FIFO queues;first-in first-out queue;hazard pointers;64-bit processor architectures;lock-free atomic synchronization;compare-and-swap;fetch-and-add","",1.0,"",33.0,"IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Harnessing the Potential of Function-Reuse in Multimedia Cloud Systems","C. Denninnart; M. A. Salehi","Center of Advanced Computer Study, University of Louisiana at Lafayette, Lafayette, LA, USA; Center of Advanced Computer Study, University of Louisiana at Lafayette, Lafayette, LA, USA","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2021",2022,33.0,3.0,617,629,"Cloud-based computing systems can get oversubscribed due to the budget constraints of their users or limitations in certain resource types. The oversubscription can, in turn, degrade the users perceived Quality of Service (QoS). The approach we investigate to mitigate both the oversubscription and the incurred cost is based on smart reusing of the computation needed to process the service requests (i.e., tasks). We propose a reusing paradigm for the tasks that are waiting for execution. This paradigm can be particularly impactful in serverless platforms where multiple users can request similar services simultaneously. Our motivation is a multimedia streaming engine that processes the media segments in an on-demand manner. We propose a mechanism to identify various types of “mergeable” tasks and aggregate them to improve the QoS and mitigate the incurred cost. We develop novel approaches to determine when and how to perform task aggregation such that the QoS of other tasks is not affected. Evaluation results show that the proposed mechanism can improve the QoS by significantly reducing the percentage of tasks missing their deadlines and reduce the overall time (and subsequently the incurred cost) of utilizing cloud services by more than 9 percent.","1558-2183","","10.1109/TPDS.2021.3097911","National Science Foundation(grant numbers:CNS-2007209,CNS-2047144); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490337","Task aggregation;over-subscription;serverless computing;cloud computing;video stream processing","Streaming media;Task analysis;Cloud computing;Containers;Engines;Delays;Admission control","cloud computing;multimedia computing;quality of service","function-reuse;multimedia cloud systems;cloud-based computing systems;budget constraints;resource types;oversubscription;QoS;incurred cost;smart reusing;service requests;reusing paradigm;serverless platforms;multiple users;similar services;multimedia streaming engine;media segments;on-demand manner;mergeable tasks;cloud services","",1.0,"",41.0,"IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Multi-Task Federated Learning for Personalised Deep Neural Networks in Edge Computing","J. Mills; J. Hu; G. Min","Department of Computer Science, University of Exeter, Exeter, U.K.; Department of Computer Science, University of Exeter, Exeter, U.K.; Department of Computer Science, University of Exeter, Exeter, U.K.","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2021",2022,33.0,3.0,630,641,"Federated Learning (FL) is an emerging approach for collaboratively training Deep Neural Networks (DNNs) on mobile devices, without private user data leaving the devices. Previous works have shown that non-Independent and Identically Distributed (non-IID) user data harms the convergence speed of the FL algorithms. Furthermore, most existing work on FL measures global-model accuracy, but in many cases, such as user content-recommendation, improving individual User model Accuracy (UA) is the real objective. To address these issues, we propose a Multi-Task FL (MTFL) algorithm that introduces non-federated Batch-Normalization (BN) layers into the federated DNN. MTFL benefits UA and convergence speed by allowing users to train models personalised to their own data. MTFL is compatible with popular iterative FL optimisation algorithms such as Federated Averaging (FedAvg), and we show empirically that a distributed form of Adam optimisation (FedAvg-Adam) benefits convergence speed even further when used as the optimisation strategy within MTFL. Experiments using MNIST and CIFAR10 demonstrate that MTFL is able to significantly reduce the number of rounds required to reach a target UA, by up to $5\times$5× when using existing FL optimisation strategies, and with a further $3\times$3× improvement when using FedAvg-Adam. We compare MTFL to competing personalised FL algorithms, showing that it is able to achieve the best UA for MNIST and CIFAR10 in all considered scenarios. Finally, we evaluate MTFL with FedAvg-Adam on an edge-computing testbed, showing that its convergence and UA benefits outweigh its overhead.","1558-2183","","10.1109/TPDS.2021.3098467","EPSRC DTP Studentship; EU Horizon 2020 INITIATE(grant numbers:101008297); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492755","Federated learning;multi-task learning;deep learning;edge computing;adaptive optimization","Training;Servers;Data models;Optimization;Convergence;Computational modeling;Adaptation models","cloud computing;deep learning (artificial intelligence);optimisation;user modelling","Adam optimisation;FedAvg-Adam;convergence speed;multitask federated learning;nonIID;user content-recommendation;federated DNN;MTFL benefits UA;personalised deep neural networks;edge computing;nonindependent and identically distributed;personalised FL algorithms;user model accuracy;multitask FL algorithm;nonfederated batch-normalization layers;iterative FL optimisation algorithms;FL optimisation strategies","",33.0,"",34.0,"IEEE","21 Jul 2021","","","IEEE","IEEE Journals"
"Propagation Pattern for Moment Representation of the Lattice Boltzmann Method","J. Gounley; M. Vardhan; E. W. Draeger; P. Valero-Lara; S. V. Moore; A. Randles","Computational Sciences and Engineering Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Department of Biomedical Engineering, Duke University, Durham, NC, USA; Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, Livermore, CA, USA; Oak Ridge National Laboratory, Computer Science and Mathematics Division, Oak Ridge, TN, USA; Department of Computer Science, University of Texas at El Paso, El Paso, TX, USA; Department of Biomedical Engineering, Duke University, Durham, NC, USA","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2021",2022,33.0,3.0,642,653,"A propagation pattern for the moment representation of the regularized lattice Boltzmann method (LBM) in three dimensions is presented. Using effectively lossless compression, the simulation state is stored as a set of moments of the lattice Boltzmann distribution function, instead of the distribution function itself. An efficient cache-aware propagation pattern for this moment representation has the effect of substantially reducing both the storage and memory bandwidth required for LBM simulations. This article extends recent work with the moment representation by expanding the performance analysis on central processing unit (CPU) architectures, considering how boundary conditions are implemented, and demonstrating the effectiveness of the moment representation on a graphics processing unit (GPU) architecture.","1558-2183","","10.1109/TPDS.2021.3098456","Oak Ridge National Laboratory; National Institutes of Health(grant numbers:1U01CA253511); American Heart Association Predoctoral Fellowship and ACM/IEEE-CS George Michael Memorial High Performance Computing Fellowship; Oak Ridge Leadership Computing Facility; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725,DE-AC52-07NA27344); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492751","Lattice Boltzmann methods;high-performance computing;fluid dynamics","Mathematical model;Lattice Boltzmann methods;Kernel;Memory management;Tensors;Boundary conditions;Bandwidth","cache storage;data compression;flow simulation;graphics processing units;lattice Boltzmann methods","moment representation;regularized lattice Boltzmann method;lattice Boltzmann distribution function;cache-aware propagation pattern;lossless compression;central processing unit architectures;boundary conditions;simulation state;graphics processing unit architecture","",2.0,"",52.0,"IEEE","21 Jul 2021","","","IEEE","IEEE Journals"
"On the Analysis of Cache Invalidation With LRU Replacement","Q. Zheng; T. Yang; Y. Kan; X. Tan; J. Yang; X. Jiang","Institute of Advanced Technology, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2021",2022,33.0,3.0,654,666,"Caching contents close to end-users can improve the network performance, while causing the problem of guaranteeing consistency. Specifically, solutions are classified into validation and invalidation, the latter of which can provide strong cache consistency strictly required in some scenarios. To date, little work on the analysis of cache invalidation has been covered. In this work, by using conditional probability to characterize the interactive relationship between existence and validity, we develop an analytical model that evaluates the performance (hit probability and server load) of four different invalidation schemes with LRU replacement under arbitrary invalidation frequency distribution. The model allows us to theoretically identify some key parameters that affect our metrics of interest and gain some common insights on parameter settings to balance the performance of cache invalidation. Compared with other cache invalidation models, our model can achieve higher accuracy in predicting the cache hit probability. We also conduct extensive simulations that demonstrate the achievable performance of our model.","1558-2183","","10.1109/TPDS.2021.3098459","National Key Research and Development Program of China(grant numbers:2020YFA0711400); Key Technologies Research and Development Program of Anhui Province(grant numbers:202004a05020078); CETC Joint Advanced Reasearch Foundation(grant numbers:6141B08080101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492756","LRU;cache;consistency;invalidation;hit probability;server load","Servers;Analytical models;Load modeling;Computational modeling;Mathematical model;Predictive models;Numerical models","cache storage;probability","LRU replacement;network performance;strong cache consistency;arbitrary invalidation frequency distribution;cache invalidation models;cache hit probability","",3.0,"",30.0,"IEEE","21 Jul 2021","","","IEEE","IEEE Journals"
"Mechanisms for Resource Allocation and Pricing in Mobile Edge Computing Systems","T. Bahreini; H. Badri; D. Grosu","Department of Computer Science, Wayne State University, Detroit, MI, USA; Department of Computer Science, Wayne State University, Detroit, MI, USA; Department of Computer Science, Wayne State University, Detroit, MI, USA","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2021",2022,33.0,3.0,667,682,"In this article, we address the resource allocation and monetization challenges in Mobile Edge Computing (MEC) systems, where users have heterogeneous demands and compete for high quality services. We formulate the Edge Resource Allocation Problem (ERAP) as a Mixed-Integer Linear Program (MILP) and prove that ERAP is NP-hard. To solve the problem efficiently, we propose two resource allocation mechanisms. First, we develop an auction-based mechanism and prove that the proposed mechanism is individually-rational and produces envy-free allocations. We also propose an LP-based approximation mechanism that does not guarantee envy-freeness, but it provides solutions that are guaranteed to be within a given distance from the optimal solution. We evaluate the performance of the proposed mechanisms by conducting an extensive experimental analysis on ERAP instances of various sizes. We use the optimal solutions obtained by solving the MILP model using a commercial solver as benchmarks to evaluate thequality of solutions. Our analysis shows that the proposed mechanisms obtain near optimal solutions for fairly large size instances of the problem in a reasonable amount of time.","1558-2183","","10.1109/TPDS.2021.3099731","National Science Foundation(grant numbers:IIS-1724227); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9496271","Edge computing;resource allocation;pricing;envy-free mechanism;approximation algorithm","Resource management;Servers;Pricing;Edge computing;Cloud computing;Cost accounting;Computational modeling","approximation theory;computational complexity;integer programming;mobile computing;pricing;resource allocation","mobile edge computing systems;heterogeneous demands;high quality services;edge resource allocation problem;mixed-integer linear program;auction-based mechanism;envy-free allocations;optimal solution;ERAP instances;LP-based approximation mechanism;NP-hard","",15.0,"",43.0,"IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Energy-Efficient Offloading for DNN-Based Smart IoT Systems in Cloud-Edge Environments","X. Chen; J. Zhang; B. Lin; Z. Chen; K. Wolter; G. Min","Fujian Provincial Key Laboratory of Network Computing and Intelligent Information Processing, Fuzhou, China; Fujian Provincial Key Laboratory of Network Computing and Intelligent Information Processing, Fuzhou, China; Fujian Provincial Collaborative Innovation Center for Optoelectronic Semiconductors and Efficient Devices, Xiamen, China; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.; Institut für Informatik, Freie Universität Berlin, Berlin, Germany; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2021",2022,33.0,3.0,683,697,"Deep Neural Networks (DNNs) have become an essential and important supporting technology for smart Internet-of-Things (IoT) systems. Due to the high computational costs of large-scale DNNs, it might be infeasible to directly deploy them in energy-constrained IoT devices. Through offloading computation-intensive tasks to the cloud or edges, the computation offloading technology offers a feasible solution to execute DNNs. However, energy-efficient offloading for DNN based smart IoT systems with deadline constraints in the cloud-edge environments is still an open challenge. To address this challenge, we first design a new system energy consumption model, which takes into account the runtime, switching, and computing energy consumption of all participating servers (from both the cloud and edge) and IoT devices. Next, a novel energy-efficient offloading strategy based on a Self-adaptive Particle Swarm Optimization algorithm using the Genetic Algorithm operators (SPSO-GA) is proposed. This new strategy can efficiently make offloading decisions for DNN layers with layer partition operations, which can lessen the encoding dimension and improve the execution time of SPSO-GA. Simulation results demonstrate that the proposed strategy can significantly reduce energy consumption compared to other classic methods.","1558-2183","","10.1109/TPDS.2021.3100298","National Natural Science Foundation of China(grant numbers:62072108); Natural Science Foundation of Fujian Province(grant numbers:2020J06014); Natural Science Foundation of Fujian Province(grant numbers:2019J01286); Young and Middle-aged Teacher Education Foundation of Fujian Province(grant numbers:JT180098); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9497712","Cloud-edge computing;IoT systems;energy-efficient offloading;deep neural networks;particle swarm optimization","Energy consumption;Internet of Things;Cloud computing;Servers;Data communication;Quality of service;Task analysis","cloud computing;energy conservation;energy consumption;genetic algorithms;Internet;Internet of Things;mobile computing;neural nets;particle swarm optimisation","DNN-based smart IoT systems;cloud-edge environments;Deep Neural Networks;essential supporting technology;important supporting technology;Internet-of-Things systems;high computational costs;large-scale DNNs;energy-constrained IoT devices;offloading computation-intensive tasks;computation offloading technology;DNN based smart IoT systems;system energy consumption model;novel energy-efficient offloading strategy;offloading decisions","",39.0,"",38.0,"IEEE","27 Jul 2021","","","IEEE","IEEE Journals"
"Timed Loops for Distributed Storage in Wireless Networks","A. Mukherjee; P. K. Deb; S. Misra","Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2021",2022,33.0,3.0,698,709,"IoT deployments that have limited memories lack sustained computation power and have limited connectivity to the Internet due to intermittent last-mile connectivity, particularly in rural and remote locations. For maintaining congestion-free operations, most of the collected data from these networks are discarded, instead of being transmitted remotely for further processing. In this article, we propose the paradigm Timed Loop Storage to distribute the data and use the underutilized bandwidth of local network links for sequentially queuing packets of computational data that are being operated on in parts in one of the IoT nodes. While the sequenced packets are executed sequentially on the target IoT device, the remaining packets, which are currently not being operated on, distribute and keep looping over the network links until they are required for processing. A time-synchronized packet deflection mechanism on each node handles data transfer and looping of individual packets. In our implementation, although we observe that the proposed approach requires data rates of 6 Mbps, it incurs only 45 Kb usage of primary storage systems even for sizeable data, ensuring scalability of the connected IoT devices' temporary storage capabilities, thereby making it useful for real-life applications.","1558-2183","","10.1109/TPDS.2021.3100780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501984","Wireless networks;Internet of Things;resource allocation;connectivity;distributed storage networks","Internet of Things;Delays;Task analysis;Servers;Performance evaluation;Hardware;Computer architecture","Internet of Things;queueing theory;radio links;radio networks;telecommunication computing","underutilized bandwidth;local network links;computational data;IoT nodes;sequenced packets;target IoT device;remaining packets;data transfer;individual packets;data rates;primary storage systems;sizeable data;connected IoT devices;distributed Storage;wireless networks;IoT deployments;computation power;rural locations;remote locations;time synchronized packet deflection mechanism;paradigm timed loop storage;congestion free operations;timed loops","",3.0,"",18.0,"IEEE","29 Jul 2021","","","IEEE","IEEE Journals"
"Efficient, Dynamic Multi-Task Execution on FPGA-Based Computing Systems","U. I. Minhas; R. Woods; D. S. Nikolopoulos; G. Karakonstantis","School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.; School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.","IEEE Transactions on Parallel and Distributed Systems","17 Aug 2021",2022,33.0,3.0,710,722,"With growing Field Programmable Gate Array (FPGA) device sizes and their integration in environments enabling sharing of computing resources such as cloud and edge computing, there is a requirement to share the FPGA area between multiple tasks. The resource sharing typically involves partitioning the FPGA space into fix-sized slots. This results in suboptimal resource utilisation and relatively poor performance, particularly as the number of tasks increase. Using OpenCL's exploration capabilities, we employ clever clustering and custom, task-specific partitioning and mapping to create a novel, area sharing methodology where task resource requirements are more effectively managed. Using models with varying resource/throughput profiles, we select the most appropriate distribution based on the runtime, workload needs to enhance temporal compute density. The approach is enabled in the system stack by a corresponding task-based virtualisation model. Using 11 high performance tasks from graph analysis, linear algebra and media streaming, we demonstrate an average 2.8× higher system throughput at 2.3× better energy efficiency over existing approaches.","1558-2183","","10.1109/TPDS.2021.3101153","European Commission(grant numbers:Grant 6876281 (VINEYARD)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502563","FPGA systems;resource sharing;scheduling and runtime, partitioning and mapping;task-based virtualisation;clustering","Task analysis;Field programmable gate arrays;Runtime;Throughput;Resource management;Dynamic scheduling;Space exploration","cloud computing;distributed processing;field programmable gate arrays;graph theory;resource allocation","dynamic multitask execution;FPGA-based computing systems;Field Programmable Gate Array device sizes;computing resources;cloud computing;edge computing;FPGA area;resource sharing;FPGA space;fix-sized slots;suboptimal resource utilisation;task-specific partitioning;area sharing methodology;task resource requirements;temporal compute density;corresponding task-based virtualisation model;OpenCL exploration capabilities;graph analysis;linear algebra;media streaming;energy efficiency;varying resource-throughput profiles","",4.0,"",36.0,"CCBY","30 Jul 2021","","","IEEE","IEEE Journals"
"ViTrack: Efficient Tracking on the Edge for Commodity Video Surveillance Systems","L. Cheng; J. Wang; Y. Li","School of Software and BNrist, Tsinghua University, Beijing, China; School of Software and BNrist, Tsinghua University, Beijing, China; School of Software and BNrist, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","17 Aug 2021",2022,33.0,3.0,723,735,"Nowadays, video surveillance systems are widely deployed in various places, e.g., schools, parks, airports, roads, etc. However, existing video surveillance systems are far from full utilization due to high computation overhead in video processing. In this work, we present ViTrack, a framework for efficient multi-video tracking using computation resource on the edge for commodity video surveillance systems. In the heart of ViTrack lies a two layer spatial/temporal compressed target detection method to significantly reduce the computation overhead by combining videos from multiple cameras. Further, ViTrack derives the video relationship and camera information even in absence of camera location, direction, etc. To alleviate the impact of variant video quality and missing targets, ViTrack leverages a Markov Model based approach to efficiently recover missing information and finally derive the complete trajectory. We implement ViTrack on a real deployed video surveillance system with 110 cameras. The experiment results demonstrate that ViTrack can provide efficient trajectory tracking with processing time 45x less than the existing approach. For 110 video cameras, ViTrack can run on a Dell OptiPlex 390 computer to track given targets in almost real time. We believe ViTrack can enable practical video analysis for widely deployed commodity video surveillance systems.","1558-2183","","10.1109/TPDS.2021.3081254","National Key Research and Development Program of China(grant numbers:2017YFB1003000); National Natural Science Foundation of China(grant numbers:61932013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452000","Video tracking;edge computing","Cameras;Video surveillance;Trajectory;Image edge detection;Target tracking;Object detection;Trajectory tracking","microcomputers;object detection;video cameras;video signal processing;video surveillance","ViTrack;efficient tracking;existing video surveillance systems;high computation overhead;video processing;efficient multivideo tracking;video relationship;camera information;variant video quality;missing targets;deployed video surveillance system;110 video cameras;practical video analysis;widely deployed commodity video surveillance systems","",2.0,"",32.0,"IEEE","11 Jun 2021","","","IEEE","IEEE Journals"
"Necessary Feasibility Analysis for Mixed-Criticality Real-Time Embedded Systems","H. S. Chwa; H. Baek; J. Lee","Department of Information and Communication Engineering, DGIST, Daegu, South Korea; Department of Computer Science and Engineering, Incheon National University, Incheon, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Parallel and Distributed Systems","4 Nov 2021",2022,33.0,7.0,1520,1537,"As multiple software components with different safety-criticality levels are integrated on a shared computing platform, a real-time embedded system becomes a mixed-criticality (MC) system, which should provide timing guarantees at all different levels of assurance to software components with different criticality levels. In the real-time systems community, the concept of an MC system is regarded as a promising, emerging solution to solve an inherent challenge of real-time systems: pessimistic reservation of computing resources, which yields a low resource-utilization for the sake of guaranteeing timing requirements. Since a timing guarantee should be provided before a real-time system starts to operate, its feasibility has been extensively studied for single-criticality systems; however, the same cannot be said for MC systems. In this article, we develop necessary feasibility tests for MC real-time embedded systems, which is the first study that yields non-trivial results for MC necessary feasibility on both uniprocessor and multiprocessor platforms. To this end, we investigate characteristics of MC necessary feasibility conditions, and identify new challenges posed by the characteristics. By addressing those challenges, we develop two collective necessary feasibility tests and their simplified versions, which are able to exploit a tradeoff between capability in finding infeasible task sets and time-complexity. The simulation results demonstrate that the proposed tests find a number of additional infeasible task sets for both uniprocessor and multiprocessor platforms, which have been proven neither feasible nor infeasible by any existing studies.","1558-2183","","10.1109/TPDS.2021.3118610","National Research Foundation of Korea(grant numbers:2020R1F1A1076058,2021R1A2B5B02001758,2019R1F1A1059663,2017M3A9G8084463); Institute for Information & communications Technology Planning & Evaluation(grant numbers:2014-3-00065); DGIST R&D Program of MSIT(grant numbers:20-CoE-IT-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563206","Real-time embedded systems;mixed-criticality systems;necessary feasibility analysis;timing guarantees;uniprocessor and multiprocessor platforms","Task analysis;Real-time systems;Timing;Scheduling algorithms;Embedded systems;Complexity theory;Program processors","computational complexity;embedded systems;multiprocessing systems;object-oriented programming;program testing;safety-critical software","necessary feasibility analysis;mixed-criticality real-time embedded systems;software components;shared computing platform;timing guarantee;real-time systems community;computing resources;timing requirements;single-criticality systems;uniprocessor platforms;multiprocessor platforms;MC necessary feasibility conditions;collective necessary feasibility tests;time-complexity;safety-criticality levels;resource-utilization;infeasible task sets","",4.0,"",30.0,"IEEE","7 Oct 2021","","","IEEE","IEEE Journals"
"Topology-Aware Neural Model for Highly Accurate QoS Prediction","J. Li; H. Wu; J. Chen; Q. He; C. -H. Hsu","School of Information Science and Engineering, Yunnan University, Kunming, Yunnan, China; School of Information Science and Engineering, Yunnan University, Kunming, Yunnan, China; School of Information Science and Engineering, Yunnan University, Kunming, Yunnan, China; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; Department of Medical Research, China Medical University Hospital, China Medical University, Taiwan","IEEE Transactions on Parallel and Distributed Systems","4 Nov 2021",2022,33.0,7.0,1538,1552,"With the widespread deployment of various cloud computing and service-oriented systems, there is a rapidly increasing demand for collaborative quality-of-service (QoS) prediction. Existing QoS prediction methods have made great progress in modeling users and services as well as exploiting contexts of service invocations. However, they ignore the completion of service requests/responses relies on the underlying network topology and the complex interactions between Autonomous Systems. To tackle this challenge, we propose a topology-aware neural (TAN) model for collaborative QoS prediction. In the TAN model, the features of users, services, and intermediate nodes on the communication path are projected to a shared latent space as input features. To jointly characterize the invocation process, the path features and end-cross features are captured respectively through an explicit path modeling layer and an implicit cross-modeling layer. After that, a gating layer fuses and transmits these features to the prediction layer for estimating unknown QoS values. In this way, TAN provides a flexible framework that can comprehensively capture the invocation context for making accurate QoS prediction. Experimental results on two real-world datasets demonstrate that TAN significantly outperforms state-of-the-art methods on the tasks of response time, throughput, and reliability prediction. Also, TAN shows better extensibility of using auxiliary information.","1558-2183","","10.1109/TPDS.2021.3116865","National Natural Science Foundation of China(grant numbers:61962061,61562090,61872084); Yunnan Provincial Foundation for Leaders of Disciplines in Science and Technology(grant numbers:202005AC160005); Top Young Talents of ”Ten Thousand Plan” in Yunnan Province(grant numbers:YNWR-QNBJ-2019-188); Yunnan University; Guangdong-Hong Kong-Macao Joint Laboratory for Intelligent Micro-Nano Optoelectronic Technology(grant numbers:2020B1212030010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555220","QoS prediction;deep neural networks;topology-aware modeling;end-to-end interaction;communication path","Quality of service;Cloud computing;Predictive models;Neural networks;Computational modeling;Network topology;Collaboration","cloud computing;quality of service;service-oriented architecture;Web services","topology-aware neural model;highly accurate QoS prediction;cloud computing;service-oriented systems;rapidly increasing demand;quality-of-service prediction;QoS prediction methods;service invocations;underlying network topology;complex interactions;Autonomous Systems;collaborative QoS prediction;TAN model;communication path;shared latent space;invocation process;path features;end-cross features;explicit path modeling layer;implicit cross-modeling layer;gating layer fuses;transmits these features;prediction layer;unknown QoS values;invocation context;reliability prediction","",5.0,"",63.0,"IEEE","30 Sep 2021","","","IEEE","IEEE Journals"
"Exploring Data Analytics Without Decompression on Embedded GPU Systems","Z. Pan; F. Zhang; Y. Zhou; J. Zhai; X. Shen; O. Mutlu; X. Du","Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, ETH Zurich, Zurich, Switzerland; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","4 Nov 2021",2022,33.0,7.0,1553,1568,"With the development of computer architecture, even for embedded systems, GPU devices can be integrated, providing outstanding performance and energy efficiency to meet the requirements of different industries, applications, and deployment environments. Data analytics is an important application scenario for embedded systems. Unfortunately, due to the limitation of the capacity of the embedded device, the scale of problems handled by the embedded system is limited. In this paper, we propose a novel data analytics method, called G-TADOC, for efficient text analytics directly on compression on embedded GPU systems. A large amount of data can be compressed and stored in embedded systems, and can be processed directly in the compressed state, which greatly enhances the processing capabilities of the systems. Particularly, G-TADOC has three innovations. First, a novel fine-grained thread-level workload scheduling strategy for GPU threads has been developed, which partitions heavily-dependent loads adaptively in a fine-grained manner. Second, a GPU thread-safe memory pool has been developed to handle inconsistency with low synchronization overheads. Third, a sequence-support strategy is provided to maintain high GPU parallelism while ensuring sequence information for lossless compression. Moreover, G-TADOC involves special optimizations for embedded GPUs, such as utilizing the CPU-GPU shared unified memory. Experiments show that G-TADOC provides 13.2× average speedup compared to the state-of-the-art TADOC. G-TADOC also improves performance-per-cost by 2.6× and energy efficiency by 32.5× over TADOC.","1558-2183","","10.1109/TPDS.2021.3119402","National Key Research and Development Program of China(grant numbers:2018YFB1004401); National Natural Science Foundation of China(grant numbers:61732014,62172419,U20A20226,61802412); Natural Science Foundation of Beijing Municipality(grant numbers:4202031); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program(grant numbers:20191080594); State Key Laboratory of Computer Architecture(grant numbers:CARCHA202007); GHfund A(grant numbers:20210701); CCF-Tencent Open Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9568774","TADOC;embedded GPU systems;compression;data analytics","Graphics processing units;Embedded systems;Data analysis;Parallel processing;Instruction sets;Optimization;Random access memory","data compression;embedded systems;graphics processing units;multiprocessing systems;multi-threading;storage management;synchronisation","embedded GPU systems;embedded system;GPU devices;energy efficiency;embedded device;novel data analytics method;efficient text analytics;G-TADOC;novel fine-grained thread-level workload scheduling strategy;GPU threads;GPU thread-safe memory pool;embedded GPUs","",7.0,"",78.0,"IEEE","12 Oct 2021","","","IEEE","IEEE Journals"
"SaPus: Self-Adaptive Parameter Update Strategy for DNN Training on Multi-GPU Clusters","Z. Zhang; C. Wang","Department of Computer Science, The University of Hong Kong, Hong Kong; Department of Computer Science, The University of Hong Kong, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","4 Nov 2021",2022,33.0,7.0,1569,1580,"Parameter server architecture has been identified as an efficient framework for scaling DNNs training on clusters. For large-scale deployment, communication becomes the bottleneck, and the parameter updating strategy strongly impacts the training performance and accuracy. Recent state-of-art solutions have adopted the local SGD approach, which enables workers to update their local version of models and only aggregate them to update the global parameters after finishing a number of iterations, to alleviate heavy communication pressure on the parameter server and improving the training performance. We identify three limitations of these works. First, these works do not provide an approach for determining when the worker is to update the parameter with the server under asynchronous communication strategies that can guarantee the training performance. Second, local SGD suffers from the problem of unbounded gradient delay. Previous work works well for a short delay while can not guarantee the performance with an increase of gradient delay. Third, they do not consider the system performance when determining the update interval of the local SGD, including the CPU, memory, and network, which affects the training performance extremely. We provide a self-adaptive parameter updating strategy called SaPus, which allows each worker to detect their training results through quantification of the accumulated gradient updates and determine when to update the parameter with the server adaptively and individually. Theoretical lower and upper bound of the update interval is also provided. We also propose a weighted aggregation algorithm based on a global-loss window, which is used to collect the most recent loss value of other workers to calculate a weight for the accumulated gradients of each worker to solve the unbounded delay problem in asynchronous local SGD. To increase the robustness of our parameter updating strategy, a performance model is built to provide a resource-aware lower bound for the update interval. Extensive experimental results generated on GPU cluster indicate that our model improves the training performance of DNNs, achieving up to $66.67\%$66.67% speedup as compared with state-of-art solutions. Further, results show the CPU utilization of server dropped by up to $81.1\%$81.1% and network bandwidth usage reduced to less than $1~Gbps$1Gbps on an average during the training.","1558-2183","","10.1109/TPDS.2021.3118609","Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); RGC Collaborative Research Fund(grant numbers:C6021-19EF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563229","Distributed DNN training;self-driven;distributed optimization;communication reduction;local SGD","Training;Servers;Delays;Graphics processing units;Computer architecture;Bandwidth;System performance","deep learning (artificial intelligence);gradient methods;graphics processing units;storage management","asynchronous communication strategies;unbounded gradient delay;update interval;asynchronous local SGD;self-adaptive parameter update strategy;DNN training;parameter server architecture;SaPus;multiGPU clusters;CPU;weighted aggregation algorithm;global-loss window;resource-aware lower bound","",1.0,"",31.0,"IEEE","7 Oct 2021","","","IEEE","IEEE Journals"
"LOCUS: User-Perceived Delay-Aware Service Placement and User Allocation in MEC Environment","Y. Chen; S. Zhang; Y. Jin; Z. Qian; M. Xiao; J. Ge; S. Lu","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Computer Science and Technology/Suzhou Institute for Advanced Study, University of Science and Technology of China, Hefei, China; State Key Laboratory for Novel Software Technology, Software Institute, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","4 Nov 2021",2022,33.0,7.0,1581,1592,"In the multi-access edge computing environment, app vendors deploy their services and applications at the network edges, and edge users offload their computation tasks to edge servers. We study the user-perceived delay-aware service placement and user-allocation problem in edge environment. We model the MEC-enabled network, where the user-perceived delay consists of computing delay and transmission delay. The total cost in the offloading system is defined as the sum of service placement, edge server usage and energy consumption cost, and we need to minimize the total cost by determining the overall service-placing decision and user-allocation decision, while guaranteeing that the user-perceived delay requirement of each user is fulfilled. Our considered problem is formulated as a Mixed Integer Linear Programming problem, and we prove its NP-hardness. Due to the intractability of the considered problem, we propose a LOCal-search based algorithm for USer-perceived delay-aware service placement and user-allocation in edge environment, named LOCUS, which starts with a feasible solution and then repeatedly reduces the total cost by performing local-search steps. After that, we analyze the time complexity of LOCUS and prove that it achieves provable guaranteed performance. Finally, we compare LOCUS with other existing methods and show its good performance through experiments.","1558-2183","","10.1109/TPDS.2021.3119948","National Key Research and Development Program of China(grant numbers:2017YFB1001801); National Natural Science Foundation of China(grant numbers:61872175,61832008); Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573423","User-perceived delay;service placement;user allocation;edge computing;local search","Servers;Task analysis;Resource management;Delays;Costs;Cloud computing;Heuristic algorithms","computational complexity;delays;integer programming;linear programming;mobile computing;search problems","user-perceived delay-aware service placement;edge environment;user allocation;multiaccess edge computing environment;edge users;user-allocation problem;transmission delay;user-allocation decision;user-perceived delay requirement;MEC environment;LOCal-search based algorithm;local-search steps;mixed integer linear programming problem","",6.0,"",46.0,"IEEE","14 Oct 2021","","","IEEE","IEEE Journals"
"A Fast $f(r,k+1)/k$f(r,k+1)/k-Diagnosis for Interconnection Networks Under MM* Model","Y. Huang; L. Lin; S. -Y. Hsieh","School of Computer Science and Mathematics, Fujian University of Technology, Fuzhou, Fujian, China; College of Computer and Cyber Security, Key Laboratory of Network Security and Cryptology, Fujian Normal University, Fuzhou, Fujian, China; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan","IEEE Transactions on Parallel and Distributed Systems","4 Nov 2021",2022,33.0,7.0,1593,1604,"Cyberspace is not a “vacuum space”, and it is normal that there are inevitable viruses and worms in cyberspace. Cyberspace security threats stem from the problem of endogenous security, which is caused by the incompleteness of theoretical system and technology of the information field itself. Thus it is impossible and unnecessary for us to build an “aseptic” cyberspace. On the contrast, we must focus on improving the “self-immunity” of network. Literally, endogenous security is an endogenous effect from its own structural factors rather than external ones. The $t/k$t/k-diagnosis strategy plays a very important role in measuring endogenous network security without prior knowledge, which can significantly enhance the self-diagnosing capability of network. As far as we know, few research involves $t/k$t/k-diagnosis algorithm and $t/k$t/k-diagnosability of interconnection networks under MM* model. In this article, we propose a fast $f(r,k+1)/k$f(r,k+1)/k-diagnosis algorithm of complexity $O(Nr^2)$O(Nr2), say $G$GMIS$k$kDIAGMM*, for a general $r$r-regular network $G$G under MM* model by designing a 0-comparison subgraph $M_0(G)$M0(G), where $N$N is the size of $G$G. We determine that the $t/k$t/k-diagnosability $(t(G)/k)^M$(t(G)/k)M of $G$G under MM* model is $f(r,k+1)$f(r,k+1) by $G$GMIS$k$kDIAGMM* algorithm. Moreover, we establish the $(t(G)/k)^M$(t(G)/k)M of some interconnection networks under MM* model, including BC networks, $(n,l)$(n,l)-star graph networks, and data center network DCells. Finally, we compare $(t(G)/k)^M$(t(G)/k)M with diagnosability, conditional diagnosability, pessimistic diagnosability, extra diagnosability, and good-neighbor diagnosability under MM* model. It can be seen that $(t(G)/k)^M$(t(G)/k)M is greater than other fault diagnosabilities in most cases.","1558-2183","","10.1109/TPDS.2021.3122440","National Natural Science Foundation of China(grant numbers:62102088,62171132,U1905211,61773415); Fok Ying Tung Education Foundation(grant numbers:171061); Natural Science Foundation of Fujian Province(grant numbers:2021J05228); Fujian University of Technology(grant numbers:GJ-YB-20-06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585362","Endogenous security;fault diagnosis;reliability; $t/k$    t / k     -diagnosability;interconnection networks;MM* model","Security;Program processors;Cyberspace;Hypercubes;Data centers;Multiprocessing systems;Computational modeling","computational complexity;computer centres;fault diagnosis;graph theory;multiprocessor interconnection networks;security of data","fast f(r,k+1)/kf(r,k+1)/k-diagnosis;interconnection networks;MM model;cyberspace security threats;information field;self-immunity;endogenous network security;self-diagnosing capability;t/kt/k-diagnosis algorithm;t/kt/k-diagnosability;complexity;GGMISkkDIAGMM;Υ-regular network;0-comparison subgraph;star graph networks;data center network;DCells;conditional diagnosability;pessimistic diagnosability;extra diagnosability;good-neighbor diagnosability;fault diagnosabilities","",1.0,"",37.0,"IEEE","26 Oct 2021","","","IEEE","IEEE Journals"
"Cooperative Scheduling Schemes for Explainable DNN Acceleration in Satellite Image Analysis and Retraining","W. -J. Kim; C. -H. Youn","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Transactions on Parallel and Distributed Systems","4 Nov 2021",2022,33.0,7.0,1605,1618,"The deep learning-based satellite image analysis and retraining systems are getting emerging technologies to enhance the capability of the sophisticated analysis of terrestrial objects. In principle, to apply the explainable DNN model for the process of satellite image analysis and retraining, we consider a new acceleration scheduling mechanism. Especially, the conventional DNN acceleration schemes cause serious performance degradation due to computational complexity and costs in satellite image analysis and retraining. In this article, to overcome the performance degradation, we propose cooperative scheduling schemes for explainable DNN acceleration in analysis and retraining process. For the purpose of it, we define the latency and energy cost modeling to derive the optimized processing time and cost required for explainable DNN acceleration. Especially, we show a minimum processing cost considered in the proposed scheduling via layer-level management of the explainable DNN on FPGA-GPU acceleration system. In addition, we evaluate the performance using an adaptive unlabeled data selection scheme with confidence threshold and a semi-supervised learning driven data parallelism scheme in accelerating retraining process. The experimental results demonstrate that the proposed schemes reduce the energy cost of the conventional DNN acceleration systems by up to about 40% while guaranteeing the latency constraints.","1558-2183","","10.1109/TPDS.2021.3122454","Defense Challengeable Future Technology Program; Samsung Electronics Co., Ltd(grant numbers:IO201210-07976-01); Institute for Information and Communications Technology Promotion(grant numbers:2017-0-00294); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585555","Cooperative satellite image analysis and retraining;DNN acceleration;distributed deep learning","Satellites;Image analysis;Field programmable gate arrays;Costs;Graphics processing units;Task analysis;Labeling","deep learning (artificial intelligence);field programmable gate arrays;graphics processing units;image processing;remote sensing;scheduling","explainable DNN model;acceleration scheduling mechanism;explainable DNN acceleration;energy cost;FPGA-GPU acceleration system;adaptive unlabeled data selection scheme;semisupervised learning;deep learning;retraining systems;data parallelism scheme;satellite image analysis;cooperative scheduling schemes","",2.0,"",32.0,"CCBY","26 Oct 2021","","","IEEE","IEEE Journals"
"Wukong+G: Fast and Concurrent RDF Query Processing Using RDMA-Assisted GPU Graph Exploration","Z. Yao; R. Chen; B. Zang; H. Chen","Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","4 Nov 2021",2022,33.0,7.0,1619,1635,"RDF graph has been increasingly used to store and represent information shared over the Web, including social graphs and knowledge bases. With the increasing scale of RDF graphs and the concurrency level of SPARQL queries, current RDF systems are confronted with inefficient concurrent query processing on massive data parallelism. The situation becomes more severe in the face of data-intensive queries (aka heavy query), which usually lead to suboptimal response time (latency) as well as throughput collapse. In this article, we present Wukong+G, the first graph-based distributed RDF query processing system that efficiently exploits the hybrid parallelism of CPU and GPU. Wukong+G is made fast and concurrent with four key designs. First, Wukong+G tames massive random memory accesses in graph exploration by efficiently mapping data between CPU and GPU for latency hiding, including a set of techniques like query-aware prefetching, pattern-aware pipelining and fine-grained swapping. Second, Wukong+G scales up by introducing a GPU-friendly RDF store to support RDF graphs exceeding GPU memory size, by using techniques like predicate-based grouping, pairwise caching and look-ahead replacing to narrow the gap between host and device memory scale. Third, Wukong+G scales out through a communication layer that decouples the transferring process for query metadata and intermediate results, and further leverages both native and GPUDirect RDMA to enable efficient communication on a CPU/GPU cluster. Finally, Wukong+G simultaneously runs multiple queries on a single GPU to improve overall throughput and fully exploits hardware heterogeneity (CPU/GPU) by scheduling a single query on CPU and GPU adaptively. We have implemented Wukong+G by extending a state-of-the-art distributed RDF store (i.e., Wukong) with distributed GPU support. Evaluation on a heterogeneous CPU/GPU cluster with RDMA-capable network shows that Wukong+G outperforms Wukong by up to 9.0× (from 2.3×) and scales well on 10 GPU cards for heavy queries. Wukong+G can also improve both latency and throughput by more than one order of magnitude when facing hybrid workloads.","1558-2183","","10.1109/TPDS.2021.3121568","National Key Research and Development Program of China(grant numbers:2020YFB2104100); National Natural Science Foundation of China(grant numbers:61772335,61925206); Huawei Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582823","RDF;SPARQL;graph exploration;hardware heterogeneity;RDMA","Resource description framework;Graphics processing units;History;Hardware;Throughput;Query processing;Indexes","cache storage;concurrency control;graph theory;graphics processing units;Linked Data;meta data;parallel processing;pipeline processing;query languages;query processing","concurrent RDF query processing;RDMA-assisted GPU graph exploration;RDF graph;social graphs;knowledge bases;concurrency level;SPARQL queries;data parallelism;data-intensive queries;random memory accesses;pattern-aware pipelining;Wukong+G scales;GPU-friendly RDF store;GPU memory size;device memory scale;query metadata;distributed GPU support;GPU cards;distributed RDF store;heavy query;suboptimal response time;throughput collapse;graph-based distributed RDF query processing system;hybrid parallelism;CPU;graph exploration;data mapping;latency hiding;query-aware prefetching;fine-grained swapping;predicate-based grouping;pairwise caching;look-ahead replacing;communication layer;GPUDirect RDMA;GPU cluster;hardware heterogeneity;query scheduling;hybrid workloads","",1.0,"",84.0,"IEEE","20 Oct 2021","","","IEEE","IEEE Journals"
"Pistis: Issuing Trusted and Authorized Certificates With Distributed Ledger and TEE","Z. Li; H. Wu; L. H. Lao; S. Guo; Y. Yang; B. Xiao","Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; College of Computer Science, Chongqing University, Chongqing, China; Department of Computer Science, Stony Brook University, Stony Brook, NY, USA; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","4 Nov 2021",2022,33.0,7.0,1636,1649,"The security of HTTPS fundamentally relies on SSL/TLS certificates issued by Certificate Authorities (CAs), which, however, are vulnerable to be compromised to issue unauthorized certificates (i.e., certificates issued without domains’ permission). Current countermeasures such as Certificate Transparency (CT) can only detect unauthorized certificates rather than preventing them. In this article, we present Pistis, a framework for issuing authorized and trusted certificates with the distributed ledger and Trusted Execution Environment (TEE) technology. In Pistis, TEE nodes validate whether the domain in a requested certificate passes the domain ownership validation (i.e., under corresponding applicants’ control) and submit attested results to a smart contract in the distributed ledger. The smart contract issues a certificate to the applicant when an attested result shows a pass. Therefore, Pistis can ensure its issued certificates are authorized due to the domain ownership validation mechanism in the TEE. Furthermore, as the issued certificates are stored in a Merkle Patricia Tree (MPT) in Pistis, they are trusted and can be verified by a normal user easily. The security of Pistis is formally proved in the Universally Composable (UC) framework. Compared with state-of-the-art, Pistis avoids potential damages by preventing unauthorized certificates from issuing.","1558-2183","","10.1109/TPDS.2021.3121562","HK RGC GRF PolyU(grant numbers:15217321,15216220); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582795","Distributed ledger;blockchain;smart contract;trusted execution environment (TEE);certificate issuance","Smart contracts;Blockchains;Distributed ledger;Public key;Servers;Internet;Software","blockchains;trusted computing","Pistis;unauthorized certificates;distributed ledger;TEE;Certificate Authorities;Certificate Transparency;Trusted Execution Environment technology;smart contract issues;domain ownership validation mechanism;SSL certificates;TLS certificates;Merkle Patricia Tree","",3.0,"",37.0,"IEEE","20 Oct 2021","","","IEEE","IEEE Journals"
"TODG: Distributed Task Offloading With Delay Guarantees for Edge Computing","S. Yue; J. Ren; N. Qiao; Y. Zhang; H. Jiang; Y. Zhang; Y. Yang","School of Computer and Engineering, Central South University, Changsha, Hunan, China; School of Computer and Engineering, Central South University, Changsha, Hunan, China; School of Computer and Engineering, Central South University, Changsha, Hunan, China; School of Computer and Engineering, Central South University, Changsha, Hunan, China; School of Information Science and Engineering, Hunan University, Changsha, Hunan, China; School of Computer and Engineering, Central South University, Changsha, Hunan, China; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA","IEEE Transactions on Parallel and Distributed Systems","23 Nov 2021",2022,33.0,7.0,1650,1665,"Edge computing has been an efficient way to provide prompt and near-data computing services for resource-and-delay sensitive IoT applications via computation offloading. Effective computation offloading strategies need to comprehensively cope with several major issues, including 1) the allocation of dynamic communication and computational resources, 2) delay constraints of heterogeneous tasks, and 3) requirements for computationally inexpensive and distributed algorithms. However, most of the existing works mainly focus on part of these issues, which would not suffice to achieve expected performance in complex and practical scenarios. To tackle this challenge, in this paper, we systematically study a distributed computation offloading problem with delay constraints, where heterogeneous computational tasks require continually offloading to a set of edge servers via a limiting number of stochastic communication channels. The task offloading problem is formulated as a delay-constrained long-term stochastic optimization problem under unknown prior statistical knowledge. To solve this problem, we first provide a technical path to transform and decompose it into several slot-level sub-problems. Then, we devise a distributed online algorithm, namely TODG, to efficiently allocate resources and schedule offloading tasks. Further, we present a comprehensive analysis for TODG in terms of the optimality gap, the worst-case delay, and the impact of system parameters. Extensive simulation results demonstrate the effectiveness and efficiency of TODG.","1558-2183","","10.1109/TPDS.2021.3123535","National Natural Science Foundation of China(grant numbers:62122095,62072472,U19A2067); National Key Research and Development Program of China(grant numbers:2019YFA0706403); Natural Science Foundation of Hainan Province(grant numbers:2020JJ2050); Higher Education Discipline Innovation Project(grant numbers:B18059); Young Talents Plan of Hunan Province of China(grant numbers:2019RS2001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591418","distributed task offloading;edge computing;delay guarantee;channel allocation;stochastic optimization","Task analysis;Delays;Servers;Edge computing;Resource management;Mobile handsets;Optimization","cloud computing;distributed algorithms;Internet of Things;mobile computing;optimisation;resource allocation;scheduling","TODG;distributed task offloading;delay guarantees;edge computing;prompt data computing services;near-data computing services;effective computation offloading strategies;dynamic communication;computational resources;heterogeneous tasks;computationally inexpensive distributed algorithms;distributed computation offloading problem;delay constraints;heterogeneous computational tasks;edge servers;stochastic communication channels;task offloading problem;delay-constrained long-term;stochastic optimization problem;slot-level sub-problems;schedule offloading tasks;worst-case delay","",10.0,"",51.0,"IEEE","27 Oct 2021","","","IEEE","IEEE Journals"
"Design and Simulation of Content-Aware Hybrid DRAM-PCM Memory System","Y. Fu; Y. Lu; Z. Chen; Y. Wu; N. Xiao","Army Engineering University, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, Guangdong, China; College of Command and Control Engineering, Army Engineering University, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","23 Nov 2021",2022,33.0,7.0,1666,1677,"Phase Change Memory (PCM) can directly connect persistent memory to main memory bus, while it achieves high read throughput and low standby power, the critical concerns are its poor write performance and limited durability. A naturally in-spired design is the hybrid memory architecture that fuses DRAM and PCM, so as to exploit the positive aspects of both types of memory. Unfortunately, existing solutions are seriously challenged by the limited main memory size, which is the primary bottleneck of in-memory computing. In this paper, we introduce a novel Content Aware Hybrid DRAM-PCM memory system framework—CAHRAM, which exploits deduplication to improve line sharing with high memory efficiency. It reduces write traffic to hybrid memory by removing unnecessary duplicate line writes, thereby further enhancing the write endurance of PCM. And it also substantially extends available free memory space by coalescing redundant lines in hybrid memory. We also design a reference-based page migration technique to minimize the access overheads caused by the performance gap between DRAM and PCM. Compared with the state-of-the-art in a hybrid memory simulator, our experiment results show that CAHRAM can achieve the highest I/O performance and the longest PCM lifetime with the competitive efficiencies in space and energy.","1558-2183","","10.1109/TPDS.2021.3123539","National Natural Science Foundation of China(grant numbers:61832020,61872392); Natural Science Foundation of Jiangsu Province(grant numbers:BK20191327); Zhejiang Lab(grant numbers:2021KC0AB04); Guangdong Major Project of Basic and Applied Basic Research(grant numbers:2019B030302002); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); Natural Science Foundation of Guangdong Province(grant numbers:2018B030312002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591354","Hybrid memory management;data deduplication;content awareness;page migration","Random access memory;Phase change materials;Memory management;Nonvolatile memory;Memory architecture;Power demand;Metadata","DRAM chips;phase change memories","persistent memory;memory bus;hybrid memory architecture;in-memory computing;high memory efficiency;free memory space;hybrid memory simulator;phase change memory;content aware hybrid DRAM-PCM memory system framework;write traffic;write endurance;reference-based page migration technique;access overheads","",1.0,"",45.0,"IEEE","27 Oct 2021","","","IEEE","IEEE Journals"
"TherMa-MiCs: Thermal-Aware Scheduling for Fault-Tolerant Mixed-Criticality Systems","S. Safari; H. Khdr; P. Gohari-Nazari; M. Ansari; S. Hessabi; J. Henkel","Karlsruhe Institute of Technology, Karlsruhe, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Karlsruhe Institute of Technology, Karlsruhe, Germany","IEEE Transactions on Parallel and Distributed Systems","23 Nov 2021",2022,33.0,7.0,1678,1694,"Multicore platforms are becoming the dominant trend in designing Mixed-Criticality Systems (MCSs), which integrate applications of different levels of criticality into the same platform. A well-known MCS is the dual-criticality system that is composed of low-criticality and high-criticality tasks. The availability of multiple cores on a single chip provides opportunities to employ fault-tolerant techniques, such as N-Modular Redundancy (NMR), to ensure the reliability of MCSs. However, applying fault-tolerant techniques will increase the power consumption on the chip, and thereby on-chip temperatures might increase beyond safe limits. To prevent thermal emergencies, urgent countermeasures, like Dynamic Voltage and Frequency Scaling (DVFS) or Dynamic Power Management (DPM) will be triggered to cool down the chip. Such countermeasures, however, might not only lead to suspending low-criticality tasks, but also it might lead to violating timing constraints of high-criticality tasks. In order to prevent such severe scenarios, it is indispensable to consider a temperature constraint within the scheduling process of fault-tolerant MCSs. Therefore, this paper presents, for the first time, a thermal-aware scheduling scheme for fault-tolerant MCSs, named TherMa-MiCs. In particular, TherMa-MiCs, satisfies the temperature constraint jointly with the timing constraints of the high-criticality tasks, while attempting to maximize the QoS of low-criticality tasks under the predefined constraints. At the same time, a reliability target is satisfied by employing the well-known N-Modular Redundancy (NMR) fault-tolerant technique. Experimental results show that our proposed scheme meets the temperature and timing constraints, while at the same time, improving the QoS of low-criticality tasks, with an average of 44%.","1558-2183","","10.1109/TPDS.2021.3123544","Deutsche Forschungsgemeinschaft(grant numbers:146371743); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591366","Multicores;N Modular Redundancy (NMR);mixed-criticality systems;QoS;temperature","Multicore processing;Fault tolerant systems;Timing;System-on-chip;Power demand;Redundancy;Quality of service;Temperature control;Thermal management","multiprocessing systems;power consumption;power system management;processor scheduling;quality of service","dual-criticality system;thermal-aware scheduling;fault-tolerant mixed-criticality systems;mixed-criticality systems;MCS;power consumption;dynamic voltage-frequency scaling;DVFS;dynamic power management;DPM;N-modular redundancy;NMR","",6.0,"",76.0,"IEEE","27 Oct 2021","","","IEEE","IEEE Journals"
"Performance and Cost-Efficient Spark Job Scheduling Based on Deep Reinforcement Learning in Cloud Computing Environments","M. T. Islam; S. Karunasekera; R. Buyya","Cloud Computing and Distributed Systems (CLOUDS) Lab, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Lab, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Lab, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia","IEEE Transactions on Parallel and Distributed Systems","23 Nov 2021",2022,33.0,7.0,1695,1710,"Big data frameworks such as Spark and Hadoop are widely adopted to run analytics jobs in both research and industry. Cloud offers affordable compute resources which are easier to manage. Hence, many organizations are shifting towards a cloud deployment of their big data computing clusters. However, job scheduling is a complex problem in the presence of various Service Level Agreement (SLA) objectives such as monetary cost reduction, and job performance improvement. Most of the existing research does not address multiple objectives together and fail to capture the inherent cluster and workload characteristics. In this article, we formulate the job scheduling problem of a cloud-deployed Spark cluster and propose a novel Reinforcement Learning (RL) model to accommodate the SLA objectives. We develop the RL cluster environment and implement two Deep Reinforce Learning (DRL) based schedulers in TF-Agents framework. The proposed DRL-based scheduling agents work at a fine-grained level to place the executors of jobs while leveraging the pricing model of cloud VM instances. In addition, the DRL-based agents can also learn the inherent characteristics of different types of jobs to find a proper placement to reduce both the total cluster VM usage cost and the average job duration. The results show that the proposed DRL-based algorithms can reduce the VM usage cost up to 30%.","1558-2183","","10.1109/TPDS.2021.3124670","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9599497","Cloud computing;cost-efficiency;performance improvement;deep reinforcement learning","Sparks;Cloud computing;Costs;Task analysis;Service level agreements;Big Data;Reinforcement learning","Big Data;cloud computing;cost reduction;data handling;deep learning (artificial intelligence);parallel processing;reinforcement learning;scheduling;virtual machines","cost-efficient Spark job scheduling;cloud computing environments;big data frameworks;analytics jobs;affordable compute resources;cloud deployment;big data computing clusters;complex problem;monetary cost reduction;job performance improvement;inherent cluster;workload characteristics;job scheduling problem;cloud-deployed Spark cluster;reinforcement learning model;SLA objectives;RL cluster environment;TF-Agents framework;DRL-based scheduling agents work;fine-grained level;DRL-based agents;total cluster VM usage;average job duration;DRL-based algorithms;deep reinforce learning based schedulers;service level agreement objectives;deep reinforcement learning","",13.0,"",40.0,"IEEE","2 Nov 2021","","","IEEE","IEEE Journals"
"A Highly-Available Move Operation for Replicated Trees","M. Kleppmann; D. P. Mulligan; V. B. F. Gomes; A. R. Beresford","University of Cambridge, Cambridge, U.K.; Arm Research, Cambridge, U.K.; Google, Mountain View, CA, USA; University of Cambridge, Cambridge, U.K.","IEEE Transactions on Parallel and Distributed Systems","17 Nov 2021",2022,33.0,7.0,1711,1724,"Replicated tree data structures are a fundamental building block of distributed filesystems, such as Google Drive and Dropbox, and collaborative applications with a JSON or XML data model. These systems need to support a move operation that allows a subtree to be moved to a new location within the tree. However, such a move operation is difficult to implement correctly if different replicas can concurrently perform arbitrary move operations, and we demonstrate bugs in Google Drive and Dropbox that arise with concurrent moves. In this article we present a CRDT algorithm that handles arbitrary concurrent modifications on trees, while ensuring that the tree structure remains valid (in particular, no cycles are introduced), and guaranteeing that all replicas converge towards the same consistent state. Our algorithm requires no synchronous coordination between replicas, making it highly available in the face of network partitions. We formally prove the correctness of our algorithm using the Isabelle/HOL proof assistant, and evaluate the performance of our formally verified implementation in a geo-replicated setting.","1558-2183","","10.1109/TPDS.2021.3118603","Boeing; Engineering and Physical Sciences Research Council; REMS: Rigorous Engineering for Mainstream Systems” programme(grant numbers:EP/K008528); Leverhulme Trust Early Career Fellowship; Isaac Newton Trust; Nokia Bell Labs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563274","Conflict-free replicated data types (CRDTs);formal verification;distributed filesystems;distributed collaboration","Internet;Synchronization;Computer bugs;XML;Software;Drives;Data models","data models;formal verification;replicated databases;theorem proving;tree data structures;trees (mathematics)","CRDT algorithm;arbitrary concurrent modifications;replicated tree data structures;distributed filesystems;move operations;Isabelle/HOL proof assistant","",1.0,"",57.0,"IEEE","7 Oct 2021","","","IEEE","IEEE Journals"
"FARNN: FPGA-GPU Hybrid Acceleration Platform for Recurrent Neural Networks","H. Cho; J. Lee; J. Lee","Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea","IEEE Transactions on Parallel and Distributed Systems","17 Nov 2021",2022,33.0,7.0,1725,1738,"GPU-based platforms provide high computation throughput for large mini-batch deep neural network computations. However, a large batch size may not be ideal for some situations, such as aiming at low latency, training on edge/mobile devices, partial retraining for personalization, and having irregular input sequence lengths. GPU performance suffers from low utilization especially for small-batch recurrent neural network (RNN) applications where sequential computations are required. In this article, we propose a hybrid architecture, called FARNN, which combines a GPU and an FPGA to accelerate RNN computation for small batch sizes. After separating RNN computations into GPU-efficient and GPU-inefficient tasks, we design special FPGA computation units that accelerate the GPU-inefficient RNN tasks. FARNN off-loads the GPU-inefficient tasks to the FPGA. We evaluate FARNN with synthetic RNN layers of various configurations on the Xilinx UltraScale+ FPGA and the NVIDIA P100 GPU in addition to evaluating it with real RNN applications. The evaluation result indicates that FARNN outperforms the P100 GPU platform for RNN training by up to 4.2$\times {}$× with small batch sizes, long input sequences, and many RNN cells per layer.","1558-2183","","10.1109/TPDS.2021.3124125","National Research Foundation of Korea(grant numbers:NRF-2016M3C4A7952587,NRF-2019M3E4A1080386,NRF-2019R1F1A1062335); Institute for Information and Communications Technology Promotion(grant numbers:2018-0-00581); CUDA Programming Environment for FPGA Clusters; Ministry of Science and ICT, South Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9600618","FPGA;GPU;hybrid platform;RNN","Task analysis;Training;Graphics processing units;Field programmable gate arrays;Logic gates;Recurrent neural networks;Throughput","deep learning (artificial intelligence);field programmable gate arrays;graphics processing units;logic design;recurrent neural nets","Xilinx UltraScale+ FPGA;NVIDIA P100 GPU;RNN applications;P100 GPU platform;long input sequences;FPGA-GPU hybrid acceleration platform;recurrent neural networks;GPU-based platforms;mini-batch deep neural network computations;small-batch recurrent neural network applications;sequential computations;hybrid architecture;GPU-inefficient RNN tasks;synthetic RNN layers;FPGA computation units;FARNN","",3.0,"",66.0,"IEEE","3 Nov 2021","","","IEEE","IEEE Journals"
"FFNLFD: Fault Diagnosis of Multiprocessor Systems at Local Node With Fault-Free Neighbors Under PMC Model and MM* Model","L. Lin; Y. Huang; Y. Lin; S. -Y. Hsieh; L. Xu","Key Laboratory of Network Security and Cryptology, and Center for Applied Mathematics of Fujian Province (Fujian Normal University), College of Computer and Cyber Security, Fujian Normal University, Fuzhou, Fujian, China; College of Computer and Cyber Security, Fujian Normal University, Fuzhou, Fujian, China; Key Laboratory of Network Security and Cryptology, and Center for Applied Mathematics of Fujian Province (Fujian Normal University), College of Computer and Cyber Security, Fujian Normal University, Fuzhou, Fujian, China; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Key Laboratory of Network Security and Cryptology, and Center for Applied Mathematics of Fujian Province (Fujian Normal University), College of Computer and Cyber Security, Fujian Normal University, Fuzhou, Fujian, China","IEEE Transactions on Parallel and Distributed Systems","24 Nov 2021",2022,33.0,7.0,1739,1751,"Fault diagnosability is utilized as a significant measure that reflects the reliability of a multiprocessor system. However, people frequently pay close attention to the entire system’s diagnosability while ignoring the system’s important local information. The $m$m-fault-free-neighbor local fault diagnosability (for short, $m$m-FFNLFD) is a novel indicator, which describes the diagnosability of a system at a local node with $m$m fault-free neighbors. In this paper, we propose the $m$m-FFNLFD of general networks at local node under the Preparata Metze Chien model. Moreover, we also characterize some important properties of $m$m-FFNLFD of a multiprocessor system under the comparison model. Furthermore, we apply our proposed conclusions to directly obtain the $m$m-FFNLFD of 11 well-known networks under PMC-M and MM*-M, including hypercubes, locally twisted cubes, $k$k-ary $n$n-cubes, crossed cubes, twisted hypercubes, exchanged hypercubes, star graphs, $(n,k)$(n,k)-star graphs, $(n,k)$(n,k)-arrangement graphs, data center network DCells and BCDCs. Finally, we compare the $m$m-FFNLFD with both diagnosability and conditional diagnosability, and it is shown that the $m$m-FFNLFD is greater than all the other fault diagnosabilities.","1558-2183","","10.1109/TPDS.2021.3126257","National Natural Science Foundation of China(grant numbers:62171132,62102088,U1905211,61771140); Fok Ying Tung Education Foundation(grant numbers:171061); Natural Science Foundation of Fujian Province(grant numbers:2021J05228); Fujian University of Technology(grant numbers:GJ-YB-20-06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609660","Interconnection networks;reliability;fault diagnosis;fault-free-neighbor local fault diagnosability","Multiprocessing systems;Hypercubes;Data models;Fault diagnosis;Data centers;Computer science;Computer crime","fault diagnosis;fault tolerant computing;graph theory;hypercube networks;network theory (graphs);parallel processing;program diagnostics;program testing;software reliability","FFNLFD;fault diagnosis;multiprocessor system reliability;fault-free neighbors;PMC model;MM* model;Preparata Metze Chien model;locally twisted cubes;twisted hypercubes;kk-ary nn-cubes;crossed cubes;exchanged hypercubes;star graphs;arrangement graphs;data center network DCells;BCDCs;test-based diagnostic model","",2.0,"",45.0,"IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Hamiltonian Paths of $k$k-ary $n$n-cubes Avoiding Faulty Links and Passing Through Prescribed Linear Forests","Y. Yang; L. Zhang","School of Mathematics and Information Science, Henan Normal University, Xinxiang, Henan, China; School of Mathematics and Information Science, Henan Normal University, Xinxiang, Henan, China","IEEE Transactions on Parallel and Distributed Systems","24 Nov 2021",2022,33.0,7.0,1752,1760,"The $k$k-ary $n$n-cube $Q_n^k$Qnk is one of the most attractive interconnection networks for parallel and distributed systems. Let $F$F be a set of faulty links in $Q_n^k$Qnk and let $L$L be a linear forest in $Q_n^k-F$Qnk-F such that $|E(L)|+|F|\leq 2n-3$|E(L)|+|F|≤2n-3. For any two distinct nodes $u$u and $v$v of $Q_n^k$Qnk with $n\geq 2$n≥2 and odd $k\geq 3$k≥3, we prove that $Q_n^k-F$Qnk-F admits a Hamiltonian path between $u$u and $v$v passing through $L$L if and only if none of the paths in $L$L has $u$u or $v$v as internal nodes or both of them as end-nodes. The upper bound $2n-3$2n-3 on $|E(L)|+|F|$|E(L)|+|F| is optimal in the worst case. The main results in this paper generalized some known results.","1558-2183","","10.1109/TPDS.2021.3126254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609537","Interconnection networks; $k$   k    -ary  $n$   n    -cubes;fault tolerance;prescribed linear forests;hamiltonian paths","Forestry;Fault tolerant systems;Fault tolerance;Program processors;Routing;Upper bound;Information science","computational complexity;fault tolerant computing;hypercube networks;trees (mathematics)","Hamiltonian path;faulty links;linear forests;interconnection networks;parallel system;distributed systems","",6.0,"",28.0,"IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"A Low-Power Transprecision Floating-Point Cluster for Efficient Near-Sensor Data Analytics","F. Montagna; S. Mach; S. Benatti; A. Garofalo; G. Ottavi; L. Benini; D. Rossi; G. Tagliavini","Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, Italy; Department of Information Technology and Electrical Engineering (D-ITET), ETH Zürich, 8092, Zürich, Switzerland; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, Italy; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, Italy; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, Italy; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, Italy; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, Italy; Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, Italy","IEEE Transactions on Parallel and Distributed Systems","26 Oct 2021",2022,33.0,5.0,1038,1053,"Recent applications in low-power (1-20 mW) near-sensor computing require the adoption of floating-point arithmetic to reconcile high precision results with a wide dynamic range. In this article, we propose a low-power multi-core computing cluster that leverages the fined-grained tunable principles of transprecision computing to provide support to near-sensor applications at a minimum power budget. Our solution – based on the open-source RISC-V architecture – combines parallelization and sub-word vectorization with a dedicated interconnect design capable of sharing floating-point units (FPUs) among the cores. On top of this architecture, we provide a full-fledged software stack support, including a parallel low-level runtime, a compilation toolchain, and a high-level programming model, with the aim to support the development of end-to-end applications. We performed an exhaustive exploration of the design space of the transprecision cluster on a cycle-accurate FPGA emulator, varying the number of cores and FPUs to maximize performance. Orthogonally, we performed a vertical exploration to identify the most efficient solutions in terms of non-functional requirements (operating frequency, power, and area). We conducted an experimental assessment on a set of benchmarks representative of the near-sensor processing domain, complementing the timing results with a post place-&-route analysis of the power consumption. A comparison with the state-of-the-art shows that our solution outperforms the competitors in energy efficiency, reaching a peak of 97 Gflop/s/W on single-precision scalars and 162 Gflop/s/W on half-precision vectors. Finally, a real-life use case demonstrates the effectiveness of our approach in fulfilling accuracy constraints.","1558-2183","","10.1109/TPDS.2021.3101764","European Union's Horizon 2020 research and innovation programme(grant numbers:732631 (OPRECOMP),863337 (WiPLASH),857191 (IOTWINS)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506919","RISC-V;transprecision;parallel computing;sub-word vectorization;FPU interconnect;near-sensor computing","Hardware;Dynamic range;Pipelines;Open area test sites;Optimization;Open source software;Energy consumption","field programmable gate arrays;floating point arithmetic;logic design;low-power electronics;multiprocessing systems;power consumption;reduced instruction set computing","FPUs;full-fledged software stack support;parallel low-level runtime;high-level programming model;end-to-end applications;transprecision cluster;cycle-accurate FPGA emulator;nonfunctional requirements;near-sensor processing domain;power consumption;half-precision vectors;low-power transprecision floating-point cluster;efficient near-sensor data analytics;floating-point arithmetic;high precision results;low-power multicore computing cluster;transprecision computing;near-sensor applications;minimum power budget;open-source RISC-V;dedicated interconnect design;floating-point units;low-power near-sensor computing;power 1.0 mW to 20.0 mW","",4.0,"",54.0,"IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"Exploring the Galaxyfly Family to Build Flexible-Scale Interconnection Networks","F. Lei; D. Dong; X. Liao","College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China","IEEE Transactions on Parallel and Distributed Systems","18 Oct 2021",2022,33.0,5.0,1054,1068,"Interconnection networks play an essential role in the architecture of high-performance computing (HPC) systems. In this article, we explore the Galaxyfly family to build flexible-scale interconnection networks. Galaxyfly is guaranteed to retain a small constant diameter while achieving a flexible tradeoff between network scale and bisection bandwidth. Galaxyfly not only supports small-scale interconnection networks with smaller diameter but also lowers the demands for high-radix routers and is able to utilize routers with moderate radix to build exascale interconnection networks. We analyze the constructible configuration of Galaxyfly and evaluate the properties of Galaxyfly. We conduct extensive simulations and analysis to evaluate the performance, cost, and power consumption of Galaxyfly on physical layout against state-of-the-art topologies. The results show that our design achieves better performance than most existing topologies under typical HPC workloads, and is cost-effective to deploy for exascale HPC systems.","1558-2183","","10.1109/TPDS.2021.3100783","National Key Research and Development Program of China(grant numbers:2018YFB0204300); Excellent Youth Foundation of Hunan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501493","High-performance computing;interconnection;topology;flexible-radix;low-diameter","Topology;Network topology;Multiprocessor interconnection;Scalability;Generators;Routing;Power demand","multiprocessor interconnection networks;network routing;parallel processing;power aware computing","network scale;bisection bandwidth;small-scale interconnection networks;high-radix routers;exascale interconnection networks;Galaxyfly family;high-performance computing systems;flexible-scale interconnection networks;power consumption;HPC systems","","","",48.0,"IEEE","29 Jul 2021","","","IEEE","IEEE Journals"
"Efficient and Accurate Flow Record Collection With HashFlow","Z. Zhao; X. Shi; Z. Wang; Q. Li; H. Zhang; X. Yin","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology (BNRIST), Beijing, China; Beijing National Research Center for Information Science and Technology (BNRIST), Beijing, China; Southern University of Science and Technology, Shenzhen, China; Beijing National Research Center for Information Science and Technology (BNRIST), Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China","IEEE Transactions on Parallel and Distributed Systems","18 Oct 2021",2022,33.0,5.0,1069,1083,"Traditional tools like NetFlow face great challenges as both the speed and the complexity of the network traffic increase. To keep the pace up, we propose HashFlow for more efficient and accurate collection of flow records. HashFlow keeps large flows in its main flow table and uses an ancillary table to summarize the other flows when the main table is full. With our flow collision resolution and flow record promotion schemes, a flow in the ancillary table is promoted back to the main flow table with a guaranteed probability when it becomes large enough. These operations can be performed highly efficiently, so HashFlow can keep up with ultra-high traffic speed. We implement HashFlow in a Tofino switch, and using traces from different operational networks, we compare its performance against some state-of-the-art flow measurement algorithms. Our experiments show that, for various types of traffic analysis applications, HashFlow consistently demonstrates clearly better performance than its competitors. For example, the performance of HashFlow in flow size estimation, flow size distribution estimation and heavy hitter detection is up to 21, 60 and 35 percent better than those of the best competitors respectively, and these merits of HashFlow come with almost no degradation of throughput.","1558-2183","","10.1109/TPDS.2021.3099442","National Key Research and Development Program of China(grant numbers:2018YFB1800401); National Natural Science Foundation of China(grant numbers:61972189,62002009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495221","Network measurement;flow record measurement;sketch","Switches;Data structures;Tools;Random access memory;Hash functions;Throughput;Telecommunication traffic","data flow computing;data structures;parallel algorithms;probability","flow record collection;ancillary table;ultra-high traffic speed;flow measurement algorithms;flow size estimation;flow size distribution estimation;heavy hitter detection;NetFlow;network traffic;Tofino switch;HashFlow algorithm;guaranteed probability;data structure","",1.0,"",42.0,"IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Parallel and Distributed Structured SVM Training","J. Jiang; Z. Wen; Z. Wang; B. He; J. Chen","Zhejiang University, Hangzhou, Zhejiang, China; The University of Western Australia, Crawley, WA, Australia; Zhejiang University, Hangzhou, Zhejiang, China; National University of Singapore, Singapore; South China University of Technology, Guangzhou, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1084,1096,"Structured Support Vector Machines (structured SVMs) are a fundamental machine learning algorithm, and have solid theoretical foundation and high effectiveness in applications such as natural language parsing and computer vision. However, training structured SVMs is very time-consuming, due to the large number of constraints and inferior convergence rates, especially for large training data sets. The high cost of training structured SVMs has hindered its adoption to new applications. In this article, we aim to improve the efficiency of structured SVMs by proposing a parallel and distributed solution (namely FastSSVM) for training structured SVMs building on top of MPI and OpenMP. FastSSVM exploits a series of optimizations (e.g., optimizations on data storage and synchronization) to efficiently use the resources of the nodes in a cluster and the cores of the nodes. Moreover, FastSSVM tackles the large constraint set problem by batch processing and addresses the slow convergence challenge by adapting stop conditions based on the improvement of each iteration. We theoretically prove that our solution is guaranteed to converge to a global optimum. A comprehensive experimental study shows that FastSSVM can achieve at least four times speedup over the existing solutions, and in some cases can achieve two to three orders of magnitude speedup.","1558-2183","","10.1109/TPDS.2021.3101155","National Key Research and Development Program of China(grant numbers:2020AAA0103800); MoE AcRF Tier 1(grant numbers:T1 251RES1824); National Natural Science Foundation of China(grant numbers:62072186); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019B1515130001); Oracle for Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502528","Parallel and distributed training;structured machine learning;support vector machines","Training;Optimization;Support vector machines;Task analysis;Convergence;Synchronization;Natural languages","application program interfaces;learning (artificial intelligence);message passing;parallel processing;support vector machines","OpenMP;MPI;structured SVM training;FastSSVM;machine learning;structured support vector machines","",2.0,"",50.0,"IEEE","30 Jul 2021","","","IEEE","IEEE Journals"
"Parallel and Asynchronous Smart Contract Execution","J. Liu; P. Li; R. Cheng; N. Asokan; D. Song","Zhejiang University, Hangzhou, China; Tsinghua University, Beijing, China; University of San Francisco, San Francisco, CA, USA; University of Waterloo, Waterloo, ON, Canada; University of California, Berkeley, Berkeley, CA, USA","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1097,1108,"Today's blockchains suffer from low throughput and high latency, which impedes their widespread adoption of more complex applications like smart contracts. In this article, we propose a novel paradigm for smart contract execution. It distinguishes between consensus nodes and execution nodes: different groups of execution nodes can execute transactions in parallel; meanwhile, consensus nodes can asynchronously order transactions and process execution results. Moreover, it requires no coordination among execution nodes and can effectively prevent livelocks. We show two ways of applying this paradigm to blockchains. First, we show how we can make Ethereum support parallel and asynchronous contract execution without hard-forks. Then, we propose a new public, permissionless blockchain. Our benchmark shows that, with a fast consensus layer, it can provide a high throughput even for complex transactions like Cryptokitties gene mixing. It can also protect simple transactions from being starved by complex transactions.","1558-2183","","10.1109/TPDS.2021.3095234","Zhejiang Key R&D Plans(grant numbers:2021C01116,2019C03133); National Natural Science Foundation of China(grant numbers:62002319,U20A20222); China Zheshang Bank; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477197","Blockchain;smart contract;parallel execution;asynchronous execution","Blockchain;Smart contracts;Servers;Cryptography;Protocols;Throughput;Bitcoin","blockchains;contracts;transaction processing","blockchains;smart contract execution;consensus nodes;execution nodes;process execution;asynchronous contract execution;complex transactions;Ethereum;Cryptokitties gene mixing","",6.0,"",41.0,"IEEE","7 Jul 2021","","","IEEE","IEEE Journals"
"Addictive Incentive Mechanism in Crowdsensing From the Perspective of Behavioral Economics","J. Liu; S. Huang; D. Li; S. Wen; H. Liu","School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia; Department of Computer Science, Missouri State University, Springfield, MO, USA","IEEE Transactions on Parallel and Distributed Systems","20 Oct 2021",2022,33.0,5.0,1109,1127,"In mobile crowdsensing, many mobile devices are collectively used to complete complex sensing tasks. Most tasks require users to consume resources to ensure continuous performance over multiple periods of time. Therefore, it is important to incentivize enough users to continuously participate in the tasks. However, there are two issues with current incentive mechanisms. First, most studies are designed for maximizing the revenue of a single round of tasks rather than long-term incentives. Second, although some studies use historical data to design mechanisms for long-term operation, the law of diminishing marginal utility is not considered; thus, the actual performance is lower than expected. In this study, the concepts of capital deposit and intertemporal choice from behavioral economics are introduced to explain the principle of addiction, which is a representative long-term incentive. Consequently, an Addiction Incentive Mechanism (AIM) is proposed. It influences the utility and demand functions of users by accelerating the accumulation of capital deposits and promoting users to become addicted to cooperative behavior. It also mitigates the effect of diminishing marginal utility through intertemporal choice theory to maintain user engagement. Simulations demonstrate that AIM improves participation and repetition rates compared with the state-of-the-art mechanisms.","1558-2183","","10.1109/TPDS.2021.3104247","National Natural Science Foundation of China(grant numbers:61873352); Natural Science Foundation of Hunan Province(grant numbers:2020JJ5770); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511828","Behavioural economics;crowdsensing;incentive mechanism;intertemporal choice","Task analysis;Sensors;Economics;Crowdsensing;Computer science;Monitoring;Maintenance engineering","behavioural sciences computing;economics;incentive schemes;mobile computing","AIM;addictive incentive mechanism;complex sensing tasks;mobile devices;mobile crowdsensing;user engagement;capital deposits;behavioral economics;capital deposit","",4.0,"",59.0,"IEEE","11 Aug 2021","","","IEEE","IEEE Journals"
"Elastic Parameter Server: Accelerating ML Training With Scalable Resource Scheduling","S. Wang; A. Pi; X. Zhou","Department of Computer Science, University of Colorado, Boulder, CO, USA; Department of Computer Science, University of Colorado, Boulder, CO, USA; Department of Computer Science, University of Colorado, Boulder, CO, USA","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1128,1143,"Parameter server (PS) based on worker-server communication is designed for distributed machine learning (ML) training in clusters. In feedback-driven exploration of ML model training, users exploit early feedback from each job to decide whether to kill the job or keep it running so as to find the optimal model configuration. However, PS does not support adjusting the number of workers and servers of a job at runtime. It becomes the bottleneck of scalable distributed ML training because the cluster resources cannot be dynamically allocated or deallocated to jobs, resulting in significant early feedback latency and resource under-utilization. This article rethinks the principle of PS architecture. We present Elastic Parameter Server (EPS), a lightweight and user-transparent PS that accelerates feedback-driven exploration for distributed ML training. EPS allows to remove a subset of workers and servers from running jobs and allocate the released resources to an incoming job at runtime so as to reduce its early feedback latency. It can also use the released resources from a killed job to add workers and servers to running jobs to improve resource utilization and the training speed. We develop a heuristic scheduler that leverages EPS and offers scalable resource scheduling for multiple ML jobs. We implement EPS in Tencent Angel and the scheduler in Apache Yarn, and conduct evaluations with various ML models. Experimental results show that EPS achieves up to 1.5x improvement on the ML training speed compared to PS.","1558-2183","","10.1109/TPDS.2021.3104242","National Science Foundation(grant numbers:SHF-1816850); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511790","Parameter server;feedback-driven exploration;ML model training;resource scheduling;elasticity","Servers;Training;Runtime;Clocks;Yarn;Resource management;Training data","feedback;file servers;learning (artificial intelligence);resource allocation;scheduling","resource utilization;scalable resource scheduling;multiple ML jobs;EPS;ML models;ML training speed;Elastic Parameter Server;worker-server communication;distributed machine learning training;feedback-driven exploration;ML model training;optimal model configuration;scalable distributed ML training;cluster resources;significant early feedback;PS architecture;lightweight user-transparent PS;running jobs;released resources;incoming job;killed job","",4.0,"",56.0,"IEEE","11 Aug 2021","","","IEEE","IEEE Journals"
"Data, User and Power Allocations for Caching in Multi-Access Edge Computing","X. Xia; F. Chen; Q. He; G. Cui; J. C. Grundy; M. Abdelrazek; X. Xu; H. Jin","School of Information Technology, Deakin University, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Melbourne, VIC, Australia; Department of Computer Science and Software Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; Department of Computer Science and Software Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Information Technology, Deakin University, Melbourne, VIC, Australia; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1144,1155,"In the multi-access edge computing (MEC) environment, app vendors’ data can be cached on edge servers to ensure low-latency data retrieval. Massive users can simultaneously access edge servers with high data rates through flexible allocations of transmit power. The ability to manage networking resources offers unique opportunities to app vendors but also raises unprecedented challenges. To ensure fast data retrieval for users in the MEC environment, edge data caching must take into account the allocations of data, users, and transmit power jointly. We make the first attempt to study the Data, User, and Power Allocation (DUPA$^3$3) problem, aiming to serve the most users and maximize their overall data rate. First, we formulate the DUPA$^3$3 problem and prove its $\mathcal {NP}$NP-completeness. Then, we model the DUPA$^3$3 problem as a potential DUPA$^3$3 game admitting at least one Nash equilibrium and propose a two-phase game-theoretic decentralized algorithm named DUPA$^3$3Game to achieve the Nash equilibrium as the solution to the DUPA$^3$3 problem. To evaluate DUPA$^3$3Game, we analyze its theoretical performance and conduct extensive experiments to demonstrate its effectiveness and efficiency.","1558-2183","","10.1109/TPDS.2021.3104241","Australian Research Council(grant numbers:FL190100035); Discovery Projects(grant numbers:DP180100212,DP200102491); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511831","Edge computing;data allocation;user allocation;power allocation;optimization;multi-access","Servers;Resource management;Games;Interference;NOMA;Intercell interference;Distributed databases","cache storage;distributed processing;game theory;multi-access systems;network servers","potential DUPA3 game;two-phase game-theoretic decentralized algorithm;Nash equilibrium;NP-completeness;DUPA3 problem;edge data caching;data retrieval;networking resources;transmit power;flexible allocations;access edge servers;low-latency data retrieval;app vendors data;MEC environment;data-user-power allocations;multiaccess edge computing caching","",16.0,"",33.0,"IEEE","11 Aug 2021","","","IEEE","IEEE Journals"
"Online Reconfiguration of IoT Applications in the Fog: The Information-Coordination Trade-Off","B. Donassolo; A. Legrand; P. Mertikopoulos; I. Fajjari","CNRS, Inria, Grenoble INP, LIG, Université Grenoble Alpes, Grenoble, France; CNRS, Inria, Grenoble INP, LIG, Université Grenoble Alpes, Grenoble, France; CNRS, Inria, Grenoble INP, LIG, Université Grenoble Alpes, Grenoble, France; Orange Labs, France","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1156,1172,"The evolution of the Internet of Things (IoT) is driving an extraordinary growth of traffic and processing demands, persuading 5G players to change their infrastructures. In this context, Fog computing emerges as a potential solution, providing nearby resources to run IoT applications. However, the Fog raises several challenges which hinders its adoption. In this article, we consider the reconfiguration problem, i.e., how to dynamically adapt the placement of IoT applications running on the Fog, depending on application needs and evolution of resource usage. We propose and evaluate a series of reconfiguration algorithms, based on both online scheduling and online learning approaches. Through an extensive set of experiments in a realistic testbed, we demonstrate that the performance strongly depends on the quality and availability of information from both Fog infrastructure and IoT applications. This information mainly concerns the application’s resource usage (estimated by the user during the design of the application) and the availability of resources in the infrastructure (collected by commercial off-the-shelf monitoring tools). Finally, we show that a reactive and greedy strategy, which relies on this additional information, can overcome the performance of state-of-the-art online learning algorithms, even in a scenario with inaccurate information.","1558-2183","","10.1109/TPDS.2021.3097281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9485041","Fog computing;Internet of Things;reconfiguration;online learning;online scheduling","Internet of Things;Sensors;Cloud computing;Quality of service;Performance evaluation;Automobiles;Sensor phenomena and characterization","greedy algorithms;Internet of Things;learning (artificial intelligence);resource allocation;scheduling","online reconfiguration;IoT applications;resource usage;fog computing;Internet of Things;reconfiguration algorithms;online scheduling;greedy strategy;online learning algorithms","",2.0,"",32.0,"IEEE","14 Jul 2021","","","IEEE","IEEE Journals"
"DIESEL+: Accelerating Distributed Deep Learning Tasks on Image Datasets","L. Wang; Q. Luo; S. Yan","Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; SenseTime Research, Shenzhen, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1173,1184,"We observe that data access and processing takes a significant amount of time in large-scale deep learning training tasks (DLTs) on image datasets. Three factors contribute to this problem: (1) the massive and recurrent accesses to large numbers of small files; (2) the repeated, expensive decoding computation on each image, and (3) the frequent communication between computation nodes and storage nodes. Existing work has addressed some aspects of these problems; however, no end-to-end solutions have been proposed. In this article, we propose DIESEL+, an all-in-one system which accelerates the entire I/O pipeline of deep learning training tasks. DIESEL+ contains several components: (1) local metadata snapshot; (2) per-task distributed caching; (3) chunk-wise shuffling; (4) GPU-assisted image decoding and (5) online region-of-interest (ROI) decoding. The metadata snapshot removes the bottleneck on metadata access in frequent reading of large numbers of files. The per-task distributed cache across the worker nodes of a DLT task to reduce the I/O pressure on the underlying storage. The chunk-based shuffle method converts small file reads into large chunk reads, so that the performance is improved without sacrificing the training accuracy. The GPU-assisted image decoding and the online ROI method minimize the image decoding workloads and reduce the cost of data movement between nodes. These techniques are seamlessly integrated into the system. In our experiments, DIESEL+ outperforms existing systems by a factor of two to three times on the overall training time.","1558-2183","","10.1109/TPDS.2021.3104252","SenseTime Group(grant numbers:18191980); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511819","Storage system;dataset management;deep learning;distributed cache;dataset shuffling;image decoding;GPU","Decoding;Training;Task analysis;Transform coding;Deep learning;Metadata;Image coding","cache storage;decoding;file organisation;learning (artificial intelligence);meta data;parallel databases;storage management","DIESEL;distributed deep learning tasks;image datasets;data access;large-scale deep learning training tasks;massive accesses;recurrent accesses;repeated decoding computation;expensive decoding computation;frequent communication;computation nodes;storage nodes;end-to-end solutions;per-task distributed caching;region-of-interest decoding;training time;image decoding workloads;online ROI method;training accuracy;chunk-based shuffle method;DLT task;worker nodes;per-task distributed cache;frequent reading;metadata access","",2.0,"",44.0,"IEEE","11 Aug 2021","","","IEEE","IEEE Journals"
"Towards Revenue-Driven Multi-User Online Task Offloading in Edge Computing","Z. Ma; S. Zhang; Z. Chen; T. Han; Z. Qian; M. Xiao; N. Chen; J. Wu; S. Lu","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of the Electrical and Computer Engineering, University of New Jersey Institute of Technology, Newark, NJ, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Computer Science and Technology / Suzhou Institute for Advanced Study, University of Science and Technology of China, Hefei, Anhui, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China","IEEE Transactions on Parallel and Distributed Systems","20 Oct 2021",2022,33.0,5.0,1185,1198,"Mobile Edge Computing (MEC) has become an attractive solution to enhance the computing and storage capacity of mobile devices by leveraging available resources on edge nodes. In MEC, the arrivals of tasks are highly dynamic and are hard to predict precisely. It is of great importance yet very challenging to assign the tasks to edge nodes with guaranteed system performance. In this article, we aim to optimize the revenue earned by each edge node by optimally offloading tasks to the edge nodes. We formulate the revenue-driven online task offloading (ROTO) problem, which is proved to be NP-hard. We first relax ROTO to a linear fractional programming problem, for which we propose the Level Balanced Allocation (LBA) algorithm. We then show the performance guarantee of LBA through rigorous theoretical analysis, and present the LB-Rounding algorithm for ROTO using the primal-dual technique. The algorithm achieves an approximation ratio of $2(1+\xi)\ln (d+1)$2(1+ξ)ln(d+1) with a considerable probability, where $d$d is the maximum number of process slots of an edge node and $\xi$ξ is a small constant. The performance of the proposed algorithm is validated through both trace-driven simulations and testbed experiments. Results show that our proposed scheme is more efficient compared to baseline algorithms.","1558-2183","","10.1109/TPDS.2021.3105325","National Key Research and Development Program of China(grant numbers:2017YFB1001801); National Natural Science Foundation of China(grant numbers:61872175,61832008); Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516964","Mobile edge computing;primal-dual technique;online computation offloading;revenue-optimal","Task analysis;Cloud computing;Approximation algorithms;Wireless communication;Software;Resource management;Optimization","approximation theory;computational complexity;linear programming;mobile computing;probability;resource allocation","edge node;revenue-driven multiuser online task offloading;mobile edge computing;ROTO;NP-hard;linear fractional programming;level balanced allocation;LBA;LB-Rounding algorithm;primal-dual technique;probability","",7.0,"",42.0,"IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"Maximizing User Service Satisfaction for Delay-Sensitive IoT Applications in Edge Computing","J. Li; W. Liang; W. Xu; Z. Xu; X. Jia; W. Zhou; J. Zhao","School of Computing, The Australian National University, Canberra, ACT, Australia; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong, China; College of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Software, Dalian University of Technology, Dalian, Liaoning, China; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong, China; Institute of Data Science, City University of Macau, Macau, China; Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1199,1212,"The Internet of Things (IoT) technology provisions unprecedented opportunities to evolve the interconnection among human beings. However, the latency brought by unstable wireless networks and computation failures caused by limited resources on IoT devices prevents users from experiencing high efficiency and seamless user experience. To address these shortcomings, the integrated Mobile Edge Computing (MEC) with remote clouds is a promising platform to enable delay-sensitive service provisioning for IoT applications, where edge-clouds (cloudlets) are co-located with wireless access points in the proximity of IoT devices. Thus, computation-intensive and sensing data from IoT devices can be offloaded to the MEC network immediately for processing, and the service response latency can be significantly reduced. In this paper, we first formulate two novel optimization problems for delay-sensitive IoT applications, i.e., the total utility maximization problems under both static and dynamic offloading task request settings, with the aim to maximize the accumulative user satisfaction on the use of the services provided by the MEC, and show the NP-hardness of the defined problems. We then devise efficient approximation and online algorithms with provable performance guarantees for the problems in a special case where the bandwidth capacity constraint is negligible. We also develop efficient heuristic algorithms for the problems with the bandwidth capacity constraint. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms are promising in reducing service delays and enhancing user satisfaction, and the proposed algorithms outperform their counterparts by at least 10.8 percent.","1558-2183","","10.1109/TPDS.2021.3107137","Australian Research Council(grant numbers:DP200101985); National Natural Science Foundation of China(grant numbers:61602330); Sichuan Science and Technology Program(grant numbers:2018GZDZX0010,2017GZDZX0003); National Key Research and Development Program of China(grant numbers:2017YFB0202403); National Natural Science Foundation of China(grant numbers:61802048); Dalian University of Technology; Research Grants Council of Hong Kong(grant numbers:CityU 11214316); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521690","Cost modeling;resource optimization and allocation;service provisioning;delay-sensitive IoT applications;maximum profit generalized assignment problems;approximation algorithms;online algorithms;task offloading and scheduling;service delay;user satisfaction of using services;mobile edge computing (MEC)","Internet of Things;Task analysis;Cloud computing;Delays;Heuristic algorithms;Approximation algorithms;Bandwidth","cloud computing;computational complexity;distributed processing;Internet of Things;optimisation;resource allocation;telecommunication traffic","user service satisfaction;delay-sensitive IoT applications;unstable wireless networks;computation failures;IoT devices;seamless user experience;delay-sensitive service;edge-clouds;wireless access points;computation-intensive;MEC network;service response;total utility maximization problems;static offloading task request settings;dynamic offloading task request settings;accumulative user satisfaction;bandwidth capacity constraint;service delays;edge computing;Internet of Things technology;integrated mobile edge computing;remote clouds;optimization problems;NP-hardness","",7.0,"",27.0,"IEEE","24 Aug 2021","","","IEEE","IEEE Journals"
"Mapping-Aware Kernel Partitioning Method for CGRAs Assisted by Deep Learning","T. Kojima; A. Ohwada; H. Amano","RIKEN Center for Computational Science, Kobe, Hyogo, Japan; Keio University, Yokohama, Kanagawa, Japan; Keio University, Yokohama, Kanagawa, Japan","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1213,1230,"Coarse-grained reconfigurable architectures (CGRAs) provide high energy efficiency with word-level programmability rather than bit-level ones such as FPGAs. The coarser reconfigurability brings about higher energy efficiency and reduces the complexity of compiler tasks compared to the FPGAs. However, application mapping process for CGRAs is still time-consuming. When the compiler tries to map a large and complicated application data-flow-graph(DFG) onto the reconfigurable fabric, it tends to result in inefficient resource use or to fail in mapping. In case of failure, the compiler must divide it into several sub-DFGs and goes back to the same flow. In this work, we propose a novel partitioning method based on a genetic algorithm to eliminate the unmappable DFGs and improve the mapping quality. In order not to generate unmappable sub-DFGs, we also propose an estimation model which predicts the mappability and resource requirements using a DGCNN (Deep Graph Convolutional Neural Network). The genetic algorithm with this model can seek the most resource-efficient mapping without the back-end mapping process. Our model can predict the mappability with more than 98% accuracy and resource usage with a negligible error for two studied CGRAs. Besides, the proposed partitioning method demonstrates 53-75% of memory saving, 1.28-1.39x higher throughput, and better mapping quality over three comparative approaches.","1558-2183","","10.1109/TPDS.2021.3107746","JSPS KAKENHI(grant numbers:19J21493); JST CREST(grant numbers:JPMJCR19K1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524520","Coarse-grained reconfigurable architecture;CGRA;deep learning;genetic algorithm;graph partitioning","Registers;Partitioning algorithms;Arrays;Routing;Optimization;Estimation;Pipelines","convolutional neural nets;data flow graphs;deep learning (artificial intelligence);field programmable gate arrays;genetic algorithms;reconfigurable architectures","FPGA;coarser reconfigurability;compiler tasks;application mapping process;reconfigurable fabric;genetic algorithm;resource requirements;deep graph convolutional neural network;resource-efficient mapping;back-end mapping process;resource usage;mapping-aware kernel partitioning method;CGRA;deep learning;coarse-grained reconfigurable architectures;high energy efficiency;word-level programmability;bit-level ones","",2.0,"",64.0,"IEEE","27 Aug 2021","","","IEEE","IEEE Journals"
"Workload Balancing via Graph Reordering on Multicore Systems","Y. Chen; Y. -C. Chung","The Chinese University of Hong Kong, Shenzhen, Guangdong, China; The Chinese University of Hong Kong, Shenzhen, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1231,1245,"In a shared-memory multicore system, the intrinsic irregular data structure of graphs leads to poor cache utilization, and therefore deteriorates the performance of graph analytics. To address the problem, prior works have proposed a variety of lightweight reordering methods with focus on the optimization of cache locality. However, there is a compromise between cache locality and workload balance. Little insight has been devoted into the issue of workload imbalance for the underlying multicore system, which degrades the effectiveness of parallel graph processing. In this work, a measurement approach is proposed to quantify the imbalance incurred by the concentration of vertices. Inspired by it, we present Cache-aware Reorder (Corder), a lightweight reordering method exploiting the cache hierarchy of multicore systems. At the shared-memory level, Corder promotes even distribution of computation loads amongst multicores. At the private-cache level, Corder facilitates cache efficiency by applying further refinement to local vertex order. Comprehensive performance evaluation of Corder is conducted on various graph applications and datasets. Experimental results show that Corder yields speedup of up to $2.59\times$2.59× and on average $1.45\times$1.45×, which significantly outperforms existing lightweight reordering methods. To identify the root causes of performance boost delivered by Corder, multicore activities are investigated in terms of thread behavior, cache efficiency, and memory utilization. Statistical analysis demonstrates that the issue of imbalanced thread execution time dominates other factors in determining the overall graph processing time. Moreover, Corder achieves remarkable advantages in cross-platform scalability and reordering overhead.","1558-2183","","10.1109/TPDS.2021.3105323","National Key Research and Development Program of China(grant numbers:2018YFB1003505); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516878","Multicore system;cache locality;workload balance;graph processing","Multicore processing;Sorting;Social networking (online);Instruction sets;Blogs;Performance evaluation;Parallel processing","cache storage;data structures;graph theory;multi-threading;performance evaluation;resource allocation;shared memory systems;statistical analysis","workload balancing;graph reordering;shared-memory multicore system;cache locality;parallel graph processing;Corder;cache hierarchy;private-cache level;cache-aware reorder;lightweight reordering;data structure;cache utilization;graph analytics;measurement approach;computation load distribution;local vertex order;performance evaluation;thread behavior;memory utilization;statistical analysis;imbalanced thread execution time;graph processing time;cross-platform scalability;reordering overhead","",1.0,"",50.0,"IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"A Practical Framework for Secure Document Retrieval in Encrypted Cloud File Systems","J. Fu; N. Wang; B. Cui; B. K. Bhargava","School of Cyberspace Security and National Engineering Lab for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing, China; School of Cyber Science and Technology, Beihang University, Beijing, China; School of Cyberspace Security and National Engineering Lab for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computer Science, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1246,1261,"With the development of cloud computing, more and more data owners are motivated to outsource their documents to the cloud and share them with the authorized data users securely and flexibly. To protect data privacy, the documents are generally encrypted before being outsourced to the cloud and hence their searchability decreases. Though many privacy-preserving document search schemes have been proposed, they cannot reach a proper balance among functionality, flexibility, security and efficiency. In this paper, a new encrypted document retrieval system is designed and a proxy server is integrated into the system to alleviate data owner's workload and improve the whole system's security level. In this process, we consider a more practical and stronger threat model in which the cloud server can collude with a small number of data users. To support multiple document search patterns, we construct two AVL trees for the filenames and authors, and a Hierarchical Retrieval Features tree (HRF tree) for the document vectors. A depth-first search algorithm is designed for the HRF tree and the Enhanced Asymmetric Scalar-Product-Preserving Encryption (Enhanced ASPE) algorithm is utilized to encrypt the HRF tree. All the three index trees are linked with each other to efficiently support the search requests with multiple parameters. Theoretical analysis and simulation results illustrate the security and efficiency of the proposed framework.","1558-2183","","10.1109/TPDS.2021.3107752","National Natural Science Foundation of China(grant numbers:62001055,62102017); Natural Science Foundation of Beijing Municipality(grant numbers:4204107); Funds of “YinLing”(grant numbers:A02B01C03-201902D0); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524492","Cloud computing;privacy-preserving;searchable encryption;document ranked retrieval","Cloud computing;Servers;Indexes;Security;Encryption;Search problems;Simulation","cloud computing;cryptography;data protection;document handling;file organisation;query formulation;trees (mathematics)","enhanced asymmetric scalar-product-preserving encryption algorithm;hierarchical retrieval features tree;privacy-preserving document search;index trees;depth-first search algorithm;document vectors;AVL trees;multiple document search patterns;cloud server;proxy server;encrypted document retrieval system;data privacy protection;authorized data users;cloud computing;encrypted cloud file systems","",1.0,"",43.0,"IEEE","27 Aug 2021","","","IEEE","IEEE Journals"
"DH-SVRF: A Reconfigurable Unicast/Multicast Forwarding for High-Performance Packet Forwarding Engines","Z. Jin; W. -K. Jia","College of Photonic and Electronic Engineering, Fujian Normal University, Fuzhou, Fujian, China; College of Photonic and Electronic Engineering, Fujian Normal University, Fuzhou, Fujian, China","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2021",2022,33.0,5.0,1262,1275,"High-performance multicast-enabled packet forwarding engines (PFEs), as an essential component of high-end switches, use a polynomial-time membership query algorithm to determine which port(s) the data packet should be forwarded. The currently widely used query algorithm is Bloom Filter (BF), which has been proven to have many fatal flaws. Another error-free membership query algorithm includes Scalar-pair Vectors Routing Forwarding (SVRF), Fractional-N Scalar-pair Vectors Routing Forwarding (Frac-N SVRF), and the Per-Port Prime Filter Array (P3FA) also have some shortcomings in space and time efficiencies. In this paper, we proposed a hybrid strategy: Divaricate Heterogeneous SVRF (DH-SVRF) scheme, which based on the P3FA and Frac-N SVRF, which randomly divides all member ships into N groups, and each group has the same structure and is independent of each other to obtain higher time efficiency and space utilization. Finally, we also discussed the selection of the optimal egress-diversity threshold. Through mathematical modeling and simulation, we validate that the proposed DH-SVRF scheme is superior to the SVRF/Frac-N SVRF and traditional BF in terms of scalability, space utilization, and time efficiency in specific conditions such as appropriate egress-diversity thresholds.","1558-2183","","10.1109/TPDS.2021.3108899","National Natural Science Foundation of China(grant numbers:61871131,U1805262); Key Laboratory of OptoElectronic Science and Technology for Medicine of Ministry of Education; Fujian Provincial Key Laboratory of Photonics Technology; Fujian Normal University, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9527084","Multicast;packet forwarding engine (PFE);membership querying;bloom filter (BF);scalar-pair vectors routing and forwarding (SVRF);divaricate heterogeneous","Engines;Routing;Unicast;Scalability;Multicast algorithms;Switches;Filtering algorithms","computational complexity;data structures;IP networks;multicast communication;optimisation;packet radio networks;routing protocols;telecommunication traffic","Frac-N SVRF;space utilization;DH-SVRF scheme;high-performance packet forwarding engines;high-end switches;polynomial-time membership query algorithm;data packet;Bloom Filter;error-free membership query algorithm;time efficiencies;per-port prime filter array;scalar-pair vectors;widely used query algorithm;divaricate heterogeneous SVRF scheme;DH-SVRF;multicast-enabled packet forwarding engines;reconfigurable unicast-multicast forwarding;scalar-pair vectors routing forwarding;fractional-N scalar-pair vectors routing forwarding;optimal egress-diversity threshold","",3.0,"",46.0,"IEEE","1 Sep 2021","","","IEEE","IEEE Journals"
"LOFS: A Lightweight Online File Storage Strategy for Effective Data Deduplication at Network Edge","G. Cheng; D. Guo; L. Luo; J. Xia; S. Gu","Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","10 Mar 2022",2022,33.0,10.0,2263,2276,"Edge computing responds to users’ requests with low latency by storing the relevant files at the network edge. Various data deduplication technologies are currently employed at edge to eliminate redundant data chunks for space saving. However, the lookup for the global huge-volume fingerprint indexes imposed by detecting redundancies can significantly degrade the data processing performance. Besides, we envision a novel file storage strategy that realizes the following rationales simultaneously: 1) space efficiency, 2) access efficiency, and 3) load balance, while the existing methods fail to achieve them at one shot. To this end, we report LOFS, a Lightweight Online File Storage strategy, which aims at eliminating redundancies through maximizing the probability of successful data deduplication, while realizing the three design rationales simultaneously. LOFS leverages a lightweight three-layer hash mapping scheme to solve this problem with constant-time complexity. To be specific, LOFS employs the Bloom filter to generate a sketch for each file, and thereafter feeds the sketches to the Locality Sensitivity hash (LSH) such that similar files are likely to be projected nearby in LSH tablespace. At last, LOFS assigns the files to real-world edge servers with the joint consideration of the LSH load distribution and the edge server capacity. Trace-driven experiments show that LOFS closely tracks the global deduplication ratio and generates a relatively low load std compared with the comparison methods.","1558-2183","","10.1109/TPDS.2021.3133098","National Natural Science Foundation of China(grant numbers:U19B2024,62002378); Research Funding of NUDT(grant numbers:ZK20-30); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645269","Data deduplication;locality sensitivity hash;edge computing;storage systems","Servers;Indexes;Costs;Redundancy;Throughput;Image edge detection;Resource management","cloud computing;computational complexity;data structures;file organisation;indexing;resource allocation;storage management","edge server capacity;global deduplication ratio;Lightweight Online File Storage strategy;effective data deduplication;network edge;edge computing responds;relevant files;data deduplication technologies;redundant data chunks;global huge-volume fingerprint indexes;detecting redundancies;data processing performance;novel file storage strategy;successful data deduplication;LOFS leverages;similar files;real-world edge servers","","","",47.0,"CCBY","10 Dec 2021","","","IEEE","IEEE Journals"
"ComboTree: A Persistent Indexing Structure With Universal Operational Efficiency and Scalability","Z. Wang; T. Yao; J. Wan; H. Jiang; Q. Cui; L. Tang; Y. Zhang; Q. Zhang","Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; University of Texas at Arlington, Arlington, TX, USA; PingCAP, Beijing, China; PingCAP, Beijing, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","15 Mar 2022",2022,33.0,10.0,2277,2290,"To leverage the larger-than-DRAM capacity and close-to-DRAM performance of persistent memory (PM) to build future memory systems, more scalable and efficient indexing structures are of paramount importance. However, as we evaluate existing PM indexing structures, we find that (1) both ordered and unordered indexing structures cannot support efficiently all KV operation types, i.e., Put, Get, Delete and Scan, and (2) the majority of indexes scale poorly as the PM capacity and dataset increases, i.e., the decreasing throughput of ordered indexes with the growth of dataset and the blocking of foreground requests by costly hash resizing of unordered indexes. To provide better operational efficiency on both point accesses and range queries for persistent memory in the ever-increasing volume of data, this article proposes ComboTree, a three-tiered indexing structure with a sorted key space. In ComboTree, we break the global B+Tree into multiple low height B+Trees (Tier C) and arrange them with a sorted array (Tier B). Further, we accelerate the lookup of the sorted array by cumulative distribution function (Tier A). Last but not least, a background resizing policy is proposed to avoid performance degradation when the capacity of the ComboTree grows. We implement and evaluate ComboTree on Intel’s Optane DCPMM. Test results show that ComboTree delivers $2.1\times -3.6\times$2.1×-3.6× put throughput and $1.5\times -2.1\times$1.5×-2.1× get throughput of the state-of-art sorted indexes. Furthermore, ComboTree is $1.27\times$1.27× faster than the efficient B+Tree variant in various scan granularities, and it is open-sourced1.","1558-2183","","10.1109/TPDS.2021.3137247","National Natural Science Foundation of China(grant numbers:62072196); National Natural Science Foundation of China(grant numbers:61821003); Science Technology and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20190809095001781); National Science Foundation(grant numbers:CNS-2008835); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658202","Indexing structure;persistent index;persistent memory;key-value store","Indexing;Scalability;Throughput;Random access memory;Memory management;Distribution functions;Currencies","","","","","",61.0,"IEEE","21 Dec 2021","","","IEEE","IEEE Journals"
"Privacy-Preserving Efficient Federated-Learning Model Debugging","A. Li; L. Zhang; J. Wang; F. Han; X. -Y. Li","School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China","IEEE Transactions on Parallel and Distributed Systems","10 Mar 2022",2022,33.0,10.0,2291,2303,"Federated learning allows large amounts of mobile clients to jointly construct a global model without sending their private data to a central server. A fundamental issue in this framework is the susceptibility to the erroneous training data. This problem is especially challenging due to the invisibility of clients’ local training data and training process, as well as the resource constraints. In this paper, we aim to solve this issue by introducing the first FL debugging framework, FLDebugger, for mitigating test error caused by erroneous training data. The proposed solution traces the global model’s bugs (test errors), jointly through the training log and the underlying learning algorithm, back to first identify the clients and subsequently their training samples that are most responsible for the errors. In addition, we devise an influence-based participant selection strategy to fix bugs as well as to accelerate the convergence of model retraining. The performance of the identification algorithm is evaluated via extensive experiments on a real AIoT system (50 clients, including 20 edge computers, 20 laptops and 10 desktops) and in larger-scale simulated environments. The evaluation results attest to that our framework achieves accurate, privacy-preserving and efficient identification of negatively influential clients and samples, and significantly improves the model performance by fixing bugs.","1558-2183","","10.1109/TPDS.2021.3137321","National Key Research and Development Program of China(grant numbers:2018YFB0803400); National Natural Science Foundation of China(grant numbers:61822209,61625205,61932016,62132018); Key Research Program of Frontier Science, Chinese Academy of Sciences(grant numbers:QYZDY-SSW-JSC002); Tencent Marketing Solution Rhino-Bird Focused Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661312","Federated learning;Influence function;data quality assessment","Training;Data models;Adaptation models;Debugging;Computational modeling;Training data;Predictive models","data privacy;learning (artificial intelligence);program debugging","training log;underlying learning algorithm;subsequently their training samples;influence-based participant selection strategy;model retraining;negatively influential clients;privacy-preserving efficient federated-learning model debugging;federated learning;mobile clients;global model;private data;central server;erroneous training data;training process;FL debugging framework;test error","",2.0,"",45.0,"IEEE","23 Dec 2021","","","IEEE","IEEE Journals"
"CoPA: Cold Page Awakening to Overcome Retention Failures in STT-MRAM Based I/O Buffers","M. Hadizadeh; E. Cheshmikhani; M. Rahmanpour; O. Mutlu; H. Asadi","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Electrical and Computer Engineering, ETH Zurich, Zurich, Switzerland; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","10 Mar 2022",2022,33.0,10.0,2304,2317,"Performance and reliability are two prominent factors in the design of data storage systems. To achieve higher performance, recently storage system designers use $Dynamic$Dynamic $RAM$RAM (DRAM)-based buffers. The volatility of DRAM brings up the possibility of data loss and data inconsistency. Thus, a part of the main storage is conventionally used as the journal area to be able of recovering unflushed data pages in the case of power failure. Moreover, periodically flushing buffered data pages to the main storage is a common mechanism to preserve a high level of reliability. This scheme, however, leads to a considerable increase in storage write traffic, which adversely affects the performance. To address this shortcoming, recent studies offer a small $Non-Volatile$Non-Volatile $Memory$Memory (NVM) as the $Persistent$Persistent $Journal$Journal $Area$Area (PJA) along with DRAM as an efficient approach to overcome DRAM vulnerability against power failure while effectively reducing storage write traffic. This approach, named $NVM-Backed$NVM-Backed $Buffer$Buffer (NVB-Buffer), features from advantages of NVMs and addresses DRAM shortcomings. In this article, we employ the most promising technologies for PJA among the emerging technologies, which is $Spin-Transfer$Spin-Transfer $Torque$Torque $Magnetic$Magnetic $Random$Random $Access$Access $Memory$Memory (STT-MRAM) to meet the requirements of efficient PJA by providing high endurance, non-volatility, and DRAM-like latency. Despite these advantages, STT-MRAM faces major reliability challenges, i.e., Retention Failure, Read Disturbance, and Write Failure, which have not been addressed in previously suggested NVB-Buffers. In this article, we first demonstrate that the retention failure is the dominant source of errors in NVB-Buffers as it suffers from long and unpredictable page idle intervals (i.e., the time interval between two consecutive accesses to a PJA page). Then, we propose a novel NVB-Buffer management scheme, named, $\underline{Co}ld$Co̲ld $\underline{P}age$P̲age $\underline{A}wakening$A̲wakening (CoPA), which predictably reduces the idle time of PJA pages. To this aim, CoPA employs $Distant$Distant $Refreshing$Refreshing to periodically overwrite the vulnerable PJA page contents by opportunistically using their replica in DRAM-based buffer. We compare CoPA with the state-of-the-art schemes over several well-known storage workloads based on physical journaling. Our evaluations show that CoPA significantly reduces the maximum page idle time, which leads to three orders of magnitude lower failure rate with negligible performance degradation (1.1%) and memory overhead (1.2%).","1558-2183","","10.1109/TPDS.2021.3137315","Sharif University of Technology; Eidgenssische Technische Hochschule Zrich; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9662259","Data storage systems;persistent journal area;STT-MRAM;retention failure","Nonvolatile memory;Random access memory;Reliability;Magnetic tunneling;Ferroelectric films;Degradation;Resistive RAM","","","",1.0,"",75.0,"IEEE","23 Dec 2021","","","IEEE","IEEE Journals"
"CAMIG: Concurrency-Aware Live Migration Management of Multiple Virtual Machines in SDN-Enabled Clouds","T. He; A. N. Toosi; R. Buyya","CLOUDS Lab, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia; Department of Software Systems and Cybersecurity, Faculty of Information Technology, Monash University, Clayton, VIC, Australia; CLOUDS Lab, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia","IEEE Transactions on Parallel and Distributed Systems","10 Mar 2022",2022,33.0,10.0,2318,2331,"By integrating Software-Defined Networking and cloud computing, virtualized networking and computing resources can be dynamically reallocated through live migration of Virtual Machines (VMs). Dynamic resource management such as load balancing and energy-saving policies can request multiple migrations when the algorithms are triggered periodically. There exist notable research efforts in dynamic resource management that alleviate single migration overheads, such as single migration time and co-location interference while selecting the potential VMs and migration destinations. However, by neglecting the resource dependency among potential migration requests, the existing solutions of dynamic resource management can result in the Quality of Service (QoS) degradation and Service Level Agreement (SLA) violations during the migration schedule. Therefore, it is essential to integrate both single and multiple migration overheads into VM reallocation planning. In this paper, we propose a concurrency-aware multiple migration selector that operates based on the maximal cliques and independent sets of the resource dependency graph of multiple migration requests. Our proposed method can be integrated with existing dynamic resource management policies. The experimental results demonstrate that our solution efficiently minimizes migration interference and shortens the convergence time of reallocation by maximizing the multiple migration performance while achieving the objective of dynamic resource management.","1558-2183","","10.1109/TPDS.2021.3139014","Australian Research Council(grant numbers:DP160102414); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664322","Live migration;dynamic resource management;migration scheduling;software-defined networking;cloud computing","Resource management;Dynamic scheduling;Heuristic algorithms;Costs;Cloud computing;Load modeling;Interference","cloud computing;computer centres;quality of service;resource allocation;virtual machines;virtualisation","concurrency-aware live migration management;multiple Virtual Machines;cloud computing;virtualized networking;computing resources;multiple migrations;single migration overheads;single migration time;migration destinations;potential migration requests;migration schedule;multiple migration overheads;concurrency-aware multiple migration selector;resource dependency graph;multiple migration requests;dynamic resource management policies;migration interference;multiple migration performance","",1.0,"",39.0,"IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"SPRINT: A High-Performance, Energy-Efficient, and Scalable Chiplet-Based Accelerator With Photonic Interconnects for CNN Inference","Y. Li; A. Louri; A. Karanth","Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA; Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA; School of Electrical Engineering and Computer Science, Ohio University, Athens, OH, USA","IEEE Transactions on Parallel and Distributed Systems","9 Mar 2022",2022,33.0,10.0,2332,2345,"Chiplet-based convolution neural network (CNN) accelerators have emerged as a promising solution to provide substantial processing power and on-chip memory capacity for CNN inference. The performance of these accelerators is often limited by inter-chiplet metallic interconnects. Emerging technologies such as photonic interconnects can overcome the limitations of metallic interconnects due to several superior properties including high bandwidth density and distance-independent latency. However, implementing photonic interconnects in chiplet-based CNN accelerators is challenging and requires combined effort of network architectural optimization and CNN dataflow customization. In this article, we propose SPRINT, a chiplet-based CNN accelerator that consists of a global buffer and several accelerator chiplets. SPRINT introduces two novel designs: (1) a photonic inter-chiplet network that can adapt to specific communication patterns in CNN inference through wavelength allocation and waveguide reconfiguration, and (2) a CNN dataflow that can leverage the broadcasting capability of photonic interconnects while minimizing the costly electrical-to-optical and optical-to-electrical signal conversions. Simulations using multiple CNN models show that SPRINT achieves up to 76% and 68% reduction in execution time and energy consumption, respectively, as compared to other state-of-the-art chiplet-based architectures with either metallic or photonic interconnects.","1558-2183","","10.1109/TPDS.2021.3139015","National Science Foundation(grant numbers:CCF-1702980,CCF-1812495,CCF-1901165,CCF-1953980,CCF-1513606,CCF-1703013,CCF-1901192); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664279","Convolution neural network;chiplet;accelerator;photonic interconnects","Photonics;Convolutional neural networks;Optical waveguides;Optical switches;Optical filters;Convolution;Optical network units","inference mechanisms;optical interconnections;optical neural nets;optical waveguides;wavelength assignment","SPRINT;scalable chiplet-based accelerator;photonic interconnects;CNN inference;chiplet-based convolution neural network accelerators;chiplet-based CNN accelerator;CNN dataflow customization;accelerator chiplets;multiple CNN models;state-of-the-art chiplet-based architectures;interchiplet metallic interconnects;photonic interchiplet network;communication patterns;wavelength allocation;waveguide reconfiguration;optical-electrical signal conversions;execution time;energy consumption","",1.0,"",65.0,"IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"Dependent Function Embedding for Distributed Serverless Edge Computing","S. Deng; H. Zhao; Z. Xiang; C. Zhang; R. Jiang; Y. Li; J. Yin; S. Dustdar; A. Y. Zomaya","Institute of Intelligence Applications, Yunnan University of Finance and Economics, Kunming, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Zhejiang University City College, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Institute of Intelligence Applications, Yunnan University of Finance and Economics, Kunming, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Distributed Systems Group, Technische Universität Wien, Vienna, Austria; School of Computer Science, University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","10 Mar 2022",2022,33.0,10.0,2346,2357,"Edge computing is booming as a promising paradigm to extend service provisioning from the centralized cloud to the network edge. Benefit from the development of serverless computing, an edge server can be configured as a carrier of limited serverless functions, in the way of deploying Docker runtime and Kubernetes engine. Meanwhile, an application generally takes the form of directed acyclic graphs (DAGs), where vertices represent dependent functions and edges represent data traffic. The status quo of minimizing the completion time (a.k.a. makespan) of the application motivates the study on optimal function placement. However, current approaches lose sight of proactively splitting and mapping the traffic to the logical data paths between the heterogeneous edge servers, which could affect the makespan significantly. To remedy that, we propose an algorithm, termed as Dependent Function Embedding (DPE), to get the optimal edge server for each function to execute and the moment it starts executing. DPE finds the best segmentation of each data traffic by exquisitely solving several infinity norm minimization problems. DPE is theoretically verified to achieve the global optimality. Extensive experiments on Alibaba cluster trace show that DPE significantly outperforms two baseline algorithms in makespan by 43.19% and 40.71%, respectively.","1558-2183","","10.1109/TPDS.2021.3137380","Key Research Project of Zhejiang Province(grant numbers:2022C01145); National Natural Science Foundation of China(grant numbers:U20A20173,62125206); Zhejiang University De-qing Institute of Advanced technology and Industrilization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665233","Edge computing;dependent function embedding;directed acyclic graph;function placement;task scheduling","Servers;Routing;Edge computing;Virtual links;Power measurement;Internet of Things;Surveillance","cloud computing;directed graphs;graph theory;minimisation;virtualisation","optimal function placement;logical data paths;heterogeneous edge servers;Dependent Function Embedding;DPE;optimal edge server;data traffic;distributed serverless edge computing;service provisioning;centralized cloud;network edge;serverless computing;serverless functions;directed acyclic graphs;dependent functions","",3.0,"",41.0,"IEEE","29 Dec 2021","","","IEEE","IEEE Journals"
"An Efficient Index-Based Approach to Distributed Set Reachability on Small-World Graphs","Y. Zeng; K. Li; X. Zhou; W. Luo; Y. Gao","National Supercomputing Center, Changsha, Hunan, China; National Supercomputing Center, Changsha, Hunan, China; National Supercomputing Center, Changsha, Hunan, China; National Supercomputing Center, Changsha, Hunan, China; Key Lab of Big Data Intelligent Computing of Zhejiang Province, Zhejiang University, Hangzhou, China","IEEE Transactions on Parallel and Distributed Systems","10 Mar 2022",2022,33.0,10.0,2358,2371,"Set reachability query in directed graphs has a plethora of graph-based applications such as dependency analysis and graph centrality calculation. Given two sets $S$S and $T$T of source and target vertices, set reachability query needs to acquire all pairs $(s,t)$(s,t) where $s{\in }S$s∈S, $t{\in }T$t∈T and $s$s can reach $t$t. The state-of-the-art approach distributed set reachability (DSR) investigates the set reachability query in a distributed environment and adopts a static graph-based index to enhance the query efficiency. Nevertheless, DSR needs to store the graph-based index in all partitions, which causes a huge space overhead. Furthermore, it cannot efficiently solve the negative query $(s,t)$(s,t) where $s$s cannot reach $t$t, since DSR needs to traverse the whole reachable paths and becomes unable to efficiently reduce the computations. To alleviate these issues, we propose a novel multi-level 2-hop (ML2hop) index for the set reachability query in a distributed environment. Based on ML2hop, we further present a bi-directional query algorithm, called MLQA, to achieve efficient support for both positive and negative queries in Pregel-like systems. Generally, MLQA is equipped with the following three significant properties: (1) Low computation costs. It reduces redundant local computations in each partition by controlling the rounds of path traversals. (2) Low communication costs. It restricts the message exchange among different partitions within one single round with guaranteed accuracy of query results. (3) High parallelism. It adopts a bi-directional query technique for message propagation, achieving the better query efficiency than the forward-traversal query strategy utilized in DSR. Experimental results over several real-world graphs demonstrate that MLQA significantly outperforms the state-of-the-art algorithm by up to two orders of magnitude speedup.","1558-2183","","10.1109/TPDS.2021.3139111","National Key Research and Development Program of China(grant numbers:2020YFB2104000); National Natural Science Foundation of China(grant numbers:62172146,62172157); Open Research Projects of Zhejiang Lab(grant numbers:2021KD0AB02); Hunan Leading plan for scientific and technological innovation of high-tech industries(grant numbers:2020GK2037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665221","2-hop labeling;distributed processing;indexing;set reachabiity","Indexes;Partitioning algorithms;Parallel processing;Directed graphs;Costs;Computational modeling;Bidirectional control","","","",2.0,"",24.0,"IEEE","29 Dec 2021","","","IEEE","IEEE Journals"
"Elastic Resource Allocation Against Imbalanced Transaction Assignments in Sharding-Based Permissioned Blockchains","H. Huang; Z. Yue; X. Peng; L. He; W. Chen; H. -N. Dai; Z. Zheng; S. Guo","School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; Department of Computing and Decision Sciences, Lingnan University, Hong Kong; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","9 Mar 2022",2022,33.0,10.0,2372,2385,"This article studies the PBFT-based sharded permissioned blockchain, which executes in either a local datacenter or a rented cloud platform. In such permissioned blockchain, the transaction (TX) assignment strategy could be malicious such that the network shards may possibly receive imbalanced transactions or even bursty-TX injection attacks. An imbalanced transaction assignment brings serious threats to the stability of the sharded blockchain. A stable sharded blockchain can ensure that each shard processes the arrived transactions timely. Since the system stability is closely related to the blockchain throughput, how to maintain a stable sharded blockchain becomes a challenge. To depict the transaction processing in each network shard, we adopt the Lyapunov Optimization framework. Exploiting drift-plus-penalty (DPP) technique, we then propose an adaptive resource-allocation algorithm, which can yield the near-optimal solution for each network shard while the shard queues can also be stably maintained. We also rigorously analyze the theoretical boundaries of both the system objective and the queue length of shards. The numerical results show that the proposed algorithm can achieve a better balance between resource consumption and queue stability than other baselines. We particularly evaluate two representative cases of bursty-TX injection attacks, i.e., the continued attacks against all network shards and the drastic attacks against a single network shard. The evaluation results show that the DPP-based algorithm can well alleviate the imbalanced TX assignment, and simultaneously maintain high throughput while consuming fewer resources than other baselines.","1558-2183","","10.1109/TPDS.2022.3141737","National Key Research and Development Program of China(grant numbers:2020YFB1006005); National Natural Science Foundation of China(grant numbers:61902445); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019A1515011798); Guangzhou Basic and Applied Basic Research Foundation(grant numbers:202102020613); Guangdong Provincial Pearl River Talents Program(grant numbers:2019QN01X130); CCF-Huawei Populus euphratica forest fund(grant numbers:CCF-HuaweiBC2021004); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,152203/20E,152244/21E); National Natural Science Foundation of China(grant numbers:61872310); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:R2020A045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678070","System stability;sharded blockchain;queueing theory;imbalanced transaction assignment","Blockchains;Protocols;Stability analysis;Throughput;Numerical stability;Bitcoin;Scalability","cloud computing;optimisation;queueing theory;resource allocation;transaction processing","single network shard;imbalanced TX assignment;elastic resource allocation;imbalanced transaction assignment;sharding-based permissioned blockchains;permissioned blockchain;transaction assignment strategy;imbalanced transactions;bursty-TX injection attacks;stable sharded blockchain;arrived transactions timely;blockchain throughput;transaction processing;adaptive resource-allocation algorithm;shard queues","",7.0,"",31.0,"IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"SelectiveEC: Towards Balanced Recovery Load on Erasure-Coded Storage Systems","L. Xu; M. Lyu; Q. Li; L. Xie; C. Li; Y. Xu","Ahhui Province Key Laboratory of High Performance Computing, Hefei, Anhui, China; Ahhui Province Key Laboratory of High Performance Computing, Hefei, Anhui, China; Ahhui Province Key Laboratory of High Performance Computing, Hefei, Anhui, China; Ahhui Province Key Laboratory of High Performance Computing, Hefei, Anhui, China; Ahhui Province Key Laboratory of High Performance Computing, Hefei, Anhui, China; Ahhui Province Key Laboratory of High Performance Computing, Hefei, Anhui, China","IEEE Transactions on Parallel and Distributed Systems","23 Mar 2022",2022,33.0,10.0,2386,2400,"Erasure coding (EC) has been commonly used to offer high data reliability with low storage cost. Upon failures, the lost blocks are recovered in batches. Due to the limited number of stripes, the data layout within a batch is non-uniform. Together with the random selection of source and replacement nodes for recovery tasks, the recovery workload among live nodes is skewed within a batch, which severely slows down failure recovery. To solve this problem, We present SelectiveEC, a new recovery task scheduling module that provides provable network traffic and recovery load balancing for large-scale EC-based storage systems. It relies on bipartite graphs to model the recovery traffic among live nodes. Then, it intelligently selects tasks to form batches and carefully determines where to read source blocks or to store recovered ones, using theories such as a perfect or maximum matching and $k$k-regular spanning subgraph. SelectiveEC supports single-node failure and multi-node failure recovery, and can be deployed in both homogeneous and heterogeneous network environments. We implement SelectiveEC in HDFS, and evaluate its recovery performance in a local cluster of 18 nodes and AWS EC2 of 50 virtual machine instances. SelectiveEC increases the recovery throughput by up to $30.68\%$30.68% compared with state-of-the-art baselines in homogeneous network environments. It further achieves $1.32\times$1.32× recovery throughput and $1.23\times$1.23× benchmark throughput of HDFS on average in heterogeneous network environments, due to the straggler avoidance by the balanced scheduling.","1558-2183","","10.1109/TPDS.2021.3129973","National Natural Science Foundation of China(grant numbers:61832011,61772486); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9625750","Distributed storage system;reliability;erasure coding;recovery traffic;load balance","Task analysis;Codes;Bandwidth;Decoding;Throughput;Encoding;Load modeling","","","","","",52.0,"IEEE","23 Nov 2021","","","IEEE","IEEE Journals"
"Blockchain Assisted Decentralized Federated Learning (BLADE-FL): Performance Analysis and Resource Allocation","J. Li; Y. Shao; K. Wei; M. Ding; C. Ma; L. Shi; Z. Han; H. V. Poor","School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Data61, CSIRO, Sydney, NSW, Australia; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, Nanjing, Jiangsu, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Department of Computer Science and Engineering, Kyung Hee University, Seoul, Korea; Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA","IEEE Transactions on Parallel and Distributed Systems","23 Mar 2022",2022,33.0,10.0,2401,2415,"Federated learning (FL), as a distributed machine learning paradigm, promotes personal privacy by local data processing at each client. However, relying on a centralized server for model aggregation, standard FL is vulnerable to server malfunctions, untrustworthy servers, and external attacks. To address these issues, we propose a decentralized FL framework by integrating blockchain into FL, namely, blockchain assisted decentralized federated learning (BLADE-FL). In a round of the proposed BLADE-FL, each client broadcasts its trained model to other clients, aggregates its own model with received ones, and then competes to generate a block before its local training on the next round. We evaluate the learning performance of BLADE-FL, and develop an upper bound on the global loss function. Then we verify that this bound is convex with respect to the number of overall aggregation rounds $K$K, and optimize the computing resource allocation for minimizing the upper bound. We also note that there is a critical problem of training deficiency, caused by lazy clients who plagiarize others’ trained models and add artificial noises to disguise their cheating behaviors. Focusing on this problem, we explore the impact of lazy clients on the learning performance of BLADE-FL, and characterize the relationship among the optimal $K$K, the learning parameters, and the proportion of lazy clients. Based on the MNIST and Fashion-MNIST datasets, we see that the experimental results are consistent with the analytical ones. To be specific, the gap between the developed upper bound and experimental results is lower than $5\%$5%, and the optimized $K$K based on the upper bound can effectively minimize the loss function.","1558-2183","","10.1109/TPDS.2021.3138848","National Natural Science Foundation of China(grant numbers:61872184,62002170); Natural Science Foundation of Jiangsu Province(grant numbers:BK20210331); National Science Foundation(grant numbers:CNS-2128368,CNS-2107216,ECCS-2039716); Toyota Motor Corporation; Amazon Catalyst; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664296","Federated learning;blockchain;lazy client;computing resource allocation","Blockchains;Training;Servers;Computational modeling;Upper bound;Resource management;Privacy","","","",19.0,"",53.0,"IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"Decentralised Data Quality Control in Ground Truth Production for Autonomic Decisions","P. Gkikopoulos; V. Schiavoni; J. Spillner","InIT, Zurich University of Applied Sciences, Winterthur, Switzerland; Department of Computer Science, University of Neuchǎtel, Neuchatel, Switzerland; InIT, Zurich University of Applied Sciences, Winterthur, Switzerland","IEEE Transactions on Parallel and Distributed Systems","23 Mar 2022",2022,33.0,10.0,2416,2427,"Autonomic decision-making based on rules and metrics is inevitably on the rise in distributed software systems. Often, the metrics are acquired from system observations such as static checks and runtime traces. To avoid bias propagation and hence reduce wrong decisions in increasingly autonomous systems due to poor observation data quality, multiple independent observers can exchange their findings and produce a majority-accepted, complete and outlier-cleaned ground truth in the form of consensus-supported metrics. In this work, we motivate the growing importance of metrics for informed and autonomic decisions in clouds and other distributed systems, present reasons for diverging observations, and describe a federated approach to produce ground truth with data-centric consensus voting for more reliable decision making processes. We validate the system design with experiments in the area of cloud software artefact observations and highlight benefits for reproducible distributed system behaviour.","1558-2183","","10.1109/TPDS.2022.3142967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681269","Distributed systems;redundant design;algorithms for data and knowledge management","Measurement;Software;Decision making;System analysis and design;Redundancy;Peer-to-peer computing;Data integrity","","","",1.0,"",49.0,"IEEE","13 Jan 2022","","","IEEE","IEEE Journals"
"Mobility-Aware Offloading and Resource Allocation for Distributed Services Collaboration","H. Chen; S. Deng; H. Zhu; H. Zhao; R. Jiang; S. Dustdar; A. Y. Zomaya","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Institute of Intelligence Applications, Yunnan University of Finance and Economics, Kunming, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Institute of Intelligence Applications, Yunnan University of Finance and Economics, Kunming, China; Distributed Systems Group, TU Wien, Vienna, Austria; High Performance Computing & Networking, School of Computer Science, University of Sydney, Camperdown, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","29 Mar 2022",2022,33.0,10.0,2428,2443,"In mobile edge computing (MEC) systems, mobile users (MUs) are capable of allocating local resources (CPU frequency and transmission power) and offloading tasks to edge servers in the vicinity in order to enhance their computation capabilities and reduce back-and-forth transmission over backhaul link. Nevertheless, mobile environment makes it hard to draw offloading and resource allocation decisions under dynamical wireless channel state and users’ locations. In real life, social relationship is also provably a significant factor affecting integral performance in collaborative work, which results in MUs decisions strongly coupled and renders this problem further intractable. Most of previous works ignore the impact of inter-user dependency (or data dependency among IoT devices). To bridge this gap, we study the service collaboration with master-slave dependency among service chains of MUs and formulate this combinational optimization problem as a mixed integer non-linear programming (MINLP) problem. To this end, we derive the closed-form expression of resource allocation solution by convex optimization and transform it to integer linear programming (ILP) problem. Subsequently, we propose a distributed algorithm based on Markov approximation which has polynomial computation complexity. Experimental result on real-world dataset substantiates the usefulness and superiority of our scheme, in terms of reducing latency and energy consumption.","1558-2183","","10.1109/TPDS.2022.3142314","Key Research Project of Zhejiang Province(grant numbers:2022C01145); National Natural Science Foundation of China(grant numbers:U20A20173,62125206); Zhejiang University Deqing Institute of Advanced technology and Industrilization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681614","Mobile edge computing;task offloading;resource allocation;dependency;collaborative computing","Resource management;Task analysis;Servers;Optimization;Energy consumption;Collaboration;Collaborative work","combinatorial mathematics;computational complexity;convex programming;distributed algorithms;integer programming;linear programming;mobile computing;nonlinear programming;resource allocation","resource allocation;master-slave dependency;combinational optimization;mixed integer nonlinear programming;integer linear programming;distributed algorithm;polynomial computation complexity;distributed services collaboration;mobile edge computing systems;mobile users;MU service chains;Markov approximation;mobility-aware offloading;closed-form expression","",6.0,"",55.0,"IEEE","13 Jan 2022","","","IEEE","IEEE Journals"
"Improving Fairness for SSD Devices through DRAM Over-Provisioning Cache Management","R. Liu; Z. Tan; L. Long; Y. Wu; Y. Tan; D. Liu","College of Computer Science, Chongqing University of Posts and Telecommunications, Chongqing, China; College of Computer Science, Chongqing University of Posts and Telecommunications, Chongqing, China; College of Computer Science, Chongqing University of Posts and Telecommunications, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China","IEEE Transactions on Parallel and Distributed Systems","23 Mar 2022",2022,33.0,10.0,2444,2454,"Modern NVMe SSDs have been widely deployed in multi-tenant cloud computing environments or multi-programming systems. When multiple applications concurrently access one SSD hardware, unfairness within the shared SSD will slow down the application significantly and lead to a violation of service level objectives. However, traditional data cache management within SSDs mainly focuses on improving cache hit ratio, which causes data cache contention and sacrifices fairness among multiple applications. In this paper, we propose a DRAM-based Over-Provisioning (OP) cache management mechanism, named Justitia, to reduce data cache contention and improve fairness for modern SSDs. Justitia consists of two stages including Static-OP stage and Dynamic-OP stage. Through the novel OP mechanism in the two stages, Justitia reduces the max slowdown by $4.5\times$4.5× on average. At the same time, Justitia increases fairness by $20.6\times$20.6× and buffer hit ratio by $19.6\%$19.6% averagely, compared with the traditional shared mechanism.","1558-2183","","10.1109/TPDS.2022.3143295","National Natural Science Foundation of China(grant numbers:61902045,62072059); Chongqing High-Tech Research Key Program(grant numbers:cstc2021jcyj-msxmX0981); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9682597","Solid state drives (SSDs);multiple applications;fairness;data cache management","Ash;Random access memory;Nonvolatile memory;Hardware;Writing;Time factors;Organizations","","","",1.0,"",33.0,"IEEE","14 Jan 2022","","","IEEE","IEEE Journals"
"Fast and Accurate Statistical Simulation of Shared-Memory Applications on Multicore Systems","F. Jiang; R. K. V. Maeda; J. Feng; S. Chen; L. Chen; X. Li; J. Xu","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","23 Mar 2022",2022,33.0,10.0,2455,2469,"Detailed cycle-accurate simulation of multicore systems is naturally slow. Statistical simulation is one alternative that permits trading off simulation speed for accuracy. However, there is a lack of effective memory locality models for multicore applications. Hence, existing statistical simulators neglect data-sharing between threads. Additionally, the standard method to speed up statistical simulations is to blindly reduce the trace length to be synthesized. While this gives good control over the speedup, it leaves the simulation error unbounded. In this work, we introduce a novel statistical simulation methodology for exploration of shared-memory multicore systems. It includes a new sharing-locality model (Shalom) that captures and reproduces data-sharing in multithread applications. Furthermore, we propose a method to bound the simulation error for a particular metric while maximizing speedup. The technique works by monitoring the convergence of the statistical synthesis. It is referred to as convergence-deterministic simulation (Condens). The combination of Shalom and Condens is around 130x faster than cycle-accurate simulations with reasonable accuracy loss. Our approach is also 5x faster than state-of-the-art sampling simulation under the same accuracy level. Compared to previous statistical simulators ignoring sharing, our approach is 2x more accurate for performance metrics and 8x more accurate for cache miss estimations.","1558-2183","","10.1109/TPDS.2022.3143535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684737","Statistical simulation;reuse distance;performance modeling;simulation speedup;locality model;multicore system","Instruction sets;Computational modeling;Multicore processing;Synchronization;Computer architecture;Message systems;Benchmark testing","","","","","",64.0,"IEEE","18 Jan 2022","","","IEEE","IEEE Journals"
"Execution Time Estimation of Multithreaded Programs With Critical Sections","D. Kagaris; S. Dutta; S. Eyerman","School of Electrical, Computer, and Biomedical Engineering, Southern Illinois University, Carbondale, IL, USA; School of Theoretical and Applied Science, Ramapo College of New Jersey, Mahwah, NJ, USA; Intel Corporation, Kontich, Belgium","IEEE Transactions on Parallel and Distributed Systems","1 Apr 2022",2022,33.0,10.0,2470,2481,"The ideal benefit of parallelizing/multithreading a program is diminished in practice by several factors such as hardware scaling, memory bandwidth, power constraints, and synchronization due to critical sections. Several models have been proposed in the past to estimate the resulting performance and extend the traditional Amdahl’s law. In this work, we focus on the effect of synchronization, and develop a model for the execution time estimation of multithreaded programs under the presence of critical sections. The proposed model is applicable to multiple different critical sections and generalizes and improves previously proposed models. Experimental results on simulated, synthetic and benchmark examples show that the proposed model provides accurate approximations.","1558-2183","","10.1109/TPDS.2022.3143455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684674","Parallel programming;performance estimation;critical sections;mutual exclusion;synchronization;concurrency;modelling","Codes;Message systems;Instruction sets;Estimation;Delays;Computational modeling;Synchronization","multi-threading;parallel processing;synchronisation","execution time estimation;multithreaded programs;hardware scaling;memory bandwidth;power constraints;synchronization;parallel program","",1.0,"",23.0,"IEEE","18 Jan 2022","","","IEEE","IEEE Journals"
"Dynamic Controller/Switch Mapping: A Service Oriented Assignment Approach","C. Pham; D. T. Nguyen; N. H. Tran; K. K. Nguyen; M. Cheriet","Synchromedia - Ecole de Technologie Superieure, Universite du Quebec, Quebec, Canada; Synchromedia - Ecole de Technologie Superieure, Universite du Quebec, Quebec, Canada; School of Computer Science, University of Sydney, NSW, Australia; Synchromedia - Ecole de Technologie Superieure, Universite du Quebec, Quebec, Canada; Synchromedia - Ecole de Technologie Superieure, Universite du Quebec, Quebec, Canada","IEEE Transactions on Parallel and Distributed Systems","4 Apr 2022",2022,33.0,10.0,2482,2495,"With the capability of decoupling the control plane and the data plane of networks, Software-Defined Network (SDN) enables flexible and efficient implementations in networks. In addition, Network Function Virtualization (NFV) with Virtual Network Function (VNF) service chain capabilities provides high-performance networks with greater scalability, elasticity, and adaptability. Such an elastic deployment of service chains results in different Service Level Agreements (SLA) and resource requirements on the control plane. In this work, we illustrate the impact of service chains on the control plane and formulate the dynamic controller/switch mapping (DCSM) problem in NFV networks in order to reduce the operational cost. We address the combinatorial optimization problem, DCSM, by designing a novel mechanism to relax DCSM into a tractable problem based on the Penalty Successive Upper Bound Minimization (PSUM) method. In doing so, we conduct several simulation scenarios to evaluate the performance. The experimental results show that our proposed algorithms can achieve a near-optimal result and reduce the operational cost up to 31.7% and 28.3% compared to K-Mean and the matching game-based approaches, respectively.","1558-2183","","10.1109/TPDS.2022.3144116","Natural Sciences and Engineering Research Council of Canada(grant numbers:506319-17); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9690940","Software-defined network;network function virtualization;service chain","Control systems;Routing;Costs;Resilience;Reliability;Optimization;Process control","contracts;game theory;minimisation;software defined networking;virtualisation","DCSM;NFV networks;operational cost;combinatorial optimization problem;control plane;data plane;network function virtualization;high-performance networks;elastic deployment;resource requirements;dynamic controller/switch mapping;service oriented assignment;software-defined network;SDN;virtual network function service chain capabilities;service level agreements;SLA;penalty successive upper bound minimization;PSUM;game-based approaches","","","",43.0,"IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Coordinated Batching and DVFS for DNN Inference on GPU Accelerators","S. M. Nabavinejad; S. Reda; M. Ebrahimi","School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran; School of Engineering, Brown University, Providence, RI, USA; KTH Royal Institute of Technology, Stockholm, Sweden","IEEE Transactions on Parallel and Distributed Systems","1 Apr 2022",2022,33.0,10.0,2496,2508,"Employing hardware accelerators to improve the performance and energy-efficiency of DNN applications is on the rise. One challenge of using hardware accelerators, including the GPU-based ones, is that their performance is limited by internal and external factors, such as power caps. A common approach to meet the power cap constraint is using the Dynamic Voltage Frequency Scaling (DVFS) technique. However, the functionally of this technique is limited and platform-dependent. To tackle this challenge, we propose a new control knob, which is the size of input batches fed to the GPU accelerator in DNN inference applications. We first evaluate the impact of batch size on power consumption and performance of DNN inference. Then, we introduce the design and implementation of a fast and lightweight runtime system, called BatchDVFS. Dynamic batching is implemented in BatchDVFS to adaptively change the batch size, and hence, trade-off throughput with power consumption. It employs an approach based on binary search to find the proper batch size within a short period of time. Combining dynamic batching with the DVFS technique, BatchDVFS can control the power consumption in wider ranges, and hence, yield higher throughput in the presence of power caps. To find near-optimal solution for long-running jobs that can afford a relatively significant profiling overhead, compared with BatchDVFS overhead, we also design an approach, called BOBD, that employs Bayesian Optimization to wisely explore the vast state space resulted by combination of the batch size and DVFS solutions. Conducting several experiments using a modern GPU and several DNN models and input datasets, we show that our BatchDVFS can significantly surpass the techniques solely based on DVFS or batching, regarding throughput (up to 11.2x and 2.2x, respectively), while successfully meeting the power cap.","1558-2183","","10.1109/TPDS.2022.3144614","National Science Foundation(grant numbers:1814920); DoD ARO(grant numbers:W911NF-19-1-0484); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689937","Deep neural networks;GPU accelerator;power consumption;throughput;batch size;dynamic voltage frequency scaling","Throughput;Graphics processing units;Power demand;Runtime;Bayes methods;Resource management;Optimization","Bayes methods;deep learning (artificial intelligence);energy conservation;graphics processing units;hardware accelerators;low-power electronics;optimisation","power cap constraint;Dynamic Voltage Frequency Scaling technique;input batches;GPU accelerator;DNN inference applications;power consumption;fast runtime system;lightweight runtime system;dynamic batching;proper batch size;DVFS technique;DVFS solutions;coordinated batching;hardware accelerators;DNN applications;GPU-based ones;internal factors;BatchDVFS","",6.0,"",58.0,"IEEE","21 Jan 2022","","","IEEE","IEEE Journals"
"Communication-Efficient $k$k-Means for Edge-Based Machine Learning","H. Lu; T. He; S. Wang; C. Liu; M. Mahdavi; V. Narayanan; K. S. Chan; S. Pasteris","Pennsylvania State University, University Park, PA, USA; Pennsylvania State University, University Park, PA, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; Pennsylvania State University, University Park, PA, USA; Pennsylvania State University, University Park, PA, USA; Army Research Laboratory, Adelphi, MD, USA; University College London, London, U.K.","IEEE Transactions on Parallel and Distributed Systems","4 Apr 2022",2022,33.0,10.0,2509,2523,"We consider the problem of computing the $k$k-means centers for a large high-dimensional dataset in the context of edge-based machine learning, where data sources offload machine learning computation to nearby edge servers. $k$k-Means computation is fundamental to many data analytics, and the capability of computing provably accurate $k$k-means centers by leveraging the computation power of the edge servers, at a low communication and computation cost to the data sources, will greatly improve the performance of these analytics. We propose to let the data sources send small summaries, generated by joint dimensionality reduction (DR), cardinality reduction (CR), and quantization (QT), to support approximate $k$k-means computation at reduced complexity and communication cost. By analyzing the complexity, the communication cost, and the approximation error of $k$k-means algorithms based on carefully designed composition of DR/CR/QT methods, we show that: (i) it is possible to compute near-optimal $k$k-means centers at a near-linear complexity and a constant or logarithmic communication cost, (ii) the order of applying DR and CR significantly affects the complexity and the communication cost, and (iii) combining DR/CR methods with a properly configured quantizer can further reduce the communication cost without compromising the other performance metrics. Our theoretical analysis has been validated through experiments based on real datasets.","1558-2183","","10.1109/TPDS.2022.3144595","Army Research Laboratory; Ministry of Defence(grant numbers:W911NF-16-3-0001); National Science Foundation(grant numbers:1822923); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9690512","k-Means;dimensionality reduction;coreset;random projection;quantization;edge-based machine learning","Costs;Soft sensors;Approximation algorithms;Servers;Quantization (signal);Dimensionality reduction;Heuristic algorithms","approximation theory;computational complexity;data analysis;learning (artificial intelligence);pattern clustering","communication-efficient;edge-based machine learning;means centers;high-dimensional dataset;data sources offload machine;nearby edge servers;means computation;data analytics;computation power;low communication;joint dimensionality reduction;means algorithms;constant communication cost;logarithmic communication cost","",1.0,"",42.0,"IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Distributed Algorithms for Multi-Resource Allocation","F. Fossati; S. Rovedakis; S. Secci","CentraleSupélec, Gif-sur-Yvette, France; Cnam, Cedric, Paris, France; Cnam, Cedric, Paris, France","IEEE Transactions on Parallel and Distributed Systems","5 Apr 2022",2022,33.0,10.0,2524,2539,"Novel network infrastructures require the distribution of computing and network resource control to meet stringent requirements in terms of latency, reliability and bitrate. 5G systems bring a key novelty in systems design that it the ‘network slice’as a new resource provisioning entity. A network slice is meant to serve end-to-end services as a composition of different network and system resources as radio, link, storage and computing resources. Conventionally, each resource is managed by a distinct decision-maker, platform, provider, orchestrator or controller. Naturally, centralized slice orchestration approaches are proposed in the literature, where a multi-domain orchestrator allocates the resources, for instance using a multi-resource allocation rule. Nonetheless, while simplifying the algorithmic approach, centralization can come at the expense of scalability and performance. In this article, we propose new ways to distribute the slice multi-resource resource allocation problem, using cascade and parallel resource allocations that are functionally compatible with novel software platforms. We also show how to adapt the proposed algorithms to make them able to guarantee service level agreements on the minimum resource needed, and to take into account deadline priority policy scheduling. We provide an exhaustive analysis of the advantages and disadvantages of the different approaches, including a numerical analysis for a realistic setting.","1558-2183","","10.1109/TPDS.2022.3144376","Agence Nationale de la Recherche(grant numbers:ANR-18-CE25-0012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695170","Multi-resource allocation;network slicing;network resource control","Resource management;Network slicing;Service level agreements;Radio access networks;Games;Distributed algorithms;Delays","5G mobile communication;computer network reliability;contracts;distributed algorithms;mobile computing;resource allocation;telecommunication scheduling","resource provisioning entity;network slice;multidomain orchestrator;multiresource allocation rule;distributed algorithms;network resource control;5G systems;slice multiresource resource allocation;parallel resource allocations;cascade resource allocations;software platforms;service level agreements;deadline priority policy scheduling;exhaustive analysis","",1.0,"",33.0,"IEEE","27 Jan 2022","","","IEEE","IEEE Journals"
"EdgeTB: A Hybrid Testbed for Distributed Machine Learning at the Edge With High Fidelity","L. Yang; F. Wen; J. Cao; Z. Wang","School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong, China; School of Software Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","4 Apr 2022",2022,33.0,10.0,2540,2553,"Distributed Machine Learning (DML) at the edge has become an essential topic for providing low-latency intelligence near the data sources. However, both the development and testing of DMLs lack sufficient support. Reusable libraries that abstract the general functionalities of DMLs are needed for rapid development. Moreover, existing physical testbeds are usually small and lack network flexibility, while virtual testbeds like simulators and emulators lack fidelity. This paper proposes a novel hybrid testbed EdgeTB, which provides numerous emulated nodes to generate large-scale and network-flexible test environments while incorporating physical nodes to guarantee fidelity. EdgeTB manages physical nodes and emulated nodes uniformly and supports arbitrary network topologies between nodes through dynamic configurations. Importantly, we propose Role-oriented development to support the rapid development of DMLs. Through case studies and experiments, we demonstrate that EdgeTB provides convenience for efficiently developing and testing DMLs in various structures with high fidelity and scalability.","1558-2183","","10.1109/TPDS.2022.3144994","National Natural Science Foundation of China(grant numbers:61972161); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515011496); Key Research and Development Program of Guangdong(grant numbers:2019B010154004); Hong Kong RGC General Research Fund(grant numbers:PolyU 152133/18,PolyU 15217919); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693130","Testbed;emulator;edge computing;distributed machine learning","Edge computing;Costs;Machine learning;Computational modeling;Peer-to-peer computing;Libraries;Cloud computing","computer networks;learning (artificial intelligence);service-oriented architecture;telecommunication network topology","EdgeTB;distributed machine learning;low-latency intelligence;data sources;reusable libraries;physical testbeds;network flexibility;virtual testbeds;numerous emulated nodes;network-flexible test environments;physical nodes;arbitrary network topologies;role-oriented development","",2.0,"",35.0,"IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"NetSync: A Network Adaptive and Deduplication-Inspired Delta Synchronization Approach for Cloud Storage Services","W. Xia; C. Wei; Z. Li; X. Wang; X. Zou","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China; Tsinghua University, Beijing, China; Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China; Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","4 Apr 2022",2022,33.0,10.0,2554,2570,"Delta sync (synchronization) is a key bandwidth-saving technique for cloud storage services. The representative delta sync utility, rsync, matches data chunks by sliding a search window byte-by-byte to maximize the redundancy detection for bandwidth efficiency. However, it is difficult for this process to cater to the forthcoming high-bandwidth cloud storage services which require lightweight delta sync that can well support large files. Moreover, rsync employs invariant chunking and compression methods during the sync process, making it unable to cater to services from various network environments which require the sync approach to perform well under different network conditions. Inspired by the Content-Defined Chunking (CDC) technique used in data deduplication, we propose NetSync, a network adaptive and CDC-based lightweight delta sync approach with less computing and protocol (metadata) overheads than the state-of-the-art delta sync approaches. Besides, NetSync can choose appropriate compressing and chunking strategies for different network conditions. The key idea of NetSync is (1) to simplify the process of chunk matching by proposing a fast weak hash called FastFP that is piggybacked on the rolling hashes from CDC, and redesigning the delta sync protocol by exploiting deduplication locality and weak/strong hash properties; (2) to minimize the sync time by adaptively choosing chunking parameters and compression methods according to the current network conditions. Our evaluation results driven by both benchmark and real-world datasets suggest NetSync performs $2\times$2×–$10\times$10× faster and supports $30\%$30%–$80\%$80% more clients than the state-of-the-art rsync-based WebR2sync+ and deduplication-based approach.","1558-2183","","10.1109/TPDS.2022.3145025","NSFC(grant numbers:61972441); Shenzhen Science and Technology(grant numbers:JCYJ20190806143405318,JCYJ20200109113427092,GXWD20201230155427003-20200821172511002); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2021A1515012634); State Key Laboratory of Computer Architecture(grant numbers:CARCHA202006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693219","rsync;content-defined chunking;network adaptive;cloud storage","Synchronization;Servers;Cloud computing;Bandwidth;Adaptive systems;Redundancy;Power capacitors","cloud computing;cryptography;data compression;file organisation;meta data;storage management;synchronisation","rsync;invariant chunking;compression methods;sync process;network environments;different network conditions;Content-Defined Chunking technique;data deduplication;NetSync;network adaptive;CDC-based lightweight delta sync approach;state-of-the-art delta sync approaches;appropriate compressing;chunking strategies;chunk matching;delta sync protocol;deduplication locality;sync time;chunking parameters;current network conditions;deduplication-based approach;deduplication-inspired delta synchronization approach;bandwidth-saving technique;representative delta sync utility;data chunks;window byte-by-byte;bandwidth efficiency;high-bandwidth cloud storage services","",2.0,"",44.0,"IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"NetEC: Accelerating Erasure Coding Reconstruction With In-Network Aggregation","Y. Qiao; M. Zhang; Y. Zhou; X. Kong; H. Zhang; M. Xu; J. Bi; J. Wang","Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Alibaba Inc., Hangzhou, China; Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Quan Cheng Laboratory, Jinan, China; Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Peng Cheng Laboratory, Shenzhen, Guangdong, P. R. China","IEEE Transactions on Parallel and Distributed Systems","4 Apr 2022",2022,33.0,10.0,2571,2583,"In distributed storage systems, Erasure Coding (EC) is a crucial technology to enable high data availability. By downloading parity data from survived machines, EC can reconstruct lost data with much lower storage overheads than data replication. However, this reduction in storage cost comes at the expense of extra performance problems: low reconstruction rate, high degraded read latency, and high host CPU utilization. Our analysis shows that these performance problems are deeply rooted in the host-based EC processing. To resolve these problems, we present NetEC, an in-network accelerating framework that fully offloads EC to the new generation programmable switching ASICs. We propose Explicit Buffer Size Notification (EBSN) to constrain decoding buffer usage, and design an on-switch one-to-many TCP proxy to integrate EBSN with TCP. We also design two parallel Galois Field (GF) offloading methods—table lookup and bitmatrix methods—to maximize parsable bytes. We implement NetEC on programmable switches and integrate it with HDFS. Extensive evaluations show that NetEC improves the reconstruction rate by 2.7x-6.8x, reduces the degraded read latency significantly, and removes the host CPU overhead completely. We also emulate multi-rack scenarios and show that NetEC is able to support $\sim$∼GB/s reconstruction rate and tens of concurrent tasks.","1558-2183","","10.1109/TPDS.2022.3145836","Joint Research on IPv6 Network Governance: Research, Development and Demonstration(grant numbers:2020YFE0200500); National Natural Science Foundation of China(grant numbers:62002009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693136","Erasure coding;distributed storage sytems;programmable switch;software-defined networks","Encoding;Decoding;Codes;Throughput;Task analysis;Switches;Bandwidth","computer centres;decoding;Galois fields;multiprocessing systems;optimisation;parallel processing;storage management;table lookup;transport protocols","NetEC;accelerating Erasure Coding reconstruction;In-Network Aggregation;distributed storage systems;crucial technology;high data availability;downloading parity data;survived machines;lower storage overheads;data replication;storage cost;extra performance problems;low reconstruction rate;high host CPU utilization;host-based EC processing;in-network accelerating framework;generation programmable;Explicit Buffer Size Notification;EBSN;decoding buffer usage;TCP;bitmatrix methods;programmable switches","",1.0,"",48.0,"IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Optimal Embedding of Aggregated Service Function Tree","D. Guo; B. Ren; G. Tang; L. Luo; T. Chen; X. Fu","Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; University of Göttingen, Göttingen, Germany","IEEE Transactions on Parallel and Distributed Systems","5 Apr 2022",2022,33.0,10.0,2584,2596,"Many hardware-based security middleboxes have been deployed in the networks to defend against different threats. However, these hardware middleboxes are hard to upgrade or migrate. The emergence of network functions virtualization (NFV), which realizes various security functions in the form of virtual network functions (VNFs), brings many benefits to network security. To improve the security level further, several VNFs are coordinated in a pre-defined order to form service function chains (SFCs). It is expected that the SFCs are embedded properly with low cost, including the VNF setup cost and the flow routing cost. In this paper, we find that when an SFC is required by multiple flows for the identical network security threats, the total cost could be reduced by embedding an aggregated service function tree (ASFT) instead of multiple independent SFCs. We formally characterize the integer programming model of this problem and prove that it is NP-hard. Then we propose a performance-guaranteed approximation algorithm and prove that the algorithm could find the optimal solution in a special case. Extensive experiments indicate that our method can reduce the total cost by $22.0\%$22.0% and $24.1\%$24.1% against two compared algorithms, respectively.","1558-2183","","10.1109/TPDS.2022.3147870","National Key Research and Development Program of China(grant numbers:2018YFE0207600,2020YFE0200500); National Natural Science Foundation of China(grant numbers:U19B2024,61802421); European Union’s Horizon 2020 Research and Innovation Program; Marie Sklodowska-Curie(grant numbers:824019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698997","Network functions virtualization;service function chain;approximation algorithm","Costs;Security;Approximation algorithms;Routing;Network security;Middleboxes;Computer crime","cloud computing;computational complexity;computer network security;integer programming;trees (mathematics);virtualisation","optimal embedding;aggregated service function tree;hardware-based security middleboxes;different threats;hardware middleboxes;network functions virtualization;security functions;virtual network functions;security level;form service function chains;VNF setup cost;flow routing cost;identical network security threats;multiple independent SFCs;ASFT;NP-hard problem;performance-guaranteed approximation algorithm","","","",33.0,"IEEE","1 Feb 2022","","","IEEE","IEEE Journals"
"Optimizing Video Caching at the Edge: A Hybrid Multi-Point Process Approach","X. Zhang; Y. Zhou; D. Wu; M. Hu; X. Zheng; M. Chen; S. Guo","Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, Guangdong, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, Guangdong, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, Guangdong, China; School of Computing, FSE, Macquarie University, Macquarie Park, NSW, Australia; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Department of Computing, The Polytechnic University of Hong Kong, Hung Hom, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","4 Apr 2022",2022,33.0,10.0,2597,2611,"It is always a challenging problem to deliver a huge volume of videos over the Internet. To meet the high bandwidth and stringent playback demand, one feasible solution is to cache video contents on edge servers based on predicted video popularity. Traditional caching algorithms (e.g., LRU, LFU) are too simple to capture the dynamics of video popularity, especially long-tailed videos. Recent learning-driven caching algorithms (e.g., DeepCache) show promising performance, however, such black-box approaches are lack of explainability and interpretability. Moreover, the parameter tuning requires a large number of historical records, which are difficult to obtain for videos with low popularity. In this paper, we optimize video caching at the edge using a white-box approach, which is highly efficient and also completely explainable. To accurately capture the evolution of video popularity, we develop a mathematical model called HRS model, which is the combination of multiple point processes, including Hawkes’ self-exciting, reactive and self-correcting processes. The key advantage of the HRS model is its explainability, and much less number of model parameters. In addition, all its model parameters can be learned automatically through maximizing the Log-likelihood function constructed by past video request events. Next, we further design an online HRS-based video caching algorithm. To verify its effectiveness, we conduct a series of experiments using real video traces collected from Tencent Video, one of the largest online video providers in China. Experiment results demonstrate that our proposed algorithm outperforms the state-of-the-art algorithms, with 15.5% improvement on average in terms of cache hit rate under realistic settings.","1558-2183","","10.1109/TPDS.2022.3147240","National Natural Science Foundation of China(grant numbers:U1911201,U2001209,62072486,61872310,62176101); Science and Technology Planning Project of Guangdong Province(grant numbers:2021A0505110008); Australian Research Council(grant numbers:DE180100950); Major Key Project of PCL(grant numbers:PCL2021A08); Australian Research Council(grant numbers:LP190100676); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,152203/20E,152244/21E); Shenzhen Science and Technology Innovation Commission(grant numbers:R2020A045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9699396","Video caching;edge servers;point process;Monte Carlo;gradient descent","Streaming media;Servers;Mathematical models;Prediction algorithms;Internet;Heuristic algorithms;Computational modeling","cache storage;distributed processing;Internet;learning (artificial intelligence);video servers","video request events;online HRS-based video caching algorithm;Tencent Video;online video providers;cache hit rate;hybrid multipoint process approach;stringent playback demand;video contents;edge servers;long-tailed videos;black-box approaches;white-box approach;HRS model;multiple point processes;learning-driven caching algorithms;caching algorithms;self-correcting processes;reactive processes;log-likelihood function;video traces","",1.0,"",55.0,"IEEE","1 Feb 2022","","","IEEE","IEEE Journals"
"Incentive-Aware Autonomous Client Participation in Federated Learning","M. Hu; D. Wu; Y. Zhou; X. Chen; M. Chen","Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; School of Computing, Faculty of Science and Engineering, Macquarie University, Macquarie Park, NSW, Australia; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","22 Apr 2022",2022,33.0,10.0,2612,2627,"Federated learning (FL) emerges as a promising paradigm to enable a federation of clients to train a machine learning model in a privacy-preserving manner. Most existing works assumed that the central parameter server (PS) determines the participation of clients implying that clients cannot make autonomous participation decisions. The above assumption is unrealistic because the participation in FL training may incur various cost and clients also have strong desire to be rewarded for participation. To address this problem, we design a novel autonomous client participation scheme to incentivize clients. Specifically, the PS provides a certain reward shared among participating clients for each training round. Clients decide whether to participate each FL training round or not based on their own utilities (i.e., reward minus cost). The process can be modeled as a minority game (MG) with incomplete information and clients end up in the minority side win after each training round because the reward of each participating client may not cover its cost if too many clients participate and vice verse. The challenge of autonomous participation schemes lies in lowering the volatility of participating clients in each round due to the lack of coordination among clients. Through solid analysis, we prove that: 1) The volatility of participating clients in each round is very high under the standard MG scheme. 2) The volatility of participating clients can be reduced significantly under the stochastic MG scheme. 3) A coalition based MG is proposed, which can further reduce the volatility in each round. By conducting extensive experiments in real settings, we demonstrate that the stochastic MG-based scheme outperforms other state-of-the-art algorithms in terms of utility and volatility, and the coalition MG-based client participation scheme can further boost the utility by 39%-48% and reduce the volatility by 51%–100%. Moreover, our algorithms can achieve almost the same model accuracy as that obtained by centralized client participation algorithms.","1558-2183","","10.1109/TPDS.2022.3148113","National Natural Science Foundation of China(grant numbers:62072486,U1911201,U2001209,61972432,62176101); Science and Technology Planning Project of Guangdong Province(grant numbers:2021A0505110008); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011369); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Guangdong Provincial Pearl River Talents Program(grant numbers:2017GC010465); Australian Research Council(grant numbers:DE180100950); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9705080","Client participation;federated learning;minority game;volatility","Training;Games;Stochastic processes;Collaborative work;Servers;Costs;Standards","client-server systems;data privacy;learning (artificial intelligence);stochastic games","incentive-aware autonomous client participation;federated learning;coalition MG-based client participation scheme;centralized client participation algorithms;machine learning model;central parameter server;PS;FL training;minority game;coalition based MG;stochastic MG-based scheme","",2.0,"",51.0,"IEEE","4 Feb 2022","","","IEEE","IEEE Journals"
"DrTM+B: Replication-Driven Live Reconfiguration for Fast and General Distributed Transaction Processing","S. Shen; X. Wei; R. Chen; H. Chen; B. Zang","Shanghai Artificial Intelligence Laboratory, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; Engineering Research Center for Domain-Specific Operating Systems (MOE), Shanghai, China; Engineering Research Center for Domain-Specific Operating Systems (MOE), Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","22 Apr 2022",2022,33.0,10.0,2628,2643,"Recent in-memory database systems leverage advanced hardware features like RDMA to provide transaction processing at millions of transactions per second. Distributed transaction processing systems can scale to even higher rates, especially for partitionable workloads. Unfortunately, it is challenging to sustain such high rates during live reconfiguration of partitions. In this article, we observe that state-of-the-art approaches would cause notable performance disruption under fast transaction processing. To this end, this article presents DrTM+B, a live reconfiguration approach that seamlessly repartitions data with little performance disruption to running transactions. DrTM+B uses a pre-copy-based mechanism to avoid excessive data transfer by leveraging common properties in recent transactional systems. DrTM+B's reconfiguration plans reduce data movement by preferring existing data replicas, while copying data from multiple replicas asynchronously and in parallel. It further reuses the log forwarding mechanism in primary-backup replication to seamlessly track and forward dirty database tuples and avoids iterative copying costs. To commit a reconfiguration plan in a transactional-safe way, DrTM+B designs a cooperative commit protocol for synchronization of data and state among replicas. To boost the performance during data migration, DrTM+B combines the pre-copy and post-copy schemes to propose a hybrid copy scheme. The live reconfiguration approach can also coexist with fault-tolerance mechanisms of primary-backup replication to provide high availability. Evaluation on a working system based on DrTM+R with 3-way replication using typical OLTP workloads like TPC-C and SmallBank shows that DrTM+B incurs only very small performance degradation during live reconfiguration and provides high availability. Both the reconfiguration time and the downtime are also minimal.","1558-2183","","10.1109/TPDS.2022.3148251","National Key Research & Development Program(grant numbers:2020AAA0108500); National Natural Science Foundation of China(grant numbers:61732010,61925206,62172272); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9705084","Distributed transactions;load balance;live reconfiguration;data replication;RDMA","Throughput;Fault tolerant systems;Fault tolerance;Protocols;Low latency communication;Optimization;Distributed databases","database management systems;fault tolerant computing;protocols;query processing;transaction processing;virtual machines","replication-driven live reconfiguration;general distributed transaction processing;in-memory database systems leverage;distributed transaction processing systems;performance disruption;fast transaction processing;live reconfiguration approach;running transactions;pre-copy-based mechanism;excessive data transfer;transactional systems;DrTM+B's reconfiguration plans;data movement;data replicas;log forwarding mechanism;primary-backup replication;dirty database tuples;reconfiguration plan;data migration;post-copy schemes;hybrid copy scheme;DrTM+R;reconfiguration time","","","",55.0,"IEEE","4 Feb 2022","","","IEEE","IEEE Journals"
"IEEE Special Issue on Innovative R&D Toward the Exascale Era","S. R. Alam; L. C. McInnes; K. Nakajima","Swiss National Supercomputing Centre (CSCS), Lugano, Switzerland; Mathematics and Computer Science Division of Argonne National Laboratory, Lemont, IL, USA; Supercomputing Research Division, Information Technology Center, the University of Tokyo, Kashiwa, Chiba, Japan","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,736,738,"This special issue on Innovative research and development toward the exascale era explores new foundational and translational research toward enabling exascale computing for emerging scientific and societal challenges. Exascale computing is defined as the capability to perform 1018 operations per second. Productively harnessing such a scale of processing, storage, and networking capabilities for diverse domains— including high-performance computing (HPC) simulations, artificial intelligence (AI), and extreme data-driven computing— relies on not only revitalizing existing parallel and distributed computing technologies but also innovating new solutions. Papers in this special issue explore diverse topics in research encompassing parallel, distributed, and heterogeneous systems for exascale, including advances in applications, programming environments, runtimes, libraries, innovative algorithms, domain-specific frameworks, systems architecture, performance analysis, data processing, and networking technologies.","1558-2183","","10.1109/TPDS.2021.3109651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576672","","Special issues and sections;Exascale computing;Artificial intelligence;Translational research;Research and development;Systems architecture;Runtime library;Programming environments;Performance evaluation;Distributed computing;Data processing","","","","","",0.0,"IEEE","15 Oct 2021","","","IEEE","IEEE Journals"
"Anomaly Detection and Anticipation in High Performance Computing Systems","A. Borghesi; M. Molan; M. Milano; A. Bartolini","Alma Mater Research Center for Human-Centered Artificial Intelligence, Bologna, Italy; DISI and DEI Department, University of Bologna, Bologna, Italy; Alma Mater Research Center for Human-Centered Artificial Intelligence, Bologna, Italy; Alma Mater Research Center for Human-Centered Artificial Intelligence, Bologna, Italy","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,739,750,"In their quest toward Exascale, High Performance Computing (HPC) systems are rapidly becoming larger and more complex, together with the issues concerning their maintenance. Luckily, many current HPC systems are endowed with data monitoring infrastructures that characterize the system state, and whose data can be used to train Deep Learning (DL) anomaly detection models, a very popular research area. However, the lack of labels describing the state of the system is a wide-spread issue, as annotating data is a costly task, generally falling on human system administrators and thus does not scale toward exascale. In this article we investigate the possibility to extract labels from a service monitoring tool (Nagios) currently used by HPC system administrators to flag the nodes which undergo maintenance operations. This allows to automatically annotate data collected by a fine-grained monitoring infrastructure; this labelled data is then used to train and validate a DL model for anomaly detection. We conduct the experimental evaluation on a tier-0 production supercomputer hosted at CINECA, Bologna, Italy. The results reveal that the DL model can accurately detect the real failures, and, moreover, it can predict the insurgency of anomalies, by systematically anticipating the actual labels (i.e., the moment when system administrators realize when an anomalous event happened); the average advance time computed on historical traces is around 45 minutes. The proposed technology can be easily scaled toward exascale systems to easy their maintenance.","1558-2183","","10.1109/TPDS.2021.3082802","EU H2020-ICT-11-2018-2019 IoTwins(grant numbers:857191); H2020-JTI-EuroHPC-2019-1 Regale(grant numbers:956560); Emilia-Romagna POR-FESR 2014-2020; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439169","High performance computing;anomaly detection;deep learning","Supercomputers;Data models;Monitoring;Anomaly detection;Tools;Bridges;Computational modeling","deep learning (artificial intelligence);mainframes;parallel machines;parallel processing","high performance computing systems;data monitoring;system state;popular research area;wide-spread issue;human system administrators;service monitoring tool;HPC system administrators;maintenance operations;fine-grained monitoring infrastructure;labelled data;DL model;actual labels;exascale systems;deep learning anomaly detection models;CINECA","",7.0,"",40.0,"IEEE","21 May 2021","","","IEEE","IEEE Journals"
"Online Power Management for Multi-Cores: A Reinforcement Learning Based Approach","Y. Wang; W. Zhang; M. Hao; Z. Wang","School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Computing, University of Leeds, Leeds, U.K.","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,751,764,"Power and energy is the first-class design constraint for multi-core processors and is a limiting factor for future-generation supercomputers. While modern processor design provides a wide range of mechanisms for power and energy optimization, it remains unclear how software can make the best use of them. This article presents a novel approach for runtime power optimization on modern multi-core systems. Our policy combines power capping and uncore frequency scaling to match the hardware power profile to the dynamically changing program behavior at runtime. We achieve this by employing reinforcement learning (RL) to automatically explore the energy-performance optimization space from training programs, learning the subtle relationships between the hardware power profile, the program characteristics, power consumption and program running times. Our RL framework then uses the learned knowledge to adapt the chip's power budget and uncore frequency to match the changing program phases for any new, previously unseen program. We evaluate our approach on two computing clusters by applying our techniques to 11 parallel programs that were not seen by our RL framework at the training stage. Experimental results show that our approach can reduce the system-level energy consumption by 12 percent, on average, with less than 3 percent of slowdown on the application performance. By lowering the uncore frequency to leave more energy budget to allow the processor cores to run at a higher frequency, our approach can reduce the energy consumption by up to 17 percent while improving the application performance by 5 percent for specific workloads.","1558-2183","","10.1109/TPDS.2021.3092270","National Key Research and Development Program of China(grant numbers:2020YFB1406902); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101360001); Shenzhen Science and Technology Research and Development Fundation(grant numbers:JCYJ20190806143418198); National Natural Science Foundation of China(grant numbers:61872110); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465707","Power management;multi-cores;reinforcement learning;power capping;uncore frequency;phase change detection","Hardware;Optimization;Training;Runtime;Power system management;Power demand;Energy consumption","learning (artificial intelligence);microprocessor chips;multiprocessing systems;optimisation;parallel machines;parallel programming;power aware computing;power consumption","future-generation supercomputers;power optimization;power capping;uncore frequency scaling;hardware power profile;dynamically changing program behavior;reinforcement learning based approach;energy-performance optimization space;training programs;power consumption;program running times;RL framework;system-level energy consumption;online power management;first-class design constraint;multicore processors","",3.0,"",72.0,"IEEE","25 Jun 2021","","","IEEE","IEEE Journals"
"Near-Zero Downtime Recovery From Transient-Error-Induced Crashes","C. Chen; G. Eisenhauer; S. Pande","Amazon Science, Santa Clara, CA, USA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,765,778,"Due to the system scaling, transient errors caused by external noise, e.g., heat fluxes and particle strikes, have become a growing concern for the current and upcoming exa-scale high-performance-computing (HPC) systems. Applications running on these systems are expected to experience transient errors more frequently than ever before, which will either lead them to generate incorrect outputs or cause them to crash. However, since such errors are still quite rare as compared to no-fault cases, desirable solutions call for low/no-overhead systems that do not compromise the performance under no-fault conditions and also allow very fast fault recovery to minimize downtime. In this article, we present IterPro, a light-weight compiler-assisted resilience technique to quickly and accurately recover processes from transient-error-induced crashes. During the compilation of applications, IterPro constructs a set of recovery kernels for crash-prone instructions. These recovery kernels are executed to repair the corrupted process states on-the-fly upon occurrences of errors, enabling applications to continue their executions instead of being terminated. When constructing recovery kernels, IterPro exploits side effects introduced by induction variable based code optimization techniques based on loop unrolling and strength reduction to improve its recovery capability. To this end, two new code transformation passes are introduced to expose the side effects for resilience purposes. We evaluated IterPro with 4 scientific workloads as well as the NPB benchmarks suite. During their normal execution, IterPro incurs almost zero runtime overhead and a small, fixed 27MB memory overhead. Meanwhile, IterPro can recover on an average 83.55 percent of crash-causing errors within dozens of milliseconds with negligible downtime. We also evaluated IterPro with parallel jobs running on 3072 cores and showed that IterPro can successfully mask the impact of crash-causing errors by providing almost uninterrupted execution. Finally, we present our preliminary evaluation result for BLAS, which shows that IterPro is capable of recovering failures in libraries with a very high coverage rate of 83 percent and negligible overheads. With such an effective recovery mechanism, IterPro could tremendously mitigate the overheads and resource requirements of the resilience subsystem in future exa-scale systems.","1558-2183","","10.1109/TPDS.2021.3096055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9479798","Resiliency;transient fault;soft error;fault tolerance;exa-scale computing;failure;crash;segment fault;compiler","Kernel;Computer crashes;Transient analysis;Runtime;Indexes;Resilience;Optimization","fault tolerant computing;optimisation;parallel processing;program compilers;system recovery","crash-causing errors;transient-error-induced crashes;fault recovery;light-weight compiler-assisted resilience technique;crash-prone instructions;induction variable based code optimization techniques;near-zero downtime recovery;exa-scale high-performance-computing systems;HPC;IterPro;recovery kernels;code transformation passes","","","",33.0,"IEEE","9 Jul 2021","","","IEEE","IEEE Journals"
"Compiler-Assisted Compaction/Restoration of SIMD Instructions","J. M. Cebrian; T. Balem; A. Barredo; M. Casas; M. Moretó; A. Ros; A. Jimborean","Computer Engineering Department, University of Murcia, Murcia, Spain; ENS Rennes, Rennes, France; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Computer Engineering Department, University of Murcia, Murcia, Spain; Computer Engineering Department, University of Murcia, Murcia, Spain","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,779,791,"Vector processors (e.g., SIMD or GPUs) are ubiquitous in high performance systems. All the supercomputers in the world exploit data-level parallelism (DLP), for example by using single instructions to operate over several data elements. Improving vector processing is therefore key for exascale computing. However, despite its potential, vector code generation and execution have significant challenges. Among these challenges, control flow divergence is one of the main performance limiting factors. Most modern vector instruction sets, including SIMD, rely on predication to support divergence control. Nevertheless, the performance and energy consumption in predicated codes is usually insensitive to the number of active elements in a predicated mask. Since the trend is that vector register size increases, the energy efficiency of exascale computing systems will become sub-optimal. This article proposes a novel approach to improve execution efficiency in predicated vector codes, the Compiler-Assisted Compaction/Restoration (CACR) technique. Baseline CR delays predicated SIMD instructions with inactive elements, compacting active elements from instances of the same instruction of consecutive loop iterations. Compacted elements form an equivalent dense vector instruction. After executing the dense instructions, their results are restored to the original instructions. However, CR has a significant performance and energy penalty when it fails to find active elements, either due to lack of resources when unrolling or because of inter-loop dependencies. In CACR, the compiler analyzes the code looking for key information required to configure CR. Then, it passes this information to the processor via new instructions inserted in the code. This prevents CR from waiting for active elements on scenarios when it would fail to form dense instructions. Simulated results (gem5) show that CACR improves performance by up to 29 percent and reduces dynamic energy by up to 24.2 percent on average, for a a set of applications with predicated execution. The baseline CR only achieves 18.6 percent performance and 14 percent energy improvements for the same configuration and applications.","1558-2183","","10.1109/TPDS.2021.3091015","Spanish Government(grant numbers:SEV2015-0493,BES-2017-080635); Ministerio de Ciencia e Innovación(grant numbers:PID2019-107255GB-C21/AEI/10.13039/501100011033,RTI2018-098156-B-C53); ECHO and RoMoL ERC(grant numbers:819134,321253); European HiPEAC Network(grant numbers:EU-FP7-610402,EU-H2020-779877); Spanish Ministry of Economy, Industry and Competitiveness(grant numbers:RYC-2016-21104,RYC-2017-23269,RYC-2018-025200-I); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462482","SIMD;predication;LLVM;density-time performance","Registers;Parallel processing;Hardware;Computer architecture;Out of order;Delays;Energy consumption","instruction sets;memory architecture;microprocessor chips;multiprocessing systems;parallel processing;parallel programming;performance evaluation;program compilers;vector processor systems","vector instruction sets;predication;energy consumption;active elements;vector register;energy efficiency;exascale computing systems;CACR;SIMD instructions;inactive elements;compact elements;dense vector instruction;energy penalty;vector processors;data level parallelism;single instructions;vector code generation;control flow divergence;supercomputers;Compiler-Assisted Compaction/Restoration technique","","","",48.0,"IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"EXA2PRO: A Framework for High Development Productivity on Heterogeneous Computing Systems","L. Papadopoulos; D. Soudris; C. Kessler; A. Ernstsson; J. Ahlqvist; N. Vasilas; A. I. Papadopoulos; P. Seferlis; C. Prouveur; M. Haefele; S. Thibault; A. Salamanis; T. Ioakimidis; D. Kehagias","Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Computer and Information Science, Linköping University, Linköping, Sweden; Department of Computer and Information Science, Linköping University, Linköping, Sweden; Department of Computer and Information Science, Linköping University, Linköping, Sweden; Centre for Research and Technology Hellas, Chemical Process and Energy Resources Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Chemical Process and Energy Resources Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Chemical Process and Energy Resources Institute, Thessaloniki, Greece; Maison de la Simulation, CEA, CNRS, Paris, France; Université de Pau et des Pays de l'Adour, Pau, France; Bordeaux University, Bordeaux, France; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,792,804,"Programming upcoming exascale computing systems is expected to be a major challenge. New programming models are required to improve programmability, by hiding the complexity of these systems from application developers. The EXA2PRO programming framework aims at improving developers’ productivity for applications that target heterogeneous computing systems. It is based on advanced programming models and abstractions that encapsulate low-level platform-specific optimizations and it is supported by a runtime that handles application deployment on heterogeneous nodes. It supports a wide variety of platforms and accelerators (CPU, GPU, FPGA-based Data-Flow Engines), allowing developers to efficiently exploit heterogeneous computing systems, thus enabling more HPC applications to reach exascale computing. The EXA2PRO framework was evaluated using four HPC applications from different domains. By applying the EXA2PRO framework, the applications were automatically deployed and evaluated on a variety of computing architectures, enabling developers to obtain performance results on accelerators, test scalability on MPI clusters and productively investigate the degree by which each application can efficiently use different types of hardware resources.","1558-2183","","10.1109/TPDS.2021.3104257","European Union's Horizon 2020 research and innovation programme(grant numbers:801015); National Infrastructures for Research and Technology; PRACE(grant numbers:EXA2PRO); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511802","Programming models;skeleton programming;task-based runtime systems;programming productivity;heterogeneous systems;exascale computing","Programming;Skeleton;Computational modeling;Runtime;Exascale computing;Task analysis;Productivity","application program interfaces;field programmable gate arrays;message passing;parallel processing;parallel programming;resource allocation","high development productivity;upcoming exascale computing systems;new programming models;application developers;EXA2PRO programming framework;improving developers;target heterogeneous computing systems;advanced programming models;abstractions;low-level platform-specific optimizations;application deployment;heterogeneous nodes;HPC applications;EXA2PRO framework;computing architectures","",3.0,"",33.0,"CCBY","11 Aug 2021","","","IEEE","IEEE Journals"
"Kokkos 3: Programming Model Extensions for the Exascale Era","C. R. Trott; D. Lebrun-Grandié; D. Arndt; J. Ciesko; V. Dang; N. Ellingwood; R. Gayatri; E. Harvey; D. S. Hollman; D. Ibanez; N. Liber; J. Madsen; J. Miles; D. Poliakoff; A. Powell; S. Rajamanickam; M. Simberg; D. Sunderland; B. Turcksin; J. Wilke","Sandia National Laboratories, Albuquerque, NM, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Argonne National Laboratory, Lemont, IL, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Swiss National Supercomputing Centre, Lugano, Switzerland; Sandia National Laboratories, Albuquerque, NM, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Sandia National Laboratories, Albuquerque, NM, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,805,817,"As the push towards exascale hardware has increased the diversity of system architectures, performance portability has become a critical aspect for scientific software. We describe the Kokkos Performance Portable Programming Model that allows developers to write single source applications for diverse high-performance computing architectures. Kokkos provides key abstractions for both the compute and memory hierarchy of modern hardware. We describe the novel abstractions that have been added to Kokkos version 3 such as hierarchical parallelism, containers, task graphs, and arbitrary-sized atomic operations to prepare for exascale era architectures. We demonstrate the performance of these new features with reproducible benchmarks on CPUs and GPUs.","1558-2183","","10.1109/TPDS.2021.3097283","Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9485033","Performance portability;programming models;high-performance computing;heterogeneous computing;exascale","Programming;Hardware;Kernel;Graphics processing units;Layout;Laboratories;Benchmark testing","application program interfaces;multiprocessing systems;parallel architectures;parallel processing;software libraries;software portability","Kokkos 3;Model extensions;exascale hardware;system architectures;performance portability;scientific software;Kokkos Performance Portable Programming Model;single source applications;diverse high-performance computing architectures;key abstractions;memory hierarchy;modern hardware;Kokkos version;exascale era architectures","",45.0,"",24.0,"CCBY","14 Jul 2021","","","IEEE","IEEE Journals"
"Design and Performance Characterization of RADICAL-Pilot on Leadership-Class Platforms","A. Merzky; M. Turilli; M. Titov; A. Al-Saadi; S. Jha","Rutgers, The State University of New Jersey, Piscataway, NJ, USA; Rutgers, The State University of New Jersey, Piscataway, NJ, USA; Rutgers, The State University of New Jersey, Piscataway, NJ, USA; Rutgers, The State University of New Jersey, Piscataway, NJ, USA; Brookhaven National Laboratory, Upton, NY, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,818,829,"Many extreme scale scientific applications have workloads comprised of a large number of individual high-performance tasks. The Pilot abstraction decouples workload specification, resource management, and task execution via job placeholders and late-binding. As such, suitable implementations of the Pilot abstraction can support the collective execution of large number of tasks on supercomputers. We introduce RADICAL-Pilot (RP) as a portable, modular and extensible pilot-enabled runtime system. We describe RP's design, architecture and implementation. We characterize its performance and show its ability to scalably execute workloads comprised of tens of thousands heterogeneous tasks on DOE and NSF leadership-class HPC platforms. Specifically, we investigate RP's weak/strong scaling with CPU/GPU, single/multi core, (non)MPI tasks and Python functions when using most of ORNL Summit and TACC Frontera. RADICAL-Pilot can be used stand-alone, as well as the runtime for third-party workflow systems.","1558-2183","","10.1109/TPDS.2021.3105994","National Science Foundation(grant numbers:NSF-1931512,NSF-1835449); ECP CANDLE and ExaWorks; Exascale Computing Project 17-SC-20-SC(grant numbers:DESC0012704); U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); XSEDE resources(grant numbers:TG-MCB090174); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519521","Middleware;high performance computing;RADICAL-Pilot;python","Task analysis;Runtime;Resource management;Parallel processing;Tools;Throughput;Programming","application program interfaces;message passing;parallel processing","RADICAL-Pilot;leadership-class HPC platforms;extreme scale scientific applications;Pilot abstraction decouples;task execution;collective execution;pilot-enabled runtime system","",8.0,"",48.0,"IEEE","19 Aug 2021","","","IEEE","IEEE Journals"
"LB4OMP: A Dynamic Load Balancing Library for Multithreaded Applications","J. H. M. Korndörfer; A. Eleliemy; A. Mohammed; F. M. Ciorba","Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland; Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland; HPE's HPC/AI EMEA Research Lab (ERL), Switzerland; Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,830,841,"Exascale computing systems will exhibit high degrees of hierarchical parallelism, with thousands of computing nodes and hundreds of cores per node. Efficiently exploiting hierarchical parallelism is challenging due to load imbalance that arises at multiple levels. OpenMP is the most widely-used standard for expressing and exploiting the ever-increasing node-level parallelism. The scheduling options in OpenMP are insufficient to address the load imbalance that arises during the execution of multithreaded applications. The limited scheduling options in OpenMP hinder research on novel scheduling techniques which require comparison with others from the literature. This work introduces LB4OMP, an open-source dynamic load balancing library that implements successful scheduling algorithms from the literature. LB4OMP is a research infrastructure designed to spur and support present and future scheduling research, for the benefit of multithreaded applications performance. Through an extensive performance analysis campaign, we assess the effectiveness and demystify the performance of all loop scheduling techniques in the library. We show that, for numerous applications-systems pairs, the scheduling techniques in LB4OMP outperform the scheduling options in OpenMP. Node-level load balancing using LB4OMP leads to reduced cross-node load imbalance and to improved MPI+OpenMP applications performance, which is critical for Exascale computing.","1558-2183","","10.1109/TPDS.2021.3107775","Swiss National Science Foundation(grant numbers:169123); Swiss Platform for Advanced Scientific Computing; European Union's Horizon 2020 research and innovation programme(grant numbers:957407); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524500","Hierarchical parallelism;dynamic load balancing;self-scheduling;runtime library;OpenMP;multithreaded programming;shared-memory systems","Dynamic scheduling;Processor scheduling;Parallel processing;Optimal scheduling;Libraries;Standards;Load management","application program interfaces;message passing;multi-threading;resource allocation;scheduling;shared memory systems","hierarchical parallelism;computing nodes;node-level parallelism;scheduling techniques;LB4OMP;open-source dynamic load balancing library;multithreaded applications performance;node-level load balancing;MPI+OpenMP applications performance;exascale computing systems;applications-systems pairs;cross-node load imbalance","",3.0,"",45.0,"CCBY","27 Aug 2021","","","IEEE","IEEE Journals"
"The PetscSF Scalable Communication Layer","J. Zhang; J. Brown; S. Balay; J. Faibussowitsch; M. Knepley; O. Marin; R. T. Mills; T. Munson; B. F. Smith; S. Zampini","Argonne National Laboratory, Lemont, IL, USA; University of Colorado Boulder, Boulder, CO, USA; Argonne National Laboratory, Lemont, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA; University at Buffalo, Buffalo, NY, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne Associate of Global Empire, LLC, Argonne National Laboratory, Lemont, IL, USA; King Abdullah University of Science and Technology, Thuwal, Saudi Arabia","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,842,853,"PetscSF, the communication component of the Portable, Extensible Toolkit for Scientific Computation (PETSc), is designed to provide PETSc's communication infrastructure suitable for exascale computers that utilize GPUs and other accelerators. PetscSF provides a simple application programming interface (API) for managing common communication patterns in scientific computations by using a star-forest graph representation. PetscSF supports several implementations based on MPI and NVSHMEM, whose selection is based on the characteristics of the application or the target architecture. An efficient and portable model for network and intra-node communication is essential for implementing large-scale applications. The Message Passing Interface, which has been the de facto standard for distributed memory systems, has developed into a large complex API that does not yet provide high performance on the emerging heterogeneous CPU-GPU-based exascale systems. In this article, we discuss the design of PetscSF, how it can overcome some difficulties of working directly with MPI on GPUs, and we demonstrate its performance, scalability, and novel features.","1558-2183","","10.1109/TPDS.2021.3084070","Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); Office of Science(grant numbers:DE-SC0016140,DE-AC02-0000011838); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442258","Communication;GPU;extreme-scale;MPI;PETSc","Libraries;Programming;Graphics processing units;Forestry;Electronics packaging;Arrays;Scalability","application program interfaces;distributed memory systems;graph theory;graphics processing units;message passing;multiprocessing systems","communication component;portable, extensible toolkit for scientific computation;simple application programming interface;star-forest graph representation;intra-node communication;large-scale applications;message passing interface;heterogeneous CPU-GPU-based exascale systems;distributed memory systems","",5.0,"",26.0,"IEEE","26 May 2021","","","IEEE","IEEE Journals"
"An Automated Tool for Analysis and Tuning of GPU-Accelerated Code in HPC Applications","K. Zhou; X. Meng; R. Sai; D. Grubisic; J. Mellor-Crummey","Computer Science Department, Rice University, Houston, TX, USA; Computer Science Department, Rice University, Houston, TX, USA; Computer Science Department, Rice University, Houston, TX, USA; Computer Science Department, Rice University, Houston, TX, USA; Computer Science Department, Rice University, Houston, TX, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,854,865,"The US Department of Energy’s fastest supercomputers and forthcoming exascale systems employ Graphics Processing Units (GPUs) to increase the computational performance of compute nodes. However, the complexity of GPU architectures makes tailoring sophisticated applications to achieve high performance on GPU-accelerated systems a major challenge. At best, prior performance tools for GPU code only provide coarse-grained tuning advice at the kernel level. In this article, we describe GPA, a performance advisor that suggests potential code optimizations at a hierarchy of levels, including individual lines, loops, and functions. To gather the fine-grained measurements needed to produce such insights, GPA uses instruction sampling and binary instrumentation to monitor execution of GPU code. At the time of this writing, GPU instruction sampling is only available on NVIDIA GPUs. To understand performance losses, GPA uses data flow analysis to approximately attribute measured instruction stalls back to their causes. GPA then analyzes patterns of stalls using information about a program’s structure and the GPU architecture to identify optimization strategies that address inefficiencies observed. GPA then employs detailed performance models to estimate the potential speedup that each optimization might provide. Experiments with benchmarks and applications show that GPA provides useful advice for tuning GPU code. We applied GPA to analyze and tune a collection of codes on NVIDIA V100 and A100 GPUs. GPA suggested optimizations that it estimates will accelerate performance across the set of codes by a geometric mean of 1.21×. Applying these optimizations suggested by GPA accelerated these codes by a geometric mean of 1.19×.","1558-2183","","10.1109/TPDS.2021.3094169","Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; Lawrence Livermore National Laboratory(grant numbers:B639429); ExxonMobil Graduate Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9470950","High performance computing;performance analysis;parallel programming;parallel architectures","Graphics processing units;Optimization;Tools;Measurement;Instruments;Tuning;Registers","computer graphic equipment;data flow analysis;graphics processing units;operating system kernels;parallel architectures;parallel machines","automated tool;GPU-accelerated code;HPC applications;supercomputers;forthcoming exascale systems;graphics processing units;computational performance;GPU-accelerated systems;coarse-grained tuning advice;GPA;potential code optimizations;GPU instruction sampling;US Department;NVIDIA GPU","",1.0,"",45.0,"IEEE","1 Jul 2021","","","IEEE","IEEE Journals"
"Enabling Scalable and Extensible Memory-Mapped Datastores in Userspace","I. B. Peng; M. B. Gokhale; K. Youssef; K. Iwabuchi; R. Pearce","Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Virginia Tech, Blacksburg, VA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,866,877,"Exascale workloads are expected to incorporate data-intensive processing in close coordination with traditional physics simulations. These emerging scientific, data-analytics and machine learning applications need to access a wide variety of datastores in flat files and structured databases. Programmer productivity is greatly enhanced by mapping datastores into the application process's virtual memory space to provide a unified “in-memory” interface. Currently, memory mapping is provided by system software primarily designed for generality and reliability. However, scalability at high concurrency is a formidable challenge on exascale systems. Also, there is a need for extensibility to support new datastores potentially requiring HPC data transfer services. In this article, we present UMap, a scalable and extensible userspace service for memory-mapping datastores. Through decoupled queue management, concurrency aware adaptation, and dynamic load balancing, UMap enables application performance to scale even at high concurrency. We evaluate UMap in data-intensive applications, including sorting, graph traversal, database operations, and metagenomic analytics. Our results show that UMap as a userspace service outperforms an optimized kernel-based service across a wide range of intra-node concurrency by 1.22-1.9 ${\times}$×. We performed two case studies to demonstrate UMap's extensibility. First, a new datastore residing in remote memory is incorporated into UMap as an application-specific plugin. Second, we present a persistent memory allocator Metall built atop UMap for unified storage/memory.","1558-2183","","10.1109/TPDS.2021.3086302","Lawrence Livermore National Laboratory; U.S. Department of Energy(grant numbers:DE-AC52-07NA27344 (LLNL-JRNL-819817)); Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446641","Data-intensive;userspace;memory mapping;mmap;memory-mapped I/O","Concurrent computing;Kernel;Databases;Optimization;Task analysis;Scalability;Prefetching","concurrency (computers);data analysis;database management systems;graph theory;learning (artificial intelligence);natural sciences computing;parallel processing;queueing theory;resource allocation;sorting;virtual storage","extensible memory-mapped datastores;data-intensive processing;traditional physics simulations;machine learning applications;flat files;structured databases;system software;exascale systems;HPC data transfer services;decoupled queue management;concurrency aware adaptation;database operations;metagenomic analytics;application-specific plugin;persistent memory allocator;scientific data-analytics;virtual memory space;unified in-memory interface;sorting;graph traversal;dynamic load balancing;scalable extensible userspace service;Metall","",2.0,"",29.0,"IEEE","3 Jun 2021","","","IEEE","IEEE Journals"
"Improving I/O Performance for Exascale Applications Through Online Data Layout Reorganization","L. Wan; A. Huebl; J. Gu; F. Poeschel; A. Gainaru; R. Wang; J. Chen; X. Liang; D. Ganyushin; T. Munson; I. Foster; J. -L. Vay; N. Podhorszki; K. Wu; S. Klasky","Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Center for Advanced Systems Understanding (CASUS), Görlitz, Germany; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Missouri University of Science and Technology, Rolla, MO, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,878,890,"The applications being developed within the U.S. Exascale Computing Project (ECP) to run on imminent Exascale computers will generate scientific results with unprecedented fidelity and record turn-around time. Many of these codes are based on particle-mesh methods and use advanced algorithms, especially dynamic load-balancing and mesh-refinement, to achieve high performance on Exascale machines. Yet, as such algorithms improve parallel application efficiency, they raise new challenges for I/O logic due to their irregular and dynamic data distributions. Thus, while the enormous data rates of Exascale simulations already challenge existing file system write strategies, the need for efficient read and processing of generated data introduces additional constraints on the data layout strategies that can be used when writing data to secondary storage. We review these I/O challenges and introduce two online data layout reorganization approaches for achieving good tradeoffs between read and write performance. We demonstrate the benefits of using these two approaches for the ECP particle-in-cell simulation WarpX, which serves as a motif for a large class of important Exascale applications. We show that by understanding application I/O patterns and carefully designing data layouts we can increase read performance by more than 80 percent.","1558-2183","","10.1109/TPDS.2021.3100784","Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; Center of Advanced Systems Understanding; Germany's Federal Ministry of Education and Research; Saxon Ministry for Science, Culture and Tourism; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501495","Parallel IO;data layout;IO performance;WarpX;data access optimization","Layout;Arrays;Heuristic algorithms;Computational modeling;Performance evaluation;Optimization;Distributed databases","application program interfaces;input-output programs;parallel processing;resource allocation","online data layout reorganization;imminent Exascale computers;record turn-around time;particle-mesh methods;dynamic load-balancing;mesh-refinement;Exascale machines;parallel application efficiency;irregular data distributions;dynamic data distributions;Exascale simulations;data layout strategies;ECP particle-in-cell simulation WarpX;data layouts;read performance;U.S. exascale computing project","",8.0,"",38.0,"IEEE","29 Jul 2021","","","IEEE","IEEE Journals"
"Transparent Asynchronous Parallel I/O Using Background Threads","H. Tang; Q. Koziol; J. Ravi; S. Byna","Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; NC State University, Raleigh, NC, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,891,902,"Moving toward exascale computing, the size of data stored and accessed by applications is ever increasing. However, traditional disk-based storage has not seen improvements that keep up with the explosion of data volume or the speed of processors. Multiple levels of non-volatile storage devices are being added to handle bursty I/O, however, moving data across the storage hierarchy can take longer than the data generation or analysis. Asynchronous I/O can reduce the impact of I/O latency as it allows applications to schedule I/O early and to check their status later. I/O is thus overlapped with application communication or computation or both, effectively hiding some or all of the I/O latency. POSIX and MPI-I/O provide asynchronous read and write operations, but lack the support for non-data operations such as file open and close. Users also have to manually manage data dependencies and use low-level byte offsets, which requires significant effort and expertise to adopt. In this article, we present an asynchronous I/O framework that supports all types of I/O operations, manages data dependencies transparently and automatically, provides implicit and explicit modes for application flexibility, and error information retrieval. We implemented these techniques in HDF5. Our evaluation of several benchmarks and application workloads demonstrates it effectiveness on hiding the I/O cost from the application.","1558-2183","","10.1109/TPDS.2021.3090322","Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459479","Asynchronous I/O;parallel I/O;background threads","Task analysis;Connectors;Libraries;Instruction sets;Computational modeling;Monitoring;Middleware","application program interfaces;input-output programs;message passing;storage management","exascale computing;nonvolatile storage devices;storage hierarchy;application communication;data dependencies;low-level byte offsets;application flexibility;background threads;asynchronous read and write operations;transparent asynchronous parallel I/O framework","",3.0,"",32.0,"IEEE","17 Jun 2021","","","IEEE","IEEE Journals"
"Accelerating HDF5 I/O for Exascale Using DAOS","J. Soumagne; J. Henderson; M. Chaarawi; N. Fortner; S. Breitenfeld; S. Lu; D. Robinson; E. Pourmal; J. Lombardi","HDF Group, Champaign, IL, USA; HDF Group, Champaign, IL, USA; Extreme Storage Architecture & Development, Intel Corporation, Santa Clara, CA, USA; HDF Group, Champaign, IL, USA; HDF Group, Champaign, IL, USA; HDF Group, Champaign, IL, USA; HDF Group, Champaign, IL, USA; HDF Group, Champaign, IL, USA; Extreme Storage Architecture & Development, Intel Corporation, Santa Clara, CA, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,903,914,"The Hierarchical Data Format 5 (HDF5) has long been defined as one of the most prominent data models, binary file formats and I/O libraries for storing and managing scientific data. Introduced in the late 90s when POSIX I/O was the standard, the library has since then been continuously improved to respond and adapt to the ever-growing demands of high-performance computing (HPC) software and hardware. Given the limitations of POSIX I/O and with the emergence of new technologies such as object stores, non-volatile memory, and SSDs, the need for an interface that can efficiently store and access data at scale through new paradigms has become more and more pressing. The Distributed Asynchronous Object Storage (DAOS) file system is an emerging file system that aims at responding to those demands by taking disk-based storage out of the loop. We present in this article the research efforts that have been taking place to prepare the HDF5 library for Exascale using DAOS. By enabling and defining a new storage file format, we focus on the benefits that it delivers to the applications in terms of features and performance.","1558-2183","","10.1109/TPDS.2021.3097884","U.S. Department of Energy; Argonne National Laboratory(grant numbers:DE-AC02-06CH11357,8F-30005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490299","Parallel I/O;distributed file systems;data storage representations;object representation","Libraries;Semantics;Metadata;Connectors;Nonvolatile memory;Middleware;Writing","data models;disc storage;input-output programs;message passing;parallel processing;random-access storage","disk-based storage;HDF5 library;Exascale;DAOS;hierarchical data format 5;data models;binary file formats;object stores;nonvolatile memory;high-performance computing;POSIX I-O;distributed asynchronous object storage file system","",2.0,"",28.0,"IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Characterizing Performance of Graph Neighborhood Communication Patterns","S. Ghosh; N. R. Tallent; M. Halappanavar","Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA; Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA; Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,915,928,"Distributed-memory graph algorithms are fundamental enablers in scientific computing and analytics workflows. A majority of graph algorithms rely on the graph neighborhood communication pattern, i.e., repeated asynchronous communication between a vertex and its neighbors in the graph. The pattern is adversarial for communication software and hardware due to high message injection rates and input-dependent, many-to-one traffic with variable destinations and volumes. We present benchmarks and performance analysis of graph neighborhood communication on modern large-scale network interconnects from four supercomputers: ALCF Theta, NERSC Cori, OLCF Summit and R-CCS Fugaku. Our benchmarks characterize communication from the perspectives of latency and throughput. Benchmark parameters make it possible to mimic the behaviors of complex applications on real world or synthetic graphs by varying work distribution, remote edges, message volume, and per-vertex work. We find that minor changes in the input graph can substantially increase latencies; and contention can develop in memory caches and network stacks before contention in the network itself. Further, latencies and contention vary significantly for different graph neighborhoods, motivating the need for exploring asynchronous algorithms in greater detail. When adding work, load imbalance on real-world graphs can be pronounced: latencies for the 99th percentile were 8–128× than the corresponding average latencies. Our results help analysts and developers understand the performance implications of this important pattern, especially for the impending exascale platforms.","1558-2183","","10.1109/TPDS.2021.3101425","U.S. Department of Energy(grant numbers:DE-AC02-05CH11231,DE-AC02-06CH11357); RIKEN CCS; U.S. Department of Energy(grant numbers:17-SC-20-SC); Advanced Scientific Computing Research; Data-Model Convergence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9503355","Graphs and networks;neighborhood communication;MPI;network communication;network contention;distributed memories;benchmarking","Benchmark testing;Pattern matching;Heating systems;Clustering algorithms;Software;Topology;Surges","distributed memory systems;graph theory;mainframes;parallel algorithms;parallel machines;performance evaluation","performance evaluation;network stacks;memory caches;real-world graphs;input graph;synthetic graphs;high message injection rates;communication software;repeated asynchronous communication;analytics workflows;scientific computing;distributed-memory graph algorithms;graph neighborhood communication pattern","",1.0,"",52.0,"IEEE","2 Aug 2021","","","IEEE","IEEE Journals"
"A Parallel Algorithm Template for Updating Single-Source Shortest Paths in Large-Scale Dynamic Networks","A. Khanda; S. Srinivasan; S. Bhowmick; B. Norris; S. K. Das","Department of Computer Science, Missouri University of Science and Technology, Rolla, MO, USA; Department of Radiation Oncology, Virginia Commonwealth University, Richmond, VA, USA; Department of Computer Science and Engineering, University of North Texas, Denton, TX, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer Science, Missouri University of Science and Technology, Rolla, MO, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,929,940,"The Single Source Shortest Path (SSSP) problem is a classic graph theory problem that arises frequently in various practical scenarios; hence, many parallel algorithms have been developed to solve it. However, these algorithms operate on static graphs, whereas many real-world problems are best modeled as dynamic networks, where the structure of the network changes with time. This gap between the dynamic graph modeling and the assumed static graph model in the conventional SSSP algorithms motivates this work. We present a novel parallel algorithmic framework for updating the SSSP in large-scale dynamic networks and implement it on the shared-memory and GPU platforms. The basic idea is to identify the portion of the network affected by the changes and update the information in a rooted tree data structure that stores the edges of the network that are most relevant to the analysis. Extensive experimental evaluations on real-world and synthetic networks demonstrate that our proposed parallel updating algorithm is scalable and, in most cases, requires significantly less execution time than the state-of-the-art recomputing-from-scratch algorithms.","1558-2183","","10.1109/TPDS.2021.3084096","National Science Foundation(grant numbers:1725755,1725566,1725585); National Science Foundation(grant numbers:1919789); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442368","Dynamic networks;single source shortest path (SSSP);shared-memory parallel algorithm;GPU implementation","Heuristic algorithms;Graphics processing units;Parallel algorithms;Synchronization;Multicore processing;Complexity theory;Wireless sensor networks","computational complexity;graph theory;parallel algorithms;tree data structures","parallel algorithm template;updating Single-Source Shortest paths;large-scale dynamic networks;Single Source Shortest Path problem;classic graph theory problem;parallel algorithms;static graphs;dynamic graph modeling;assumed static graph model;conventional SSSP algorithms motivates this work;parallel algorithmic framework;rooted tree data structure;synthetic networks;state-of-the-art recomputing-from-scratch algorithms","",2.0,"",25.0,"IEEE","26 May 2021","","","IEEE","IEEE Journals"
"TianheGraph: Customizing Graph Search for Graph500 on Tianhe Supercomputer","X. Gan; Y. Zhang; R. Wang; T. Li; T. Xiao; R. Zeng; J. Liu; K. Lu","National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,941,951,"As the era of exascale supercomputing is coming, it is vital for next-generation supercomputers to find appropriate applications with high social and economic benefit. In recent years, it has been widely accepted that extremely-large graph computation is a promising killer application for supercomputing. Although Tianhe series supercomputers are leading in the world-wide competition of supercomputing (ranked No. 1 in the Top500 list for six times), previously they had been inefficient in graph computation according to the Graph500 list. This is mainly because the previous graph processing system cannot leverage the advanced hardware features of Tianhe supercomputers. To address the problem, in this paper we present our integrated optimizations for improving the graph computation performance on our next-generation Tianhe supercomputing system, mainly including sorting with buffering for heavy vertices, vectorized searching with SVE (Scalable Vector Extension) on matrix2000+ CPUs, and group communication on the proprietary interconnection network. Performance evaluation on a subset of the Tianhe supercomputer (with 512 nodes and 196,608 cores) shows that our customized graph processing system effectively improves the graph search performance and achieves the BFS performance of 2131.98 GTEPS.","1558-2183","","10.1109/TPDS.2021.3100785","National Key Research and Development Program of China(grant numbers:2018YFB2101102); National Natural Science Foundation of China(grant numbers:61772541,61872376,61932001); Natural Science Foundation of Hunan Province(grant numbers:2020JJ4669); Foundation of Parallel and Distributed Processing Laboratory(grant numbers:6142110190206); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502006","Graph computation;exascale supercomputer;matrix2000+;group-based communication;Graph500;BFS","Supercomputers;Next generation networking;Benchmark testing;Hardware;Multiprocessor interconnection;Image edge detection;Performance evaluation","graph theory;mainframes;parallel machines;performance evaluation;search problems","exascale supercomputing;Tianhe series supercomputers;Graph500 list;graph processing system;graph computation performance;next-generation Tianhe supercomputing system;customized graph;graph search performance;scalable vector extension;interconnection network;performance evaluation","",2.0,"",51.0,"IEEE","29 Jul 2021","","","IEEE","IEEE Journals"
"VPIC 2.0: Next Generation Particle-in-Cell Simulations","R. Bird; N. Tan; S. V. Luedtke; S. L. Harrell; M. Taufer; B. Albright","Los Alamos National Laboratory, Los Alamos, NM, USA; University of Tennessee at Knoxville, Knoxville, TN, USA; Los Alamos National Laboratory, Los Alamos, NM, USA; Texas Advanced Computing Center, University of Texas at Austin, Austin, TX, USA; University of Tennessee at Knoxville, Knoxville, TN, USA; Los Alamos National Laboratory, Los Alamos, NM, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,952,963,"VPIC is a general purpose particle-in-cell simulation code for modeling plasma phenomena such as magnetic reconnection, fusion, solar weather, and laser-plasma interaction in three dimensions using large numbers of particles. VPIC's capacity in both fidelity and scale makes it particularly well-suited for plasma research on pre-exascale and exascale platforms. In this article, we demonstrate the unique challenges involved in preparing the VPIC code for operation at exascale, outlining important optimizations to make VPIC efficient on accelerators. Specifically, we show the work undertaken in adapting VPIC to exploit the portability-enabling framework Kokkos and highlight the enhancements to VPIC's modeling capabilities to achieve performance at exascale. We assess the achieved performance-portability trade-off through a suite of studies on nine different varieties of modern pre-exascale hardware. Our performance-portability study includes weak-scaling runs on three of the top ten TOP500 supercomputers, as well as a comparison of low-level system performance of hardware from four different vendors.","1558-2183","","10.1109/TPDS.2021.3084795","U.S. Department of Energy; Los Alamos National Laboratory; LANL ASC; Experimental Sciences Programs(grant numbers:LA-UR-21-21453); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444146","Simulation;portability;plasma physics;particle-in-cell","Plasmas;Hardware;Physics;Libraries;Mathematical model;Shape;Layout","magnetic reconnection;mainframes;parallel machines;physics computing;plasma simulation","magnetic reconnection;solar weather;laser-plasma interaction;exascale platforms;VPIC's modeling capabilities;performance-portability study;particle-in-cell simulation code;plasma phenomena;performance-portability trade-off;pre-exascale hardware;VPIC 2.0;next generation particle-in-cell simulations;TOP500 supercomputers;low-level system performance","",15.0,"",45.0,"CCBY","28 May 2021","","","IEEE","IEEE Journals"
"Accelerating Geostatistical Modeling and Prediction With Mixed-Precision Computations: A High-Productivity Approach With PaRSEC","S. Abdulah; Q. Cao; Y. Pei; G. Bosilca; J. Dongarra; M. G. Genton; D. E. Keyes; H. Ltaief; Y. Sun","Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,964,976,"Geostatistical modeling, one of the prime motivating applications for exascale computing, is a technique for predicting desired quantities from geographically distributed data, based on statistical models and optimization of parameters. Spatial data are assumed to possess properties of stationarity or non-stationarity via a kernel fitted to a covariance matrix. A primary workhorse of stationary spatial statistics is Gaussian maximum log-likelihood estimation (MLE), whose central data structure is a dense, symmetric positive definite covariance matrix of the dimension of the number of correlated observations. Two essential operations in MLE are the application of the inverse and evaluation of the determinant of the covariance matrix. These can be rendered through the Cholesky decomposition and triangular solution. In this contribution, we reduce the precision of weakly correlated locations to single- or half- precision based on distance. We thus exploit mathematical structure to migrate MLE to a three-precision approximation that takes advantage of contemporary architectures offering BLAS3-like operations in a single instruction that are extremely fast for reduced precision. We illustrate application-expected accuracy worthy of double-precision from a majority half-precision computation, in a context where uniform single-precision is by itself insufficient. In tackling the complexity and imbalance caused by the mixing of three precisions, we deploy the PaRSEC runtime system. PaRSEC delivers on-demand casting of precisions while orchestrating tasks and data movement in a multi-GPU distributed-memory environment within a tile-based Cholesky factorization. Application-expected accuracy is maintained while achieving up to $1.59X$1.59X by mixing FP64/FP32 operations on 1536 nodes of HAWK or 4096 nodes of Shaheen II, and up to $2.64X$2.64X by mixing FP64/FP32/FP16 operations on 128 nodes of Summit, relative to FP64-only operations. This translates into up to 4.5, 4.7, and 9.1 (mixed) PFlop/s sustained performance, respectively, demonstrating a synergistic combination of exascale architecture, dynamic runtime software, and algorithmic adaptation applied to challenging environmental problems.","1558-2183","","10.1109/TPDS.2021.3084071","U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442267","Climate/weather prediction;dynamic runtime systems;geospatial statistics;high performance computing;multiple precisions;user-productivity","Computational modeling;Covariance matrices;Runtime;Task analysis;Predictive models;Maximum likelihood estimation;Data models","approximation theory;covariance matrices;data analysis;data structures;geophysics computing;graphics processing units;matrix decomposition;maximum likelihood estimation;optimisation","geostatistical modeling;mixed-precision computations;prime motivating applications;geographically distributed data;spatial data;stationary spatial statistics;central data structure;dense covariance matrix;symmetric positive definite covariance matrix;three-precision approximation;double-precision;majority half-precision computation;uniform single-precision;high-productivity approach;PaRSEC;exascale computing;parameter optimization;kernel fitted;Gaussian maximum log-likelihood estimation;Cholesky decomposition;triangular solution;exploit mathematical structure;migrate MLE;contemporary architectures;orchestrating tasks;multiGPU distributed-memory environment","",5.0,"",55.0,"IEEE","26 May 2021","","","IEEE","IEEE Journals"
"libEnsemble: A Library to Coordinate the Concurrent Evaluation of Dynamic Ensembles of Calculations","S. Hudson; J. Larson; J. -L. Navarro; S. M. Wild","Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,977,988,"Almost all applications stop scaling at some point; those that don't are seldom performant when considering time to solution on anything but aspirational/unicorn resources. Recognizing these tradeoffs as well as greater user functionality in a near-term exascale computing era, we present libEnsemble, a library aimed at particular scalability- and capability-stretching uses. libEnsemble enables running concurrent instances of an application in dynamically allocated ensembles through an extensible Python library. We highlight the structure, execution, and capabilities of the library on leading pre-exascale environments as well as advanced capabilities for exascale environments and beyond.","1558-2183","","10.1109/TPDS.2021.3082815","U.S. Department of Energy(grant numbers:17-SC-20-SC); ComPASS and NUCLEI SciDAC; Advanced Scientific Computing Research(grant numbers:DE-AC02-06CH11357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439163","Dynamic ensembles;simulation support systems;workflow management;HPC Python","Generators;Resource management;Libraries;Optimization;Arrays;History;Concurrent computing","libraries;Python","concurrent evaluation;dynamic ensembles;concurrent instances;dynamically allocated ensembles;extensible Python library;advanced capabilities;preexascale environments;nearterm exascale computing era","",7.0,"",38.0,"IEEE","21 May 2021","","","IEEE","IEEE Journals"
"Combinatorial BLAS 2.0: Scaling Combinatorial Algorithms on Distributed-Memory Systems","A. Azad; O. Selvitopi; M. T. Hussain; J. R. Gilbert; A. Buluç","Indiana University, Bloomington, IN, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Indiana University, Bloomington, IN, USA; University of California, Santa Barbara, Santa Barbara, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,989,1001,"Combinatorial algorithms such as those that arise in graph analysis, modeling of discrete systems, bioinformatics, and chemistry, are often hard to parallelize. The Combinatorial BLAS library implements key computational primitives for rapid development of combinatorial algorithms in distributed-memory systems. During the decade since its first introduction, the Combinatorial BLAS library has evolved and expanded significantly. This article details many of the key technical features of Combinatorial BLAS version 2.0, such as communication avoidance, hierarchical parallelism via in-node multithreading, accelerator support via GPU kernels, generalized semiring support, implementations of key data structures and functions, and scalable distributed I/O operations for human-readable files. Our article also presents several rules of thumb for choosing the right data structures and functions in Combinatorial BLAS 2.0, under various common application scenarios.","1558-2183","","10.1109/TPDS.2021.3094091","Advanced Scientific Computing Research(grant numbers:DE-AC02-05CH11231); National Science Foundation(grant numbers:1823034); Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; National Science Foundation(grant numbers:CCF-1637564); U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9470983","Sparse matrices;parallel computing;combinatorics;graph theory;communication-avoidance algorithms","Sparse matrices;Libraries;Data structures;Indexes;Three-dimensional displays;Data analysis;Computational modeling","data structures;distributed memory systems;graph theory;multi-threading;software libraries","key technical features;key data structures;combinatorial algorithms;distributed-memory systems;discrete systems;combinatorial BLAS library;in-node multithreading;GPU kernels;generalized semiring support;combinatorial BLAS 2.0","",6.0,"",46.0,"IEEE","1 Jul 2021","","","IEEE","IEEE Journals"
"Evaluating Spatial Accelerator Architectures with Tiled Matrix-Matrix Multiplication","G. E. Moon; H. Kwon; G. Jeong; P. Chatarasi; S. Rajamanickam; T. Krishna","Department of Software, Korea Aerospace University, Goyang, Gyeonggi, Republic of Korea; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; Center for Computing Research, Sandia National Laboratories, Albuquerque, NM, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,1002,1014,"There is a growing interest in custom spatial accelerators for machine learning applications. These accelerators employ a spatial array of processing elements (PEs) interacting via custom buffer hierarchies and networks-on-chip. The efficiency of these accelerators comes from employing optimized dataflow (i.e., spatial/temporal partitioning of data across the PEs and fine-grained scheduling) strategies to optimize data reuse. The focus of this work is to evaluate these accelerator architectures using a tiled general matrix-matrix multiplication (GEMM) kernel. To do so, we develop a framework that finds optimized mappings (dataflow and tile sizes) for a tiled GEMM for a given spatial accelerator and workload combination, leveraging an analytical cost model for runtime and energy. Our evaluations over five spatial accelerators demonstrate that the tiled GEMM mappings systematically generated by our framework achieve high performance on various GEMM workloads and accelerators.","1558-2183","","10.1109/TPDS.2021.3104240","U.S. Department of Energy; National Technology & Engineering Solutions of Sandia, LLC; National Nuclear Security Administration(grant numbers:DE-NA0003525); Korea Aerospace University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511786","Spatial accelerator;DNN accelerator;dataflow;GEMM mapping","Kernel;Analytical models;Runtime;Hardware;Shape;Parallel processing;Sparse matrices","data flow computing;learning (artificial intelligence);matrix algebra;matrix multiplication;multiprocessing systems;optimisation;parallel architectures;pipeline processing;power aware computing;reconfigurable architectures;scheduling;storage management","spatial accelerator architectures;tiled matrix-matrix multiplication;custom spatial accelerators;machine learning applications;spatial array;processing elements interacting;PEs;custom buffer hierarchies;networks-on-chip;optimized dataflow;fine-grained scheduling;data reuse;tiled general matrix-matrix multiplication kernel;optimized mappings;tile sizes;given spatial accelerator;workload combination;tiled GEMM mappings","",2.0,"",41.0,"IEEE","11 Aug 2021","","","IEEE","IEEE Journals"
"gSoFa: Scalable Sparse Symbolic LU Factorization on GPUs","A. Gaihre; X. S. Li; H. Liu","Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA","IEEE Transactions on Parallel and Distributed Systems","18 Oct 2021",2022,33.0,4.0,1015,1026,"Decomposing a matrix $\mathbf {A}$A into a lower matrix $\mathbf {L}$L and an upper matrix $\mathbf {U}$U, which is also known as LU decomposition, is an essential operation in numerical linear algebra. For a sparse matrix, LU decomposition often introduces more nonzero entries in the $\mathbf {L}$L and $\mathbf {U}$U factors than in the original matrix. A symbolic factorization step is needed to identify the nonzero structures of $\mathbf {L}$L and $\mathbf {U}$U matrices. Attracted by the enormous potentials of the Graphics Processing Units (GPUs), an array of efforts have surged to deploy various LU factorization steps except for the symbolic factorization, to the best of our knowledge, on GPUs. This article introduces gSoFa, the first GPU-based symbolic factorization design with the following three optimizations to enable scalable LU symbolic factorization for nonsymmetric pattern sparse matrices on GPUs. First, we introduce a novel fine-grained parallel symbolic factorization algorithm that is well suited for the Single Instruction Multiple Thread (SIMT) architecture of GPUs. Second, we tailor supernode detection into a SIMT friendly process and strive to balance the workload, minimize the communication and saturate the GPU computing resources during supernode detection. Third, we introduce a three-pronged optimization to reduce the excessive space consumption problem faced by multi-source concurrent symbolic factorization. Taken together, gSoFa achieves up to 31× speedup from 1 to 44 Summit nodes (6 to 264 GPUs) and outperforms the state-of-the-art CPU project, on average, by 5×. Notably, gSoFa also achieves up to 47 percent of the peak memory throughput of a V100 GPU in the Summit Supercomputer.","1558-2183","","10.1109/TPDS.2021.3090316","National Science Foundation(grant numbers:2000722); CAREER(grant numbers:2046102); Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459541","Sparse linear algebra;sparse linear solvers;LU decomposition;static symbolic factorization on GPU","Sparse matrices;Matrix decomposition;Graphics processing units;Memory management;Data structures;Parallel processing;Image edge detection","linear algebra;mathematics computing;matrix algebra;matrix decomposition;multi-threading;parallel algorithms;sparse matrices","numerical linear algebra;SIMT architecture;single instruction multiple thread;fine-grained parallel symbolic factorization;LU decomposition;matrix decomposition;GPU;scalable sparse symbolic LU factorization;gSoFa","",1.0,"",35.0,"IEEE","17 Jun 2021","","","IEEE","IEEE Journals"
"Accelerating Restarted GMRES With Mixed Precision Arithmetic","N. Lindquist; P. Luszczek; J. Dongarra","Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; University of Manchester, Manchester, U.K.","IEEE Transactions on Parallel and Distributed Systems","15 Oct 2021",2022,33.0,4.0,1027,1037,"The generalized minimum residual method (GMRES) is a commonly used iterative Krylov solver for sparse, non-symmetric systems of linear equations. Like other iterative solvers, data movement dominates its run time. To improve this performance, we propose running GMRES in reduced precision with key operations remaining in full precision. Additionally, we provide theoretical results linking the convergence of finite precision GMRES with classical Gram-Schmidt with reorthogonalization (CGSR) and its infinite precision counterpart which helps justify the convergence of this method to double-precision accuracy. We tested the mixed-precision approach with a variety of matrices and preconditioners on a GPU-accelerated node. Excluding the incomplete LU factorization without fill in (ILU(0)) preconditioner, we achieved average speedups ranging from 8 to 61 percent relative to comparable double-precision implementations, with the simpler preconditioners achieving the higher speedups.","1558-2183","","10.1109/TPDS.2021.3090757","University of Tennessee(grant numbers:MSE E01-1315-038); UT-Battelle(grant numbers:4000123266); National Science Foundation(grant numbers:2004541); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462418","Linear systems;multiple precision arithmetic","Iterative methods;Lifting equipment;Convergence;Stability analysis;Linear systems;Kernel;Error correction","graphics processing units;iterative methods;sparse matrices","classical Gram-Schmidt with reorthogonalization;infinite precision counterpart;double-precision accuracy;matrices;preconditioners;GPU-accelerated node;double-precision implementations;restarted GMRES;mixed precision arithmetic;generalized minimum residual method;nonsymmetric systems;linear equations;data movement;finite precision GMRES;iterative Krylov solver;sparse system","",5.0,"",41.0,"IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"EiC Editorial – Advancing Reproducibility in Parallel and Distributed Systems Research","M. Parashar","","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2010,2010,", the TPDS Reproducibility Initiative [2][3] has being exploring postpublication peer review of code associated with published articles for a few years. Authors who have published in TPDS can make their published article more reproducible and earn a reproducibility badge by submitting their associated code for post-publication peer review. To date, this pilot has largely focused on two badges: 1. Code Available: The code, including any associated data and documentation, provided by the authors is reasonable and complete and can potentially be used to support reproducibility of the published results. 2. Code Reviewed: The code, including any associated data and documentation, provided by the authors is reasonable and complete, runs to produce the outputs described, and can support reproducibility of the published results. While TPDS’ goal has always been to include badges for reproducing research results using the code and/or data provided, the nature of research in parallel and distributed systems covered by TPDS makes it challenging to evaluate code and data challenging for reproducibility. This is because such an evaluation may require access to specific hardware, system architectures and scales, OS configurations, and so on, which may not be feasible or practical. vel system/middleware services, is typically infeasible. Consequently, TPDS has piloted an alternate approach where members of the community can submit short, supplemental ‘critique’ papers that present their experiences in reproducing published results using the artifacts, and/or evaluations or experiences with published artifacts. These supplemental paper submissions are reviewed and, if accepted, are linked to the original publication and are citable, serving to help validate the reproducibility of the original publication. This approach was first implemented in a special section guest edited by Stephen Lien Harrell and Beth Plale and consisting of a primary paper and 6 critique papers that reproduce the results of the primary paper. This special section continues this effort, building on the SC20 Student Cluster Competition, which was part of the SC20 conference. It consists of 9 critique papers that reproduce the results of the primary paper.","1558-2183","","10.1109/TPDS.2021.3137871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696236","","","","","",2.0,"",4.0,"IEEE","28 Jan 2022","","","IEEE","IEEE Journals"
"Advancing Adoption of Reproducibility in HPC: A Preface to the Special Section","S. L. Harrell; S. Michael; C. Maltzahn","Texas Advanced Computing Center, University of Texas at Austin, Austin, TX, USA; Research Software, Indiana University, Bloomington, IN, USA; Center for Research in Open Source Software, University of California Santa Cruz, Santa Cruz, CA, USA","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2011,2013,"In this special section we bring you a practice and experience effort in reproducibility for large-scale computational science at SC20. This section includes nine critiques, each by a student team that reproduced results from a paper published at SC19, during the following year’s Student Cluster Competition. The paper is also included in this section and has been expanded upon, now including an analysis of the outcomes of the students’ reproducibility experiments. Lastly, this special section encapsulates a variety of advances in reproducibility in the SC conference series technical program.","1558-2183","","10.1109/TPDS.2021.3128796","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622175","Open science;computational science;reproducibility;practice and experience","Reproducibility of results;Standards;Scientific computing;Special issues and sections;High performance computing;Hardware;Graphics processing units","computer aided instruction;parallel processing;pattern clustering","student team;large-scale computational science;advancing reproducibility adoption;student cluster competition;students reproducibility experiments;SC conference series technical program;high performance computing;HPC","","","",1.0,"IEEE","19 Nov 2021","","","IEEE","IEEE Journals"
"MemXCT: Design, Optimization, Scaling, and Reproducibility of X-Ray Tomography Imaging","M. Hidayeto&#x011F;lu; T. Bi&#x00E7;er; S. G. de Gonzalo; B. Ren; D. G&#x00FC;rsoy; R. Kettimuthu; I. T. Foster; W. -M. W. Hwu","University of Illinois at Urbana-Champaign, Champaign, IL, USA; Argonne National Laboratory, Lemont, IL, USA; University of Illinois at Urbana-Champaign, Champaign, IL, USA; College of Willam &amp; Mary, Williamsburg, VA, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; University of Illinois at Urbana-Champaign, Champaign, IL, USA","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2014,2031,"This work extends our previous research entitled “MemXCT: Memory-centric X-ray CT Reconstruction with Massive Parallelization” that was originally published at SC19 conference (Hidayetoğlu <italic>et al.</italic>, 2019) with reproducibility of the computational imaging performance. X-ray computed tomography (XCT) is regularly used at synchrotron light sources to study the internal morphology of materials at high resolution. However, experimental constraints, such as radiation sensitivity, can result in noisy or undersampled measurements. Further, depending on the resolution, sample size and data acquisition rates, the resulting noisy dataset can be in the order of terabytes. Advanced iterative reconstruction techniques can produce high-quality images from noisy measurements, but their computational requirements have made their use an exception rather than the rule. We propose a novel memory-centric approach that avoids redundant computations at the expense of additional memory complexity. We develop a memory-centric iterative reconstruction system, MemXCT, that uses an optimized SpMV implementation with two-level pseudo-Hilbert ordering and multi-stage input buffering. We evaluate MemXCT on various supercomputer architectures involving KNL and GPU. MemXCT can reconstruct a large (11K×11K) mouse brain tomogram in 10 seconds using 4096 KNL nodes (256K cores). The results presented in our original article at the SC19 were based on large-scale supercomputing resources. The MemXCT application was selected for the Student Cluster Competition (SCC) Reproducibility Challenge and evaluated on a variety of cloud computing resources by universities around the world in the SC20 conference. We summarize the results of the top-ranked SCC Reproducibility Challenge teams and identify the most pertinent measures for ensuring the reproducibility of our experiments in this article.","1558-2183","","10.1109/TPDS.2021.3128032","U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:OCI-0725070,ACI-1238993); XPACC Center for Exascale Simulation of Plasma-Coupled Combustion and Department of Energy(grant numbers:DE-NA0002374); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9625857","X-ray tomography;space-filling curves;cache utilization;knights landing;GPU computing;SpMV;reproducibility","Image reconstruction;X-ray imaging;Reproducibility of results;Noise measurement;Optimization;Computed tomography;Mice","brain;computerised tomography;data acquisition;image reconstruction;iterative methods;medical image processing","optimized SpMV implementation;large-scale supercomputing resources;MemXCT application;memory-centric X-ray CT reconstruction;X-ray computed tomography;synchrotron light sources;noisy measurements;data acquisition rates;two-level pseudoHilbert ordering;multistage input buffering;supercomputer architectures;mouse brain tomogram;time 10.0 s","",2.0,"",68.0,"IEEE","23 Nov 2021","","","IEEE","IEEE Journals"
"Critique of “MemXCT: Memory-Centric X-Ray CT Reconstruction With Massive Parallelization” by SCC Team From Peking University","Z. Fan; Y. Gu; Z. Hao; Y. Pan; P. Xu; Y. Yan; F. Yang; Z. Fu; Y. Liang","School of Electronics and Computer Science (EECS), Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China; School of Physics, Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China; Computer Center, Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2032,2034,"Hidayetolu et al. (2019) proposed a novel memory-centric computation system, MemXCT. As a challenge at SC20, we reproduce the computational efficiency of MemXCT on our Azure cloud cluster. Our experiments evaluate the overall performance and the strong scalability with real datasets and verify part of the conclusions in the original article.","1558-2183","","10.1109/TPDS.2021.3092273","Omnisky; Computer Center of Peking University; Peking University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9464698","Reproducible computation;student cluster competition","Graphics processing units;Hardware;Scalability;Bandwidth;Fans;Computer science;Libraries","cloud computing;computerised tomography;image reconstruction;medical image processing","MemXCT;Azure cloud cluster;memory-centric X-ray CT reconstruction;massive parallelization;SCC team;peking university;computational efficiency;memory-centric computation system;SC20","","","",1.0,"IEEE","24 Jun 2021","","","IEEE","IEEE Journals"
"Critique of “MemXCT: Memory-Centric X-Ray CT Reconstruction With Massive Parallelization” by SCC Team From Georgia Tech","N. Prindle; A. Kazmi; A. Jain; A. Chen; M. Sorkin; S. Agarwal; R. Vuduc; V. Thakkar","College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA; Computational Science and Engineering Division, Georgia Institute of Technology, Atlanta, GA, USA; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2035,2038,"For the 2020 Student Cluster Competition, we reproduced results from “MemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization” (Hidayetoğlu et al.). Reproducibility is of critical importance to the scientific community, not just to verify correctness of results but also to see how easily others can understand and work with the given methods. MemXCT is an approach for image reconstruction in X-ray ptychography, which has a broad range of applications in materials science. MemXCT is not the only X-ray tomography algorithm, though; as opposed to compute-centric algorithms, it is designed to scale better by optimizing for memory bandwidth and memory latency. MemXCT also applies several key optimizations in order to ease memory pressure. In this article, we test the performance and strong scaling of MemXCT on 1 to 256 AMD CPU cores (1-4 nodes) and 1-16 Nvidia V100 GPUs (1-4 nodes). We confirm the impact of MemXCT’s optimizations. Still, we find that the performance of some important loops in the MemXCT kernel is much lower on the AMD processors (with AVX2) of our CPU nodes compared to the Intel CPUs (with AVX-512) used in the original article. We also confirm MemXCT performance on Tesla V100 GPUs, as reported in the article.","1558-2183","","10.1109/TPDS.2021.3108956","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696254","Reproducible computation;parallel computing;student cluster competition;MemXCT","Materials science and technology;Computed tomography;X-ray tomography;Performance gain;Hardware;Reproducibility of results;Kernel","computerised tomography;graphics processing units;image reconstruction;multiprocessing systems;X-ray microscopy","Memory-Centric X-ray CT Reconstruction;massive parallelization;2020 Student Cluster Competition;image reconstruction;X-ray ptychography;X-ray tomography algorithm;compute-centric algorithms;memory bandwidth;memory pressure;MemXCT kernel;MemXCT optimizations;AVX-512;reproducibility;memory latency;AMD processors;Tesla V100 GPU","","","",7.0,"IEEE","28 Jan 2022","","","IEEE","IEEE Journals"
"Critique of “MemXCT: Memory-Centric X-Ray CT Reconstruction With Massive Parallelization” by SCC Team From ETH Zürich","J. Kleine; R. Steiger; S. Wachter; E. İşman; S. Jacob; D. Romaniello","Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2039,2042,"This report analyzes the reproducibility of the paper “MemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization” by Hidayetoğlu et al. in the cloud as part of the SC20 Virtual Student Cluster Competition (VSCC). To reproduce the results from the original work, the ETH Zürich SC20 VSCC team performed a series of CT reconstructions and performed a scaling study using three provided sinograms. All experimental runs were performed during the SC20 VSCC on an HPC cluster hosted in the Microsoft Azure CycleCloud. In this paper, we describe discrepancies in results as a factor of the differences in experiment environments and insufficient parameter tuning. We successfully reproduce the single device performance and partially reproduce the strong scaling behavior. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108.","1558-2183","","10.1109/TPDS.2021.3127711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613757","Reproducible computation;student cluster competition;MemXCT","Graphics processing units;Performance evaluation;Bandwidth;Reproducibility of results;Image reconstruction;Codes;Computed tomography","cloud computing;computerised tomography;image reconstruction;parallel processing;X-ray imaging","MemXCT;parallelization;SCC team;ETH Zürich;memory-centric X-ray CT reconstruction;SC20 Virtual Student Cluster Competition;SC20 VSCC team;CT reconstructions;device performance;sinograms;HPC cluster;Microsoft Azure CycleCloud;parameter tuning;scaling behavior","","","",5.0,"IEEE","12 Nov 2021","","","IEEE","IEEE Journals"
"Critique of “MemXCT: Memory-Centric X-Ray CT Reconstruction With Massive Parallelization” by SCC Team From University of California San Diego","X. Li; M. Apodaca; A. Gupta; Z. Kong; H. Pan; H. Zhou; M. Thomas; M. Kandes; Z. Li; M. Tatineni; L. Carroll","University of California, San Diego, La Jolla, CA, USA; University of California, San Diego, La Jolla, CA, USA; University of California, San Diego, La Jolla, CA, USA; University of California, San Diego, La Jolla, CA, USA; University of California, San Diego, La Jolla, CA, USA; University of California, San Diego, La Jolla, CA, USA; San Diego Supercomputer Center, University of California San Diego, La Jolla, CA, USA; San Diego Supercomputer Center, University of California San Diego, La Jolla, CA, USA; San Diego Supercomputer Center, University of California San Diego, La Jolla, CA, USA; San Diego Supercomputer Center, University of California San Diego, La Jolla, CA, USA; Advancd Micro Devices, Santa Clara, CA, USA","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2043,2046,"In this article, we describe our efforts to reproduce results reported in the SC19 article by Hidayetoğlu et al., titled “MemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization”. MemXCT's single-device performance, parallelized via OpenMP and MPI, was characterized using AMD Zen2 CPU cores and NVIDIA V100 GPU devices running on the Microsoft Azure cloud. We were able to reproduce most of the results, and exceed the performance of larger inputs, on an AMD EPYC HBv2 cluster. We were also able to reproduce the strong scaling trends for optimized CPU and GPU versions. Slight variations in performance of the CPU version were observed due to differences in the underlying hardware, input size, and number of available nodes. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108","1558-2183","","10.1109/TPDS.2021.3128840","University of California; San Diego Supercomputer Center; National Science Foundation(grant numbers:#1928224); Extreme Science and Engineering Discovery Environment(grant numbers:#ACI-1548562); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9618831","Performance evaluation;random access memory;hardware;codes;graphics processing units;bandwidth;optimization","Graphics processing units;Bandwidth;Optimization;Performance evaluation;Random access memory;Hardware;Codes","application program interfaces;cloud computing;computerised tomography;graphics processing units;image reconstruction;message passing;parallel processing;storage management;X-ray imaging","memory-centric X-ray CT reconstruction;massive parallelization;SCC team;University of California San Diego;AMD Zen2 CPU cores;NVIDIA V100 GPU devices;AMD EPYC HBv2 cluster;MemXCT single-device performance;OpenMP;MPI;Microsoft Azure cloud","","","",5.0,"IEEE","17 Nov 2021","","","IEEE","IEEE Journals"
"Reproducibility: Performance Evaluation of MemXCT on Azure CycleCloud Platform","Y. Liu; Y. Meng; K. Xu; Z. Xu; T. Wu; Y. Yang; S. Yin","School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2047,2049,"Memory-Centric X-ray Computational Tomography(CT) is an iterative reconstruction technique that trades compute simplifications with higher memory accesses. MemXCT implements a sparse matrix-vector multiplication(SpMV) with multi-stage buffering and two-level pseudo-Hilbert ordering for optimization. Motivated by the need to validate conclusions from previous work, we reproduce the numerical results, the algorithm’s performance, and the scaling behavior of the algorithms as the number of MPI processes increases on Azure. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108","1558-2183","","10.1109/TPDS.2021.3127450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612082","SpStudent cluster challenge;reproducibility;scalability;MemXCT","Graphics processing units;Scalability;Bandwidth;Performance evaluation;Computer architecture;Sparse matrices;Reproducibility of results","computerised tomography;image reconstruction;iterative methods;matrix multiplication;medical image processing;message passing;parallel architectures;sparse matrices;vectors","performance evaluation;MemXCT;Azure CycleCloud platform;iterative reconstruction technique;scaling behavior;reproducibility;memory-centric X-ray computational tomography;sparse matrix-vector multiplication;sparse matrix-vector multiplication;SpMV;multistage buffering;two-level pseudoHilbert ordering;MPI process","","","",6.0,"IEEE","11 Nov 2021","","","IEEE","IEEE Journals"
"Critique of “MemXCT: Memory-Centric X-Ray CT Reconstruction With Massive Parallelization” by SCC Team From Tsinghua University","R. Zhong; J. Chen; C. Zhang; M. Zhai; Z. Song; Y. Wang; W. Han; L. Gan; J. Zhai","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2050,2053,"Hidayetoğlu et al. propose a novel memory-centric algorithm to reconstruct X-ray CT images in the SC19 article entitled “MemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization”. They formulate the reconstruction with several SpMVs, and propose two memory-centric optimizations to improve cache locality for better memory bandwidth utilization, i.e., a two-level pseudo-Hilbert ordering and a multi-stage input buffering. In this article, we present our results on reproducing that article to show its effectiveness and generality, as part of the SC20 Student Cluster Competition Reproducibility Challenge. We reproduce the execution time and memory bandwidth tests in that article on various architectures, including Intel CPUs, AMD CPUs, and NVIDIA GPUs. We further analyze the bottleneck on different architectures by comparing the achieved memory bandwidth with the peak bandwidth on those architectures. We then reproduce the strong scaling test on CPU and GPU clusters with different scales, and use the proposed algorithm to reconstruct three new X-ray computed tomograms.","1558-2183","","10.1109/TPDS.2021.3108964","National Key Research and Development Program of China(grant numbers:2016YFB0200100); National Natural Science Foundation of China(grant numbers:U20A20226); Tsinghua University Initiative Scientific Research Program(grant numbers:20191080594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9600647","Reproducible computation;scalability;cloud;student cluster competition","Bandwidth;Graphics processing units;Sockets;Image reconstruction;Computed tomography;Benchmark testing;Micromechanical devices","cache storage;computerised tomography;graphics processing units;image reconstruction;medical image processing;multiprocessing systems;optimisation;parallel architectures","memory-centric X-ray CT reconstruction;massive parallelization;memory-centric optimizations;memory bandwidth utilization;SC20 Student Cluster Competition Reproducibility Challenge;memory bandwidth tests;MemXCT;memory-centric algorithm;X-ray CT images;CPU;GPU clusters;X-ray computed tomograms","","","",9.0,"IEEE","3 Nov 2021","","","IEEE","IEEE Journals"
"Critique of “MemXCT: Memory-Centric X-Ray CT Reconstruction With Massive Parallelization” by SCC Team From Clemson University","G. Dube; C. Holt; J. Hollowell; S. Placke; S. Ranjan; N. Heitzig; J. Calhoun","Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2054,2057,"This article reproduces the results of the Supercomputing 2019 article, MemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization by Hidayetoğlu et al. as part of the Supercomputing 2020 Student Cluster Competition Reproducibility Challenge. We reproduce the single CPU-GPU performance experiments and the strong scaling experiments and compare the results to the original article. Although we are not able to use the exact HPC system from the original article, we use similar hardware and configurations to recreate the original environment in Microsoft Azure Cloud services. We were not able to demonstrate exact performance characteristics from the original article due to hardware limitations in our environment, but we are able to reproduce similar performance trends for both single-node and scaling experiments.","1558-2183","","10.1109/TPDS.2021.3108961","Clemson Computing & Information Technology; Dell Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9600648","Reproducible computation;student cluster competition;replication;high-performance computing;X-ray CT;Microsoft Azure","Graphics processing units;Hardware;Bandwidth;Market research;Computed tomography;Timing;Software","cloud computing;computerised tomography;graphics processing units;image reconstruction;medical image processing;parallel processing","original article;original environment;MemXCT;memory-centric X-ray CT reconstruction;massive parallelization;SCC team;Clemson University;Supercomputing 2020 Student Cluster Competition Reproducibility Challenge;CPU-GPU performance experiments;HPC system","","","",8.0,"IEEE","3 Nov 2021","","","IEEE","IEEE Journals"
"Critique of ”MemXCT: Memory-Centric X-Ray CT Reconstruction With Massive Parallelization” by SCC Team From Nanyang Technological University","S. Li; B. -S. Lee","Nanyang Technological University, Singapore, Singapore; Nanyang Technological University, Singapore, Singapore","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2058,2061,"In this technical report, we focus on reproducing the results reported in the paper “MemXCT: Memory-Centric X-ray CT Reconstruction with Massive Parallelization” [1]. MemXCT is a scalable approach to X-ray Computed Tomography reconstruction which removes redundant computation. We reproduced the single CPU/GPU performance as well as strong scaling experiments. We set up our configurations on Microsoft Azure CycleCloud and have two clusters. One cluster has 4 nodes with 60 CPUs on each node and the other cluster has 4 nodes with 4 NVIDIA V100 GPUs on each node. Both clusters come with InfiniBand. The original author conducted his experiments on Theta and Blue Waters supercomputers. We were able to reproduce part of the results in the original paper, however, failed to produce similar performance on other experiments. This report was submitted as part of the reproducibility challenge in SC20 Student Cluster Competition. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108.","1558-2183","","10.1109/TPDS.2021.3128040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9615005","X-ray CT;memory;reproducible computation;SC20","Graphics processing units;Bandwidth;Sockets;Image reconstruction;Computed tomography;Scalability;Performance evaluation","computerised tomography;diagnostic radiography;graphics processing units;image reconstruction;medical image processing;parallel machines","MemXCT;memory-centric X-ray CT reconstruction;Nanyang Technological University;X-ray computed tomography reconstruction;NVIDIA V100 GPU;CPU-GPU performance;Microsoft Azure CycleCloud;CPU;InfiniBand;Blue Waters supercomputers;Theta supercomputers","","","",1.0,"IEEE","15 Nov 2021","","","IEEE","IEEE Journals"
"Critique of “MemXCT: Memory-Centric X-Ray CT Reconstruction With Massive Parallelization” by SCC Team From the University of Texas at Austin","B. Davis; J. Paez; J. Gaither; J. A. Garcia","University of Texas at Austin, Austin, TX, USA; University of Texas at Austin, Austin, TX, USA; University of Arkansas, Fayetteville, AR, USA; Texas Advanced Computing Center, Austin, TX, USA","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2022",2022,33.0,9.0,2062,2065,"This report describes The University of Texas Student Cluster Competition team’s effort to reproduce the results of “MemXCT: memory-centric X-ray CT reconstruction with massive parallelization” (Hidayetoğlu et al., 2019). The article details a new memory-centric approach that reconstructs X-ray computed tomography (XCT) from noisy raw data. In our reproduction experiments, we utilized Microsoft Azure’s CycleCloud tool to provision, orchestrate, and manage our computing cluster in the cloud. In particular, we scheduled and benchmarked reconstruction workloads using Azure’s CPU-based HC44rs and GPU-based NC12s v2 virtual machine (VM) types to evaluate the scalability properties of the reconstruction approach and the performance differences between architectures. The HC44rs VMs contained 44 Intel Xeon Platinum cores, while the NC12s v2 VM was equipped with two NVIDIA P100 GPUs. We used a recent version of Intel’s compiler stack with the MKL library for our CPU code along with CUDA 11.1 on GPUs. Overall, our results confirm the findings of the original article, demonstrating similar acceleration on GPUs and scalability properties on CPUs. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108","1558-2183","","10.1109/TPDS.2021.3127826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613798","Case studies in scientific applications;usability testing","Graphics processing units;Computer architecture;Codes;Image reconstruction;Bandwidth;Throughput;Sockets","cloud computing;computerised tomography;data handling;graphics processing units;image reconstruction;parallel architectures;program compilers;virtual machines","GPU-based NC12 v2 virtual machine types;NVIDIA P100 GPU;CUDA;Intel Xeon Platinum cores;HC44rs;reconstruction approach;reconstruction workloads;computing cluster;Microsoft Azure CycleCloud tool;X-ray computed tomography;memory-centric approach;Texas student cluster competition team;SCC team;massive parallelization;memory-centric X-ray CT reconstruction;MemXCT","","","",6.0,"IEEE","12 Nov 2021","","","IEEE","IEEE Journals"
"PPOAccel: A High-Throughput Acceleration Framework for Proximal Policy Optimization","Y. Meng; S. Kuppannagari; R. Kannan; V. Prasanna","Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA; U.S. Army Research Lab, Los Angeles, CA, USA; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA","IEEE Transactions on Parallel and Distributed Systems","7 Feb 2022",2022,33.0,9.0,2066,2078,"Reinforcement Learning (RL) is a major branch of AI that enables agents to learn optimal decision making via interaction with the environment. Proximal Policy Optimization (PPO) is the state-of-the-art policy optimization based RL algorithm which achieves superior overall performance on various benchmarks. A PPO agent iteratively optimizes its policy - a function which chooses optimal actions approximated by a DNN, with each iteration consisting of two computationally intensive phases: Sample Generation - where agents inference on its policy and interact with the environment to collect data, and Model Update - where the policy is trained using the collected data. In this paper, we develop the first high-throughput PPO accelerator on CPU-FPGA heterogeneous platform. Our unified systolic-array based design accelerates both the inference and the training of the deep neural network used in a RL algorithm, and is generalizable to various MLP and CNN models across a wide range of RL applications. We develop novel optimizations to simultaneously reduce data access and computation latencies, specifically: (a) optimal data flow mapping to systolic array, (b) novel memory-blocked data layout to enable streaming stall-free data access in both forward and backward propagations, and, (c) a systolic array compute sharing technique to mitigate load imbalance in the training of two networks. We evaluate our design on widely used robotics and gaming benchmarks, achieving 1.4×–26× and 1.3×–2.7× improvements in throughput, respectively, when compared with state-of-the-art CPU/CPU-GPU implementations.","1558-2183","","10.1109/TPDS.2021.3134709","National Science Foundation(grant numbers:2009057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647962","Reinforcement learning;hardware accelerators;FPGA","Conferences;Portable document format;Indexes;Typesetting;Loading;Web sites;Warranties","AI chips;convolutional neural nets;data acquisition;decision making;deep learning (artificial intelligence);electronic engineering computing;field programmable gate arrays;generalisation (artificial intelligence);logic design;multi-agent systems;multilayer perceptrons;optimisation;reinforcement learning;systolic arrays","RL algorithm;PPO agent;optimal actions;high-throughput PPO accelerator;unified systolic-array based design;computation latencies;optimal data flow mapping;memory-blocked data layout;proximal policy optimization;high-throughput acceleration;PPOAccel;reinforcement learning;optimal decision making;DNN;sample generation;model update;data collection;CPU-FPGA heterogeneous platform;deep neural network;MLP;CNN;data access;stall-free data access streaming;backward propagations;forward propagations;systolic array compute sharing technique;load imbalance;network training","",1.0,"",49.0,"IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"Cost-Efficient Workflow Scheduling Algorithm for Applications With Deadline Constraint on Heterogeneous Clouds","X. Tang; W. Cao; H. Tang; T. Deng; J. Mei; Y. Liu; C. Shi; M. Xia; Z. Zeng","School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; Applied Economics, Beijing Normal University-Hong Kong Baptist University United International College (UIC), Xiangzhou, Zhuhai, China; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; College of Information Science and Engineering, Hunan Normal University, Changsha, Hunan, China; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; I2R, A*Star, Singapore, Singapore","IEEE Transactions on Parallel and Distributed Systems","3 Feb 2022",2022,33.0,9.0,2079,2092,"In recent years, more and more large-scale data processing and computing workflow applications run on heterogeneous clouds. Such cloud applications with precedence-constrained tasks are usually deadline-constrained and their scheduling is an essential problem faced by cloud providers. Moreover, minimizing the workflow execution cost based on cloud billing periods is also a complex and challenging problem for clouds. In realizing this, we first model the workflow applications as I/O Data-aware Directed Acyclic Graph (DDAG), according to clouds with global storage systems. Then, we mathematically state this deadline-constrained workflow scheduling problem with the goal of minimum execution financial cost. We also prove that the time complexity of this problem is NP-hard by deducing from a multidimensional multiple-choice knapsack problem. Third, we propose a heuristic cost-efficient task scheduling strategy called CETSS, which includes workflow DDAG model building, task subdeadline initialization, greedy workflow scheduling algorithm, and task adjusting method. The greedy workflow scheduling algorithm mainly consists of dynamical task renting billing period sharing method and unscheduled task subdeadline relax technique. We perform rigorous simulations on some synthetic randomly generated applications and real-world applications, such as Epigenomics, CyberShake, and LIGO. The experimental results clearly demonstrate that our proposed heuristic CETSS outperforms the existing algorithms and can effective save the total workflow execution cost. In particular, CETSS is very suitable for large workflow applications.","1558-2183","","10.1109/TPDS.2021.3134247","National Natural Science Foundation of China(grant numbers:61972146); Natural Science Foundation of Hunan Province(grant numbers:2020JJ4376); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647942","Workflow application;cost;heterogeneous clouds;schedule length;task scheduling","Cloud computing;Task analysis;Costs;Computational modeling;Scheduling;Job shop scheduling;Heuristic algorithms","cloud computing;computational complexity;directed graphs;grid computing;knapsack problems;scheduling;workflow management software","cost-efficient workflow scheduling algorithm;deadline constraint;heterogeneous clouds;large-scale data processing;computing workflow applications;cloud applications;precedence-constrained tasks;cloud providers;cloud billing periods;complex problem;Data-aware Directed Acyclic Graph;deadline-constrained workflow scheduling problem;minimum execution financial cost;multiple-choice knapsack problem;heuristic cost-efficient task scheduling strategy;workflow DDAG model building;task subdeadline initialization;greedy workflow scheduling algorithm;task adjusting method;dynamical task renting billing period;unscheduled task subdeadline;real-world applications;total workflow execution cost","",11.0,"",48.0,"IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"Cooperative Edge Caching Based on Temporal Convolutional Networks","X. Zhang; Z. Qi; G. Min; W. Miao; Q. Fan; Z. Ma","Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; School of Electronic Science and Engineering, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; Chongqing Key Laboratory of Digital Cinema Art Theory and Technology, Chongqing University, Chongqing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, Jiangsu, China","IEEE Transactions on Parallel and Distributed Systems","3 Feb 2022",2022,33.0,9.0,2093,2105,"With the rapid growth of networked multimedia services in the Internet, wireless network traffic has increased dramatically. However, the current mainstream content caching schemes do not take into account the cooperation of different edge servers, resulting in deteriorated system performance. In this paper, we propose a learning-based edge caching scheme to enable mutual cooperation among different edge servers with limited caching resources, thus effectively reducing the content delivery latency. Specifically, we formulate the cooperative content caching problem as an optimization problem, which is proven to be NP-hard. To solve this problem, we design a new learning-based cooperative caching strategy (LECS) that encompasses three key components. Firstly, a temporal convolutional network driven content popularity prediction model is developed to estimate the content popularity with high accuracy. Secondly, with the predicted content popularity, the concept of content caching value (CCV) is introduced to weigh the value of a content cached on a given edge server. Thirdly, an novel dynamic programming algorithm is developed to maximize the overall CCV. Extensive simulation results have demonstrated the superiority of our approach. Compared with the state-of-the-art caching schemes, LECS can improve the cache hit rate by 8.3%-10.1%, and reduce the average content delivery delay by 9.1%-15.1%.","1558-2183","","10.1109/TPDS.2021.3135257","National Key Research and Development Program of China(grant numbers:2018YFB2100804); EU Horizon 2020 Research and Innovation Programme; Marie Sklodowska-Curie Actions(grant numbers:898588); EU Horizon 2020 INITIATE(grant numbers:101008297); National Natural Science Foundation of China(grant numbers:61902178,92067206,62102053,61972222); Natural Science Foundation of Jiangsu Province(grant numbers:BK20190295); Leading Technology of Jiangsu Basic Research Plan(grant numbers:BK20192003); Chongqing Key Laboratory of Digital Cinema Art Theory and Technology(grant numbers:2021KF01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650595","Cooperative edge caching;temporal convolutional networks;content caching value;content popularity","Conferences;Portable document format;Indexes;Typesetting;Printing;Loading;Web sites","cache storage;convolutional neural nets;cooperative communication;dynamic programming;Internet;multimedia communication","temporal convolutional network;networked multimedia services;wireless network traffic;learning-based edge caching scheme;mutual cooperation;caching resources;content caching problem;optimization problem;learning-based cooperative caching strategy;content popularity prediction model;predicted content popularity;content caching value;edge server;caching schemes;cache hit rate;average content delivery delay","",9.0,"",49.0,"IEEE","14 Dec 2021","","","IEEE","IEEE Journals"
"Addressing the Read-Performance Impact of Reconfigurations in Replicated Key-Value Stores","A. Papaioannou; K. Magoutis","Computer Science Department, University of Crete, Heraklion, Greece; Computer Science Department, University of Crete, Heraklion, Greece","IEEE Transactions on Parallel and Distributed Systems","3 Feb 2022",2022,33.0,9.0,2106,2119,"Raw data are often orders of magnitude larger than main memory for many applications. As the performance of storage devices is still significantly slower than main memory, systems still rely on memory caching to improve performance. Data replication schemes are prevalent in data stores for high availability and reliability. In such schemes, while data updates are propagated to all replicas (either synchronously or in the background), reads are usually served by only a subset of replica group members (e.g., as in primary-backup and quorum systems). As a result, non-serving replicas cannot keep their memory cache state updated; thus, during a reconfiguration or a fail-over action, the system suffers from a high read-performance impact for a significant amount of time due to cold-cache misses. In our study we observed up to 70% hit after a reconfiguration due to cold cache misses, taking almost 18 minutes in some cases to fully restore to the pre-reconfiguration level of performance. In this article we propose a mechanism to maintain up-to-date read caches across replicas by sending read hints to the non-serving replicas to keep their caches warm. Thus the system is able to seamlessly achieve the same performance level even in the face of a replica group reorganization. This is especially important under the read-intensive workloads that are common today. Our evaluation shows that our mechanism has significant benefits during reconfigurations, with low performance impact under periods of resource strain. Given its advisory nature, the maintenance of read hints can be reduced or held off if needed during such periods.","1558-2183","","10.1109/TPDS.2021.3135137","Hellenic Foundation for Research and Innovation(grant numbers:HFRI-FM17-1998); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650699","Data replication;caching;performance","Switches;Reliability;Task analysis;Production systems;Performance evaluation;Memory management;Market research","cache storage","data updates;replica group members;primary-backup;quorum systems;nonserving replicas;memory cache state;read-performance impact;cold-cache misses;pre-reconfiguration level;up-to-date read caches;replica group reorganization;read-intensive workloads;low performance impact;replicated key-value stores;raw data;storage devices;memory caching;data replication schemes;reliability","","","",42.0,"CCBY","14 Dec 2021","","","IEEE","IEEE Journals"
"DePo: Dynamically Offload Expensive Event Processing to the Edge of Cyber-Physical Systems","M. Ma; J. Zhang; P. Wang","Science and Technology on Communication Networks Laboratory, Shijiazhuang, Hebei, China; Autonomous Driving Laboratory, Tencent, Beijing, China; Key Laboratory of High Confidence Software Technologies, Peking University, Ministry of Education, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","3 Feb 2022",2022,33.0,9.0,2120,2132,"Event processing is one of the cornerstones to manage massive data streams in Cyber-Physical Systems (CPS). Due to CPS applications' increasing complexity, detecting highly complicated events (aka. “expensive” events) leads to significant performance degradation, particularly harmful to mission-critical systems. To tackle this challenge, we define a new task - dynamic event processing offloading to CPS-edges. This paper proves the problem NP-hard and proposes a solution - DePo. DePo splits the expensive events into sub-models and offloads them to CPS edges. We design a long and short-term event memory mechanism in DePo that enables the edges and server to process expensive events collaboratively within their capabilities. Besides, we propose a concept called Edge Utility to measure the optimality of offloading schemes. A heuristic algorithm is presented in this study to guide how to dispatch events to edges, thereby helping DePo generate a sub-optimal solution in polynomial computational complexity. Our extensive experiments show that the performance gap between DePo and the optimal benchmark is less than 5%. DePo effectively reduces more than 40% redundant states and provides over 100% higher throughput than state-of-the-art approaches. Experimental results verified the high stability and scalability of DePo, especially when dealing with a large number of expensive events.","1558-2183","","10.1109/TPDS.2021.3135441","National Key Research and Development Program of China(grant numbers:2020YFB1805400); National Natural Science Foundation of China(grant numbers:62072006,92167104); Science and Technology on Communication Networks Laboratory(grant numbers:6142104200103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650526","Complex event processing;edge computing;cyber-physical systems;task offloading","Task analysis;Image edge detection;Cyber-physical systems;Servers;Heuristic algorithms;Data models;Transforms","computational complexity;cyber-physical systems;distributed processing;optimisation","dynamically offload expensive event processing;cyber-physical systems;CPS applications;highly complicated events;mission-critical systems;dynamic event processing offloading;CPS edges;long term event memory mechanism;short-term event memory mechanism;edge utility;expensive events;DePo;offloading schemes;polynomial computational complexity","",1.0,"",42.0,"IEEE","14 Dec 2021","","","IEEE","IEEE Journals"
"Exploiting Concurrency in Sharded Parallel State Machine Replication","A. Burgos; E. Alchieri; F. Dotti; F. Pedone","Universidade de Brasília, Brasília, Brazil; Universidade de Brasília, Brasília, Brazil; PUCRS - Escola Politécnica, Porto Alegre, RS, Brazil; Università della Svitzzera italiana, Lugano, Switzerland","IEEE Transactions on Parallel and Distributed Systems","3 Feb 2022",2022,33.0,9.0,2133,2147,"State machine replication (SMR) is a well-known approach to implementing fault-tolerant services, providing high availability and strong consistency. In classic SMR, commands are executed sequentially, in the same order by all replicas. To improve performance, two classes of protocols have been proposed to parallelize the execution of commands. Early scheduling protocols reduce scheduling overhead but introduce costly synchronization of worker threads; late scheduling protocols, instead, reduce the cost of thread synchronization but suffer from scheduling overhead. Depending on the characteristics of the workload, one class can outperform the other. We introduce a hybrid scheduling technique that builds on the existing protocols. An experimental evaluation has revealed that the hybrid approach not only inherits the advantages of each technique but also scales better than either one of them, improving the system performance by up to  $3\times$3× in a workload with conflicting commands.","1558-2183","","10.1109/TPDS.2021.3135761","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9652105","Parallel state machine replication;scheduling;dependability","Scheduling;Message systems;Data structures;Servers;Protocols;Synchronization;Schedules","fault tolerance;finite state machines;protocols;telecommunication scheduling","concurrency;sharded parallel state machine replication;fault-tolerant services;classic SMR;scheduling overhead;worker threads;late scheduling protocols;thread synchronization;hybrid scheduling technique;system performance;conflicting commands","","","",46.0,"IEEE","15 Dec 2021","","","IEEE","IEEE Journals"
"Performant, Multi-Objective Scheduling of Highly Interleaved Task Graphs on Heterogeneous System on Chip Devices","J. Mack; S. E. Arda; U. Y. Ogras; A. Akoglu","Electrical and Computer Engineering Department, University of Arizona, Tucson, AZ, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Electrical and Computer Engineering Department, University of Wisconsin-Madison, Madison, WI, USA; Electrical and Computer Engineering Department, University of Arizona, Tucson, AZ, USA","IEEE Transactions on Parallel and Distributed Systems","3 Feb 2022",2022,33.0,9.0,2148,2162,"Performance-, power-, and energy-aware scheduling techniques play an essential role in optimally utilizing processing elements (PEs) of heterogeneous systems. List schedulers, a class of low-complexity static schedulers, have commonly been used in static execution scenarios. However, list schedulers are not suitable for runtime decision making, particularly when multiple concurrent applications are interleaved dynamically. For such cases, the static task execution times and expectation of idle PEs assumed by list schedulers lead to inefficient system utilization and poor performance. To address this problem, we present techniques for optimizing execution of list scheduling algorithms in dynamic runtime scenarios via a family of algorithms inspired by the well-known heterogeneous earliest finish time (HEFT) list scheduler. Through dynamically arriving, realistic workload scenarios that are simulated in an open-source discrete event heterogeneous SoC simulator, we exhaustively evaluate each of the proposed algorithms across two SoCs modeled after the Xilinx Zynq Ultrascale+ ZCU102 and O-Droid XU3 development boards. Altogether, depending on the chosen variant in this family of algorithms, we are able to achieve an up to 39% execution time improvement, up to 7.24x algorithmic speedup, or up to 30% energy consumption improvement compared to the baseline HEFT implementation.","1558-2183","","10.1109/TPDS.2021.3135876","Air Force Research Laboratory; Defense Advanced Research Projects Agency(grant numbers:FA8650-18-2-7860); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653796","Scheduling and task partitioning;heterogeneous (hybrid) systems;energy-aware systems;hardware simulation;HEFT","Task analysis;Heuristic algorithms;Runtime;Dynamic scheduling;Scheduling algorithms;Schedules;Optimal scheduling","decision making;power aware computing;processor scheduling;system-on-chip","multiobjective scheduling;highly interleaved task graphs;heterogeneous system;energy-aware scheduling techniques;list schedulers;low-complexity static schedulers;static execution scenarios;static task execution times;inefficient system utilization;list scheduling algorithms;dynamic runtime scenarios;heterogeneous earliest finish time list scheduler;open-source discrete event heterogeneous SoC simulator;execution time improvement","",3.0,"",49.0,"IEEE","16 Dec 2021","","","IEEE","IEEE Journals"
"UFC2: User-Friendly Collaborative Cloud","M. Zhao; Z. Li; W. Liu; J. Chen; X. Li","School of Software and BNRist, Tsinghua University, Beijing, China; School of Software and BNRist, Tsinghua University, Beijing, China; School of Software and BNRist, Tsinghua University, Beijing, China; School of Software and BNRist, Tsinghua University, Beijing, China; School of Software and BNRist, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2022",2022,33.0,9.0,2163,2182,"This article studies how today's cloud storage services support collaborative file editing. As a tradeoff for transparency and user-friendliness, they do not ask collaborators to use version control systems but instead implement their own heuristics for handling conflicts, which however often lead to unexpected and undesired experiences. With specialized measurements and reverse engineering, we unravel a number of their design and implementation issues as the root causes of poor experiences. Driven by the findings, we propose to reconsider the collaboration support of cloud storage services from a novel perspective of operations without using any locks. To enable this idea, we design intelligent and efficient approaches to the inference and transformation of users’ editing operations, as well as optimizations to the maintenance of files’ historical versions and the update of individual files. We build an open-source system UFC2 (User-Friendly Collaborative Cloud) to embody our design, which can avoid most (98%) conflicts with little (2%) overhead.","1558-2183","","10.1109/TPDS.2021.3132496","National Natural Science Foundation of China(grant numbers:61822205,61632020,61632013,61902211); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635692","Cloud storage;collaborative editing;conflict resolution;operation inference;operation transformation","Cloud computing;Collaboration;Matlab;Transforms;Synchronization;Servers;Codes","cloud computing;configuration management;groupware;reverse engineering;storage management","unexpected experiences;undesired experiences;implementation issues;collaboration support;users;files;open-source system UFC2;user-friendly collaborative cloud;cloud storage services support collaborative file editing;user-friendliness;version control systems","","","",82.0,"IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"Multi-Swarm Co-Evolution Based Hybrid Intelligent Optimization for Bi-Objective Multi-Workflow Scheduling in the Cloud","H. Li; D. Wang; M. Zhou; Y. Fan; Y. Xia","Key Laboratory of Complex System Intelligent Control and Decision, Beijing Institute of Technology, Beijing, China; Key Laboratory of Complex System Intelligent Control and Decision, Beijing Institute of Technology, Beijing, China; Department of Cyber-Physical Systems, St. Petersburg State Marine Technical University, Lotsmanskaya St., 3, St. Petersburg, Russia; Department of Automation, Tsinghua University, Beijing, China; Key Laboratory of Complex System Intelligent Control and Decision, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2022",2022,33.0,9.0,2183,2197,"Many scientific applications can be well modelled as large-scale workflows. Cloud computing has become a suitable platform for hosting and executing them. Workflow scheduling has gained much attention in recent years. However, since cloud service providers must offer services for multiple users with various QoS demands, scheduling multiple applications with different QoS requirements is highly challenging. This work proposes a Multi-swarm Co-evolution-based Hybrid Intelligent Optimization (MCHO) algorithm for multiple-workflow scheduling to minimize total makespan and cost while meeting the deadline constraint of each workflow. First, we design a multi-swarm co-evolutionary mechanism where three swarms are adopted to sufficiently search for various elite solutions. Second, to improve global search and convergence performance, we embed local and global guiding information into the updating process of a Particle Swarm Optimizer, and develop a swarm cooperation technique. Third, we propose a Genetic Algorithm-based elite enhancement strategy to exploit more non-dominated individuals, and apply the Metropolis Acceptance rule of Simulated Annealing to update the local guiding solution for each swarm so as to prevent it from being stuck into a local optimum at an early stage. Extensive experimental results demonstrate that MCHO outperforms the state-of-art scheduling algorithms with better distributed non-dominated solutions.","1558-2183","","10.1109/TPDS.2021.3122428","National Key Research and Development Program of China(grant numbers:2018YFB1003700); National Natural Science Foundation of China(grant numbers:61836001); Ministry of Science and Higher Education of the Russian Federation(grant numbers:075-15-2020-903); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585437","Cloud computing;intelligent optimization;multi-swarm;multiple workflows;Quality of Service (QoS);scheduling","Task analysis;Cloud computing;Costs;Optimization;Heuristic algorithms;Quality of service;Dynamic scheduling","cloud computing;genetic algorithms;Pareto optimisation;particle swarm optimisation;quality of service;scheduling;search problems;simulated annealing","scientific applications;large-scale workflows;cloud computing;cloud service providers;QoS demands;MCHO;multiple-workflow scheduling;total makespan;global search;convergence performance;local guiding information;global guiding information;particle swarm optimizer;swarm cooperation technique;local guiding solution;multiswarm co-evolution based hybrid intelligent optimization;biobjective multiworkflow scheduling;genetic algorithm-based elite enhancement strategy;metropolis acceptance rule;simulated annealing","",7.0,"",54.0,"IEEE","26 Oct 2021","","","IEEE","IEEE Journals"
"Cost-Efficient Server Configuration and Placement for Mobile Edge Computing","Z. He; K. Li; K. Li","National Supercomputing Center in Changsha, Changsha, Hunan, China; National Supercomputing Center in Changsha, Changsha, Hunan, China; Department of Computer Science, State University of New York, New Paltz, NY, USA","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2022",2022,33.0,9.0,2198,2212,"Computing resource configuration and site selection of edge servers (ESs) are two critical steps to build up a mobile edge computing (MEC) platform. In this paper, the joint optimization problem of configuration and placement for ES in the MEC environment is investigated. First, we treat each ES as an M/G/m queueing model, and establish mathematical models to characterize the MEC environment, such that the performance and operational expenditures (OPEX) of the system can be calculated analytically. Then, we design a two-stage method and develop a series of algorithms based on bisection algorithm and genetic algorithm (GA) to obtain the optimal configuration scheme and sub-optimal placement scheme (including the deployment quantity) of ESs, with the goal of minimizing OPEX while maintaining system performance at a predetermined level. Finally, we conduct experiments based on a real base station dataset provided by Shanghai Telecom to show the effectiveness of the proposed algorithms. To the best of our knowledge, this work is the first research of the joint optimization problem of configuration and placement for ES in the MEC environment, where the main objective is to increase the cost efficiency.","1558-2183","","10.1109/TPDS.2021.3135955","National Natural Science Foundation of China(grant numbers:61876061); Applied Basic Research Foundation of Yunnan Province(grant numbers:202001BB050034); Open Foundation of Key Laboratory in Software Engineering of Yunnan Province(grant numbers:2020SE405); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653813","Cost efficiency;configuration scheme;edge server;mobile edge computing;placement scheme","Servers;Costs;Resource management;Optimization;Heuristic algorithms;Quality of service;Dynamic scheduling","cloud computing;computer centres;genetic algorithms;mobile computing;optimisation;queueing theory;resource allocation","cost-efficient server configuration;resource configuration;site selection;edge servers;ES;critical steps;mobile edge computing platform;joint optimization problem;MEC environment;mathematical models;operational expenditures;OPEX;bisection algorithm;genetic algorithm;optimal configuration scheme;sub-optimal placement scheme;system performance;base station dataset;cost efficiency","",1.0,"",42.0,"IEEE","16 Dec 2021","","","IEEE","IEEE Journals"
"NetSHa: In-Network Acceleration of LSH-Based Distributed Search","P. Zhang; H. Pan; Z. Li; P. Cui; R. Jia; P. He; Z. Zhang; G. Tyson; G. Xie","University of Chinese Academy of Sciences, Beijing, China; Purple Mountain Laboratories, Nanjing, China; Purple Mountain Laboratories, Nanjing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; ByteDance Inc., Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Queen Mary University of London, London, U.K.; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2022",2022,33.0,9.0,2213,2229,"Locality Sensitive Hashing (LSH) is widely adopted to index similar data in high-dimensional space for approximate nearest neighbor search. Demanding applications (e.g. web search) mean that LSH must exhibit low response times and high throughput. To achieve this, they tend to load balance between multiple machines. However, as the scale of concurrent queries and the volume of data grow, large numbers of index messages are required. Hence, the network is a key bottleneck. To address this gap, we propose NetSHa, which exploits the computational capacity of programmable switches. Specifically, we introduce a heuristic sort-reduce approach to drop potentially poor candidate answers while preserving search quality. Then, NetSHa aggregates good candidate answers from different index messages when transmitting them. Through this, it reduces the network communication cost. Furthermore, we introduce a best-effort replacement mechanism to improve its concurrency. We implement NetSHa on a Barefoot Tofino programmable switch and evaluate it using 7 real-world datasets. The experimental results show that NetSHa reduces the packet volume by $4\sim 10$4∼10 times and improves the search efficiency by least 3× in comparison with typical LSH-based distributed search frameworks.","1558-2183","","10.1109/TPDS.2021.3135842","National Key Research and Development Program of China(grant numbers:2020YFB1805600); National Natural Science Foundation of China(grant numbers:61725206,U20A20180,62002344); Informatization Plan of Chinese Academy of Sciences(grant numbers:CAS-WX2021SF-0506); CAS-Austria Joint Project(grant numbers:171111KYSB20200001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653850","Local sensitive hashing;distributed search;in-network computation","Servers;Indexes;Costs;Task analysis;Hash functions;Concurrent computing;Aggregates","nearest neighbour methods;query processing;resource allocation;search problems;sorting;storage management","computational capacity;programmable switches;heuristic sort-reduce approach;index messages;network communication cost;Barefoot Tofino programmable switch;in-network acceleration;LSH-based distributed search;locality sensitive hashing;index similar data;high-dimensional space;approximate nearest neighbor search;low response times;concurrent queries","",3.0,"",57.0,"IEEE","16 Dec 2021","","","IEEE","IEEE Journals"
"Flexible Performant GEMM Kernels on GPUs","T. Faingnaert; T. Besard; B. De Sutter","Department of Electronics and Information Systems, Ghent University, Ghent, Belgium; Julia Computing, Ghent University, Ghent, Belgium; Department of Electronics and Information Systems, Ghent University, Ghent, Belgium","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2022",2022,33.0,9.0,2230,2248,"General Matrix Multiplication or GEMM kernels take centre place in high performance computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as NVIDIA’s Tensor Cores. Their exploitation is hampered by the two-language problem: it requires either low-level programming which implies low programmer productivity or using libraries that only offer a limited set of components. Because rephrasing algorithms in terms of established components often introduces overhead, the libraries’ lack of flexibility limits the freedom to explore new algorithms. Researchers using GEMMs can hence not enjoy programming productivity, high performance, and research flexibility at once. In this paper we solve this problem. We present three sets of abstractions and interfaces to program GEMMs within the scientific Julia programming language. The interfaces and abstractions are co-designed for researchers’ needs and Julia’s features to achieve sufficient separation of concerns and flexibility to easily extend basic GEMMs in many different ways without paying a performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS and CUTLASS, we demonstrate that our performance is in the same ballpark of the libraries, and in some cases even exceeds it, without having to write a single line of code in CUDA C++ or assembly, and without facing flexibility limitations.","1558-2183","","10.1109/TPDS.2021.3136457","Fonds Wetenschappelijk Onderzoek(grant numbers:3G051318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9655458","Matrix multiplication;graphics processors;high-level programming languages","Libraries;Kernel;Graphics processing units;Codes;Programming;Instruction sets;Productivity","C++ language;graphics processing units;learning (artificial intelligence);matrix multiplication;parallel architectures;software libraries;tensors","flexibility limitations;GEMM kernels;centre place;high performance computing;machine learning;NVIDIA GPUs;two-language problem;low-level programming;Julia programming language;CUDA C++;NVIDIAs tensor cores;general matrix multiplication","","","",52.0,"IEEE","17 Dec 2021","","","IEEE","IEEE Journals"
"CoFilter: High-Performance Switch-Accelerated Stateful Packet Filter for Bare-Metal Servers","J. Cao; Y. Liu; Y. Zhou; L. He; C. Sun; Y. Wang; M. Xu","Beijing National Research Center for Information Science and Technology, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Alibaba Infrastructure Service, Alibaba Group, Hangzhou, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Alibaba Infrastructure Service, Alibaba Group, Hangzhou, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2022",2022,33.0,9.0,2249,2262,"As one of the most critical cloud services, Bare-Metal Servers (BMS) introduce stringent performance requirements on data center networks (DCN). Stateful packet filter is an integral DCN component of ensuring connection security for BMS. However, the off-the-shelf stateful packet filters either are costly for cloud DCNs or introduce significant performance bottlenecks. In this article, we present CoFilter, which leverages low-cost programmable switches to accelerate the stateful packet filter for BMS. CoFilter uses (1) stateful process partition to enable complex stateful packet filtering logic on programmability-limited switching ASICs, (2) state compression to track tens of millions of connections with constrained hardware memory, and (3) per-tenant packet rate limit and tenant-aware flow migration to achieve efficient performance isolation among different tenants. Overall, CoFilter implements a high-performance stateful packet filter via the co-design of programmable switching ASIC and CPU. We evaluate CoFilter under various data center traffic traces with real-world flow distributions. The evaluation results show that CoFilter remarkably outperforms NetFilter, i.e., forwarding packets at line rate (13x throughput of NetFilter), keeping packet delay within 1us, and freeing a significant quantity of CPU cores, with rather small memory usage, i.e., accommodating over $10^7$107 connections with only 16MB SRAM.","1558-2183","","10.1109/TPDS.2021.3136575","National Natural Science Foundation of China(grant numbers:61772307); National Key Research and Development Program of China(grant numbers:2018YFB1800405); Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9656674","Bare-metal server stateful packet filter;programmable switch","Servers;Random access memory;Hardware;Switches;Security;Delays;Data centers","cloud computing;computer centres;computer network management;network servers;resource allocation;telecommunication traffic","BMS;off-the-shelf stateful packet filters;CoFilter;low-cost programmable switches;complex stateful packet filtering logic;per-tenant packet rate limit;efficient performance isolation;high-performance stateful packet filter;packet delay;high-performance switch-accelerated stateful packet filter;critical cloud services;stringent performance requirements;data center networks;integral DCN component;Bare-metal servers;memory size 16.0 MByte","","","",38.0,"IEEE","20 Dec 2021","","","IEEE","IEEE Journals"
"Guest Editorial: Special Section on Parallel and Distributed Computing Techniques for Non-Von Neumann Technologies","S. Pakin; C. Teuscher; C. Schuman","Computer, Computational, and Statistical Sciences Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Department of Electrical and Computer Engineering, Portland State University, Portland, OR, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA","IEEE Transactions on Parallel and Distributed Systems","27 Aug 2021",2022,33.0,2.0,249,250,"The title of this special section, Parallel and Distributed Computing Techniques for Non-Von Neumann Technologies, is a bit misleading. Technically, parallel and distributed computers already diverge from the basic architecture that John von Neumann proposed in 1945, although they are still based on processors that execute a sequence of instructions, each of which performs a simple action such as computing an arithmetic result, reading or writing memory, or branching to a new location in the instruction sequence. But what would a computer that is not based on this model of execution look like? The technologies discussed in the following articles are more exotic, more innovative, and more intriguing than what you are likely to encounter in a typical collection of peer reviewed computer-science articles. We hope that these articles will help you view computing in a new light and give you a sense of what the future of computing may look like. ","1558-2183","","10.1109/TPDS.2021.3093148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524463","","Special issues and sections;Computer architecture;Neuromorphic engineering;Computational modeling;Distributed computing;Memory;Deep learning;Hardware","","","","","",0.0,"IEEE","27 Aug 2021","","","IEEE","IEEE Journals"
"Silent-PIM: Realizing the Processing-in-Memory Computing With Standard Memory Requests","C. H. Kim; W. J. Lee; Y. Paik; K. Kwon; S. Y. Kim; I. Park; S. W. Kim","Department of Electrical Engineering, Korea University, Seoul, Korea; Department of Electrical Engineering, Korea University, Seoul, Korea; Department of Electrical Engineering, Korea University, Seoul, Korea; Department of Electrical Engineering, Korea University, Seoul, Korea; Department of Electrical Engineering, Korea University, Seoul, Korea; K hynix Inc., Icheon, South Korea; Department of Electrical Engineering, Korea University, Seoul, Korea","IEEE Transactions on Parallel and Distributed Systems","27 Aug 2021",2022,33.0,2.0,251,262,"The Deep Neural Network (DNN), Recurrent Neural Network (RNN) applications, rapidly becoming attractive to the market, process a large amount of low-locality data; thus, the memory bandwidth limits their peak performance. Therefore, many data centers actively adapt high-bandwidth memory like HBM2/HBM2E to resolve the problem. However, this approach would not provide a complete solution since it still transfers the data from the memory to the computing unit. Thus, processing-in-memory (PIM), which performs the computation inside memory, has attracted attention. However, most previous methods require the modification or the extension of core pipelines and memory system components like memory controllers, making the practical implementation of PIM very challenging and expensive in development. In this article, we propose a Silent-PIM that performs the PIM computation with standard DRAM memory requests; thus, requiring no hardware modifications and allowing the PIM memory device to perform the computation while servicing non-PIM applications’ memory requests. We can achieve our design goal by preserving the standard memory request behaviors and satisfying the DRAM standard timing requirements. In addition, using standard memory requests makes it possible to use DMA as a PIM’s offloading engine, resulting in processing the PIM memory requests fast and making a core perform other tasks. We compared the performance of three Long Short-Term Memory models (LSTM) kernels on real platforms, such as the Silent-PIM modeled on the FPGA, GPU, and CPU. For $(p \times 512) \times (512 \times 2048)$(p×512)×(512×2048) matrix multiplication with a batch size $p$p varying from 1 to 128, the Silent-PIM performed up to 16.9x and 24.6x faster than GPU and CPU, respectively, $p=1$p=1, which was the case without having any data reuse. At $p=128$p=128, the highest data reuse case, the GPU performance was the highest, but the PIM performance was still higher than the CPU execution. Similarly, at $(p \times 2048)$(p×2048) element-wise multiplication and addition, where there was no data reuse, the Silent-PIM always achieved higher than both CPU and GPU. It also showed that when the PIM’s EDP performance was superior to the others in all the cases having no data reuse.","1558-2183","","10.1109/TPDS.2021.3065365","SK hynix Inc.; Korea Institute for Advancement of Technology; Korea Government(grant numbers:N0001883); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376660","Silent-PIM;in-memory processing;standard memory requests;DMA;LSTM","Random access memory;Standards;Performance evaluation;Memory management;Timing;Engines;Field programmable gate arrays","DRAM chips;field programmable gate arrays;graphics processing units;recurrent neural nets;storage management","nonPIM applications;standard memory requests;silent-PIM;processing-in-memory computing;deep neural network;recurrent neural network;data centers;DRAM memory requests","",7.0,"",29.0,"IEEE","11 Mar 2021","","","IEEE","IEEE Journals"
"Look-up-Table Based Processing-in-Memory Architecture With Programmable Precision-Scaling for Deep Learning Applications","P. R. Sutradhar; S. Bavikadi; M. Connolly; S. Prajapati; M. A. Indovina; S. M. P. Dinakarrao; A. Ganguly","Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA","IEEE Transactions on Parallel and Distributed Systems","27 Aug 2021",2022,33.0,2.0,263,275,"Processing in memory (PIM) architecture, with its ability to perform ultra-low-latency parallel processing, is regarded as a more suitable alternative to von Neumann computing architectures for implementing data-intensive applications such as Deep Neural Networks (DNN) and Convolutional Neural Networks (CNN). In this article, we present a Look-up Table (LUT) based PIM architecture aimed at CNN/DNN acceleration that replaces logic-based processing with pre-calculated results stored inside the LUTs in order to perform complex computations on the DRAM memory platform. Our LUT-based DRAM-PIM architecture offers superior performance at a significantly higher energy-efficiency compared to the more conventional bit-wise parallel PIM architectures, while at the same time avoids fabrication challenges associated with the in-memory implementation of logic circuits. Alongside, the processing elements can be programmed and re-programmed to perform virtually any operation, including operations of Convolutional, Fully Connected, Pooling, and Activating Layers of CNN/DNN. Furthermore, it is capable of operating on several combinations of bit-widths of the operand data and thereby offers a wider range of flexibility across performance, precision, and efficiency. Transmission Gate (TG) realization of the circuitry ensures minimal footprint from the PIM architecture. Our simulations demonstrate that the proposed architecture can perform AlexNet inference at a nearly 13× faster rate and 125× more efficiency compared to state-of-the-art GPU and also provides 1.35× higher throughput at 2.5× higher energy-efficiency than another recent DRAM-implemented LUT-based PIM architecture in its baseline operation mode. Moreover, it offers 12× higher frame-rate at 9× more efficiency per frame for the lowest operand precision setting, with respect to its own baseline operation mode.","1558-2183","","10.1109/TPDS.2021.3066909","National Science Foundation(grant numbers:CNS-1553264); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380930","Processing in memory (PIM);look-up table (LUT);deep neural networks (DNN);convolutional neural networks (CNN)","Computer architecture;Random access memory;Table lookup;Performance evaluation;Registers;Parallel processing;Optimization","convolutional neural nets;DRAM chips;field programmable gate arrays;learning (artificial intelligence);logic circuits;memory architecture;parallel architectures;parallel processing;table lookup","logic circuits;processing elements;energy-efficiency;baseline operation mode;processing-in-memory architecture;programmable precision-scaling;Deep learning applications;ultra-low-latency parallel processing;von Neumann computing architectures;data-intensive applications;Deep Neural Networks;Convolutional Neural Networks;logic-based processing;complex computations;DRAM memory platform;LUT-based DRAM-PIM architecture;in-memory implementation;DRAM-implemented LUT-based PIM architecture;processing in memory architecture;operand precision setting","",4.0,"",43.0,"IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"GIRAF: General Purpose In-Storage Resistive Associative Framework","L. Yavits; R. Kaplan; R. Ginosar","Department of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Department of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel; Department of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel","IEEE Transactions on Parallel and Distributed Systems","27 Aug 2021",2022,33.0,2.0,276,287,"GIRAF is a General purpose In-storage Resistive Associative Framework based on resistive content addressable memory (RCAM), which functions simultaneously as a storage and a massively parallel associative processor. GIRAF alleviates the bandwidth wall by connecting every memory bit to processing transistors and keeping computing inside the storage arrays, thus implementing deep in-data, rather than near-data, processing. We show that GIRAF outperformed a reference computer architecture with a bandwidth-limited external storage access on a variety of data-intensive workloads. The performance of GIRAF Dot Product and Sparse Matrix-Vector multiplication exceeds the attainable performance of a reference architecture by 1200 × and 130 ×, respectively.","1558-2183","","10.1109/TPDS.2021.3065448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376647","Memristors;processing in memory;processing in storage;resistive content addressable memory;associative processing","Computer architecture;Random access memory;Three-dimensional displays;Bandwidth;Arrays;Memory architecture;Central Processing Unit","content-addressable storage;matrix multiplication;memory architecture;parallel processing;sparse matrices","resistive content addressable memory;massively parallel associative processor;storage arrays;bandwidth-limited external storage access;general purpose in-storage resistive associative framework;GIRAF dot product;RCAM;reference computer architecture;sparse matrix-vector multiplication","",7.0,"",90.0,"IEEE","11 Mar 2021","","","IEEE","IEEE Journals"
"Endurance-Aware Mapping of Spiking Neural Networks to Neuromorphic Hardware","T. Titirsha; S. Song; A. Das; J. Krichmar; N. Dutt; N. Kandasamy; F. Catthoor","Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Computer Science, University of California, Irvine, CA, USA; Department of Computer Science, University of California, Irvine, CA, USA; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; KU Leuven, Leuven, Belgium","IEEE Transactions on Parallel and Distributed Systems","27 Aug 2021",2022,33.0,2.0,288,301,"Neuromorphic computing systems are embracing memristors to implement high density and low power synaptic storage as crossbar arrays in hardware. These systems are energy efficient in executing Spiking Neural Networks (SNNs). We observe that long bitlines and wordlines in a memristive crossbar are a major source of parasitic voltage drops, which create current asymmetry. Through circuit simulations, we show the significant endurance variation that results from this asymmetry. Therefore, if the critical memristors (ones with lower endurance) are overutilized, they may lead to a reduction of the crossbar's lifetime. We propose eSpine, a novel technique to improve lifetime by incorporating the endurance variation within each crossbar in mapping machine learning workloads, ensuring that synapses with higher activation are always implemented on memristors with higher endurance, and vice versa. eSpine works in two steps. First, it uses the Kernighan-Lin Graph Partitioning algorithm to partition a workload into clusters of neurons and synapses, where each cluster can fit in a crossbar. Second, it uses an instance of Particle Swarm Optimization (PSO) to map clusters to tiles, where the placement of synapses of a cluster to memristors of a crossbar is performed by analyzing their activation within the workload. We evaluate eSpine for a state-of-the-art neuromorphic hardware model with phase-change memory (PCM)-based memristors. Using 10 SNN workloads, we demonstrate a significant improvement in the effective lifetime.","1558-2183","","10.1109/TPDS.2021.3065591","National Science Foundation(grant numbers:CCF-1942697); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380671","Neuromorphic computing;spiking neural networks (SNNs);non-volatile memory (NVM);memristor;endurance","Memristors;Neurons;Hardware;Synapses;Phase change materials;Neuromorphics;Random access memory","graph theory;learning (artificial intelligence);memristors;neuromorphic engineering;particle swarm optimisation;phase change memories","phase-change memory-based memristors;endurance-aware mapping;spiking neural networks;neuromorphic computing systems;low power synaptic storage;memristive crossbar;neuromorphic hardware model;Kernighan-Lin graph partitioning algorithm;PCM-based memristors;particle swarm optimization;PSO;mapping machine learning workload","",8.0,"",71.0,"IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"Fast Post-Hoc Normalization for Brain Inspired Sparse Coding on a Neuromorphic Device","K. Henke; G. T. Kenyon; B. Migliori","Computer, Computational, and Statistical Sciences (CCS-3), Los Alamos National Laboratory, NM, USA; Computer, Computational, and Statistical Sciences (CCS-3), Los Alamos National Laboratory, NM, USA; Computer, Computational, and Statistical Sciences (CCS-3), Los Alamos National Laboratory, NM, USA","IEEE Transactions on Parallel and Distributed Systems","27 Aug 2021",2022,33.0,2.0,302,309,"Exploration of novel computational platforms is critical for the advancement of artificial intelligence as we approach the physical limitations of traditional hardware. Biologically accurate, energy efficient neuromorphic systems are particularly promising for enabling future breakthroughs because of their ability to process information in parallel and to scale using extremely low power. Sparse coding is a signal processing technique which has been known to model the information encoding in the primary visual cortex. When sparse solutions are solved using local neuron competition along with the unsupervised dictionary learning that mimics cortical development, we can build an end to end, hardware to software, brain inspired solution to a machine learning problem. In this article, we perform a detailed comparison of sparse coding solutions generated classically by orthogonal matching pursuit (OMP) implemented on a conventional digital processor with spike-based solutions obtained using the Intel Loihi neuromorphic processor. A novel “post-hoc” normalization technique to shorten simulation time for Loihi is presented along with analysis of optimal parameter selection, reconstruction errors, and unsupervised dictionary learning for Loihi approaches and their classical counterparts. Preliminary results show that both the Loihi full simulation approach and the post-hoc normalization approach are well suited to neuromorphic processors and operate in a size, weight and power regime that is not accessible by classical approaches. Ultimately, the use of this normalization technique allows for faster and, often, better solutions than demonstrated previously.","1558-2183","","10.1109/TPDS.2021.3068777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385919","Neuromorphic computing;machine learning;artificial intelligence;neurocomputers;computer vision;signal processing","Encoding;Neurons;Neuromorphics;Matching pursuit algorithms;Dictionaries;Image reconstruction;Heuristic algorithms","artificial intelligence;biocomputing;compressed sensing;energy conservation;iterative methods;neural nets;parallel processing;power aware computing;unsupervised learning","machine learning;orthogonal matching pursuit;spike-based solutions;Intel Loihi neuromorphic processor;optimal parameter selection;unsupervised dictionary learning;neuromorphic processors;brain inspired sparse coding;neuromorphic device;computational platforms;artificial intelligence;energy efficient neuromorphic systems;signal processing;information encoding;post-hoc normalization;cortical development mimicking;parallel processing;information processing","","","",17.0,"IEEE","24 Mar 2021","","","IEEE","IEEE Journals"
"Inferring the Dynamics of the State Evolution During Quantum Annealing","E. Pelofske; G. Hahn; H. Djidjev","Los Alamos National Laboratory, CCS-3 Information Sciences, Los Alamos, NM, USA; T.H. Chan School of Public Health, Harvard University, Cambridge, MA, USA; Los Alamos National Laboratory, CCS-3 Information Sciences, Los Alamos, NM, USA","IEEE Transactions on Parallel and Distributed Systems","27 Aug 2021",2022,33.0,2.0,310,321,"To solve an optimization problem using a commercial quantum annealer, one has to represent the problem of interest as an Ising or a quadratic unconstrained binary optimization (QUBO) problem and submit its coefficients to the annealer, which then returns a user-specified number of low-energy solutions. It would be useful to know what happens in the quantum processor during the anneal process so that one could design better algorithms or suggest improvements to the hardware. However, existing quantum annealers are not able to directly extract such information from the processor. Hence, in this article we propose to use advanced features of D-Wave 2000Q to indirectly infer information about the dynamics of the state evolution during the anneal process. Specifically, D-Wave 2000Q allows the user to customize the anneal schedule, that is, the schedule with which the anneal fraction is changed from the start to the end of the anneal. Using this feature, we design a set of modified anneal schedules whose outputs can be used to generate information about the states of the system at user-defined time points during a standard anneal. With this process, called slicing, we obtain approximate distributions of lowest-energy anneal solutions as the anneal time evolves. We use our technique to obtain a variety of insights into the annealer, such as the state evolution during annealing, when individual bits in an evolving solution flip during the anneal process and when they stabilize, and we introduce a technique to estimate the freeze-out point of both the system as well as of individual qubits.","1558-2183","","10.1109/TPDS.2020.3044846","U.S. Department of Energy(grant numbers:20190065DR,20180267ER); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9293405","D-Wave 2000Q;quantum annealing;quenching,slicing;state evolution;qubits;freezeout point","Annealing;Qubit;Schedules;Standards;Visualization;Optimization;NP-hard problem","annealing;optimisation;quantum computing;simulated annealing","anneal process;existing quantum annealers;state evolution;anneal schedule;anneal fraction;modified anneal schedules whose outputs;user-defined time points;standard anneal;lowest-energy anneal solutions;anneal time evolves;quantum annealing;commercial quantum annealer;quadratic unconstrained binary optimization problem;user-specified number;low-energy solutions;quantum processor","",2.0,"",28.0,"IEEE","14 Dec 2020","","","IEEE","IEEE Journals"
"Protein Structured Reservoir Computing for Spike-Based Pattern Recognition","K. -A. Tsakalos; G. C. Sirakoulis; A. Adamatzky; J. Smith","Department of Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, Greece; Department of Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, Greece; Unconventional Computing Laboratory, University of the West of England, Bristol, U.K; Computer Science Research Centre, University of the West of England, Bristol, U.K.","IEEE Transactions on Parallel and Distributed Systems","27 Aug 2021",2022,33.0,2.0,322,331,"Nowadays we witness a miniaturisation trend in the semiconductor industry backed up by groundbreaking discoveries and designs in nanoscale characterisation and fabrication. To facilitate the trend and produce ever smaller, faster and cheaper computing devices, the size of nanoelectronic devices is now reaching the scale of atoms or molecules - a technical goal undoubtedly demanding for novel devices. Following the trend, we explore an unconventional route of implementing reservoir computing on a single protein molecule and introduce neuromorphic connectivity with a small-world networking property. We have chosen Izhikevich spiking neurons as elementary processors, corresponding to the atoms of verotoxin protein, and its molecule as a `hardware' architecture of the communication networks connecting the processors. We apply on a single readout layer, various training methods in a supervised fashion to investigate whether the molecular structured Reservoir Computing (RC) system is capable to deal with machine learning benchmarks. We start with the Remote Supervised Method, based on Spike-Timing-Dependent-Plasticity, and carry on with linear regression and scaled conjugate gradient back-propagation training methods. The RC network is evaluated as a proof-of-concept on the handwritten digit images from the standard MNIST and the extended MNIST datasets and demonstrates acceptable classification accuracies in comparison with other similar approaches.","1558-2183","","10.1109/TPDS.2021.3068826","Hellenic Foundation for Research and Innovation(grant numbers:1228); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387584","Molecular networks;reservoir computing;liquid state machine;izhikevich model;remote supervised learning;pattern recognition","Reservoirs;Proteins;Training;Topology;Three-dimensional displays;Neurons;Nanoscale devices","learning (artificial intelligence);nanoelectronics;neural nets;pattern recognition;production engineering computing;proteins;semiconductor industry","small-world networking property;Izhikevich spiking neurons;elementary processors;verotoxin protein;hardware architecture;communication networks;single readout layer;supervised fashion;molecular structured Reservoir;machine learning benchmarks;remote supervised method;Spike-Timing-Dependent-Plasticity;RC network;Spike-based pattern recognition;miniaturisation trend;semiconductor industry;groundbreaking discoveries;nanoscale characterisation;computing devices;nanoelectronic devices;single protein molecule;neuromorphic connectivity;protein structured reservoir computing;conjugate gradient back-propagation training","",2.0,"",64.0,"IEEE","26 Mar 2021","","","IEEE","IEEE Journals"
"Monodirectional Evolutional Symport Tissue P Systems With Promoters and Cell Division","B. Song; K. Li; X. Zeng","College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","27 Aug 2021",2022,33.0,2.0,332,342,"Monodirectional tissue P systems with promoters are natural inspired parallel computing paradigms, where only symport rules are permitted, and with the restriction of “monodirectionality”, objects for two given regions are transferred in one direction. In this article, a novel kind of P systems, monodirectional evolutional symport tissue P systems with promoters (MESTP P systems) is raised, where objects may be revised during the movement between two regions. The computational theory of MESTP P systems that rules are employed in a flat maximally parallel pattern is investigated. We prove that finite natural number sets are created by MESTP P systems applying one cell, at most 1 promoter and all evolutional symport rules having a maximal length 2 or with arbitrary number of cells, promoters and all evolutional symport rules having a maximal length 2. MESTP P systems are Turing universal when two cells, at most 1 promoter and all evolutional symport rules having a maximal length 2 are employed. In addition, with the help of cell division mechanism, monodirectional evolutional symport tissue P systems with promoters and cell division (MESTPD P systems) are employed to solve NP-complete (the SAT) problem, where system uses at most 1 promoter and all evolutional symport rules having a maximal length 3. These results show that MESTP(D) P systems are still computationally powerful even if monodirectionality control mechanism is imposed, thereby developing membrane algorithms for MESTP(D) P systems is theoretically possible as well as potentially exploitable.","1558-2183","","10.1109/TPDS.2021.3065397","National Natural Science Foundation of China(grant numbers:61972138,61872309); Fundamental Research Funds for the Central Universities(grant numbers:531118010355); Natural Science Foundation of Hunan Province(grant numbers:2020JJ4215); Key Research and Development Program of Changsha(grant numbers:kq2004016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376640","Bio-inspired computing;membrane computing;tissue-like network;universality;NP-complete problem","Biomembranes;Computational modeling;Skin;NP-complete problem;Micromechanical devices;Biological systems;Biological system modeling","biocomputing;computational complexity;set theory","monodirectional evolutional symport tissue P systems;cell division;natural inspired parallel computing paradigms;MESTP P systems;evolutional symport rules;MESTPD P systems;maximally parallel pattern;NP-complete problem","",12.0,"",71.0,"IEEE","11 Mar 2021","","","IEEE","IEEE Journals"
"EdgeDR: An Online Mechanism Design for Demand Response in Edge Clouds","S. Chen; L. Jiao; F. Liu; L. Wang","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab in the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab in the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Department of Computer Science, Technische Universität Darmstadt, Darmstadt, Germany","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2021",2022,33.0,2.0,343,358,"The computing frontier is moving from centralized mega datacenters towards distributed cloudlets at the network edge. We argue that cloudlets are well-suited for handling power demand response to help the grid maintain stability due to more flexible workload management attributed to their distributed nature. However, they also require computing demand response to avoid overload and maintain reliability. To this end, we propose a novel online market mechanism, EdgeDR, to achieve cost efficiency in edge demand response programs. At a high level, we observe that the cloudlet operator can dynamically switch on/off entire cloudlets to compensate for the energy reduction required by the power grid or provide enough computing resources to the edge service. We formulate a long-term social cost minimization problem and decompose it into a series of one-round procurement auctions. In each auction instance, we propose to let the cloudlet tenants bid with cost functions of their two-dimension service quality degradation tolerance, and let the cloudlet operator choose the service quality, manage the workload, and schedule the cloudlet activation status. In addition, we present a dynamic payment mechanism for the operator to balance the tradeoff between short-term profit and long-term benefit in more practical scenarios. Via rigorous analysis, we exhibit that our bidding policy is individually rational and truthful; our workload management algorithm has near-optimal performance in each auction; and our overall online algorithm achieves a provable competitive ratio. We further confirm the performance of our mechanism through extensive trace-driven simulations.","1558-2183","","10.1109/TPDS.2021.3087360","National Natural Science Foundation of China(grant numbers:61722206,61761136014,61520106005); DFG(grant numbers:392046569); National Key Research and Development Program of China(grant numbers:2017YFB1001703); Fundamental Research Funds for the Central Universities(grant numbers:2017KFKJXX009,3004210116); National Program for Support of Top-notch Young Professionals; Ripple Faculty Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448461","Edge demand response;energy saving;cloudlet control;online mechanism","Cloud computing;Throughput;Degradation;Load management;Switches;Propagation delay;Delays","cloud computing;commerce;computer centres;demand side management;electronic commerce;minimisation;optimisation;power grids;procurement;resource allocation;scheduling;smart power grids","cost functions;two-dimension service quality degradation tolerance;cloudlet operator;cloudlet activation status;dynamic payment mechanism;short-term profit;long-term benefit;workload management algorithm;online algorithm;EdgeDR;online mechanism design;edge clouds;computing frontier;centralized mega datacenters;distributed cloudlets;network edge;power demand response;flexible workload management;online market mechanism;cost efficiency;edge demand response programs;entire cloudlets;power grid;computing resources;edge service;long-term social cost minimization problem;one-round procurement auctions;auction instance;cloudlet tenants bid","",11.0,"",55.0,"IEEE","8 Jun 2021","","","IEEE","IEEE Journals"
"Optimization of Reactive Force Field Simulation: Refactor, Parallelization, and Vectorization for Interactions","P. Gao; X. Duan; B. Schmidt; W. Zhang; L. Gan; H. Fu; W. Xue; W. Liu; G. Yang","National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; Institute for Computer Science, Johannes Gutenberg University, Mainz, Germany; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2021",2022,33.0,2.0,359,373,"Molecular dynamics (MD) simulations are playing an increasingly important role in many areas ranging from chemical materials to biological molecules. With the continuing development of MD models, the potentials are getting larger and more complex. In this article, we focus on the reactive force field (ReaxFF) potential from LAMMPS to optimize the computation of interactions. We present our efforts on refactoring for neighbor list building, bond order computation, as well as valence angles and torsion angles computation. After redesigning these kernels, we develop a vectorized implementation for non-bonded interactions, which is nearly 100 × faster than the management processing element (MPE) on the Sunway TaihuLight supercomputer. Furthermore, we have implemented the three-body-list free torsion angles computation, and propose a line-locked software cache method to eliminate write conflicts in the torsion angle and valence angle interactions resulting in an order-of-magnitude speedup on a single Sunway TaihuLight node. In addition, we achieve a speedup of up to 3.5 compared to the KOKKOS package on an Intel Xeon Gold 6148 core. When executed on 1,024 processes, our implementation enables the simulation of 21,233,664 atoms on 66,560 cores with a performance of 0.032 ns/day and a weak scaling efficiency of 95.71 percent.","1558-2183","","10.1109/TPDS.2021.3091408","National Key Research and Development Program of China(grant numbers:2020YFB0204700); National Natural Science Foundation of China(grant numbers:61972231,U1806205); Key Project of Joint Fund of Shandong Province(grant numbers:ZR2019LZH007); Key Technology Research and Development Program of Shandong(grant numbers:2018CXGC1211); CSC and DAAD; Center for High Performance Computing and System Simulation; Pilot National Laboratory for Marine Science and Technology; Engineering Research Center of Digital Media Technology, Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462358","High performance computing;molecular dynamics;computational science;Sunway TaihuLight supercomputer (TaihuLight)","Mathematical model;Optimization;Software;Force;Supercomputers;Computational modeling;Chemicals","bond angles;cache storage;chemistry computing;librational states;molecular dynamics method;parallel machines;parallel programming;physics computing;potential energy functions","scaling efficiency;MPE;management processing element;LAMMPS;ReaxFF;molecular dynamics simulations;parallelization;line-locked software cache;nonbonded interactions;bond order computation;neighbor list building;reactive force field potential;MD models;biological molecules;chemical materials;vectorization;refactor;reactive force field simulation;order-of-magnitude speedup;valence angle interactions;three-body-list free torsion angles computation;Sunway TaihuLight supercomputer","",2.0,"",34.0,"IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"PostMan: Rapidly Mitigating Bursty Traffic via On-Demand Offloading of Packet Processing","Y. Niu; P. Jin; J. Guo; Y. Xiao; R. Shi; F. Liu; C. Qian; Y. Wang","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, Ohio State University, Columbus, OH, USA; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, University of California Santa Cruz, Santa Cruz, CA, USA; Department of Computer Science and Engineering, Ohio State University, Columbus, OH, USA","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2021",2022,33.0,2.0,374,387,"Unexpected bursty traffic brought by certain sudden events, such as news in the spotlight on a social network or discounted items on sale, can cause severe load imbalance in backend services. Migrating hot data - the standard approach to achieve load balance - meets a challenge when handling such unexpected load imbalance, because migrating data will slow down the server that is already under heavy pressure. This article proposes PostMan, an alternative approach to rapidly mitigate load imbalance for services processing small requests. Motivated by the observation that processing large packets incurs far less CPU overhead than processing small ones, PostMan deploys a number of middleboxes called helpers to assemble small packets into large ones for the heavily-loaded server. This approach essentially offloads the overhead of packet processing from the heavily-loaded server to helpers. To minimize the overhead, PostMan activates helpers on demand, only when bursty traffic is detected. The heavily-loaded server determines when clients connect/disconnect to/from helpers based on the real-time load statistics. To tolerate helper failures, PostMan can migrate connections across helpers and can ensure packet ordering despite such migration. Driven by real-world workloads, our evaluation shows that, with the help of PostMan, a Memcached server can mitigate bursty traffic within hundreds of milliseconds, while migrating data takes tens of seconds and increases the latency during migration.","1558-2183","","10.1109/TPDS.2021.3092266","National Natural Science Foundation of China(grant numbers:61722206,61761136014,392046569); NSFC-DFG(grant numbers:61520106005); National Key Research and Development Program of China(grant numbers:2017YFB1001703); Fundamental Research Funds for the Central Universities(grant numbers:2017KFKJXX009,3004210116); National Program for Support of Top-notch Young Professionals; National Science Foundation(grant numbers:CNS-1566403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9464666","Bursty traffic;packet offloading;packet batching;high-performance network stack","Servers;Bandwidth;Throughput;Redundancy;Load management;Linux;Computer science","cache storage;file servers;Internet;resource allocation;statistical analysis;telecommunication traffic","Memcached server;packet ordering;middleboxes;CPU overhead;bursty traffic mitigation;tolerate helper failures;real-time load statistics;heavily-loaded server;helpers;unexpected load imbalance;load balance;hot data migration;load imbalance mitigation;backend services;unexpected bursty traffic;packet processing;On-Demand Offloading;PostMan","",2.0,"",54.0,"IEEE","24 Jun 2021","","","IEEE","IEEE Journals"
"Repurposing GPU Microarchitectures with Light-Weight Out-Of-Order Execution","K. Iliakis; S. Xydis; D. Soudris","National Technical University of Athens, Athens, Greece; Harokopio University of Athens, Athens, Greece; National Technical University of Athens, Athens, Greece","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2021",2022,33.0,2.0,388,402,"GPU is the dominant platform for accelerating general-purpose workloads due to its computing capacity and cost-efficiency. GPU applications cover an ever-growing range of domains. To achieve high throughput, GPUs rely on massive multi-threading and fast context switching to overlap computations with memory operations. We observe that among the diverse GPU workloads, there exists a significant class of kernels that fail to maintain a sufficient number of active warps to hide the latency of memory operations, and thus suffer from frequent stalling. We argue that the dominant Thread-Level Parallelism model is not enough to efficiently accommodate the variability of modern GPU applications. To address this inherent inefficiency, we propose a novel micro-architecture with lightweight Out-Of-Order execution capability enabling Instruction-Level Parallelism to complement the conventional Thread-Level Parallelism model. To minimize the hardware overhead, we carefully design our extension to highly re-use the existing micro-architectural structures and study various design trade-offs to contain the overall area and power overhead, while providing improved performance. We show that the proposed architecture outperforms traditional platforms by 23 percent on average for low-occupancy kernels, with an area and power overhead of 1.29 and 10.05 percent, respectively. Finally, we establish the potential of our proposal as a micro-architecture alternative by providing 16 percent speedup over a wide collection of 60 general-purpose kernels.","1558-2183","","10.1109/TPDS.2021.3093231","European Union's Horizon 2020 Research and Innovation programme(grant numbers:825061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468415","General purpose GPU;micro-architecture;out-of-order execution;instruction level parallelism;parallel systems","Graphics processing units;Kernel;Out of order;Computer architecture;Parallel processing;Context;Resource management","coprocessors;graphics processing units;microprocessor chips;multiprocessing systems;multi-threading;parallel processing;performance evaluation;storage management","GPU microarchitecture;light-weight out-of-order execution;general-purpose workloads;computing capacity;cost-efficiency;massive multithreading;memory operations;modern GPU applications;out-of-order execution capability;instruction-level parallelism;hardware overhead;microarchitectural structures;low-occupancy kernels;thread-level parallelism model;dominant thread-level parallelism model;efficiency 23.0 percent;efficiency 10.05 percent;efficiency 16.0 percent","","","",56.0,"IEEE","29 Jun 2021","","","IEEE","IEEE Journals"
"Optimizing Network Transfers for Data Analytic Jobs Across Geo-Distributed Datacenters","L. Chen; S. Liu; B. Li","School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2021",2022,33.0,2.0,403,414,"It has become a recent trend that large volumes of data are generated, stored, and processed across geographically distributed datacenters. When popular data parallel frameworks, such as MapReduce and Spark, are employed to process such geo-distributed data, optimizing the network transfer in communication stages becomes increasingly crucial to application performance, as the inter-datacenter links have much lower bandwidth than intra-datacenter links. In this article, we focus on exploiting the flexibility of multi-path routing for inter-datacenter flows of data analytic jobs, with the hope of better utilizing inter-datacenter links and thus improve job performance. We design an optimal multi-path routing and scheduling strategy to achieve the best possible network performance for all concurrent jobs, based on our formulation of an optimization problem that can be transformed into an equivalent linear programming (LP) problem to be efficiently solved. As a highlight of this article, we have implemented our proposed algorithm in the controller of an application-layer software-defined inter-datacenter overlay testbed, designed to provide transfer optimization service for Spark jobs. With extensive evaluations of our real-world implementation on Google Cloud, we have shown convincing evidence that our optimal multi-path routing and scheduling strategies have achieved significant improvements in terms of job performance.","1558-2183","","10.1109/TPDS.2021.3093232","Louisiana Board of Regents(grant numbers:LEQSF(2019-22)-RD-A-21,LEQSF(2021-22)-RD-D-07); National Science Foundation(grant numbers:OIA-2019511); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468370","Geo-distributed datacenters;network transfer;data analytics;optimization","Data analysis;Routing;Bandwidth;Task analysis;Sparks;Optimization;Internet","cloud computing;computer centres;data analysis;data handling;linear programming;parallel processing;scheduling","optimization problem;equivalent linear programming problem;application-layer software-defined inter-datacenter;transfer optimization service;Spark jobs;optimal multipath routing;scheduling strategy;job performance;network transfer;data analytic jobs;geo-distributed datacenters;geographically distributed datacenters;popular data parallel frameworks;geo-distributed data;application performance;intra-datacenter links;inter-datacenter flows;utilizing inter-datacenter links;possible network performance;concurrent jobs","",3.0,"",39.0,"IEEE","29 Jun 2021","","","IEEE","IEEE Journals"
"A Pessimistic Fault Diagnosability of Large-Scale Connected Networks via Extra Connectivity","L. Lin; Y. Huang; L. Xu; S. -Y. Hsieh","College of Computer and Cyber Security, and Key Laboratory of Network Security and Cryptology, Fujian Normal University, Fuzhou, Fujian, China; School of Computer Science and Mathematics, Fujian University of Technology, Fuzhou, Fujian, China; College of Computer and Cyber Security, and Key Laboratory of Network Security and Cryptology, Fujian Normal University, Fuzhou, Fujian, China; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2021",2022,33.0,2.0,415,428,"The t/kt/k-diagnosability and hh-extra connectivity are regarded as two important indicators to improve the network reliability. The t/k-diagnosis strategy can significantly improve the self-diagnosing capability of a network at the expense of no more than k fault-free nodes being mistakenly diagnosed as faulty. The h-extra connectivity can tremendously improve the real fault tolerability of a network by insuring that each remaining component has no fewer than h+1 nodes. However, there is few result on the inherent relationship between these two indicators. In this article, we investigate the reason that caused the serious flawed results in (Liu, 2020), and we propose a diagnosis algorithm to establish the t/k-diagnosability for a large-scale connected network G under the PMC model by considering its h-extra connectivity. Let κh(G) be the h-extra connectivity of G. Then, we can deduce that G is κh(G)/h-diagnosable under the PMC model with some basic conditions. All κh(G)faulty nodes can be correctly diagnosed in the large-scale connected network G and at most h fault-free nodes would be misdiagnosed as faulty. The complete fault tolerant method adopts combinatorial properties and linearly many fault analysis to conquer the core of our proofs. We will apply the newly found relationship to directly obtain the κh(G)/h-diagnosability of a series of well known networks, including hypercubes, folded hypercubes, balanced hypercubes, dual-cubes, BC graphs, star graphs, Cayley graphs generated by transposition trees, bubble-sort star graphs, alternating group graphs, split-star networks, k-ary n-cubes and (n,k)-star graphs.","1558-2183","","10.1109/TPDS.2021.3093243","Fok Ying Tung Education Foundation(grant numbers:171061); National Natural Science Foundation of China(grant numbers:61702100,U1905211,61702103,61771140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468369","Reliability; $t/k$    t / k     -diagnosability;fault diagnosis;extra connectivity;large-scale connected networks","Hypercubes;Fault tolerant systems;Fault tolerance;Mathematical model;Fault diagnosis;Multiprocessing systems;Informatics","network theory (graphs);trees (mathematics)","pessimistic fault diagnosability;large-scale connected networks;t/kt/k-diagnosability;hh-extra connectivity;hypercubes;folded hypercubes;balanced hypercubes;dual-cubes;BC graphs;star graphs;Cayley graphs;transposition trees;bubble-sort star graphs;alternating group graphs;split-star networks;kk-ary nn-cubes","",5.0,"",42.0,"IEEE","29 Jun 2021","","","IEEE","IEEE Journals"
"Tensorox: Accelerating GPU Applications via Neural Approximation on Unused Tensor Cores","N. -M. Ho; W. -F. Wong","National University of Singapore, Singapore, Singapore; National University of Singapore, Singapore, Singapore","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2021",2022,33.0,2.0,429,443,"Driven by the demands of deep learning, many hardware accelerators, including GPUs, have begun to include specialized tensor processing units to accelerate matrix operations. However, general-purpose GPU applications that have little or no large dense matrix operations cannot benefit from these tensor units. This article proposes Tensorox, a framework that exploits the half-precision tensor cores available on recent GPUs for approximable, non deep learning applications. In essence, a shallow neural network is trained based on the input-output mapping of the function to be approximated. The key innovation in our implementation is the use of the small and dimension-restricted tensor operations in Nvidia GPUs to run multiple instances of the approximation neural network in parallel. With the proper scaling and training methods, our approximation yielded an overall accuracy that is higher than naïvely running the original programs with half-precision. Furthermore, Tensorox allows for the runtime adjustment of the degree of approximation. For the 10 benchmarks we tested, we achieved speedups from 2× to 112× compared to the original in single precision floating point, while maintaining the error caused by the approximation to below 10 percent in most applications.","1558-2183","","10.1109/TPDS.2021.3093239","Ministry of Education - Singapore(grant numbers:T1-251RES1818,MOE2016-T2-2-150); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468379","Graphics processing units;parallel programming;approximate computing;neural networks;tensor processing unit;GPGPU","Hardware;Tensors;Neural networks;Deep learning;Graphics processing units;Task analysis;Training","approximation theory;floating point arithmetic;graphics processing units;hardware accelerators;learning (artificial intelligence);matrix algebra;neural nets;parallel processing;tensors","training methods;Tensorox;single precision floating point;neural approximation;unused tensor cores;hardware accelerators;shallow neural network;input-output mapping;GPU applications acceleration;specialized tensor processing units;matrix operations operation acceleration;half-precision tensor cores;dimension-restricted tensor operations;small tensor operations","",2.0,"",54.0,"IEEE","29 Jun 2021","","","IEEE","IEEE Journals"
"A Block-Based Triangle Counting Algorithm on Heterogeneous Environments","A. Yaşar; S. Rajamanickam; J. W. Berry; Ü. V. Çatalyürek","School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Center for Computing Research, Sandia National Laboratories, Albuquerque, USA; Center for Computing Research, Sandia National Laboratories, Albuquerque, USA; School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2021",2022,33.0,2.0,444,458,"Triangle counting is a fundamental building block in graph algorithms. In this article, we propose a block-based triangle counting algorithm to reduce data movement during both sequential and parallel execution. Our block-based formulation makes the algorithm naturally suitable for heterogeneous architectures. The problem of partitioning the adjacency matrix of a graph is well-studied. Our task decomposition goes one step further: it partitions the set of triangles in the graph. By streaming these small tasks to compute resources, we can solve problems that do not fit on a device. We demonstrate the effectiveness of our approach by providing an implementation on a compute node with multiple sockets, cores and GPUs. The current state-of-the-art in triangle enumeration processes the Friendster graph in 2.1 seconds, not including data copy time between CPU and GPU. Using that metric, our approach is 20 percent faster. When copy times are included, our algorithm takes 3.2 seconds. This is 5.6 times faster than the fastest published CPU-only time.","1558-2183","","10.1109/TPDS.2021.3093240","National Science Foundation(grant numbers:CCF-1919021); Sandia National Laboratories; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468328","Triangle counting;task-based;block-based;sub-graph;multi-core;multi-GPU","Task analysis;Partitioning algorithms;Heuristic algorithms;Kernel;Hardware;Graphics processing units;Ear","data reduction;data structures;graph theory;parallel algorithms","block-based triangle counting algorithm;heterogeneous environments;graph algorithms;data movement reduction;sequential execution;parallel execution;block-based formulation;heterogeneous architectures;task decomposition;Friendster graph;triangle enumeration processes","",1.0,"",48.0,"IEEE","29 Jun 2021","","","IEEE","IEEE Journals"
"POCLib: A High-Performance Framework for Enabling Near Orthogonal Processing on Compression","F. Zhang; J. Zhai; X. Shen; O. Mutlu; X. Du","Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, ETH Zurich, Zurich, Switzerland; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2021",2022,33.0,2.0,459,475,"Parallel technology boosts data processing in recent years, and parallel direct data processing on hierarchically compressed documents exhibits great promise. The high-performance direct data processing technique brings large savings in both time and space by removing the need for decompressing data. However, its benefits have been limited to data traversal operations; for random accesses, direct data processing is several times slower than the state-of-the-art baselines. This article proposes a novel concept, orthogonal processing on compression (orthogonal POC), which means that text analytics can be efficiently supported directly on compressed data, regardless of the type of the data processing – that is, the type of data processing is orthogonal to its capability of conducting POC. Previous proposals, such as TADOC, are not orthogonal POC. This article presents a set of techniques that successfully eliminate the limitation, and for the first time, establishes the near orthogonal POC feasibility of effectively handling both data traversal operations and random data accesses on hierarchically-compressed data. The work focuses on text data and yields a unified high-performance library, called POCLib. In a ten-node distributed Spark cluster on Amazon EC2, POCLib achieves 3.1× speedup over the state-of-the-art on random data accesses to compressed data, while preserving the capability of supporting traversal operations efficiently and providing large (3.9×) space savings.","1558-2183","","10.1109/TPDS.2021.3093234","National Key Research and Development Program of China(grant numbers:2018YFB1004401); National Natural Science Foundation of China(grant numbers:61732014,U20A20226,61802412); Natural Science Foundation of Beijing Municipality(grant numbers:4202031); Beijing Academy of Artificial Intelligence; State Key Laboratory of Computer Architecture(grant numbers:CARCHA202007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468343","Near orthogonal processing on compression;direct processing on compressed data;TADOC;orthogonal POC","Task analysis;Indexing;Data analysis;Technological innovation;Dictionaries;Data structures;Standards","data compression;parallel processing;text analysis","Amazon EC2;distributed Spark cluster;POCLib;text analytics;orthogonal processing on compression;parallel technology;orthogonal POC;high-performance direct data processing technique;hierarchically compressed documents;parallel direct data processing;high-performance framework;high-performance library;text data;hierarchically-compressed data;random data access;data traversal operations","",46.0,"",57.0,"IEEE","29 Jun 2021","","","IEEE","IEEE Journals"
"A Practical and Efficient Bidirectional Access Control Scheme for Cloud-Edge Data Sharing","J. Cui; B. Li; H. Zhong; G. Min; Y. Xu; L. Liu","Institute of Physical Science and Information Technology, Anhui University, Hefei, Anhui, China; Institute of Physical Science and Information Technology, Anhui University, Hefei, Anhui, China; Institute of Physical Science and Information Technology, Anhui University, Hefei, Anhui, China; Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; Institute of Physical Science and Information Technology, Anhui University, Hefei, Anhui, China; School of Informatics, University of Leicester, Leicester, U.K.","IEEE Transactions on Parallel and Distributed Systems","30 Jul 2021",2022,33.0,2.0,476,488,"The cloud computing paradigm provides numerous tempting advantages, enabling users to store and share their data conveniently. However, users are naturally resistant to directly outsourcing their data to the cloud since the data often contain sensitive information. Although several fine-grained access control schemes for cloud-data sharing have been proposed, most of them focus on the access control of the encrypted data (e.g., restricting the decryption capabilities of the receivers). Distinct from the existing work, this article aims to address this challenging problem by developing a more practical bidirectional fine-grained access control scheme that can restrict the capabilities of both senders and receivers. To this end, we systematically investigate the access control for cloud data sharing. Inspired by the access control encryption (ACE), we propose a novel data sharing framework that combines the cloud side and the edge side. The edge server is located in the middle of all the communications, checking and preventing illegal communications according to the predefined access policy. Next, we develop an efficient access control algorithm by exploiting the attribute-based encryption and proxy re-encryption for the proposed framework. The experimental results show that our scheme exhibits superior performance in the encryption and decryption compared to the prior work.","1558-2183","","10.1109/TPDS.2021.3094126","National Natural Science Foundation of China(grant numbers:U1936220,62011530046,61872001); Special Fund for Key Program of Science and Technology of Anhui Province, China(grant numbers:202003A05020043); Open Fund for Discipline Construction; Institute of Physical Science and Information Technology, Anhui University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9470970","Cloud computing;data sharing;access control;encryption;edge computing","Cloud computing;Access control;Receivers;Encryption;Servers;Cryptography;Search problems","authorisation;cloud computing;cryptography;outsourcing","cloud-edge data sharing;cloud computing paradigm;numerous tempting advantages;fine-grained access control scheme;cloud-data sharing;encrypted data;cloud data sharing;access control encryption;predefined access policy;efficient access control algorithm;attribute-based encryption;proxy re-encryption","",5.0,"",43.0,"IEEE","1 Jul 2021","","","IEEE","IEEE Journals"
"Guest Editorial","J. Zhai; M. Si; A. J. Peña","Tsinghua University, Beijing, China; Meta Platforms, Inc., Menlo Park, CA, USA; Barcelona Supercomputing Center (BSC), Barcelona, Spain","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2644,2647,"This special section focuses on the state-of-the-art technologies on parallel and distributed computing techniques for artificial intelligence (AI), machine learning (ML), and deep learning (DL). AI, ML, and DL can enable computers the ability to learn from a large amount of data and use the learned model to optimize a complex problem or discover rules in a complicated system. AI, ML and DL can be applied to push forward the boundaries for many domains and significantly influence our daily life. ","1558-2183","","10.1109/TPDS.2022.3166681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782144","","Special issues and sections;Artificial intelligence;Machine learning;Distributed computing;Deep learning;Computational model;Parallel processing","","","","","",0.0,"IEEE","25 May 2022","","","IEEE","IEEE Journals"
"DONE: Distributed Approximate Newton-type Method for Federated Edge Learning","C. T. Dinh; N. H. Tran; T. D. Nguyen; W. Bao; A. R. Balef; B. B. Zhou; A. Y. Zomaya","School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Computing, The Australian National University, Canberra, ACT, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; Sharif University of Technology, Tehran, Iran; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2648,2660,"There is growing interest in applying distributed machine learning to edge computing, forming federated edge learning. Federated edge learning faces non-i.i.d. and heterogeneous data, and the communication between edge workers, possibly through distant locations and with unstable wireless networks, is more costly than their local computational overhead. In this work, we propose ${{\sf DONE}}$DONE, a distributed approximate Newton-type algorithm with fast convergence rate for communication-efficient federated edge learning. First, with strongly convex and smooth loss functions, ${{\sf DONE}}$DONE approximates the Newton direction in a distributed manner using the classical Richardson iteration on each edge worker. Second, we prove that ${{\sf DONE}}$DONE has linear-quadratic convergence and analyze its communication complexities. Finally, the experimental results with non-i.i.d. and heterogeneous data show that ${{\sf DONE}}$DONE attains a comparable performance to Newton's method. Notably, ${{\sf DONE}}$DONE requires fewer communication iterations compared to distributed gradient descent and outperforms DANE, FEDL, and GIANT, state-of-the-art approaches, in the case of non-quadratic loss functions.","1558-2183","","10.1109/TPDS.2022.3146253","Vietnam National Foundation for Science and Technology Development(grant numbers:102.02-2019.321); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695269","Distributed machine learning;federated learning;optimization decomposition","Newton method;Distributed databases;Machine learning;Convergence;Costs;Complexity theory;Approximation algorithms","approximation theory;communication complexity;convergence of numerical methods;distributed processing;gradient methods;iterative methods;learning (artificial intelligence);Newton method","distributed approximate Newton-type method;distributed machine;federated edge learning faces;heterogeneous data;edge worker;local computational overhead;distributed approximate Newton-type algorithm;communication-efficient federated edge learning;Newton direction;distributed gradient descent;DONE;edge computing;classical Richardson iteration;DANE;FEDL;GIANT;nonquadratic loss functions","",2.0,"",36.0,"IEEE","27 Jan 2022","","","IEEE","IEEE Journals"
"Flexible Clustered Federated Learning for Client-Level Data Distribution Shift","M. Duan; D. Liu; X. Ji; Y. Wu; L. Liang; X. Chen; Y. Tan; A. Ren","College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; School of Computer Science and Engineering, Xi'an Jiaotong University, Xi'an, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2661,2674,"Federated Learning (FL) enables the multiple participating devices to collaboratively contribute to a global neural network model while keeping the training data locally. Unlike the centralized training setting, the non-IID, imbalanced (statistical heterogeneity) and distribution shifted training data of FL is distributed in the federated network, which will increase the divergences between the local models and the global model, further degrading performance. In this paper, we propose a flexible clustered federated learning (CFL) framework named FlexCFL, in which we 1) group the training of clients based on the similarities between the clients’ optimization directions for lower training divergence; 2) implement an efficient newcomer device cold start mechanism for framework scalability and practicality; 3) flexibly migrate clients to meet the challenge of client-level data distribution shift. FlexCFL can achieve improvements by dividing joint optimization into groups of sub-optimization and can strike a balance between accuracy and communication efficiency in the distribution shift environment. The convergence and complexity are analyzed to demonstrate the efficiency of FlexCFL. We also evaluate FlexCFL on several open datasets and made comparisons with related CFL frameworks. The results show that FlexCFL can significantly improve absolute test accuracy by $+10.6\%$+10.6% on FEMNIST compared with FedAvg, $+3.5\%$+3.5% on FashionMNIST compared with FedProx, $+8.4\%$+8.4% on MNIST compared with FeSEM, $+4.7\%$+4.7% on Sentiment140 compare with IFCA. The experiment results show that FlexCFL is also communication efficient in the distribution shift environment.","1558-2183","","10.1109/TPDS.2021.3134263","National Natural Science Foundation of China(grant numbers:61672116,61601067,61802038,61672115); Chongqing High-Tech Research Key Program(grant numbers:cstc2019jscx-mbdx0063); Fundamental Research Funds for the Central Universities(grant numbers:0214005207005,2019CDJGFJSJ001); Chongqing Youth Talent Support Program(grant numbers:cstc2020jcyj-jqX0012); China Postdoctoral Science Foundation(grant numbers:2017M620412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647969","Federated learning;distributed machine learning;neural networks","Training;Servers;Convergence;Optimization;Data models;Collaborative work;Training data","data handling;learning (artificial intelligence);neural nets;optimisation;pattern classification;pattern clustering","client-level data distribution shift;global neural network;training data;newcomer device cold start mechanism;flexible clustered federated learning;FlexCFL;clients optimization directions","",7.0,"",47.0,"IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"Reputation-Aware Hedonic Coalition Formation for Efficient Serverless Hierarchical Federated Learning","J. S. Ng; W. Y. B. Lim; Z. Xiong; X. Cao; J. Jin; D. Niyato; C. Leung; C. Miao","Alibaba Group and Alibaba-NTU Joint Research Institute (JRI), Nanyang Technological University (NTU), Singapore; Alibaba Group and Alibaba-NTU Joint Research Institute (JRI), Nanyang Technological University (NTU), Singapore; Information Systems Technology and Design (ISTD) Pillar, Singapore University of Technology and Design (SUTD), Singapore; School of Electronic and Information Engineering, Beihang University, Beijing, China; TuSimple, Beijing, China; School of Computer Science and Engineering (SCSE), NTU, Singapore; Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Singapore; SCSE, NTU, Singapore, Alibaba-NTU JRI, LILY, Singapore","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2675,2686,"Amid growing concerns on data privacy, Federated Learning (FL) has emerged as a promising privacy preserving distributed machine learning paradigm. Given that the FL network is expected to be implemented at scale, several studies have proposed system architectures towards improving the network scalability and efficiency. Specifically, the Hierarchical FL (HFL) network utilizes cluster heads, e.g., base stations, for the intermediate aggregation and relay of model parameters. Serverless FL is also proposed recently, in which the data owners, i.e., workers, exchange the local model parameters among a neighborhood of workers. This decentralized approach reduces the risk of a single point of failure but inevitably incurs significant communication overheads. To achieve the best of both worlds, we propose the Serverless Hierarchical Federated Learning (SHFL) framework in this article. The SHFL framework adopts a two-layer system architecture. In the lower layer, the FL workers are grouped into clusters under cluster heads. In the upper layer, the cluster heads exchange the intermediate parameters with their one-hop neighbors without the aid of a central server. To improve the sustainable efficiency of the FL system while taking into account the incentive design for workers’ marginal contributions in the system, we propose the reputation-aware hedonic coalition formation game in this article. Specifically, the workers are rewarded for their marginal contribution to the cluster, whereas the reputation opinions of each cluster head is updated in a decentralized manner, thereby deterring malicious behaviors by the cluster head. This improves the performance of the network since cluster heads with higher reputation scores are more reliable in relaying the intermediate model parameters. The simulation results show that our proposed hedonic coalition formation algorithm converges to a Nash-stable partition and improves the network efficiency.","1558-2183","","10.1109/TPDS.2021.3139039","National Research Foundation, Prime Minister's Office, Singapore; Alibaba Innovative Research; National Research Foundation Singapore(grant numbers:AISG2-RP-2020-019); WASP/NTU(grant numbers:M4082187 (4080)); Singapore Ministry of Education(grant numbers:RG16/20); SUTD(grant numbers:SRG-ISTD-2021-165); SUTD-ZJU IDEA(grant numbers:SUTD-ZJU (VP) 202102); SUTD-ZJU IDEA Seed(grant numbers:SUTD-ZJU SD 202101); NSFC(grant numbers:61827901,62071343); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665214","Federated learning;serverless federated learning;decentralized edge intelligence;hedonic coalition formation","Magnetic heads;Servers;Training;Base stations;Costs;Computational modeling;Collaborative work","data privacy;game theory;learning (artificial intelligence);wireless sensor networks","efficient Serverless Hierarchical Federated Learning;data privacy;promising privacy;machine learning paradigm;system architectures;network scalability;Hierarchical FL network;intermediate aggregation;Serverless FL;local model parameters;Serverless Hierarchical Federated Learning framework;two-layer system architecture;FL workers;cluster head;intermediate parameters;sustainable efficiency;FL system;reputation-aware hedonic coalition formation game;intermediate model parameters;hedonic coalition formation algorithm converges;network efficiency","",2.0,"",40.0,"IEEE","29 Dec 2021","","","IEEE","IEEE Journals"
"Min-Max Cost Optimization for Efficient Hierarchical Federated Learning in Wireless Edge Networks","J. Feng; L. Liu; Q. Pei; K. Li","Shaanxi Key Laboratory of Information Communication Network and Security, Xi'an University of Posts & Telecommunications, Xi'an, Shaanxi, China; State Key Laboratory of ISN, School of Telecommunication Engineering, Xidian University, Xi'an, Shannxi, China; State Key Laboratory of ISN, School of Telecommunication Engineering, Xidian University, Xi'an, Shannxi, China; Department of Computer Science, State University of New York, New Paltz, NY, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2687,2700,"Federated learning is a distributed machine learning technology that can protect users’ data privacy, so it has attracted more and more attention in the industry and academia. Nonetheless, most of the existing works focused on the cost optimization of the entire process, while the cost of individual participants cannot be considered. In this article, we explore a min-max cost-optimal problem to guarantee the convergence rate of federated learning in terms of cost in wireless edge networks. In particular, we minimize the cost of the worst-case participant subject to the delay, local CPU-cycle frequency, power allocation, local accuracy, and subcarrier assignment constraints. Considering that the formulated problem is a mixed-integer nonlinear programming problem, we decompose it into several sub-problems to derive its solutions, in which the subcarrier assignment and power allocation are obtained by utilizing the Lagrangian dual decomposition method, the CPU-cycle frequency is obtained by a heuristic algorithm, and the local accuracy is obtained by an iteration algorithm. Simulation results show the convergence of the proposed algorithm and reveal that the proposed scheme can accomplish a tradeoff between the cost and fairness by comparing the proposed scheme with the existing schemes.","1558-2183","","10.1109/TPDS.2021.3131654","National Key Research and Development Program of China(grant numbers:2020YFB1807500); National Natural Science Foundation of China(grant numbers:62102297,62001357); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2020A1515110496,2020A1515110079); China Postdoctoral Science Foundation(grant numbers:2021M692501); Fundamental Research Funds for the Central Universities(grant numbers:XJS210105,XJS210107); Open Project of Shaanxi Key Laboratory of Information Communication Network and Security(grant numbers:ICNS202005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629331","CPU-cycle frequency;federated learning;local accuracy;min-max cost;wireless edge networks","Computational modeling;Servers;Smart devices;Collaborative work;Training;Data models;Resource management","data privacy;integer programming;iterative methods;learning (artificial intelligence);minimax techniques;nonlinear programming;OFDM modulation;optimisation","local CPU-cycle frequency;power allocation;local accuracy;subcarrier assignment constraints;mixed-integer nonlinear programming problem;min-max cost optimization;efficient hierarchical federated learning;wireless edge networks;distributed machine;users;min-max cost-optimal problem;convergence rate;worst-case participant subject","",30.0,"",28.0,"IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"LightFed: An Efficient and Secure Federated Edge Learning System on Model Splitting","J. Guo; J. Wu; A. Liu; N. N. Xiong","School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; Department of Computer Science and Mathematics, Sul Ross State University, Alpine, TX, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2701,2713,"With the integration of Artificial Intelligence (AI) and Internet of Things (IoT), the Federated Edge Learning (FEL), a promising computing framework is developing. However, there are still unsolved issues on communication efficiency and data security due to the huge models and unreliable transmission links. To address these issues, this paper proposes a novel federated edge learning system, called LightFed, where the edge nodes upload only vital partial local models, and successfully achieve lightweight communication and model aggregation. First, a novel model aggregation method Model Splitting and Splicing (MSS) and a Selective Parameter Transmission (SPT) scheme are proposed. By detecting the updating gradients of local parameters and filtering significant parameters, selective rotated transmission and efficient aggregation of local models are achieved. Second, a Training Filling Model (TFM) is proposed to infer the total data distribution of edge nodes, and train a filling model to mitigate the unbalanced training data without violating the data privacy of individual users. Moreover, a blockchain-powered confusion transmission mechanism is proposed for defending the attacks from external adversaries and protecting the model information. Finally, extensive experimental results demonstrate that our LightFed significantly outperforms the existing FEL systems in terms of communication efficiency and privacy security.","1558-2183","","10.1109/TPDS.2021.3127712","National Natural Science Foundation of China(grant numbers:62072475,61772554); Graduate Students of Central South University(grant numbers:2021zzts0750); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613755","Federated edge learning;communication efficiency;privacy protection;deep neural network","Data models;Computational modeling;Training;Mathematical models;Servers;Security;Data privacy","blockchains;data privacy;Internet of Things;learning (artificial intelligence)","data security;unreliable transmission links;LightFed;edge nodes;lightweight communication;model aggregation;selective parameter transmission scheme;local parameters;selective rotated transmission;total data distribution;data privacy;blockchain-powered confusion transmission mechanism;model information;privacy security;artificial intelligence;secure federated edge learning system;Internet of Things;model splitting and splicing;training filling model;external adversaries;FEL systems;computing framework;partial local models","",4.0,"",38.0,"IEEE","12 Nov 2021","","","IEEE","IEEE Journals"
"Differentially Private Federated Temporal Difference Learning","Y. Zeng; Y. Lin; Y. Yang; J. Liu","Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA; Department of Applied Mathematics and Statistics, Stony Brook University, Stony Brook, NY, USA; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2714,2726,"This article considers a federated temporal difference (TD) learning algorithm and provides both asymptotic and finite-time analyses. To protect each worker agent's cost information from being acquired by possible attackers, we propose a privacy-preserving variant of the algorithm by adding perturbation to the exchanged information. We show the rigorous differential privacy guarantee by using moments accountant and derive an upper bound of the utility loss for the privacy-preserving algorithm. Evaluations are also provided to corroborate the efficiency of the algorithms.","1558-2183","","10.1109/TPDS.2021.3133898","National Science Foundation(grant numbers:1513719,1730291); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645233","Multi-agent reinforcement learning;TD learning;federated learning;differential privacy","Privacy;Costs;Approximation algorithms;Markov processes;Differential privacy;Gaussian noise;Games","data privacy;finite element analysis;learning (artificial intelligence)","private federated temporal difference learning;federated temporal difference learning algorithm;finite-time analyses;worker agent;possible attackers;privacy-preserving variant;exchanged information;rigorous differential privacy guarantee;privacy-preserving algorithm","",2.0,"",53.0,"IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"On the Benefits of Multiple Gossip Steps in Communication-Constrained Decentralized Federated Learning","A. Hashemi; A. Acharya; R. Das; H. Vikalo; S. Sanghavi; I. Dhillon","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA; Department of Computer Science, University of Texas at Austin, Austin, TX, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA; Department of Computer Science, University of Texas at Austin, Austin, TX, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2727,2739,"Federated learning (FL) is an emerging collaborative machine learning (ML) framework that enables training of predictive models in a distributed fashion where the communication among the participating nodes are facilitated by a central server. To deal with the communication bottleneck at the server, decentralized FL (DFL) methods advocate rely on local communication of nodes with their neighbors according to a specific communication network. In DFL, it is common algorithmic practice to have nodes interleave (local) gradient descent iterations with gossip (i.e., averaging over the network) steps. As the size of the ML models grows, the limited communication bandwidth among the nodes does not permit communication of full-precision messages; hence, it is becoming increasingly common to require that messages be lossy, compressed versions of the local parameters. The requirement of communicating compressed messages gives rise to the important question: given a fixed communication budget, what should be our communication strategy to minimize the (training) loss as much as possible? In this article, we explore this direction, and show that in such compressed DFL settings, there are benefits to having multiple gossip steps between subsequent gradient iterations, even when the cost of doing so is appropriately accounted for, e.g., by means of reducing the precision of compressed information. In particular, we show that having ${\mathcal O}(\log \frac{1}{\epsilon })$O(log1ε) gradient iterations with constant step size - and ${\mathcal O}(\log \frac{1}{\epsilon })$O(log1ε) gossip steps between every pair of these iterations - enables convergence to within $\epsilon$ε of the optimal value for a class of non-convex problems that arise in the training of deep learning models, namely, smooth non-convex objectives satisfying Polyak-Łojasiewicz condition. Empirically, we show that our proposed scheme bridges the gap between centralized gradient descent and DFL on various machine learning tasks across different network topologies and compression operators.","1558-2183","","10.1109/TPDS.2021.3138977","National Science Foundation(grant numbers:ECCS-1809327,CCF-1564000,IIS-1546452,HDR-1934932); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664349","Federated learning;decentralized learning;communication-constrained distributed optimization;compressed communication;nonconvex optimization","Optimization;Convergence;Task analysis;Training;Signal processing algorithms;Linear programming;Collaborative work","deep learning (artificial intelligence);gradient methods;optimisation","multiple gossip steps;communication-constrained decentralized federated learning;emerging collaborative machine learning framework;predictive models;participating nodes;central server;communication bottleneck;decentralized FL methods;local communication;specific communication network;common algorithmic practice;gradient descent iterations;ML models;communication bandwidth;full-precision messages;compressed versions;local parameters;compressed messages;fixed communication budget;communication strategy;compressed DFL settings;subsequent gradient iterations;compressed information;constant step size;deep learning models;centralized gradient descent;compression operators","",1.0,"",35.0,"IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"The Supermarket Model With Known and Predicted Service Times","M. Mitzenmacher; M. Dell'Amico","School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA; University of Genoa, Genova, Italy","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2740,2751,"The supermarket model refers to a system with a large number of queues, where new customers choose $d$d queues at random and join the one with the fewest customers. This model demonstrates the power of even small amounts of choice, as compared to simply joining a queue chosen uniformly at random, for load balancing systems. In this work we perform simulation-based studies to consider variations where service times for a customer are predicted, as might be done in modern settings using machine learning techniques or related mechanisms. Our primary takeaway is that using even seemingly weak predictions of service times can yield significant benefits over blind First In First Out queueing in this context. However, some care must be taken when using predicted service time information to both choose a queue and order elements for service within a queue; while in many cases using the information for both choosing and ordering is beneficial, in many of our simulation settings we find that simply using the number of jobs to choose a queue is better when using predicted service times to order jobs in a queue. In our simulations, we evaluate both synthetic and real-world workloads–in the latter, service times are predicted by machine learning. Our results provide practical guidance for the design of real-world systems; moreover, we leave many natural theoretical open questions for future work, validating their relevance to real-world situations.","1558-2183","","10.1109/TPDS.2022.3146195","National Science Foundation(grant numbers:CCF-2101140,DMS-2023528,CCF-1563710,CCF-1535795); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695323","Supermarket model;prediction methods;scheduling;queueing analysis","Queueing analysis;Predictive models;Analytical models;Load modeling;Standards;Prediction algorithms;Machine learning","human computer interaction;learning (artificial intelligence);queueing theory;resource allocation","supermarket model;predicted service times;fewest customers;load balancing systems;seemingly weak predictions;service time information","",1.0,"",34.0,"IEEE","27 Jan 2022","","","IEEE","IEEE Journals"
"A Global Cost-Aware Container Scheduling Strategy in Cloud Data Centers","S. Long; W. Wen; Z. Li; K. Li; R. Yu; J. Zhu","School of Computer Science, Xiangtan University, Xiangtan, Hunan, China; School of Computer Science, Xiangtan University, Xiangtan, Hunan, China; School of Computer Science, Xiangtan University, Xiangtan, Hunan, China; National Supercomputing Center in Changsha, Changsha, Hunan, China; School of Automation, Guangdong University of Technology, Guangzhou, Guangdong, China; School of Automation and Electronics Information, Xiangtan University, Xiangtan, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2752,2766,"Large-scale Internet applications running on data centers are typically instantiated as a set of containers. Assigning a container to its affinity machine can reduce communication and transport costs while assigning it to the anti-affinity machine may affect the proper operation of the container. Existing container scheduling methods cannot accommodate these two types of requirements. In order to reduce the operation and maintenance cost of data centers, this article focuses on the container instance allocation problem in heterogeneous server cluster, and proposes a global cost-aware scheduling algorithm (GCCS) to solve it. The purpose is to minimize the total power consumption of the cluster from a global perspective, while trying to meet the affinity/anti-affinity requirements of applications. We study the number of containers per server selected by the application, model it as an integer linear program (ILP), and then propose a heuristic search algorithm to repair the relaxation solution of the ILP into a suboptimal feasible solution. In particular, we use Bayesian optimizer to perform a number of automated development and exploration processes for the selection of the cost coefficient. The experiments are carried out with the best cost coefficient recommended by Bayesian optimizer. Finally, the results demonstrate that GCCS can significantly reduce the total power consumption of the cluster, while maintaining a high affinity satisfaction ratio.","1558-2183","","10.1109/TPDS.2021.3133868","National Natural Science Foundation of China(grant numbers:62172350,62032020,62172349,62076214); Hunan Province Department of Education(grant numbers:21B0120); Natural Science Foundation of Hunan Province(grant numbers:2021JJ40544); Hunan Science and Technology Planning(grant numbers:2019RS3019); Hunan Provincial Natural Science Foundation of China for Distinguished Young Scholars(grant numbers:2018JJ1025); Hunan Province Science and Technology(grant numbers:2018TP1036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645367","Container scheduling;Bayesian optimization;container-based clouds;power efficiency;cost optimization","Containers;Costs;Cloud computing;Servers;Power demand;Data centers;Scheduling","cloud computing;computer centres;integer programming;Internet;linear programming;optimisation;power aware computing;power consumption;resource allocation;search problems;telecommunication scheduling","global cost-aware container scheduling strategy;cloud data centers;large-scale Internet applications;transport costs;anti-affinity machine;container scheduling methods;maintenance cost;container instance allocation problem;heterogeneous server cluster;global cost-aware scheduling algorithm;power consumption;cost coefficient;affinity satisfaction ratio;GCCS;ILP;integer linear program;heuristic search algorithm;Bayesian optimizer","","","",44.0,"IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"WAMP$^2$2S: Workload-Aware GPU Performance Model Based Pseudo-Preemptive Real-Time Scheduling for the Airborne Embedded System","Y. Yao; S. Liu; S. Wu; J. Wang; J. Ni; G. Yang; Y. Zhang","School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2767,2780,"New generation airborne embedded system has deployed Graphical Processing Units (GPUs) to raise processing capability to meet growing computational demands. Comparing with the cloud system, the airborne embedded system usually has a fixed application set, but strict real-time constraints. Unfortunately, the inherent GPU scheduler does not consider the application priority, which cannot provide the sufficient real-time capability to the airborne embedded system. To meet timeliness requirements, it is necessary to predict timing behaviors of those applications and design a real-time scheduling policy based on priority and deadline. We therefore propose WAMP$^2$2S, a workload-aware GPU performance model based pseudo-preemptive real-time scheduling algorithm for the airborne embedded system. The workload-aware GPU performance model can accurately predict the execution time of an application, which is running concurrently with other applications on GPU. The pseudo-preemptive real-time scheduling algorithm can provide the approximate preemption by dynamically adjusting GPU computing resources for active applications. Unlike previous work on GPU performance model and GPU real-time scheduling, WAMP$^2$2S considers the impact of co-executing workload on the execution time estimation and provides a software-only approach for preemption support. In addition, WAMP$^2$2S implements a prototype GPU scheduler without any source code analysis. We evaluate the proposed GPU performance model and real-time scheduling algorithm in both simulated and realistic application sets. Experimental results illustrate that WAMP$^2$2S can achieve low prediction error and high scheduling success ratio.","1558-2183","","10.1109/TPDS.2021.3134269","National Natural Science Foundation of China(grant numbers:61876151,62032018); Shanghai Pujiang Program(grant numbers:19PJ1430900); Fundamental Research Funds for the Central Universities(grant numbers:3102019DX1005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647957","Real-time scheduling;GPU performance model;embedded system;preemptive scheduling","Graphics processing units;Real-time systems;Kernel;Embedded systems;Atmospheric modeling;Predictive models;Computational modeling","embedded systems;graphics processing units;processor scheduling","workload-aware GPU performance model;new generation airborne embedded system;inherent GPU scheduler;real-time scheduling policy;pseudopreemptive real-time scheduling algorithm;graphical processing units;WAMP2S","","","",40.0,"IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"ASTRAEA: A Fair Deep Learning Scheduler for Multi-Tenant GPU Clusters","Z. Ye; P. Sun; W. Gao; T. Zhang; X. Wang; S. Yan; Y. Luo","SenseTime Research, Beijing, China; SenseTime Research, Beijing, China; S-Lab, Nanyang Technological University, Singapore; S-Lab, Nanyang Technological University, Singapore; Peng Cheng Laboratory, Shenzhen, China; SenseTime Research, Beijing, China; Peng Cheng Laboratory, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2781,2793,"Modern GPU clusters are designed to support distributed Deep Learning jobs from multiple tenants concurrently. Each tenant may have varied and dynamic resource demands. Unfortunately, existing GPU schedulers fail to thoroughly consider the fairness among the tenants and jobs, which can result in unbalanced resource allocation and unfair user experience. In this article, we present an efficient solution to provide strong fairness while maintaining high scheduling effectiveness in multi-tenant GPU clusters. First, we introduce a novel Long-Term GPU-time Fairness metric, which can comprehensively evaluate the fairness at both the tenant and job levels, based on both the temporal and spatial impacts of resource allocation. Second, we design a new and practical GPU scheduler, Astraea, to enforce the desired fairness among tenants and jobs. Large-scale evaluations show that Astraea can improve tenant fairness by up to 9.42× compared to state-of-the-art schedulers, without sacrificing the average job completion time.","1558-2183","","10.1109/TPDS.2021.3136245","National Natural Science Foundation of China(grant numbers:62032001,61672053,U1611461,62032008); RIE2020 Industry Alignment Fund - Industry Collaboration Projects; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9655467","Distributed systems;deep learning;GPU cluster scheduling","Graphics processing units;Resource management;Training;Measurement;Deep learning;Venus;Dynamic scheduling","cloud computing;graphics processing units;resource allocation;scheduling","Astraea;fair Deep Learning scheduler;multitenant GPU clusters;modern GPU clusters;multiple tenants;dynamic resource demands;GPU schedulers;unbalanced resource allocation;strong fairness;high scheduling effectiveness;novel Long-Term GPU-time Fairness metric;job levels;new GPU scheduler;practical GPU scheduler;desired fairness;tenant fairness;state-of-the-art schedulers;average job completion time","","","",55.0,"IEEE","17 Dec 2021","","","IEEE","IEEE Journals"
"MCDS: AI Augmented Workflow Scheduling in Mobile Edge Cloud Computing Systems","S. Tuli; G. Casale; N. R. Jennings","Department of Computing, Imperial College London, London, U.K.; Loughborough University, Loughborough, U.K.; Loughborough University, Loughborough, U.K.","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2794,2807,"Workflow scheduling is a long-studied problem in parallel and distributed computing (PDC), aiming to efficiently utilize compute resources to meet user's service requirements. Recently proposed scheduling methods leverage the low response times of edge computing platforms to optimize application Quality of Service (QoS). However, scheduling workflow applications in mobile edge-cloud systems is challenging due to computational heterogeneity, changing latencies of mobile devices and the volatile nature of workload resource requirements. To overcome these difficulties, it is essential, but at the same time challenging, to develop a long-sighted optimization scheme that efficiently models the QoS objectives. In this work, we propose MCDS: Monte Carlo Learning using Deep Surrogate Models to efficiently schedule workflow applications in mobile edge-cloud computing systems. MCDS is an Artificial Intelligence (AI) based scheduling approach that uses a tree-based search strategy and a deep neural network-based surrogate model to estimate the long-term QoS impact of immediate actions for robust optimization of scheduling decisions. Experiments on physical and simulated edge-cloud testbeds show that MCDS can improve over the state-of-the-art methods in terms of energy consumption, response time, SLA violations and cost by at least 6.13, 4.56, 45.09 and 30.71 percent respectively.","1558-2183","","10.1109/TPDS.2021.3135907","Imperial College London; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653818","AI for PDC;edge computing;cloud computing;deep learning;monte carlo learning;workflow scheduling","Quality of service;Task analysis;Processor scheduling;Optimization;Time factors;Optimal scheduling;Costs","cloud computing;decision making;deep learning (artificial intelligence);mobile computing;Monte Carlo methods;optimisation;quality of service;resource allocation;scheduling;search problems;trees (mathematics)","MCDS;deep neural network-based surrogate model;QoS;scheduling decisions;AI augmented workflow scheduling;mobile edge cloud computing systems;long-sighted optimization scheme;tree-based search strategy;quality of service;Monte Carlo learning using deep surrogate models","",5.0,"",56.0,"IEEE","16 Dec 2021","","","IEEE","IEEE Journals"
"Liquid: Intelligent Resource Estimation and Network-Efficient Scheduling for Deep Learning Jobs on Distributed GPU Clusters","R. Gu; Y. Chen; S. Liu; H. Dai; G. Chen; K. Zhang; Y. Che; Y. Huang","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2808,2820,"Deep learning (DL) is becoming increasingly popular in many domains, including computer vision, speech recognition, self-driving automobiles, etc. GPU can train DL models efficiently but is expensive, which motivates users to share GPU resource to reduce money costs in practice. To ensure efficient sharing among multiple users, it is necessary to develop efficient GPU resource management and scheduling solutions. However, existing ones have several shortcomings. First, they require the users to specify the job resource requirement which is usually quite inaccurate and leads to cluster resource underutilization. Second, when scheduling DL jobs, they rarely take the cluster network characteristics into consideration, resulting in low job execution performance. To overcome the above issues, we propose Liquid, an efficient GPU resource management platform for DL jobs with intelligent resource requirement estimation and scheduling. First, we propose a regression model based method for job resource requirement estimation to avoid users over-allocating computing resources. Second, we propose intelligent cluster network-efficient scheduling methods in both immediate and batch modes based on the above resource requirement estimation techniques. Third, we further propose three system-level optimizations, including pre-scheduling data transmission, fine-grained GPU sharing, and event-driven communication. Experimental results show that our Liquid can accelerate the job execution speed by 18% on average and shorten the average job completion time (JCT) by 21% compared with cutting-edge solutions. Moreover, the proposed optimization methods are effective in various scenarios.","1558-2183","","10.1109/TPDS.2021.3138825","China National Science Foundation(grant numbers:62072230); Alibaba Group; China National Science Foundation(grant numbers:U1811461,61832005,61702254); Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664375","Job scheduling;resource management;deep learning;GPU clusters","Graphics processing units;Processor scheduling;Resource management;Estimation;Liquids;Optimization;Training","deep learning (artificial intelligence);graphics processing units;optimisation;pattern clustering;regression analysis;resource allocation;scheduling","deep learning jobs;distributed GPU clusters;computer vision;cluster network characteristics;low job execution performance;intelligent resource requirement estimation;job resource requirement estimation;computing resources;intelligent cluster network-efficient scheduling methods;pre-scheduling data transmission;fine-grained GPU sharing;job execution speed;average job completion time;GPU resource management platform;regression model;system-level optimizations;event-driven communication;DL jobs scheduling","",12.0,"",37.0,"IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"GOSH: Task Scheduling Using Deep Surrogate Models in Fog Computing Environments","S. Tuli; G. Casale; N. R. Jennings","Department of Computing, Imperial College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.; Loughborough University, Loughborough, U.K.","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2821,2833,"Recently, intelligent scheduling approaches using surrogate models have been proposed to efficiently allocate volatile tasks in heterogeneous fog environments. Advances like deterministic surrogate models, deep neural networks (DNN) and gradient-based optimization allow low energy consumption and response times to be reached. However, deterministic surrogate models, which estimate objective values for optimization, do not consider the uncertainties in the distribution of the Quality of Service (QoS) objective function that can lead to high Service Level Agreement (SLA) violation rates. Moreover, the brittle nature of DNN training and the limited exploration with low agility in gradient-based optimization prevent such models from reaching minimal energy or response times. To overcome these difficulties, we present a novel scheduler that we call GOSH for Gradient Based Optimization using Second Order derivatives and Heteroscedastic Deep Surrogate Models. GOSH uses a second-order gradient based optimization approach to obtain better QoS and reduce the number of iterations to converge to a scheduling decision, subsequently lowering the scheduling time. Instead of a vanilla DNN, GOSH uses a Natural Parameter Network (NPN) to approximate objective scores. Further, a Lower Confidence Bound (LCB) optimization approach allows GOSH to find an optimal trade-off between greedy minimization of the mean latency and uncertainty reduction by employing error-based exploration. Thus, GOSH and its co-simulation based extension GOSH*, can adapt quickly and reach better objective scores than baseline methods. We show that GOSH* reaches better objective scores than GOSH, but it is suitable only for high resource availability settings, whereas GOSH is apt for limited resource settings. Real system experiments for both GOSH and GOSH* show significant improvements against the state-of-the-art in terms of energy consumption, response time and SLA violations by up to 18, 27 and 82 percent, respectively.","1558-2183","","10.1109/TPDS.2021.3136672","Imperial College London; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9656655","DL for PDC;fog computing;scheduling;heteroscedastic models;lower confidence bound;QoS optimization;second-order optimization","Quality of service;Adaptation models;Optimization;Task analysis;Uncertainty;Computational modeling;Time factors","contracts;deep learning (artificial intelligence);distributed processing;gradient methods;optimisation;quality of service;resource allocation;scheduling","energy consumption;second-order gradient based optimization approach;greedy minimization;co-simulation based extension GOSH;objective scores;response time;task scheduling;fog computing environments;intelligent scheduling approaches;deterministic surrogate models;deep neural networks;DNN;service level agreement violation rates;service objective function;SLA;quality of service;QoS;heteroscedastic deep surrogate models;natural parameter network;NPN;lower confidence bound optimization approach;LCB","",6.0,"",56.0,"IEEE","20 Dec 2021","","","IEEE","IEEE Journals"
"SGCNAX: A Scalable Graph Convolutional Neural Network Accelerator With Workload Balancing","J. Li; H. Zheng; K. Wang; A. Louri","Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA; Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA; Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA; Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2834,2845,"Convolutional Neural Networks (GCNs) have emerged as promising tools for graph-based machine learning applications. Given that GCNs are both compute- and memory-intensive, this constitutes a major challenge for the underlying hardware to efficiently process large-scale GCNs. In this article, we introduce SGCNAX, a scalable GCN accelerator architecture for the high-performance and energy-efficient acceleration of GCNs. Unlike prior GCN accelerators that either employ limited loop optimization techniques, or determine the design variables based on random sampling, we systematically explore the loop optimization techniques for GCN acceleration and propose a flexible GCN dataflow that adapts to different GCN configurations to achieve optimal efficiency. We further propose two hardware-based techniques to address the workload imbalance problem caused by the unbalanced distribution of zeros in GCNs. Specifically, SGCNAX exploits an outer-product-based computation architecture that mitigates the intra-PE (Processing Elements) workload imbalance, and employs a group-and-shuffle approach to mitigate the inter-PE workload imbalance. Simulation results show that SGCNAX performs 9.2×, 1.6× and 1.2× better, and reduces DRAM accesses by a factor of 9.7×, 2.9× and 1.2× compared to HyGCN, AWB-GCN, and GCNAX, respectively.","1558-2183","","10.1109/TPDS.2021.3133691","National Science Foundation(grant numbers:CCF-1702980,CCF-1812495,CCF-1901165,CCF-2131946); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645224","Graph convolutional neural networks;dataflow accelerators;domain-specific accelerators;memory access optimization","Random access memory;Optimization;Engines;Registers;Neural networks;Computational modeling;Accelerator architectures","AI chips;circuit optimisation;convolutional neural nets;DRAM chips;energy conservation;graph theory;hardware accelerators;learning (artificial intelligence);low-power electronics;memory architecture","AWB-GCN;convolutional neural networks;graph-based machine learning applications;GCN accelerator architecture;energy-efficient acceleration;loop optimization techniques;flexible GCN dataflow;hardware-based techniques;workload imbalance problem;outer-product-based computation architecture;SGCNAX performs;interPE workload imbalance;intraPE workload imbalance;compute-memory-intensive;scalable graph convolutional neural network accelerator;group-and-shuffle approach","","","",61.0,"IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"Bridging the Gap between Deep Learning and Frustrated Quantum Spin System for Extreme-Scale Simulations on New Generation of Sunway Supercomputer","M. Li; J. Chen; Q. Xiao; F. Wang; Q. Jiang; X. Zhao; R. Lin; H. An; X. Liang; L. He","School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Institute for Advanced Study, Tsinghua University, Beijing, China; CAS Key Lab of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China","IEEE Transactions on Parallel and Distributed Systems","26 May 2022",2022,33.0,11.0,2846,2859,"Efficient numerical methods are promising tools for delivering unique insights into the fascinating properties of physics, such as the highly frustrated quantum many-body systems. However, the computational complexity of obtaining the wave functions for accurately describing the quantum states increases exponentially with respect to particle number. Here we present a novel convolutional neural network (CNN) for simulating the two-dimensional highly frustrated spin-$1/2$   1 / 2    $J_1-J_2$    J 1  -  J 2     Heisenberg model, meanwhile the simulation is performed at an extreme scale system with low cost and high scalability. By ingenious employment of transfer learning and CNN’s translational invariance, we successfully investigate the quantum system with the lattice size up to $24\times 24$   24 × 24   , within 30 million cores of the new generation of sunway supercomputer. The final achievement demonstrates the effectiveness of CNN-based representation of quantum-state and brings the state-of-the-art record up to a brand-new level from both aspects of remarkable accuracy and unprecedented scales.","1558-2183","","10.1109/TPDS.2022.3145163","National Key Research and Development Program of China(grant numbers:2016YFB1000403); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693260","Quantum system;deep learning;new generation sunway supercomputer;spin-1/2 J1 – J2 Heisenberg model","Quantum system;Lattices;Supercomputers;Stationary state;Wave functions;Monte Carlo methods;Convolutional neural networks","computational complexity;convolutional neural nets;deep learning (artificial intelligence);Heisenberg model;parallel machines","convolutional neural network;two-dimensional highly frustrated spin;Heisenberg model;extreme scale system;transfer learning;quantum system;sunway supercomputer;CNN-based representation;quantum-state;state-of-the-art record;unprecedented scales;deep learning;frustrated quantum spin system;extreme-scale simulations;numerical methods;unique insights;quantum many-body systems;computational complexity;quantum states;particle number;CNN translational invariance","",2.0,"",47.0,"IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Heterogeneous Systolic Array Architecture for Compact CNNs Hardware Accelerators","R. Xu; S. Ma; Y. Wang; Y. Guo; D. Li; Y. Qiao","Institute of Microelectronics, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Parallel and Distributed Processing Laboratory, National University of Defense Technology, Changsha, Hunan, China; Institute of Microelectronics, National University of Defense Technology, Changsha, Hunan, China; Institute of Microelectronics, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Parallel and Distributed Processing Laboratory, National University of Defense Technology, Changsha, Hunan, China; Institute of Microelectronics, National University of Defense Technology, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2860,2871,"Compact convolutional neural networks have become a hot research topic. However, we find that the systolic array accelerators are extremely inefficient in dealing with compact models, especially when processing depthwise convolutional layers in the neural networks. To make systolic arrays more efficient for compact convolutional neural networks, we propose the heterogeneous systolic array (HeSA) architecture. It introduces heterogeneous processing elements that support multiple dataflows, which can further exploit the reuse data chance of depthwise convolutional layers and without changing the structure of the naÃ¯ve systolic array. By increasing the utilization rate of processing elements in the array, the HeSA improves the performance, throughput, and energy efficiency compared to the standard baseline. In addition, we design the flexible buffer structure for the HeSA. Through configuring it, the HeSA can allocate bandwidth flexibly to maintaining high performance and low communication cost. Based on our evaluation with typical workloads, the HeSA improves the utilization rate of the computing resource in depthwise convolutional layers by 4.5× - 11.2× and acquires 1.6 - 3.1× total performance speedup compared to the standard systolic array architecture. In the large-scale array design, the HeSA can reduce the data traffic by 40% while maintaining the same performance as the scaling-out method. By improving the on-chip data reuse opportunities and reducing data traffic, the HeSA saves over 20% in energy consumption. Meanwhile, the area of the HeSA is basically unchanged compared to the baseline due to its simple design.","1558-2183","","10.1109/TPDS.2021.3129647","NSFC(grant numbers:61802420,62025208,62172430); Natural Science Foundation of Hunan Province(grant numbers:2021JJ10052); STIP of Hunan Province(grant numbers:2019RS2027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623517","Hardware accelerator;architecture;convolutional neural network;depthwise separable convolution;systolic array","Arrays;Systolic arrays;Convolution;Computer architecture;Standards;Scalability;Kernel","convolutional neural nets;neural net architecture;systolic arrays","compact convolutional neural networks;heterogeneous systolic array architecture;HeSA;heterogeneous processing elements;reuse data chance;depthwise convolutional layers;naAve systolic array;compact CNNs hardware accelerators","",1.0,"",35.0,"CCBY","22 Nov 2021","","","IEEE","IEEE Journals"
"ReHy: A ReRAM-Based Digital/Analog Hybrid PIM Architecture for Accelerating CNN Training","H. Jin; C. Liu; H. Liu; R. Luo; J. Xu; F. Mao; X. Liao","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2872,2884,"Processing-In-Memory(PIM) has emerged as a high-performance and energy-efficient computing paradigm for accelerating convolutional neural network (CNN) applications. Resistive random access memory (ReRAM) has been widely used in PIM architectures due to its extremely high efficiency for accelerating matrix-vector multiplications through analog computing. However, because CNN training usually requires high-precision computation in the backward propagation (BP) stage, the limited precision of analog PIM accelerators impedes their adoption in CNN training. In this article, we propose ReHy, a hybrid PIM accelerator to support CNN training in ReRAM arrays. It is composed of Analog PIM (APIM) and Digital PIM (DPIM) modules. ReHy uses APIM to accelerate the feed-forward propagation (FP) stage for high performance, and DPIM to process the BP stage for high accuracy. We exploit the capability of ReRAM for Boolean logic operations to design the DPIM architecture. Particularly, we design floating-point multiplication and addition operators to support matrix multiplications in ReRAM arrays. We also propose a performance model to offload high-precision matrix multiplications to DPIM according to the data parallelism. Experimental results show that ReHy can speed up CNN training by 48.8× and 2.4×, and reduce energy consumption by 35.1× and 2.33×, compared with CPU/GPU architectures (baseline) and the state-of-the-art FloatPIM, respectively.","1558-2183","","10.1109/TPDS.2021.3138087","National Natural Science Foundation of China(grant numbers:62072198,61832006,61825202,61929103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9663036","Resistive random access memory;convolutional neural network training;digital-analog hybrid accelerator","Training;Convolutional neural networks;Computer architecture;Parallel processing;Resistance;Memristors;Arrays","convolutional neural nets;floating point arithmetic;graphics processing units;matrix multiplication;resistive RAM","convolutional neural network applications;resistive random access memory;PIM architectures;matrix-vector multiplications;analog computing;CNN training;high-precision computation;backward propagation stage;analog PIM accelerators;ReHy;hybrid PIM accelerator;ReRAM arrays;Digital PIM modules;feed-forward propagation stage;DPIM architecture;floating-point multiplication;addition operators;high-precision matrix multiplications;energy-efficient computing paradigm;processing-in-memory","",1.0,"",28.0,"CCBYNCND","24 Dec 2021","","","IEEE","IEEE Journals"
"Automatic Generation of High-Performance Convolution Kernels on ARM CPUs for Deep Learning","J. Meng; C. Zhuang; P. Chen; M. Wahib; B. Schmidt; X. Wang; H. Lan; D. Wu; M. Deng; Y. Wei; S. Feng","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; RIKEN Center for Computational Science (R-CCS), Kobe, Hyogo, Japan; RIKEN Center for Computational Science (R-CCS), Kobe, Hyogo, Japan; Institute of Computer Science, Johannes Gutenberg University Mainz, Mainz, Germany; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Tencent AI Lab, Shenzhen, Guangdong, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; Tencent AI Lab, Shenzhen, Guangdong, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; National Supercomputer Center in Shenzhen, Shenzhen, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2885,2899,"We present FastConv, a template-based code auto-generation open-source library that can automatically generate high-performance deep learning convolution kernels of arbitrary matrices/tensors shapes. FastConv is based on the Winograd algorithm, which is reportedly the highest performing algorithm for the time-consuming layers of convolutional neural networks. ARM CPUs cover a wide range of designs and specifications, from embedded devices to HPC-grade CPUs. The leads to the dilemma of how to consistently optimize Winograd-based convolution solvers for convolution layers of different shapes. FastConv addresses this problem by using templates to auto-generate multiple shapes of tuned kernels variants suitable for skinny tall matrices. As a performance portable library, FastConv transparently searches for the best combination of kernel shapes, cache tiles, scheduling of loop orders, packing strategies, access patterns, and online/offline computations. Auto-tuning is used to search the parameter configuration space for the best performance for a given target architecture and problem size. Results show 1.02x to 1.40x, 1.14x to 2.17x, and 1.22x and 2.48x speedup is achieved over NNPACK, ARM NN, and FeatherCNN on Kunpeng 920. Furthermore, performance portability experiments with various convolution shapes show that FastConv achieves 1.2x to 1.7x speedup and 2x to 22x speedup over NNPACK and ARM NN inference engine using Winograd on Kunpeng 920. CPU performance portability evaluation on VGG–16 show an average speedup over NNPACK of 1.42x, 1.21x, 1.26x, 1.37x, 2.26x, and 11.02x on Kunpeng 920, Snapdragon 835, 855, 888, Apple M1, and AWS Graviton2, respectively.","1558-2183","","10.1109/TPDS.2022.3146257","National Key Research and Development Program of China(grant numbers:2018YFB0204403); Strategic Priority CAS(grant numbers:XDB38050100); National Science Foundation of China(grant numbers:U1813203); Shenzhen Basic Research Fund(grant numbers:RCYX2020071411473419,KQTD20200820113106007,JSGG20190220164202211); CAS Key Lab(grant numbers:2011DP173015); JST, PRESTO; JPMJPR20MA; JSPS KAKENHI(grant numbers:JP21K17750); AIST Emerging Research, Japan(grant numbers:AAZ2029701B); Artificial Intelligence Initiative at Oak Ridge National Laboratory; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695263","AI;convolution;deep learning","Convolution;Program processors;Libraries;Tensors;Shape;Codes;Artificial intelligence","convolution;graphics processing units;learning (artificial intelligence);multiprocessing systems;neural nets;parallel processing;tensors","Winograd algorithm;reportedly the highest performing algorithm;time-consuming layers;convolutional neural networks;ARM CPUs;HPC-grade CPUs;Winograd-based convolution solvers;convolution layers;FastConv addresses this problem;auto-generate multiple shapes;tuned kernels;skinny tall matrices;performance portable library;kernel shapes;auto-tuning;given target architecture;Kunpeng 920;performance portability experiments;convolution shapes;1.7x speedup;ARM NN inference engine;CPU performance portability evaluation;Apple M1;automatic generation;high-performance convolution kernels;template-based code auto-generation open-source library;high-performance deep learning convolution kernels","",1.0,"",60.0,"IEEE","27 Jan 2022","","","IEEE","IEEE Journals"
"Predicting Throughput of Distributed Stochastic Gradient Descent","Z. Li; M. Paolieri; L. Golubchik; S. -H. Lin; W. Yan","Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Meta, Menlo Park, CA, USA; Department of Computer Science, University of Southern California, Los Angeles, CA, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2900,2912,"Training jobs of deep neural networks (DNNs) can be accelerated through distributed variants of stochastic gradient descent (SGD), where multiple nodes process training examples and exchange updates. The total throughput of the nodes depends not only on their computing power, but also on their networking speeds and coordination mechanism (synchronous or asynchronous, centralized or decentralized), since communication bottlenecks and stragglers can result in sublinear scaling when additional nodes are provisioned. In this paper, we propose two classes of performance models to predict throughput of distributed SGD: fine-grained models, representing many elementary computation/communication operations and their dependencies; and coarse-grained models, where SGD steps at each node are represented as a sequence of high-level phases without parallelism between computation and communication. Using a PyTorch implementation, real-world DNN models and different cloud environments, our experimental evaluation illustrates that, while fine-grained models are more accurate and can be easily adapted to new variants of distributed SGD, coarse-grained models can provide similarly accurate predictions when augmented with ad hoc heuristics, and their parameters can be estimated with profiling information that is easier to collect.","1558-2183","","10.1109/TPDS.2022.3151739","National Science Foundation(grant numbers:CCF-1763747,CNS-1816887); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714870","Distributed machine learning;stochastic gradient descent;performance prediction;scalability;PyTorch","Computational modeling;Predictive models;Training;Throughput;Servers;Computer architecture;Uplink","gradient methods;learning (artificial intelligence);neural nets;stochastic processes","distributed stochastic gradient descent;training jobs;deep neural networks;DNN;distributed variants;multiple nodes;exchange updates;computing power;networking speeds;coordination mechanism;communication bottlenecks;stragglers;sublinear scaling;performance models;distributed SGD;fine-grained models;coarse-grained models;SGD steps;high-level phases;real-world DNN models;similarly accurate predictions;PyTorch implementation;profiling information","","","",37.0,"IEEE","16 Feb 2022","","","IEEE","IEEE Journals"
"Building High-Throughput Neural Architecture Search Workflows via a Decoupled Fitness Prediction Engine","A. Keller Rorabaugh; S. Caíno-Lores; T. Johnston; M. Taufer","University of Tennessee at Knoxville, Knoxville, TN, USA; University of Tennessee at Knoxville, Knoxville, TN, USA; Striveworks, Austin, TX, USA; University of Tennessee at Knoxville, Knoxville, TN, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2913,2926,"Neural networks (NN) are used in high-performance computing and high-throughput analysis to extract knowledge from datasets. Neural architecture search (NAS) automates NN design by generating, training, and analyzing thousands of NNs. However, NAS requires massive computational power for NN training. To address challenges of efficiency and scalability, we propose PENGUIN, a decoupled fitness prediction engine that informs the search without interfering in it. PENGUIN uses parametric modeling to predict fitness of NNs. Existing NAS methods and parametric modeling functions can be plugged into PENGUIN to build flexible NAS workflows. Through this decoupling and flexible parametric modeling, PENGUIN reduces training costs: it predicts the fitness of NNs, enabling NAS to terminate training NNs early. Early termination increases the number of NNs that fixed compute resources can evaluate, thus giving NAS additional opportunity to find better NNs. We assess the effectiveness of our engine on 6,000 NNs across three diverse benchmark datasets and three state of the art NAS implementations using the Summit supercomputer. Augmenting these NAS implementations with PENGUIN can increase throughput by a factor of 1.6 to 7.1. Furthermore, walltime tests indicate that PENGUIN can reduce training time by a factor of 2.5 to 5.3.","1558-2183","","10.1109/TPDS.2022.3140681","National Science Foundation(grant numbers:1741057,1740990,1741040,1841758); Joint Directed Research Development; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9674227","Machine learning;artificial intelligence;performance prediction;neural networks","Training;Artificial neural networks;Predictive models;Parametric statistics;Engines;Search problems;Data models","neural net architecture;parallel machines","high-throughput neural architecture search;decoupled fitness prediction engine;neural networks;high-performance computing;high-throughput analysis;NN design;massive computational power;NN training;PENGUIN;parametric modeling functions;flexible NAS workflows;flexible parametric modeling;training costs;training NNs;fixed compute resources;NAS additional opportunity;training time;Summit supercomputer","",3.0,"",55.0,"CCBY","7 Jan 2022","","","IEEE","IEEE Journals"
"Lightweight and Accurate DNN-Based Anomaly Detection at Edge","Q. Zhang; R. Han; G. Xin; C. H. Liu; G. Wang; L. Y. Chen","Beijing Institute of Technology, Beijing, P.R. China; Beijing Institute of Technology, Beijing, P.R. China; Beijing Institute of Technology, Beijing, P.R. China; Beijing Institute of Technology, Beijing, P.R. China; Beijing Institute of Technology, Beijing, P.R. China; TU Delft, Delft, The Netherlands","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2927,2942,"Deep neural networks (DNNs) have been showing significant success in various anomaly detection applications such as smart surveillance and industrial quality control. It is increasingly important to detect anomalies directly on edge devices, because of high responsiveness requirements and tight latency constraints. The accuracy of DNN-based solutions rely on large model capacity and thus long training and inference time, making them inapplicable on resource strenuous edge devices. It is hence imperative to scale DNN model sizes in correspondence to the run-time system requirements, i.e., meeting deadlines with minimal accuracy losses, which are highly dependent on the platforms and real-time system status. Existing scaling techniques either take long training time to pre-generate scaling options or disturb the unsteady training process of anomaly detection DNNs, lacking the adaptability to heterogeneous edge systems and incurring low inference accuracies. In this article, we present LightDNN to scale DNN models for anomaly detection applications at edge, featuring high detection accuracies with lightweight training and inference time. To this end, LightDNN quickly extracts and compresses blocks in a DNN, and provides large scaling space (e.g., 1 million options) by dynamically combining these compressed blocks online. At run-time, LightDNN predicts the DNN’s inference latency according to the monitored system status, and optimizes the combination of blocks to maximize its accuracy under deadline constraints. We implement and extensively evaluate LightDNN on both CPU and GPU edge platforms using 8 popular anomaly detection workloads. Comparative experiments with state-of-the-art methods show that our approach provides 145.8 to 0.56 trillion times more scaling options without increasing training and inference overheads, thus achieving as much as 15.05% increase in accuracy under the same deadlines.","1558-2183","","10.1109/TPDS.2021.3137631","National Natural Science Foundation of China(grant numbers:61872337,62132019); National Research and Development Program of China(grant numbers:2019YQ1700); Swiss National Science Foundation NRP75(grant numbers:407540_167266); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665270","Anomaly detection;edge inference;DNN;model scaling;predictable latency","Training;Anomaly detection;Videos;Computational modeling;Predictive models;Image edge detection;Graphics processing units","cloud computing;data handling;deep learning (artificial intelligence);graphics processing units;object detection","deep neural networks;anomaly detection applications;resource strenuous edge devices;DNN model;run-time system requirements;heterogeneous edge systems;pre-generate scaling options;CPU edge platforms;GPU edge platforms","","","",58.0,"IEEE","29 Dec 2021","","","IEEE","IEEE Journals"
"Dynamic GPU Energy Optimization for Machine Learning Training Workloads","F. Wang; W. Zhang; S. Lai; M. Hao; Z. Wang","School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Computing, University of Leeds, Leeds, U.K.","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2943,2954,"GPUs are widely used to accelerate the training of machine learning workloads. As modern machine learning models become increasingly larger, they require a longer time to train, leading to higher GPU energy consumption. This paper presents GPOEO, an online GPU energy optimization framework for machine learning training workloads. GPOEO dynamically determines the optimal energy configuration by employing novel techniques for online measurement, multi-objective prediction modeling, and search optimization. To characterize the target workload behavior, GPOEO utilizes GPU performance counters. To reduce the performance counter profiling overhead, it uses an analytical model to detect the training iteration change and only collects performance counter data when an iteration shift is detected. GPOEO employs multi-objective models based on gradient boosting and a local search algorithm to find a trade-off between execution time and energy consumption. We evaluate the GPOEO by applying it to 71 machine learning workloads from two AI benchmark suites running on an NVIDIA RTX3080Ti GPU. Compared with the NVIDIA default scheduling strategy, GPOEO delivers a mean energy saving of 16.2% with a modest average execution time increase of 5.1%.","1558-2183","","10.1109/TPDS.2021.3137867","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101360001); National Key Research and Development Program of China(grant numbers:2020YFB1406902); Shenzhen Science and Technology Research and Development Foundation(grant numbers:JCYJ20190806143418198); National Natural Science Foundation of China(grant numbers:61872110,61872294); Fundamental Research Funds for the Central Universities(grant numbers:HIT.OCEF.2021007); Peng Cheng Laboratory Project(grant numbers:PCL2021A02); CCF-Huawei(grant numbers:CCF-HuaweiHP2021002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661449","Dynamic energy optimization;online application iteration detection;multi-objective machine learning;GPU","Clocks;Graphics processing units;Optimization;Training;Micromechanical devices;Machine learning;Predictive models","computer graphic equipment;graphics processing units;learning (artificial intelligence);multiprocessing systems;optimisation;performance evaluation;power aware computing;processor scheduling;search problems","GPOEO;multiobjective models;machine learning workloads;NVIDIA RTX3080Ti GPU;mean energy saving;modest average execution time increase;dynamic GPU energy optimization;machine learning training workloads;modern machine learning models;GPU energy consumption;online GPU energy optimization framework;optimal energy configuration;multiobjective prediction modeling;search optimization;target workload behavior;GPU performance counters;performance counter profiling;analytical model","",3.0,"",25.0,"IEEE","23 Dec 2021","","","IEEE","IEEE Journals"
"FarSpot: Optimizing Monetary Cost for HPC Applications in the Cloud Spot Market","A. C. Zhou; J. Lao; Z. Ke; Y. Wang; R. Mao","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; Shenzhen Institute of Computing Sciences, Shenzhen University, Shenzhen, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2955,2967,"Recently, we have witnessed many HPC applications developed and hosted in the cloud, which can benefit from the elastic and diversified resources on the cloud, while on the other hand confronting high costs for executing the long-running HPC applications. Although public clouds such as Amazon EC2 offer spot instances with dynamic and usually low prices compared to on-demand ones, the spot prices can vary significantly and sometimes can even be more expensive than on-demand prices of the same type. Previous work on reducing the monetary cost for HPC applications using spot instances focused on designing fault tolerance techniques or selecting appropriate instance types/bid prices to make good usage of the low spot prices. However, with the recent update of spot pricing model on Amazon EC2, these work may become either inefficient or invalid. In this article, we present FarSpot which is an optimization framework for HPC applications in the latest cloud spot market with the goal of minimizing application cost while ensuring performance constraints. FarSpot provides accurate long-term price prediction for a wide range of spot instance types using ensemble-based learning method. It further incorporates a cost-aware deadline assignment algorithm to distribute application deadline to each task according to spot price changes. With the assigned subdeadline of each task, FarSpot dynamically migrates tasks among spot instances to reduce execution cost. Evaluation results using real HPC benchmark show that 1) the prediction error of FarSpot is very low (below 3%), 2) FarSpot reduced the monetary cost by 32% on average compared to state-of-the-art algorithms, and 3) FarSpot satisfies the user-specified deadline constraints at all time.","1558-2183","","10.1109/TPDS.2021.3134644","National Natural Science Foundation of China(grant numbers:62172282,61802260,62072311,61972259,62122056,U2001212); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2020B1515120028,2019B151502055); Guangdong NSF(grant numbers:2019A1515012053); Shenzhen Science and Technology Foundation(grant numbers:JCYJ20210324094402008,JCYJ20210324093212034); Tencent; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9648022","Cloud computing;spot market;price prediction;ensemble models","Costs;Predictive models;Pricing;Task analysis;Cloud computing;Prediction algorithms;Fault tolerant systems","cloud computing;fault tolerance;learning (artificial intelligence);optimisation;parallel processing;power markets;pricing;virtual machines","FarSpot;optimizing monetary cost;HPC applications;public clouds;Amazon EC2 offer spot instances;dynamic prices;usually low prices;on-demand prices;fault tolerance techniques;low spot prices;spot pricing model;latest cloud spot market;minimizing application cost;long-term price prediction;spot instance types;cost-aware deadline assignment;application deadline;spot price changes;execution cost","","","",42.0,"IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"Microservice Deployment in Edge Computing Based on Deep Q Learning","W. Lv; Q. Wang; P. Yang; Y. Ding; B. Yi; Z. Wang; C. Lin","School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2968,2978,"The microservice deployment strategy is promising in reducing the overall service response time in the microservice-oriented edge computing platform. However, existing works ignore the effect of different interaction frequencies among microservices and the decrease in service execution performance caused by the increased node loads. In this article, we first model the invocation relationships among microservices as an undirected and weighted interaction graph to characterize the communication overhead. Then, we propose a multi-objective microservice deployment problem (MMDP) in edge computing. MMDP aims to minimize the communication overhead while achieving load balance between edge nodes. Without the requirement for domain experts, we propose Reward Sharing Deep Q Learning (RSDQL), a learning-based algorithm, to solve MMDP and obtain the optimal deployment strategy. In addition, to improve the scalability of the services, we propose an Elastic Scaling algorithm (ES) based on heuristics to deal with the dynamic pressure of requests. Finally, we conduct a series of experiments in Kubernetes to evaluate the performance of our approach. Experimental results indicate that, compared with interaction-aware strategy and Kubernetes default strategy, RSDQL has shorter response times, more balanced resource loads, and makes services scale elastically according to the request pressure.","1558-2183","","10.1109/TPDS.2022.3150311","National Natural Science Foundation of China(grant numbers:61972302,61962019); Shaanxi Key Technology R&D Program(grant numbers:2021ZDLGY07-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712168","Microservice;interaction awareness;load balancing;multi-objective model;deep Q learning;elastic scaling","Microservice architectures;Containers;Load modeling;Edge computing;Time factors;Load management;Scalability","distributed processing;graph theory;learning (artificial intelligence);minimisation;resource allocation","microservices;undirected interaction graph;weighted interaction graph;communication overhead;multiobjective microservice deployment problem;MMDP;load balance;edge nodes;Reward Sharing Deep Q;learning-based algorithm;optimal deployment strategy;Elastic Scaling algorithm;interaction-aware strategy;Kubernetes default strategy;shorter response times;balanced resource loads;services scale;Deep Q Learning;microservice deployment strategy;service response time;microservice-oriented edge computing platform;different interaction frequencies;service execution performance;increased node loads;invocation relationships","",1.0,"",49.0,"IEEE","11 Feb 2022","","","IEEE","IEEE Journals"
"An Efficient Parallel Reinforcement Learning Approach to Cross-Layer Defense Mechanism in Industrial Control Systems","K. Zhong; Z. Yang; G. Xiao; X. Li; W. Yang; K. Li","College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Hunan Province Key Laboratory of Industrial Internet Technology and Security, Changsha University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2979,2990,"The ongoing digitalization enables stable control processes and smooth operations of Industrial Control Systems (ICSs). A direct consequence of the highly interconnected architecture of ICSs is the introduced cyber vulnerability and increasing cyber security threats to ICSs. Numerous researches pay attention to the security problem of ICSs. However, most current studies face two challenges. First, the interaction problem between the cyber layer and the physical layer of ICSs may result in incorrect attack response strategies. Second, ICSs are real-time systems, but existing defense decision algorithms based on game theory or reinforcement learning techniques have high computational complexity, which prevents them from making decisions quickly. In this paper, we design a new multi-attribute based method for quantifying rewards and propose a multi-attribute based Q-learning algorithm to resolve the interaction problem. In addition, to overcome the limitation of slow convergence, we develop an effective parallel Q-learning (PQL) algorithm to quickly find the optimal strategy. The experimental results show the effectiveness of the PQL algorithm. Compared with the Q-learning algorithm (QL) and the deep Q-network (DQN) algorithm, our proposed solution can reduce the average completion time by 12.5 to 37 percent.","1558-2183","","10.1109/TPDS.2021.3135412","NSFC(grant numbers:61772182,62172146,62172157,61802032,61802444); NSFC(grant numbers:61661146006); Hunan Province Key Laboratory of Industrial Internet Technology and Security(grant numbers:2019TP1011); National Key Research and Development Program of China(grant numbers:2021YFF0901001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650577","Industrial control system (ICS);interaction;multiple attributes;parallel q-learning;stochastic game","Games;Q-learning;Security;Integrated circuit modeling;Process control;Physical layer;Stochastic processes","control engineering computing;industrial control;production engineering computing;reinforcement learning;security of data","parallel reinforcement learning;cross-layer defense mechanism;industrial control systems;ICSs;cyber vulnerability;cyber layer;physical layer;parallel Q-learning;cyber security threats;multi-attribute based method","",2.0,"",42.0,"IEEE","14 Dec 2021","","","IEEE","IEEE Journals"
"Adaptive DRL-Based Virtual Machine Consolidation in Energy-Efficient Cloud Data Center","J. Zeng; D. Ding; K. Kang; H. Xie; Q. Yin","School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Lab of Traffic Data Analysis and Mining, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,2991,3002,"The dramatic increasing of data and demands for computing capabilities may result in excessive use of resources in cloud data centers, which not only causes the raising of energy consumption, but also leads to the violation of Service Level Agreement (SLA). Dynamic consolidation of virtual machines (VMs) is proven to be an efficient way to tackle this issue. In this paper, we present an Adaptive Deep Reinforcement Learning (DRL)-based Virtual Machine Consolidation (ADVMC) framework for energy-efficient cloud data centers. ADVMC has two phases. In the first phase, Influence Coefficient is introduced to measure the impact of a VM on producing host overload, and a dynamic Influence Coefficient-based VM selection algorithm (ICVMS) is proposed to preferentially choose those VMs with the greatest impact for migration in order to remove the excessive workloads of the overloaded host quickly and accurately. In the second phase, a Prediction Aware DRL-based VM placement method (PADRL) is further proposed to automatically find suitable hosts for VMs to be migrated, in which a state prediction network is designed based on LSTM to provide DRL-based model more reasonable environment states so as to accelerate the convergence of DRL. Simulation experiments on the real-world workload provided by Google Cluster Trace have shown that our ADVMC approach can largely cut down system energy consumption and reduce SLA violation of users as compared to many other VM consolidation policies.","1558-2183","","10.1109/TPDS.2022.3147851","R&D Program of Beijing Municipal Education Commission(grant numbers:KJZD20191000402); Beijing Natural Science Foundation(grant numbers:L211015); Fundamental Research Funds for the Central Universities(grant numbers:2019JBM025,2019JBZ104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698981","Cloud computing;vm consolidation;energy efficient;influence coefficient;deep reinforcement learning","Cloud computing;Energy consumption;Data centers;Heuristic algorithms;Resource management;Predictive models;Costs","cloud computing;computer centres;learning (artificial intelligence);power aware computing;virtual machines","ADVMC;dynamic Influence Coefficient-based VM selection algorithm;Prediction Aware DRL-based VM placement method;DRL-based model;system energy consumption;VM consolidation policies;Adaptive DRL-based Virtual Machine Consolidation;energy-efficient cloud data center;cloud data centers;dynamic consolidation;virtual machines;Adaptive Deep Reinforcement","",3.0,"",42.0,"IEEE","1 Feb 2022","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning Enhanced Greedy Optimization for Online Scheduling of Batched Tasks in Cloud HPC Systems","Y. Yang; H. Shen","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3003,3014,"In a large cloud data center HPC system, a critical problem is how to allocate the submitted tasks to heterogeneous servers that will achieve the goal of maximizing the system’s gain defined as the value of completed tasks minus system operation costs. We consider this problem in the online setting that tasks arrive in batches and propose a novel deep reinforcement learning (DRL) enhanced greedy optimization algorithm of two-stage scheduling interacting task sequencing and task allocation. For task sequencing, we deploy a DRL module to predict the best allocation sequence for each arriving batch of tasks based on the knowledge (allocation strategies) learnt from previous batches. For task allocation, we propose a greedy strategy that allocates tasks to servers one by one online following the allocation sequence to maximize the total gain increase. We show that our greedy strategy has a performance guarantee of competitive ratio $\frac{1}{1+\kappa }$11+κ to the optimal offline solution, which improves the existing result for the same problem, where $\kappa$κ is upper bounded by the maximum cost-to-gain ratio of each task. While our DRL module enhances the greedy algorithm by providing the likely-optimal allocation sequence for each batch of arriving tasks, our greedy strategy bounds DRL’s prediction error within a proven worst-case performance guarantee for any allocation sequence. It enables a better solution quality than that obtainable from both DRL and greedy optimization alone. Extensive experiment evaluation results in both simulation and real application environments demonstrate the effectiveness and efficiency of our proposed algorithm. Compared with the state-of-the-art baselines, our algorithm increases the system gain by about 10% to 30%. Our algorithm provides an interesting example of combining machine learning (ML) and greedy optimization techniques to improve ML-based solutions with a worst-case performance guarantee for solving hard optimization problems.","1558-2183","","10.1109/TPDS.2021.3138459","Key-Area Research and Development Plan of Guangdong Province(grant numbers:#2020B010164003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664254","Task scheduling;deep reinforcement learning;greedy optimization;approximation algorithm","Task analysis;Servers;Costs;Resource management;Processor scheduling;Optimization;Approximation algorithms","cloud computing;computational complexity;computer centres;deep learning (artificial intelligence);greedy algorithms;optimisation;parallel processing;reinforcement learning;resource allocation;scheduling","deep reinforcement learning;online scheduling;batched tasks;cloud data center HPC system;completed tasks minus system operation costs;greedy optimization algorithm;two-stage scheduling interacting task sequencing;task allocation;DRL module;arriving batch;greedy strategy;total gain increase;optimal offline solution;maximum cost-to-gain ratio;greedy algorithm;likely-optimal allocation sequence;arriving tasks;worst-case performance guarantee;system gain;hard optimization problems;DRL prediction error","","","",30.0,"IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"G-SLIDE: A GPU-Based Sub-Linear Deep Learning Engine via LSH Sparsification","Z. Pan; F. Zhang; H. Li; C. Zhang; X. Du; D. Deng","School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; Computer Science Department, Rutgers University, New Brunswick, NJ, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3015,3027,"Deep learning has been one of the trendiest research topics. However, as data quantities rise exponentially, training large neural networks can become prohibitively expensive with billions of parameters. Fortunately, recent research has discovered that not all of the computations in traditional network training are necessary. By selectively sparsifying the majority of the neurons during training, we can still obtain acceptable accuracy. SLIDE, a C++ OpenMP-based sub-linear deep learning engine, has been developed in this situation. SLIDE uses the algorithm of locality sensitive hashing (LSH) to query neurons with high activation in sub-linear time. It achieves a remarkable speedup in training large fully-connected networks by making use of the network sparsity as well as multi-core parallelism. However, SLIDE is limited to CPUs, ignoring the popular GPU devices with greater parallel potential and computational capability. In this article, we propose G-SLIDE, a GPU-based sub-linear deep learning engine, which combines the benefits of SLIDE’s adaptive sparsification algorithms with GPUs’ high performance. The main challenges in developing G-SLIDE are efficiently using LSH to sparsify networks and training the special sparse neural networks on the GPU. To address these challenges, we propose several novel solutions, such as specific data formats and appropriate workload partitioning for threads to fully utilize the GPU resources. We evaluate G-SLIDE on two extremely sparse datasets with a 2080 Ti GPU, and the results demonstrate that for the time of one training epoch, G-SLIDE can achieve more than 16.4× speedup over SLIDE on a 32-core/64-thread CPU. Furthermore, on the same platform, G-SLIDE can earn an average of 16.2× speedup over TensorFlow-GPU and 30.8× speedup over TensorFlow-CPU.","1558-2183","","10.1109/TPDS.2021.3132493","National Key Research and Development Program of China(grant numbers:2018YFB1004401); National Natural Science Foundation of China(grant numbers:61732014,62172419,61802412); GHfund A(grant numbers:20210701); Alibaba Innovative Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635657","GPU;machine learning system;adaptive sparsity;sparse neural network;LSH","Graphics processing units;Training;Deep learning;Neurons;Biological neural networks;Engines;Message systems","C++ language;deep learning (artificial intelligence);graphics processing units;multiprocessing systems;parallel architectures","LSH sparsification;trendiest research topics;traditional network training;sub-linear time;fully-connected networks;network sparsity;popular GPU devices;G-SLIDE;special sparse neural networks;training epoch;SLIDE adaptive sparsification algorithms;GPU-based sublinear deep learning engine;OpenMP-based sublinear deep learning engine","",1.0,"",51.0,"IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"Scalable Unsupervised ML: Latency Hiding in Distributed Sparse Tensor Decomposition","N. Abubaker; M. O. Karsavuran; C. Aykanat","Department of Computer Engineering, Bilkent University, Ankara, Turkey; Department of Computer Engineering, Bilkent University, Ankara, Turkey; Department of Computer Engineering, Bilkent University, Ankara, Turkey","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3028,3040,"Latency overhead in distributed-memory parallel CPD-ALS scales with the number of processors, limiting the scalability of computing CPD of large irregularly sparse tensors. This overhead comes in the form of sparse reduce and expand operations performed on factor-matrix rows via point-to-point messages. We propose to hide the latency overhead through embedding all of the point-to-point messages incurred by the sparse reduce and expand into dense collective operations which already exist in the CPD-ALS. The conventional parallel CPD-ALS algorithm is not amenable for embedding so we propose a computation/communication rearrangement to enable the embedding. We embed the sparse expand and reduce into a hypercube-based ALL-REDUCE operation to limit the latency overhead to $O(\log _2 K)$O(log2K) for a $K$K-processor system. The embedding comes with the cost of increased bandwidth overhead due to the multi-hop routing of factor-matrix rows during the embedded-ALL-REDUCE. We propose an embedding scheme that takes advantage of the expand/reduce properties to reduce this overhead. Furthermore, we propose a novel recursive bipartitioning framework that enables simultaneous hypergraph partitioning and subhypergraph-to-subhypercube mapping to achieve subtensor-to-processor assignment with the objective of reducing the bandwidth overhead during the embedded-ALL-REDUCE. We also propose a bin-packing-based algorithm for factor-matrix row to processor assignment aiming at reducing processors’ maximum send and receive volumes during the embedded-ALL-REDUCE. Experiments on up to 4096 processors show that the proposed framework scales significantly better than the state-of-the-art point-to-point method.","1558-2183","","10.1109/TPDS.2021.3128827","Scientific and Technological Research Council of Turkey(grant numbers:EEEAG-116E043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9618826","Sparse tensor;tensor decomposition;CANDECOMP/PARAFAC;canonical polyadic decomposition;latency hiding;embedded communication;communication cost;concurrent communication;recursive bipartitioning;hypergraph partitioning","Tensors;Program processors;Sparse matrices;Routing;Tools;Matrix decomposition;Costs","graph theory;hypercube networks;microprocessor chips;tensors;unsupervised learning","scalable unsupervised ML;distributed sparse tensor decomposition;distributed-memory parallel CPD-ALS scales;irregularly sparse tensors;factor-matrix row;point-to-point messages;latency overhead;dense collective operations;conventional parallel CPD-ALS algorithm;hypercube-based ALL-REDUCE operation;processor system;increased bandwidth overhead;embedded-ALL-REDUCE;embedding scheme;subtensor-to-processor assignment;state-of-the-art point-to-point method","",1.0,"",40.0,"IEEE","17 Nov 2021","","","IEEE","IEEE Journals"
"Accelerating Large Sparse Neural Network Inference Using GPU Task Graph Parallelism","D. -L. Lin; T. -W. Huang","Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA; Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3041,3052,"The ever-increasing size of modern deep neural network (DNN) architectures has put increasing strain on the hardware needed to implement them. Sparsified DNNs can greatly reduce memory costs and increase throughput over standard DNNs, if the loss of accuracy can be adequately controlled. However, sparse DNNs present unique computational challenges. Efficient model or data parallelism algorithms are extremely hard to design and implement. The recent effort MIT/IEEE/Amazon HPEC Graph Challenge has drawn attention to high-performance inference methods for large sparse DNNs. In this article, we introduce SNIG, an efficient inference engine for large sparse DNNs. SNIG develops highly optimized inference kernels and leverages the power of CUDA Graphs to enable efficient decomposition of model and data parallelisms. Our decomposition strategy is flexible and scalable to different partitions of data volumes, model sizes, and GPU numbers. We have evaluated SNIG on the official benchmarks of HPEC Sparse DNN Challenge and demonstrated its promising performance scalable from a single GPU to multiple GPUs. Compared to the champion of the 2019 HPEC Sparse DNN Challenge, SNIG can finish all inference workloads using only a single GPU. At the largest DNN, which has more than 4 billion parameters across 1920 layers each of 65536 neurons, SNIG is up to 2.3× faster than a state-of-the-art baseline under a machine of 4 GPUs. SNIG receives the Champion Award in 2020 HPEC Sparse DNN Challenge.","1558-2183","","10.1109/TPDS.2021.3138856","National Science Foundation(grant numbers:CCF-2126672); NumFOCUS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664223","Task graph parallelism","Graphics processing units;Kernel;Task analysis;Parallel processing;Programming;Neurons;Data models","deep learning (artificial intelligence);Gaussian processes;graph theory;graphics processing units;inference mechanisms;parallel algorithms;parallel architectures","sparse neural network inference;modern deep neural network architectures;unique computational challenges;data parallelism algorithms;high-performance inference methods;SNIG;efficient inference engine;highly optimized inference kernels;CUDA graphs;efficient decomposition;data parallelisms;model sizes;single GPU;inference workloads;2020 HPEC sparse DNN challenge;2019 HPEC sparse DNN challenge;GPU task graph parallelism","",1.0,"",29.0,"IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"MIPD: An Adaptive Gradient Sparsification Framework for Distributed DNNs Training","Z. Zhang; C. Wang","Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, Hong Kong; Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3053,3066,"Asynchronous training based on the parameter server architecture is widely used for scaling up the DNN training over large datasets and DNN models. Communication has been identified as the major bottleneck when deploying the DNN training over the large-scale distributed deep learning systems. Recent studies try to reduce the communication traffic through gradient sparsification and quantization approaches. We identify three limitations in previous studies. First, the fundamental guideline for gradient sparsification of their work is the magnitude of the gradient. However, the gradients’ magnitude represents the current optimization direction while it cannot indicate the significance of the parameters, which potentially results in delayed updating for the significant parameters. Second, their gradient quantization methods based on the entire model often lead to error accumulation for gradients aggregation since the gradients from different layers of the DNN model follow different distributions. Third, previous quantization approaches are CPU intensive, which generates strong overhead for the server. We propose MIPD, an adaptive and layer-wise gradient sparsification framework that compresses the gradients based on model interpretability and probability distribution of gradients. MIPD compresses the gradients according to the corresponding significance of its parameters, which is defined by model interpretability. An Exponential Smoothing method is also proposed to compensate for the dropped gradients on the server to reduce the gradients error. MIPD proposes to update half of the parameters for each training step to reduce the CPU overhead of the server. It encodes the gradients based on their probability distribution, thereby minimizing the approximated errors. Extensive experimental results generated on the GPU cluster indicate that the proposed framework effectively improves the training performance of DNNs by up to 36.2%, which ensures high accuracy as compared to state-of-art solutions. Accordingly, the CPU and network usage of the server dropped by up to 42.0% and 32.7% respectively.","1558-2183","","10.1109/TPDS.2022.3154387","Hong Kong RGC Collaborative Research Fund(grant numbers:C5026-18G); Research Impact Fund(grant numbers:R5060-19); RGC Collaborative Research Fund(grant numbers:C6021-19EF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721697","Exponential smoothing prediction;gradients sparsification;model interpretability;probability distribution;quantization","Training;Servers;Quantization (signal);Convergence;Probability distribution;Degradation;Adaptation models","deep learning (artificial intelligence);gradient methods;optimisation;smoothing methods;statistical distributions","MIPD;adaptive gradient sparsification;distributed DNNs training;asynchronous training;parameter server architecture;DNN model;large-scale distributed deep learning;gradient quantization;model interpretability;dropped gradients;gradients error;training step;layer-wise gradient sparsification;Exponential Smoothing method","",1.0,"",35.0,"IEEE","25 Feb 2022","","","IEEE","IEEE Journals"
"$TC-Stream$TC-Stream: Large-Scale Graph Triangle Counting on a Single Machine Using GPUs","J. Huang; H. Wang; X. Fei; X. Wang; W. Chen","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Technology and Appications, Qinghai University, Qinghai, China; Department of Information Technology Center, Qinghai University, Qinghai, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3067,3078,"In this paper, we build a $TC$TC-$Stream$Stream, a high-performance graph processing system specific for a triangle counting algorithm on graph data with up to tens of billions of edges, which significantly exceeds the device memory capacity of Graphics Processing Units (GPUs). The triangle counting problem is a broad research topic in data mining and social network analysis in the graph processing field. As the scale of the graph data grows, a portion of the graph data must be loaded iteratively. In the existing literature, graphs with billions of edges need to be done distributively, which is cost-intensive. Also, many disk-based triangle counting systems are proposed for CPU architectures, but their tackling performances are inefficient. To solve the above problem, we propose $TC$TC-$Stream$Stream, and it focuses on three issues: 1) For power-law graphs, because the amount of tasks of each vertex or edge is inconsistent, it is bound to cause different demands of computing and memory resources for different task types. We propose a parallel vertex approach and the reordering of vertices for graph data that can be placed in the GPU device memory to ensure the maximum workload balancing; 2) A binary-search-based set intersection method is designed to achieve the maximum parallelism in GPU; 3) For the graph data that exceeds the GPU device memory capacity, we develop a novel vertical partition algorithm to guarantee the independent computing on each partition so that the three computation processes, i.e., the computation on GPU, the data transmission between main memory of CPU and SSD, and the communication between the CPU and the GPU can be perfectly overlapped. Moreover, the $TC$TC-$Stream$Stream optimizes edge-iterator models and benefits from multi-thread parallelism. Extensive experiments conducted on large-scale datasets showed that the $TC$TC-$stream$stream running on a single Tesla V100 GPU performs $2.4-6\times$2.4-6× and $1.8-4.4\times$1.8-4.4× faster than the state-of-the-art single-machine in-memory triangle counting system and GPU-based triangle counting system, respectively, and achieves $2.4\times$2.4× faster than the state-of-the-art out-of-core distributed system PDTL running on an 8-node cluster when processing the graph data with 42.5 billion edges, which demonstrates the high performance and cost-effectiveness of the $TC$TC-$Stream$Stream.","1558-2183","","10.1109/TPDS.2021.3135329","National Natural Science Foundation of China(grant numbers:62062059,62162053,61762074); National Natural Science Foundation of Qinghai Province(grant numbers:2019-ZJ-7034); Qinghai University(grant numbers:2020-ZZ-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650723","Triangle counting;vertical partition;out-of-core;GPU;parallel processing","Graphics processing units;Instruction sets;Computer architecture;Parallel processing;Task analysis;Partitioning algorithms;Data mining","computational complexity;computer graphic equipment;coprocessors;data mining;graph theory;graphics processing units;iterative methods;multi-threading;parallel processing","graph data;large-scale graph triangle counting;high-performance graph processing system;triangle counting problem;data mining;social network analysis;graph processing field;disk-based triangle counting systems;power-law graphs;GPU device memory capacity;state-of-the-art single-machine in-memory triangle;GPU-based triangle counting system","","","",73.0,"IEEE","14 Dec 2021","","","IEEE","IEEE Journals"
"Secure Deep Neural Network Models Publishing Against Membership Inference Attacks Via Training Task Parallelism","Y. Mao; W. Hong; B. Zhu; Z. Zhu; Y. Zhang; S. Zhong","Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3079,3091,"Vast data and computing resources are commonly needed to train deep neural networks, causing an unaffordable price for individual users. Motivated by the increasing demands of deep learning applications, sharing well-trained models becomes popular. The owner of a pre-trained model can share it by publishing the model directly or providing a prediction interface. Either way, individual users can benefit from deep learning without much cost, and computing resources can be saved. However, recent studies of machine learning security have identified severe threats to these model publishing approaches. This article will focus on the privacy leakage issue of publishing well-trained deep neural network models. To tackle this problem, we propose a series of secure model publishing solutions based on training task parallelism. Specifically, we show how to estimate private model parameters through parallel model training and generate new model parameters in a privacy-preserving manner to replace the original ones for publishing. Based on data parallelism and parameter generating techniques, we design another two solutions concentrating on model quality and parameter privacy, respectively. Through privacy leakage analysis and experimental attack evaluation, we conclude that deep neural network models published with our solutions can provide on-demand model quality guarantees and resist membership inference attacks.","1558-2183","","10.1109/TPDS.2021.3129612","National Key R&D Program of China(grant numbers:2020YFB1005900,NSFC-61902176,BK20190294,NSFC-61872179,NSFC-61872176); Leading-edge Technology Program of Jiangsu NSF(grant numbers:BK20202001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623516","Machine learning security;membership inference attack;data parallelism;deep neural network","Computational modeling;Data models;Publishing;Training;Privacy;Deep learning;Analytical models","computer crime;data privacy;deep learning (artificial intelligence)","secure deep neural network models publishing;membership inference attacks;training task parallelism;computing resources;deep learning;machine learning security;private model parameters;parallel model training;data parallelism;parameter generating techniques;parameter privacy;on-demand model quality guarantees","",1.0,"",52.0,"IEEE","22 Nov 2021","","","IEEE","IEEE Journals"
"Boosting Graph Embedding on a Single GPU","A. A. Aljundi; T. A. Akyildiz; K. Kaya","Faculty of Engineering and Natural Sciences, Sabancı University, Istanbul, Turkey; Faculty of Engineering and Natural Sciences, Sabancı University, Istanbul, Turkey; Faculty of Engineering and Natural Sciences, Sabancı University, Istanbul, Turkey","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3092,3105,"Graphs are ubiquitous, and they can model unique characteristics and complex relations of real-life systems. Although using machine learning (ML) on graphs is promising, their raw representation is not suitable for ML algorithms. Graph embedding represents each node of a graph as a $d$d-dimensional vector which is more suitable for ML tasks. However, the embedding process is expensive, and CPU-based tools do not scale to real-world graphs. In this work, we present GOSH, a GPU-based tool for embedding large-scale graphs with minimum hardware constraints. GOSH employs a novel graph coarsening algorithm to enhance the impact of updates and minimize the work for embedding. It also incorporates a decomposition schema that enables any arbitrarily large graph to be embedded with a single GPU. As a result, GOSH sets a new state-of-the-art in link prediction both in accuracy and speed, and delivers high-quality embeddings for node classification at a fraction of the time compared to the state-of-the-art. For instance, it can embed a graph with over 65 million vertices and 1.8 billion edges in less than 30 minutes on a single GPU.","1558-2183","","10.1109/TPDS.2021.3129617","Scientific and Technological Research Council of Turkey; EuroHPC Joint Undertaking(grant numbers:220N254,956213); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623416","Parallel graph embedding;graph coarsening;machine learning;GPU;link prediction;node classification","Graphics processing units;Training;Tools;Task analysis;Kernel;Classification algorithms;Distributed computing","graph theory;graphics processing units;learning (artificial intelligence);pattern classification","real-world graphs;GOSH;GPU-based tool;large-scale graphs;graph coarsening algorithm;single GPU;high-quality embeddings;graph embedding;ML algorithms;ML tasks;embedding process;CPU-based tools","","","",34.0,"CCBY","22 Nov 2021","","","IEEE","IEEE Journals"
"Lossy Compression of Communication Traces Using Recurrent Neural Networks","J. Sun; T. Yan; H. Sun; H. Lin; G. Sun","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3106,3116,"In high performance computing (HPC) systems, collecting and replaying communication traces are fundamental approaches to analyze performance. With increasingly large-scale HPC systems and applications, tracing tools can produce huge trace data that is costly and challenging to store and analyze. Due to the inherent repetition of behaviors of HPC applications, domain-aware data compression methods can effectively reduce the storage cost of trace data. This study proposes LCR (Lossy Compression and Replay), a framework that aggressively compresses and replays MPI communication traces. Differing from existing trace compression methods, which explicitly identify loop and synchronization structures of communication events, LCR models traces as time series and compactly represents them by lightweight recurrent neural networks. Experimental results demonstrate that LCR can further reduce the size of irregular traces by three orders of magnitude at most, compared with existing structural methods. Meanwhile, LCR accurately reproduces performance and communication patterns of original MPI programs.","1558-2183","","10.1109/TPDS.2021.3132417","NSF of China(grant numbers:61772485); Youth Innovation Promotion Association of CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635662","High performance computing;performance modeling;MPI trace;machine learning","Recurrent neural networks;Sun;Codes;Time series analysis;Runtime;Pattern matching;Libraries","application program interfaces;data compression;message passing;parallel processing;recurrent neural nets;synchronisation;time series","HPC applications;domain-aware data compression methods;LCR;MPI communication traces;trace compression methods;lightweight recurrent neural networks;communication patterns;high performance computing;lossy compression and replay;synchronization structures;time series","",1.0,"",44.0,"IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"Auto-GNAS: A Parallel Graph Neural Architecture Search Framework","J. Chen; J. Gao; Y. Chen; B. M. Oloulade; T. Lyu; Z. Li","School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; State Grid Hunan Electric Power Company Limited, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, Hangzhou, Zhejiang, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3117,3128,"Graph neural networks (GNNs) have received much attention as GNNs have recently been successfully applied on non-euclidean data. However, artificially designed graph neural networks often fail to get satisfactory model performance for a given graph data. Graph neural architecture search effectively constructs the GNNs that achieve the expected model performance with the rise of automatic machine learning. The challenge is efficiently and automatically getting the optimal GNN architecture in a vast search space. Existing search methods serially evaluate the GNN architectures, severely limiting system efficiency. To solve these problems, we develop an Automatic Graph Neural Architecture Search framework (Auto-GNAS) with parallel estimation to implement an automatic graph neural search process that requires almost no manual intervention. In Auto-GNAS, we design the search algorithm with multiple genetic searchers. Each searcher can simultaneously use evaluation feedback information, information entropy, and search results from other searchers based on sharing mechanism to improve the search efficiency. As far as we know, this is the first work using parallel computing to improve the system efficiency of graph neural architecture search. According to the experiment on the real datasets, Auto-GNAS obtain competitive model performance and better search efficiency than other search algorithms. Since the parallel estimation ability of Auto-GNAS is independent of search algorithms, we expand different search algorithms based on Auto-GNAS for scalability experiments. The results show that Auto-GNAS with varying search algorithms can achieve nearly linear acceleration with the increase of computing resources.","1558-2183","","10.1109/TPDS.2022.3151895","National Natural Science Foundation of China(grant numbers:61873288,61836016); State Grid Hunan Electirc Power Company(grant numbers:5216A6210075); Hunan Key Laboratory for Internet of Things in Electricity; CAAI-Huawei MindSpore Open Fund; Hunan Provincial Science and Technology Program(grant numbers:2021JJ30055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714826","Neural architecture search;parallel search;graph neural network","Computer architecture;Estimation;Graph neural networks;Genetics;Search problems;Prediction algorithms;Parallel processing","genetic algorithms;graph theory;neural net architecture;search problems","system efficiency;Auto-GNAS;search algorithms;artificially designed graph neural networks;satisfactory model performance;expected model performance;optimal GNN architecture;vast search space;search methods;GNN architectures;automatic graph neural search process;automatic graph neural architecture search framework;graph data;parallel graph neural architecture search framework","",2.0,"",29.0,"IEEE","16 Feb 2022","","","IEEE","IEEE Journals"
"An Accurate and Efficient Large-Scale Regression Method Through Best Friend Clustering","K. Li; L. Yuan; Y. Zhang; G. Chen","School of Computer Science and Technology, University of Chinese Academy of Sciences (UCAS), Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences (UCAS), Beijing, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3129,3140,"As the data size in Machine Learning fields grows exponentially, it is inevitable to accelerate the computation by utilizing the ever-growing large number of available cores provided by high-performance computing hardware. However, existing parallel methods for clustering or regression often suffer from problems of low accuracy, slow convergence, and complex hyperparameter-tuning. Furthermore, the parallel efficiency is usually difficult to improve while striking a balance between preserving model properties and partitioning computing workloads on distributed systems. In this article, we propose a novel and simple data structure capturing the most important information among data samples. It has several advantageous properties supporting a hierarchical clustering strategy that contains well-defined metrics for determining optimal hierarchy, balanced partition for maintaining the clustering property, and efficient parallelization for accelerating computation phases. Then we combine the clustering with regression techniques as a parallel library and utilize a hybrid structure of data and model parallelism to make predictions. Experiments illustrate that our library obtains remarkable performance on convergence, accuracy, and scalability.","1558-2183","","10.1109/TPDS.2021.3134336","National Natural Science Foundation of China(grant numbers:61972376,62072431,62032023); Science Foundation of Beijing(grant numbers:L182053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647869","Distributed machine learning;scalable algorithm;large-scale clustering;parallel regression","Clustering algorithms;Training;Mathematical models;Computational modeling;Libraries;Kernel;Support vector machines","data structures;learning (artificial intelligence);parallel processing;pattern clustering;regression analysis","efficient parallelization;computation phases;regression techniques;parallel library;model parallelism;data size;Machine Learning fields;available cores;high-performance computing hardware;parallel methods;complex hyperparameter-tuning;parallel efficiency;model properties;partitioning computing workloads;distributed systems;simple data structure;data samples;hierarchical clustering strategy;balanced partition;clustering property","","","",54.0,"IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"BLB-gcForest: A High-Performance Distributed Deep Forest With Adaptive Sub-Forest Splitting","Z. Chen; T. Wang; H. Cai; S. K. Mondal; J. P. Sahoo","Shanghai Key Laboratory of Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, China; Shanghai Key Laboratory of Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, China; Shanghai Key Laboratory of Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, China; Faculty of Information Technology, Macau University of Science and Technology, Macao, China; Department of Computer Science & Information Technology, Institute of Technical Education and Research, Siksha ‘O’ Anusandhan University, Bhubaneswar, Odisha, India","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3141,3152,"As an emulous alternative to deep neural networks, Deep Forest emerges with features like low complexity, fewer hyper-parameters, and good robustness, which are predominantly desired in distributed computing applications and ecosystems. Recently, an efficient distributed Deep Forest system, named ForestLayer, was proposed, designing a fine-grained sub-Forest-based task-parallel algorithm to improve the parallel computing efficiency of Deep Forest. However, the sub-Forest splitting of ForestLayer is static and one-off without adaptability to the computing environment, nevertheless, the size of splitting granularity has a significant impact on the system performance. To further improve the computing efficiency and scalability of the distributed Deep Forest, in this paper, we propose a novel distributed Deep Forest algorithm, named BLB-gcForest (Bag of Little Bootstraps-gcForest), which augments the gcForest (multi-Grained Cascade Forest) approach for constructing Deep Forest. BLB-gcForest carries out parallel computation for each tree in sub-Forests at a finer parallel granularity and integrates with the Bag of Little Bootstraps (BLB) mechanism to reduce massive transmitted feature instances for Cascade Forest Layers, utterly improving both computation efficiency and communication efficiency. Moreover, to solve the problem of the forest splitting granularity, we further design an adaptive sub-Forest splitting algorithm to ensure the maximum resource utilization for parallel computation of each sub-Forest. Experimental results on four well-known large-scale datasets, namely YEAST, LETTER, MNIST, CIFAR10, show that the training efficiency of BLB-gcForest achieves up to 20.3x and 1.64x speedups compared with the state-of-the-art gcForest and ForestLayer, respectively while guaranteeing higher accuracy and better robustness","1558-2183","","10.1109/TPDS.2021.3133544","National Key Research and Development Program of China(grant numbers:2018AAA0100503,2020AAA0107400); Fundamental Research Funds for the Central Universities(grant numbers:40500-20103-222131); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645202","Deep forest;distributed computing;big data bootstrap;distributed AI","Forestry;Computational modeling;Training;Parallel processing;Distributed computing;Data models;Adaptation models","deep learning (artificial intelligence);parallel algorithms;resource allocation","high-performance distributed Deep Forest;distributed computing;ecosystems;Deep Forest system;fine-grained sub-Forest-based task-parallel algorithm;parallel computing;ForestLayer;parallel granularity;Cascade Forest Layers;forest splitting granularity;BLB-gcForest;Bag of Little Bootstraps-gcForest;adaptive sub-Forest splitting","",1.0,"",30.0,"IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"FRuDA: Framework for Distributed Adversarial Domain Adaptation","S. Gan; A. Mathur; A. Isopoussu; F. Kawsar; N. Berthouze; N. D. Lane","ETH Zurich, Zrich, Switzerland; Nokia Bell Labs, Cambridge, U.K.; Invenia Labs, Cambridge, U.K.; Nokia Bell Labs, Cambridge, U.K.; University College London, London, U.K.; University of Cambridge, Cambridge, U.K.","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3153,3164,"Breakthroughs in unsupervised domain adaptation (uDA) can help in adapting models from a label-rich source domain to unlabeled target domains. Despite these advancements, there is a lack of research on how uDA algorithms, particularly those based on adversarial learning, can work in distributed settings. In real-world applications, target domains are often distributed across thousands of devices, and existing adversarial uDA algorithms – which are centralized in nature – cannot be applied in these settings. To solve this important problem, we introduce FRuDA: an end-to-end framework for distributed adversarial uDA. Through a careful analysis of the uDA literature, we identify the design goals for a distributed uDA system and propose two novel algorithms to increase adaptation accuracy and training efficiency of adversarial uDA in distributed settings. Our evaluation of FRuDA with five image and speech datasets show that it can boost target domain accuracy by up to 50% and improve the training efficiency of adversarial uDA by at least $11\times$11×.","1558-2183","","10.1109/TPDS.2021.3136673","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9656647","Distributed domain adaptation;domain shift;adversarial learning","Training;Feature extraction;Adaptation models;Costs;Electronics packaging;Data models;Classification algorithms","image processing;speech processing;unsupervised learning","distributed adversarial uDA;uDA literature;distributed uDA system;adaptation accuracy;distributed settings;FRuDA;target domain accuracy;distributed adversarial domain adaptation;unsupervised domain adaptation;adapting models;label-rich source domain;unlabeled target domains;adversarial learning;adversarial uDA algorithms;end-to-end framework","","","",46.0,"IEEE","20 Dec 2021","","","IEEE","IEEE Journals"
"Mixing Activations and Labels in Distributed Training for Split Learning","D. Xiao; C. Yang; W. Wu","Guangdong Key Laboratory of Big Data Analysis and Processing, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory of Big Data Analysis and Processing, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3165,3177,"Split Learning (SL) is a distributed machine learning setting that allows several nodes to train neural networks based on model parallelism. Since SL avoids sharing raw data among training nodes, it can protect data privacy by nature. However, recent studies show that, raw data may be reconstructed from activations in training, which may cause data privacy leakage. Besides raw data, label sharing in SL may also cause privacy problems. In order to address these issues, we propose a novel mechanism called multiple activations and labels mix (MALM). By taking advantage of the diversity of sample categories, MALM generates mixed activations that preserve a low distance correlation with the raw data so as to reduce the risk of reconstruction attacks. To protect label information, MALM creates obfuscated labels associated with the raw data so as to prevent adversaries from inferring ground-truth labels. Since clients with few sample categories may not effectively generate mixed activations and obfuscated labels, we propose a bipartite graph based assistant client match technique for MALM, which lets clients with a large number of categories provide mixed activations and obfuscated labels for clients with few categories. Those clients with few categories can mix the obtained mixed activations and obfuscated labels with their own activations and labels. Experimental results show that, compared with baselines, MALM can reduce the risk of raw data and label information leakage with lower cost, while achieving comparable even better model performance.","1558-2183","","10.1109/TPDS.2021.3139191","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101090005); National Natural Science Foundation of China(grant numbers:U1711263,U1801266); Guangdong Provincial Natural Science Foundation of China(grant numbers:2018B030312002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665231","Distributed training;split learning;privacy preservation;model parallelism;neural networks","Training;Privacy;Servers;Data models;Parallel processing;Distributed databases;Data privacy","data privacy;graph theory;learning (artificial intelligence);neural nets","obfuscated labels;MALM;raw data;mixing activations;distributed training;split Learning;Split Learning;training nodes;data privacy leakage;label sharing;labels mix;sample categories;mixed activations;label information;ground-truth labels","",2.0,"",31.0,"IEEE","29 Dec 2021","","","IEEE","IEEE Journals"
"Elastic Significant Bit Quantization and Acceleration for Deep Neural Networks","C. Gong; Y. Lu; K. Xie; Z. Jin; T. Li; Y. Wang","Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, State Key Laboratory of Computer Architecture (ICT,CAS), Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, State Key Laboratory of Computer Architecture (ICT,CAS), Tianjin, China; Department of Electrical and Computer Engineering, Khoury College of Computer Science (Affiliated), Northeastern University, Boston, MA, USA","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3178,3193,"Quantization has been proven to be a vital method for improving the inference efficiency of deep neural networks (DNNs). However, it is still challenging to strike a good balance between accuracy and efficiency while quantizing DNN weights or activation values from high-precision formats to their quantized counterparts. We propose a new method called elastic significant bit quantization (ESB) that controls the number of significant bits of quantized values to obtain better inference accuracy with fewer resources. We design a unified mathematical formula to constrain the quantized values of the ESB with a flexible number of significant bits. We also introduce a distribution difference aligner (DDA) to quantitatively align the distributions between the full-precision weight or activation values and quantized values. Consequently, ESB is suitable for various bell-shaped distributions of weights and activation of DNNs, thus maintaining a high inference accuracy. Benefitting from fewer significant bits of quantized values, ESB can reduce the multiplication complexity. We implement ESB as an accelerator and quantitatively evaluate its efficiency on FPGAs. Extensive experimental results illustrate that ESB quantization consistently outperforms state-of-the-art methods and achieves average accuracy improvements of 4.78%, 1.92%, and 3.56% over AlexNet, ResNet18, and MobileNetV2, respectively. Furthermore, ESB as an accelerator can achieve 10.95 GOPS peak performance of 1k LUTs without DSPs on the Xilinx ZCU102 FPGA platform. Compared with CPU, GPU, and state-of-the-art accelerators on FPGAs, the ESB accelerator can improve the energy efficiency by up to 65×, 11×, and 26×, respectively.","1558-2183","","10.1109/TPDS.2021.3129615","National Key Research and Development Program of China(grant numbers:2018YFB2100300); National Natural Science Foundation of China(grant numbers:62002175); Natural Science Foundation of Tianjin City(grant numbers:19JCQNJC00600); State Key Laboratory of Computer Architecture(grant numbers:CARCHB202016,CARCH201905); Innovation Fund of Chinese Universities Industry-University-Research(grant numbers:2020HYA01003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623507","DNN quantization;significant bits;fitting distribution;cheap projection;distribution aligner;FPGA accelerator","Quantization (signal);Distributed databases;Field programmable gate arrays;Degradation;Computer science;Open area test sites;Hardware","deep learning (artificial intelligence);field programmable gate arrays;image classification;parallel architectures;quantisation (signal)","elastic significant bit quantization;deep neural networks;activation values;quantized values;full-precision weight;ESB quantization;ESB accelerator;DNNs;distribution difference aligner;DDA;bell-shaped distributions;multiplication complexity;AlexNet;ResNet18;MobileNetV2;Xilinx ZCU102 FPGA platform;CPU;GPU;energy efficiency","",3.0,"",47.0,"IEEE","22 Nov 2021","","","IEEE","IEEE Journals"
"DLS: A Fast and Flexible Neural Network Training System With Fine-grained Heterogeneous Device Orchestration","P. Park; J. Lee; H. Jeong; J. Kim","Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; Facebook, Menlo Park, CA, USA; Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3194,3206,"Neural network accelerators (e.g., TPUs) have become mainstream devices in computing systems. Unfortunately, the existing accelerator-based systems for neural networks fail to fully leverage the acceleration opportunities due to the limited flexibility. Specifically, the majority of the accelerators focus on only the compute-intensive operations of neural networks (e.g., convolution and fully-connected layers). However, we identify that sub-optimal handling of auxiliary operations such as embedding and compression can incur non-trivial loss in terms of accuracy, training speed, and adaptability to new domains. The problem persists considering that recent advancements in neural networks often come from auxiliary operations. To effectively handle rapidly evolving auxiliary operations and maximize acceleration opportunities, we propose DLS, a holistic neural network acceleration system using heterogeneous computing devices. The key idea is to distribute compute-intensive operations on highly specialized ASICs for maximum performance, and auxiliary operations on flexible devices (e.g., FPGA, GPU) for better adaptability. We emphasize that a naïve integration of different devices fails to deliver high performance due to high communication overheads. To address this communication inefficiency, we propose an efficient FPGA-based device orchestration utilizing direct device-to-device communication and fine-grained operation scheduling. In this way, our system alleviates the communication overhead between heterogeneous devices by removing expensive kernel stack traversal and leveraging computation units and communication links in parallel. The evaluation using popular neural networks with emerging auxiliary operations shows that our system achieves both flexibility and high performance for various cases from single-accelerator training to distributed training (2.6–8.9× speedup).","1558-2183","","10.1109/TPDS.2022.3144453","Samsung(grant numbers:SRFC-IT1901-12); Seoul National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689955","Neural network;accelerators;GPU;FPGA;flexibility;heterogeneous devices;inter-device communication","Artificial neural networks;Training;Performance evaluation;Field programmable gate arrays;Device-to-device communication;Task analysis;Predictive models","AI chips;application specific integrated circuits;distributed processing;field programmable gate arrays;learning (artificial intelligence);neural nets;processor scheduling","neural network accelerators;compute-intensive operations;holistic neural network acceleration system;heterogeneous computing devices;efficient FPGA-based device orchestration;flexible neural network training system;fast neural network training system;fine-grained heterogeneous device;DLS;auxiliary operations suboptimal handling;training speed;highly specialized ASIC;maximum performance;direct device-to-device communication;kernel stack traversal;distributed training;device-optimal layer distribution system","","","",37.0,"IEEE","21 Jan 2022","","","IEEE","IEEE Journals"
"E3NE: An End-to-End Framework for Accelerating Spiking Neural Networks With Emerging Neural Encoding on FPGAs","D. Gerlinghoff; Z. Wang; X. Gu; R. S. M. Goh; T. Luo","Institute of High Performance Computing, Agency for Science Technology and Research (A*STAR), Singapore, Singapore; Institute of High Performance Computing, Agency for Science Technology and Research (A*STAR), Singapore, Singapore; Future Network of Intelligence Institute, Chinese University of Hong Kong, Shenzhen, China; Institute of High Performance Computing, Agency for Science Technology and Research (A*STAR), Singapore, Singapore; Institute of High Performance Computing, Agency for Science Technology and Research (A*STAR), Singapore, Singapore","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3207,3219,"Compiler frameworks are crucial for the widespread use of FPGA-based deep learning accelerators. They allow researchers and developers, who are not familiar with hardware engineering, to harness the performance attained by domain-specific logic. There exists a variety of frameworks for conventional artificial neural networks. However, not much research effort has been put into the creation of frameworks optimized for spiking neural networks (SNNs). This new generation of neural networks becomes increasingly interesting for the deployment of AI on edge devices, which have tight power and resource constraints. Our end-to-end framework E3NE automates the generation of efficient SNN inference logic for FPGAs. Based on a PyTorch model and user parameters, it applies various optimizations and assesses trade-offs inherent to spike-based accelerators. Multiple levels of parallelism and the use of an emerging neural encoding scheme result in an efficiency superior to previous SNN hardware implementations. For a similar model, E3NE uses less than 50% of hardware resources and 20% less power, while reducing the latency by an order of magnitude. Furthermore, scalability and generality allowed the deployment of the large-scale SNN models AlexNet and VGG.","1558-2183","","10.1109/TPDS.2021.3128945","Singapore Governments Research, Innovation and Enterprise 2020 Plan(grant numbers:A1687b0033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619972","Spiking neural network;neuromorphic computing;neural encoding;compiler framework;FPGA","Hardware;Neurons;Field programmable gate arrays;Encoding;Biological neural networks;Computational modeling;Optimization","AI chips;encoding;field programmable gate arrays;learning (artificial intelligence)","end-to-end framework;spiking neural networks;compiler frameworks;FPGA-based deep learning accelerators;domain-specific logic;artificial neural networks;neural encoding scheme;SNN hardware implementations;SNN inference logic;E3NE;spike-based accelerators;hardware resources;AlexNet;VGG;large-scale SNN models;edge devices;PyTorch model","",4.0,"",53.0,"IEEE","18 Nov 2021","","","IEEE","IEEE Journals"
"NeoFlow: A Flexible Framework for Enabling Efficient Compilation for High Performance DNN Training","S. Zheng; R. Chen; Y. Jin; A. Wei; B. Wu; X. Li; S. Yan; Y. Liang","School of EECS, Peking University, Beijing, China; School of EECS, Peking University, Beijing, China; School of EECS, Peking University, Beijing, China; School of EECS, Peking University, Beijing, China; School of EECS, Peking University, Beijing, China; SenseTime Research & Shanghai AI Lab, Beijing, China; SenseTime Research, Beijing, China; School of EECS, Peking University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3220,3232,"Deep neural networks (DNNs) are increasingly deployed in various image recognition and natural language processing applications. The continuous demand for accuracy and high performance has led to innovations in DNN design and a proliferation of new operators. However, existing DNN training frameworks such as PyTorch and TensorFlow only support a limited range of operators and rely on hand-optimized libraries to provide efficient implementations for these operators. To evaluate novel neural networks with new operators, the programmers have to either replace the holistic new operators with existing operators or provide low-level implementations manually. Therefore, a critical requirement for DNN training frameworks is to provide high-performance implementations for the neural networks containing new operators automatically in the absence of efficient library support. In this article, we introduce NeoFlow, which is a flexible framework for enabling efficient compilation for high-performance DNN training. NeoFlow allows the programmers to directly write customized expressions as new operators to be mapped to graph representation and low-level implementations automatically, providing both high programming productivity and high performance. First, NeoFlow provides expression-based automatic differentiation to support customized model definitions with new operators. Then, NeoFlow proposes an efficient compilation system that partitions the neural network graph into subgraphs, explores optimized schedules, and generates high-performance libraries for subgraphs automatically. Finally, NeoFlow develops an efficient runtime system to combine the compilation and training as a whole by overlapping their execution. In the experiments, we examine the numerical accuracy and performance of NeoFlow. The results show that NeoFlow can achieve similar or even better performance at the operator and whole graph level for DNNs compared to deep learning frameworks. Especially, for novel networks training, the geometric mean speedups of NeoFlow to PyTorch, TensorFlow, and CuDNN are 3.16X, 2.43X, and 1.92X, respectively.","1558-2183","","10.1109/TPDS.2021.3138862","National Natural Science Foundation of China(grant numbers:U21B2017); Shanghai Committee of Science and Technology, China(grant numbers:20DZ1100800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664259","Deep learning;training;code generation;compiler optimization;automatic differentiation","Training;Libraries;Convolution;Codes;Deep learning;Tensors;Schedules","deep learning (artificial intelligence);graph theory;program compilers;programming;software libraries","high performance DNN training;deep neural networks;natural language processing;DNN design;hand-optimized libraries;low-level implementations;high-performance implementations;library support;programming productivity;expression-based automatic differentiation;compilation system;neural network graph;high-performance libraries;runtime system;networks training;NeoFlow;image recognition;PyTorch;TensorFlow;subgraphs;graph representation;geometric mean speedups;CuDNN","",3.0,"",54.0,"CCBY","28 Dec 2021","","","IEEE","IEEE Journals"
"Heterogeneous Multi-Agent System for Brain-Computer Interaction in Routing and Forwarding With Memristive Neuron Networks","Y. Zhou; W. Chen; L. Li; L. Gong; T. Zhang","College of Communication Engineering, Jilin University, Changchun, Jilin, China; College of Communication Engineering, Jilin University, Changchun, Jilin, China; College of Communication Engineering, Jilin University, Changchun, Jilin, China; College of Communication Engineering, Jilin University, Changchun, Jilin, China; College of Communication Engineering, Jilin University, Changchun, Jilin, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3233,3248,"In this research, we aimed to design a novel brain-computer interaction (BCI) approach in routing and forwarding mechanisms to realize a networking multi-modal and multi-brain linked control. In recent years, the field programmable gate array (FPGA) has become a popular choice to construct a heterogeneous network for routing and forwarding processes due to its ability to complete a high performance computation in single node. Conventional BCI mode focus on capturing the dynamic EEG signals from multiple electrode channels and this concept neglects the existing complexity and diversity of connecting information between two brain regions. To this moment, we build a heterogeneous multi-agent system (HMAS) architecture to simulate the memristive neuron networks (MNN) with a set of logic units for multi-agent in the Internet. Specifically, we first feed the non-linear features into an intelligent router to adjust their routing and forwarding processes. Subsequently, at one-time step, the output of all testing nodes is written into a heterogeneous logic space, which mainly consists of the sub-agent, forward port, and memory memristive neuron. In the FPGA, each programmable cell selectively integrates and stores a huge number of motion information between multiple interacting channels. We can achieve the networking cooperative functions of forward and route through FPGA large-scale computation. According to the extensive experimental results on several data, we validate the effectiveness of the proposed HMAS-BCI architecture to compare the optimization method of the multiple simulations.","1558-2183","","10.1109/TPDS.2021.3137837","Department of Science and Technology of Jilin Province(grant numbers:20190302034GX); Domaion Foundation of Equipment Advance Research of 13th Five-year Plan(grant numbers:ZX7Y201902120-06001); China Postdoctoral Science Foundation(grant numbers:2020M670856); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661451","Heterogeneous multi-agent system;brain-computer interaction;routing and forwarding mechanism;MNN","Brain modeling;Routing;Computational modeling;Multi-layer neural network;Task analysis;Multi-agent systems;Flexible printed circuits","brain;brain-computer interfaces;electroencephalography;field programmable gate arrays;medical signal processing;multi-agent systems;neural nets;neurophysiology;optimisation;signal classification;telecommunication network routing","multibrain linked control;field programmable gate array;FPGA;heterogeneous network;high performance computation;conventional BCI mode focus;multiple electrode channels;existing complexity;brain regions;heterogeneous multiagent system architecture;memristive neuron networks;forwarding processes;testing nodes;heterogeneous logic space;memory memristive neuron;multiple interacting channels;networking cooperative functions;forward route;large-scale computation;HMAS-BCI architecture;multiple simulations;novel brain-computer interaction approach;networking multimodal","",1.0,"",69.0,"IEEE","23 Dec 2021","","","IEEE","IEEE Journals"
"NITI: Training Integer Neural Networks Using Integer-Only Arithmetic","M. Wang; S. Rasoulinezhad; P. H. W. Leong; H. K. . -H. So","ACCESS – AI Chip Center for Emerging Smart Systems, InnoHK Centers, Hong Kong Science Park, Hong Kong, China; School of Electrical and Information Engineering, The University of Sydney, Camperdown, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Camperdown, NSW, Australia; Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","25 May 2022",2022,33.0,11.0,3249,3261,"Low bitwidth integer arithmetic has been widely adopted in hardware implementations of deep neural network inference applications. However, despite the promised energy-efficiency improvements demanding edge applications, the use of low bitwidth integer arithmetic for neural network training remains limited. Unlike inference, training demands high dynamic range and numerical accuracy for high quality results, making the use of low-bitwidth integer arithmetic particularly challenging. To address this challenge, we present a novel neural network training framework called NITI that exclusively utilizes low bitwidth integer arithmetic. NITI stores all parameters and accumulates intermediate values as 8-bit integers while using no more than 5 bits for gradients. To provide the necessary dynamic range during the training process, a per-layer block scaling exponentiation scheme is utilized. By deeply integrating with the rounding procedures and integer entropy loss calculation, the proposed scaling scheme incurs only minimal overhead in terms of storage and additional computation. Furthermore, a hardware-efficient pseudo-stochastic rounding scheme that eliminates the need for external random number generation is proposed to facilitate conversion from wider intermediate arithmetic results to lower precision for storage. Since NITI operates only with standard 8-bit integer arithmetic and storage, it is possible to accelerate it using existing low bitwidth operators originally developed for inference in commodity accelerators. To demonstrate this, an open-source software implementation of end-to-end training, using native 8-bit integer operations in modern GPUs is presented. In addition, experiments have been conducted on an FPGA-based training accelerator to evaluate the hardware advantage of NITI. When compared with an equivalent training setup implemented with floating point storage and arithmetic, NITI has no accuracy degradation on the MNIST and CIFAR10 datasets. On ImageNet, NITI achieves similar accuracy as state-of-the-art integer training frameworks without relying on full-precision floating-point first and last layers.","1558-2183","","10.1109/TPDS.2022.3149787","Innovation Award; ACCESS – AI Chip Center for Emerging Smart Systems; Innovation and Technology Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709160","Neural network training;integer arithmetic;NITI;GPU;tensor core;hardware accelerator","Training;Arithmetic;Neural networks;Dynamic range;Degradation;Tensors;Standards","deep learning (artificial intelligence);digital arithmetic;entropy;field programmable gate arrays;graphics processing units;hardware accelerators;inference mechanisms;neural chips;public domain software;stochastic processes","NITI;8-bit integer arithmetic;low bitwidth operators;8-bit integer operations;low bitwidth integer arithmetic;deep neural network inference;integer entropy loss calculation;hardware-efficient pseudostochastic rounding scheme;intermediate arithmetic results;integer neural network training;low-bitwidth integer arithmetic;integer-only arithmetic;hardware implementations;energy-efficiency improvements;edge applications;per-layer block scaling exponentiation scheme;rounding procedures;commodity accelerators;open-source software implementation;end-to-end training;GPUs;FPGA-based training accelerator;CIFAR10 datasets;MNIST datasets;ImageNet;word length 8 bit","",2.0,"",41.0,"IEEE","9 Feb 2022","","","IEEE","IEEE Journals"
"DeTraS: Delaying Stores for Friendly-Fire Mitigation in Hardware Transactional Memory","R. Titos-Gil; R. Fernández-Pascual; A. Ros; M. E. Acacio","Deptartment de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Deptartment de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Deptartment de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Deptartment de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain","IEEE Transactions on Parallel and Distributed Systems","29 Jun 2021",2022,33.0,1.0,1,13,"Commercial Hardware Transactional Memory (HTM) systems are best-effort designs that leverage the coherence substrate to detect conflicts eagerly. Resolving conflicts in favor of the requesting core is the simplest option for ensuring deadlock freedom, yet it is prone to livelocks. In this work, we propose and evaluate DeTraS (Delayed Transactional Stores), an HTM-aware store buffer design aimed at mitigating such livelocks. DeTraS takes advantage of the fact that modern commercial processors implement a large store buffer, and uses it to prevent transactional stores predicted to conflict from performing early in the transaction. By leveraging existing processor structures, we propose a simple design that improves the ability of requester-wins HTM systems to achieve forward progress in spite of high contention while side-stepping the performance penalty of falling back to mutual exclusion. With just over 50 extra bytes, DeTraS captures the advantages of lazy conflict management without the complexity brought into the coherence fabric by commit arbitration schemes nor the relaxation of the single-writer invariant of prior works. Through detailed simulations of a 16-core tiled CMP using gem5, we demonstrate that DeTraS brings reductions in average execution time of 25 percent when compared to an Intel RTM-like design.","1558-2183","","10.1109/TPDS.2021.3085210","Spanish MCIU; AEI; European Commission(grant numbers:RTI2018-098156-B-C53); Spanish MCIU(grant numbers:ERC2018-092826); European Research Council(grant numbers:819134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444639","Multicore architectures;transactional memory;store buffer;coherence protocol","Buffer storage;Program processors;Coherence;Hardware;Out of order;Complexity theory;System recovery","cache storage;memory architecture;storage management chips","lazy conflict management;DeTraS;Intel RTM-like design;friendly-fire mitigation;commercial hardware transactional memory systems;deadlock freedom;livelocks;HTM-aware store buffer design;requester-wins HTM systems;16-core tiled CMP;gem5;delayed transactional stores","",1.0,"",31.0,"IEEE","31 May 2021","","","IEEE","IEEE Journals"
"Error-Compensated Sparsification for Communication-Efficient Decentralized Training in Edge Environment","H. Wang; S. Guo; Z. Qu; R. Li; Z. Liu","Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","25 Jun 2021",2022,33.0,1.0,14,25,"Communication has been considered as a major bottleneck in large-scale decentralized training systems since participating nodes iteratively exchange large amounts of intermediate data with their neighbors. Although compression techniques like sparsification can significantly reduce the communication overhead in each iteration, errors caused by compression will be accumulated, resulting in a severely degraded convergence rate. Recently, the error compensation method for sparsification has been proposed in centralized training to tolerate the accumulated compression errors. However, the analog technique and the corresponding theory about its convergence in decentralized training are still unknown. To fill in the gap, we design a method named ECSD-SGD that significantly accelerates decentralized training via error-compensated sparsification. The novelty lies in that we identify the component of the exchanging information in each iteration (i.e., the sparsified model update) and make targeted error compensation over the component. Our thorough theoretical analysis shows that ECSD-SGD supports arbitrary sparsification ratio and achieves the same convergence rate as the non-sparsified decentralized training methods. We also conduct extensive experiments on multiple deep learning models to validate our theoretical findings. Results show that ECSD-SGD outperforms all the start-of-the-art sparsified methods in terms of both the convergence speed and the final generalization accuracy.","1558-2183","","10.1109/TPDS.2021.3084104","National Key Research and Development Program of China(grant numbers:2016QY01W0202); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19,R5034-18); General Research Fund(grant numbers:152221/19E,15220320/20E); Collaborative Research Fund(grant numbers:C5026-18G); National Natural Science Foundation of China(grant numbers:61872310,U1836204,U1936108); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:R2020A045); Shenzhen Basic Research Funding Scheme(grant numbers:JCYJ20170818103849343); Fundamental Research Funds for the Central Universities(grant numbers:B210202079); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442310","Distributed machine learning;decentralized training;communication compression;error compensation","Training;Deep learning;Design methodology;Error compensation;Convergence","convergence;data compression;distributed processing;error compensation;gradient methods;iterative methods;learning (artificial intelligence);stochastic processes","edge environment;large-scale decentralized training systems;iteration;error compensation;compression errors;analog technique;ECSD-SGD;error-compensated sparsification;communication-efficiency decentralized training;nonsparsified decentralized training;convergence speed;sparsified model update;distributed machine learning","",4.0,"",47.0,"IEEE","26 May 2021","","","IEEE","IEEE Journals"
"Capelin: Data-Driven Compute Capacity Procurement for Cloud Datacenters Using Portfolios of Scenarios","G. Andreadis; F. Mastenbroek; V. van Beek; A. Iosup","Electrical Engineering, Mathematics and Computer Science Faculty, Delft University of Technology, Delft, CD, The Netherlands; Electrical Engineering, Mathematics and Computer Science Faculty, Delft University of Technology, Delft, CD, The Netherlands; Solvinity, Amsterdam, ED, The Netherlands; Computer Science, Vrije Universiteit Amsterdam, Amsterdam, HV, The Netherlands","IEEE Transactions on Parallel and Distributed Systems","25 Jun 2021",2022,33.0,1.0,26,39,"Cloud datacenters provide a backbone to our digital society. Inaccurate capacity procurement for cloud datacenters can lead to significant performance degradation, denser targets for failure, and unsustainable energy consumption. Although this activity is core to improving cloud infrastructure, relatively few comprehensive approaches and support tools exist for mid-tier operators, leaving many planners with merely rule-of-thumb judgement. We derive requirements from a unique survey of experts in charge of diverse datacenters in several countries. We propose Capelin, a data-driven, scenario-based capacity planning system for mid-tier cloud datacenters. Capelin introduces the notion of portfolios of scenarios, which it leverages in its probing for alternative capacity-plans. At the core of the system, a trace-based, discrete-event simulator enables the exploration of different possible topologies, with support for scaling the volume, variety, and velocity of resources, and for horizontal (scale-out) and vertical (scale-up) scaling. Capelin compares alternative topologies and for each gives detailed quantitative operational information, which could facilitate human decisions of capacity planning. We implement and open-source Capelin, and show through comprehensive trace-based experiments it can aid practitioners. The results give evidence that reasonable choices can be worse by a factor of 1.5-2.0 than the best, in terms of performance degradation or energy consumption.","1558-2183","","10.1109/TPDS.2021.3084816","NWO Vidi MagnaData; TOP OffSense; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444213","Cloud;procurement;capacity planning;datacenter;practitioner survey;simulation","Capacity planning;Tools;Cloud computing;Containers;Topology;Procurement;Planning","capacity management (computers);cloud computing;computer centres;discrete event simulation;procurement","data-driven compute capacity procurement;portfolios;cloud infrastructure;mid-tier cloud datacenters;open-source Capelin;scenario-based capacity planning system;data-driven capacity planning system;trace-based discrete-event simulator","","","",67.0,"IEEE","28 May 2021","","","IEEE","IEEE Journals"
"$run$ runData: Re-Distributing Data via Piggybacking for Geo-Distributed Data Analytics Over Edges","Y. Jin; Z. Qian; S. Guo; S. Zhang; L. Jiao; S. Lu","Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Hong Kong Polytechnic University Shenzhen Research Institute, Shenzhen, Guangdong, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China","IEEE Transactions on Parallel and Distributed Systems","29 Jun 2021",2022,33.0,1.0,40,55,"Efficiently analyzing geo-distributed datasets is emerging as a major demand in a cloud-edge system. Since the datasets are often generated in closer proximity to end users, traditional works mainly focus on offloading proper tasks from those hotspot edges to the datacenter to decrease the overall completion time of submitted jobs in a one-shot manner. However, optimizing the completion time of current job alone is insufficient in a long-term scope since some datasets would be used multiple times. Instead, optimizing the data distribution is much more efficient and could directly benefit forthcoming jobs, although it may postpone the execution of current one. Unfortunately, due to the throwaway feature of data fetcher, existing data analytics systems fail to re-distribute corresponding data out of hotspot edges after the execution of data analytics. In order to minimize the overall completion time for a sequence of jobs as well as to guarantee the performance of current one, we propose to re-distribute the data along with task offloading, and formulate corresponding ε-bounded data-driven task scheduling problem over wide area network under the consideration of edge heterogeneity. We design an online schema run Data, which offloads proper tasks and related data via piggybacking to the datacenter based on delicately calculated probabilities. Through rigorous theoretical analysis, run Data is proved concentrated on its optimum with high probability. We implement run Data based on Spark and HDFS. Both testbed results and trace-driven simulations show that run Data re-distributes proper data via piggybacking and achieves up to 37 percent reduction on average response time compared with state-of-the-art schemas.","1558-2183","","10.1109/TPDS.2021.3086274","National Key Research and Development Program of China(grant numbers:2017YFB1001801); National Science Foundation of China(grant numbers:61832005,61872175); Ripple Faculty Fellowship; Natural Science Foundation of Jiangsu Province(grant numbers:BK20181252); Collaborative Innovation Center of Novel Software Technology and Industrialization; Nanjing University Innovation and Creative Program(grant numbers:CXCY19-25); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19,R5034-18); General Research Fund(grant numbers:152221/19E,15220320/20E); Collaborative Research Fund(grant numbers:C5026-18G); National Natural Science Foundation of China(grant numbers:61872310); Shenzhen Science and Technology Innovation Commission(grant numbers:R2020A045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446574","Cloud-edge system;data re-distribution;heterogeneity;online schema","Task analysis;Data analysis;Wide area networks;Data models;Servers;Optimization;Videos","cloud computing;computer centres;data analysis;probability;scheduling","geo-distributed data analytics;cloud-edge system;datacenter;ε-bounded data-driven task scheduling problem;wide area network;piggybacking;online schema run runData","",1.0,"",67.0,"IEEE","3 Jun 2021","","","IEEE","IEEE Journals"
"Optimal Repair-Scaling Trade-off in Locally Repairable Codes: Analysis and Evaluation","S. Wu; Z. Shen; P. P. C. Lee; Y. Xu","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; College of Informatics, Xiamen University, Xiamen, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China","IEEE Transactions on Parallel and Distributed Systems","29 Jun 2021",2022,33.0,1.0,56,69,"How to improve the repair performance of erasure-coded storage is a critical issue for maintaining high reliability of modern large-scale storage systems. Locally repairable codes (LRC) are one popular family of repair-efficient erasure codes that mitigate the repair bandwidth and are deployed in practice. To adapt to the changing demands of access efficiency and fault tolerance, modern storage systems also conduct frequent scaling operations on erasure-coded data. In this article, we analyze the optimal trade-off between the repair and scaling performance of LRC in clustered storage systems. Specifically, we focus on two optimal repair-scaling trade-offs, and design placement strategies that operate along the two optimal repair-scaling trade-off curves subject to the fault tolerance constraints. We prototype and evaluate our placement strategies on a LAN testbed, and show that they outperform the conventional placement schemes in repair and scaling operations.","1558-2183","","10.1109/TPDS.2021.3087352","NSFC(grant numbers:61832011,62072381); CCF-Tencent Open Fund WeBank Special Fund(grant numbers:2021KF0AB01); Fundamental Research Funds for the Central Universities(grant numbers:WK2150110022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448455","LRC;repair;scaling;clustered storage","Maintenance engineering;Encoding;Fault tolerant systems;Bandwidth;Redundancy;Switches;Computer science","block codes;error correction codes;fault tolerance;local area networks;storage management","fault tolerance constraints;LAN testbed;LRC;repair bandwidth;optimal repair-scaling trade-off curves;clustered storage systems;scaling performance;erasure-coded data;frequent scaling operations;repair-efficient erasure codes;large-scale storage systems;erasure-coded storage;locally repairable codes","",1.0,"",36.0,"IEEE","8 Jun 2021","","","IEEE","IEEE Journals"
"Optimizing Depthwise Separable Convolution Operations on GPUs","G. Lu; W. Zhang; Z. Wang","School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Computing, University of Leeds, Leeds, U.K.","IEEE Transactions on Parallel and Distributed Systems","28 Jun 2021",2022,33.0,1.0,70,87,"The depthwise separable convolution is commonly seen in convolutional neural networks (CNNs), and is widely used to reduce the computation overhead of a standard multi-channel 2D convolution. Existing implementations of depthwise separable convolutions target accelerating model training with large batch sizes with a large number of samples to be processed at once. Such approaches are inadequate for small-batch-sized model training and the typical scenario of model inference where the model takes in a few samples at once. This article aims to bridge the gap of optimizing depthwise separable convolutions by targeting the GPU architecture. We achieve this by designing two novel algorithms to improve the column and row reuse of the convolution operation to reduce the number of memory operations performed on the width and the height dimensions of the 2D convolution. Our approach employs a dynamic tile size scheme to adaptively distribute the computational data across GPU threads to improve GPU utilization and to hide the memory access latency. We apply our approach on two GPU platforms: an NVIDIA RTX 2080Ti GPU and an embedded NVIDIA Jetson AGX Xavier GPU, and two data types: 32-bit floating point (FP32) and 8-bit integer (INT8). We compared our approach against cuDNN that is heavily tuned for the NVIDIA GPU architecture. Experimental results show that our approach delivers over 2× (up to 3×) performance improvement over cuDNN. We show that, when using a moderate batch size, our approach averagely reduces the end-to-end training time of MobileNet and EfficientNet by 9.7 and 7.3 percent respectively, and reduces the end-to-end inference time of MobileNet and EfficientNet by 12.2 and 11.6 percent respectively.","1558-2183","","10.1109/TPDS.2021.3084813","National Key Research and Development Program of China(grant numbers:2017YFB0202901); Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010136001); National Natural Science Foundation of China(grant numbers:61672186,61872294); Shenzhen Technology Research and Development(grant numbers:JCYJ20190806143418198); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444208","Performance optimization;convolution;depthwise;pointwise;memory optimization;GPU utilization","Convolution;Graphics processing units;Instruction sets;Kernel;Standards;Training;Registers","convolutional neural nets;graphics processing units;learning (artificial intelligence);neural net architecture;parallel architectures","embedded NVIDIA Jetson AGX Xavier GPU;NVIDIA GPU architecture;moderate batch size;depthwise separable convolution operations;convolutional neural networks;multichannel 2D convolution;small-batch-sized model training;convolution operation;NVIDIA RTX 2080Ti GPU;CNN;model inference;dynamic tile size scheme;accelerating model training;FP32;INT8","",7.0,"",55.0,"IEEE","28 May 2021","","","IEEE","IEEE Journals"
"Horus: Interference-Aware and Prediction-Based Scheduling in Deep Learning Systems","G. Yeung; D. Borowiec; R. Yang; A. Friday; R. Harper; P. Garraghan","School of Computing & Communications, Lancaster University, Lancaster, U.K; School of Computing & Communications, Lancaster University, Lancaster, U.K; School of Computing, University of Leeds, Leeds, U.K; School of Computing & Communications, Lancaster University, Lancaster, U.K; School of Computing & Communications, Lancaster University, Lancaster, U.K; School of Computing & Communications, Lancaster University, Lancaster, U.K","IEEE Transactions on Parallel and Distributed Systems","28 Jun 2021",2022,33.0,1.0,88,100,"To accelerate the training of Deep Learning (DL) models, clusters of machines equipped with hardware accelerators such as GPUs are leveraged to reduce execution time. State-of-the-art resource managers are needed to increase GPU utilization and maximize throughput. While co-locating DL jobs on the same GPU has been shown to be effective, this can incur interference causing slowdown. In this article we propose Horus: an interference-aware and prediction-based resource manager for DL systems. Horus proactively predicts GPU utilization of heterogeneous DL jobs extrapolated from the DL model's computation graph features, removing the need for online profiling and isolated reserved GPUs. Through micro-benchmarks and job co-location combinations across heterogeneous GPU hardware, we identify GPU utilization as a general proxy metric to determine good placement decisions, in contrast to current approaches which reserve isolated GPUs to perform online profiling and directly measure GPU utilization for each unique submitted job. Our approach promotes high resource utilization and makespan reduction; via real-world experimentation and large-scale trace driven simulation, we demonstrate that Horus outperforms other DL resource managers by up to 61.5 percent for GPU resource utilization, 23.7-30.7 percent for makespan reduction and 68.3 percent in job wait time reduction.","1558-2183","","10.1109/TPDS.2021.3079202","Engineering and Physical Sciences Research Council(grant numbers:EP/P031617/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428512","Distributed systems;deep learning;interference;GPU utilization;cloud computing;workload prediction","Graphics processing units;Interference;Kernel;Predictive models;Computational modeling;Production;Load modeling","deep learning (artificial intelligence);graphics processing units;resource allocation;scheduling","Horus;interference-aware scheduling;prediction-based scheduling;deep learning systems;hardware accelerators;state-of-the-art resource managers;GPU utilization;co-locating DL jobs;prediction-based resource manager;heterogeneous DL jobs;DL model;online profiling;reserved GPUs;heterogeneous GPU hardware;high resource utilization;makespan reduction;DL resource managers;GPU resource utilization","",8.0,"",67.0,"CCBY","11 May 2021","","","IEEE","IEEE Journals"
"COSCO: Container Orchestration Using Co-Simulation and Gradient Based Optimization for Fog Computing Environments","S. Tuli; S. R. Poojara; S. N. Srirama; G. Casale; N. R. Jennings","Department of Computing, Imperial College London, London, U.K.; Institute of Computer Science, University of Tartu, Tartu, Estonia; School of Computer and Information Sciences, University of Hyderabad, Telangana, India; Department of Computing, Imperial College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.","IEEE Transactions on Parallel and Distributed Systems","7 Jul 2021",2022,33.0,1.0,101,116,"Intelligent task placement and management of tasks in large-scale fog platforms is challenging due to the highly volatile nature of modern workload applications and sensitive user requirements of low energy consumption and response time. Container orchestration platforms have emerged to alleviate this problem with prior art either using heuristics to quickly reach scheduling decisions or AI driven methods like reinforcement learning and evolutionary approaches to adapt to dynamic scenarios. The former often fail to quickly adapt in highly dynamic environments, whereas the latter have run-times that are slow enough to negatively impact response time. Therefore, there is a need for scheduling policies that are both reactive to work efficiently in volatile environments and have low scheduling overheads. To achieve this, we propose a Gradient Based Optimization Strategy using Back-propagation of gradients with respect to Input (GOBI). Further, we leverage the accuracy of predictive digital-twin models and simulation capabilities by developing a Coupled Simulation and Container Orchestration Framework (COSCO). Using this, we create a hybrid simulation driven decision approach, GOBI*, to optimize Quality of Service (QoS) parameters. Co-simulation and the back-propagation approaches allow these methods to adapt quickly in volatile environments. Experiments conducted using real-world data on fog applications using the GOBI and GOBI* methods, show a significant improvement in terms of energy consumption, response time, Service Level Objective and scheduling time by up to 15, 40, 4, and 82 percent respectively when compared to the state-of-the-art algorithms.","1558-2183","","10.1109/TPDS.2021.3087349","Imperial College London; European Social Fund; EU's Horizon 2020 Program(grant numbers:825040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448450","Fog computing;coupled simulation;container orchestration;back-propagation to input;QoS optimization","Optimization;Quality of service;Containers;Adaptation models;Genetic algorithms;Time factors;Task analysis","backpropagation;distributed processing;learning (artificial intelligence);optimisation;quality of service;scheduling","COSCO;gradient based optimization;fog computing environments;large-scale fog platforms;highly volatile nature;modern workload applications;sensitive user requirements;low energy consumption;container orchestration platforms;scheduling decisions;AI driven methods;reinforcement learning;evolutionary approaches;dynamic scenarios;highly dynamic environments;run-times;scheduling policies;volatile environments;low scheduling overheads;digital-twin models;simulation capabilities;hybrid simulation driven decision approach;back-propagation approaches;fog applications;GOBI* methods;scheduling time;response time;coupled simulation and container orchestration framework;quality of service parameters;QoS","",25.0,"",63.0,"IEEE","8 Jun 2021","","","IEEE","IEEE Journals"
"FlitZip: Effective Packet Compression for NoC in MultiProcessor System-on-Chip","D. Deb; R. M.K.; J. Jose","Department of Computer Science and Engineering, Indian Institute of Technology Guwahati, Guwahati, Assam, India; Department of Electronics and Communication Engineering, R. V. College of Engineering, Bangalore, India; Department of Computer Science and Engineering, Indian Institute of Technology Guwahati, Guwahati, Assam, India","IEEE Transactions on Parallel and Distributed Systems","7 Jul 2021",2022,33.0,1.0,117,128,"Applications running on Network on Chip (NoC) based multicore systems demand increased on-chip network bandwidth that can cater to the need for intensive communication among the cores and caches. Due to strict area and power budget, the bandwidth offered by NoC is very limited. Data-intensive and communication-centric applications on encountering a cache miss lead to a considerable burden on the underlying network for transferring blocks from multiple cache hierarchies to the requesting core as packets. This increases the packet transmission latency, thereby slowing down the system performance. Also, NoC being the highest component of power consumption after the cores, an increase in packets increases the dynamic power consumption of NoC. The article proposes FlitZip that addresses the problem by reducing on-chip traffic through compressing network packets. Hence, the compressed packet requires less bandwidth during its transfer, reducing the network's power consumption. Experimental analysis shows that FlitZip achieves a better compression ratio of 52 percent, reduces packet latency and bandwidth utilization by 19.28 and 27 percent, respectively. It also reduces the area and power consumption of the de/compression units by 53.33 and 62.3 percent, respectively, compared to the state-of-the-art packet compression technique, NoΔ.","1558-2183","","10.1109/TPDS.2021.3090315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459512","Network on chip;packet;delta compression;multicore processor;router","Image coding;Bandwidth;Encoding;Power demand;Random access memory;System-on-chip;Multicore processing","multiprocessing systems;network-on-chip;system-on-chip","Flitzip;packet compression;NoC;multicore systems;intensive communication;power budget;data intensive applications;cache miss;multiple cache hierarchies;packet transmission;system performance;dynamic power consumption;network packets;compressed packet;compression ratio;bandwidth utilization;Network on Chip;transferring blocks;packet transmission latency;NoΔ;packet compression technique;on chip traffic;communication centric applications;MultiProcessor system on chip","",2.0,"",39.0,"IEEE","17 Jun 2021","","","IEEE","IEEE Journals"
"Efficient Distributed Approaches to Core Maintenance on Large Dynamic Graphs","T. Weng; X. Zhou; K. Li; P. Peng; K. Li","College of Computer Science and Electronic Engineering, Hunan University, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Hunan, China; Department of Computer Science, State University of New York, New Paltz, NY, USA","IEEE Transactions on Parallel and Distributed Systems","5 Jul 2021",2022,33.0,1.0,129,143,"As a fundamental problem in graph analysis, core decomposition aims to compute the core numbers of vertices in a given graph. It is a powerful tool for mining important graph structures. For dynamic graphs with real-time updates of vertices/edges, core maintenance has been utilized to update the core numbers of vertices. The previous approaches to core maintenance face challenges in terms of storage and efficiency. In this article, we investigate distributed approaches to core maintenance on a pregel-like system, which is a famous graph computing system. We first design a core decomposition algorithm to obtain core numbers of vertices in a given graph. Based on it, a distributed batch-stream combined algorithm (DBCA) is devised to efficiently maintain the core numbers when vertex/edge updates happen. In particular, we introduce a new task assignment strategy to DBCA based on diversity of the edge-cores of updated edges. To ensure that DBCA can accurately process core maintenance, we develop a message interaction protocol to resolve the problem of crosstalk among different tasks. Comprehensive experiments have been conducted on real/synthetic graphs, more specifically, in two typical distributed environments built on Supercomputing Center and Alibaba Cloud. The experiment results demonstrate that our proposed algorithms are efficient and scalable.","1558-2183","","10.1109/TPDS.2021.3090759","National Key Research and Development Program of China(grant numbers:2018YFB0204302); National Natural Science Foundation of China(grant numbers:61772182); Key Area Research Program of Hunan(grant numbers:2019GK2091); Zhejiang Laboratory(grant numbers:2021KD0AB02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462402","Core decomposition;core maintenance;distributed system;dynamic graphs","Task analysis;Maintenance engineering;Heuristic algorithms;Crosstalk;Synchronization;Protocols;Distributed algorithms","data analysis;data mining;distributed processing;graph theory","distributed approach;graph analysis;graph computing system;core decomposition algorithm;distributed batch-stream combined algorithm;graph structure mining;task assignment strategy;Supercomputing Center;Alibaba Cloud","",9.0,"",39.0,"IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"Elastic Deep Learning in Multi-Tenant GPU Clusters","Y. Wu; K. Ma; X. Yan; Z. Liu; Z. Cai; Y. Huang; J. Cheng; H. Yuan; F. Yu","Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Huawei Technologies Co. Ltd, Shenzhen, China; Huawei Technologies Co. Ltd, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","8 Jul 2021",2022,33.0,1.0,144,158,"We study how to support elasticity, that is, the ability to dynamically adjust the parallelism (i.e., the number of GPUs), for deep neural network (DNN) training in a GPU cluster. Elasticity can benefit multi-tenant GPU cluster management in many ways, for example, achieving various scheduling objectives (e.g., job throughput, job completion time, GPU efficiency) according to cluster load variations, utilizing transient idle resources, and supporting performance profiling, job migration, and straggler mitigation. We propose EDL, which enables elastic deep learning with a simple API and can be easily integrated with existing deep learning frameworks such as TensorFlow and PyTorch. EDL also incorporates techniques that are necessary to reduce the overhead of parallelism adjustments, such as stop-free scaling and dynamic data pipeline. We demonstrate with experiments that EDL can indeed bring significant benefits to the above-listed applications in GPU cluster management.","1558-2183","","10.1109/TPDS.2021.3064966","GRF(grant numbers:14208318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373916","Deep learning system;elastic deep learning;GPU cluster management","Graphics processing units;Training;Parallel processing;Elasticity;Throughput;Deep learning;Transient analysis","application program interfaces;deep learning (artificial intelligence);graphics processing units;parallel processing","parallelism adjustments;EDL;elastic deep learning;elasticity;deep neural network training;multitenant GPU cluster management;cluster load variations;transient idle resources;job migration;performance profiling;DNN training;API","",4.0,"",61.0,"IEEE","9 Mar 2021","","","IEEE","IEEE Journals"
"A Pattern-Based SpGEMM Library for Multi-Core and Many-Core Architectures","Z. Xie; G. Tan; W. Liu; N. Sun","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, China University of Petroleum, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","12 Jul 2021",2022,33.0,1.0,159,175,"General sparse matrix-matrix multiplication (SpGEMM) is one of the most important mathematical library routines in a number of applications. In recent years, several efficient SpGEMM algorithms have been proposed, however, most of them are based on the compressed sparse row (CSR) format, and the possible performance gain from exploiting other formats has not been well studied. And some specific algorithms are restricted to parameter tuning that has a significant impact on performance. So the particular format, algorithm, and parameter that yield the best performance for SpGEMM remain undetermined. In this article, we conduct a prospective study on format-specific parallel SpGEMM algorithms and analyze their pros and cons. We then propose a pattern-based SpGEMM library, that provides a unified programming interface in the CSR format, analyses the pattern of two input matrices, and automatically determines the best format, algorithm, and parameter for arbitrary matrix pairs. For this purpose, we build an algorithm set that integrates three new designed algorithms with existing popular libraries, and design a hybrid deep learning model called MatNet to quickly identify patterns of input matrices and accurately predict the best solution by using sparse features and density representations. The evaluation shows that this library consistently outperforms the state-of-the-art library. We also demonstrate its adaptability in an AMG solver and a BFS algorithm with 30 percent performance improvement.","1558-2183","","10.1109/TPDS.2021.3090328","National Key Research and Development Program of China(grant numbers:2017YFB0202105,2016YFB0201305,2016YFB0200803,2016YFB0200300); National Natural Science Foundation of China(grant numbers:61521092,91430218,31327901,61472395,61432018,61671151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459513","SpGEMM;spare BLAS;sparse format;auto-tuning;neural network","Libraries;Sparse matrices;Prediction algorithms;Neural networks;Predictive models;Memory management;Tuners","deep learning (artificial intelligence);matrix multiplication;multiprocessing systems;parallel processing;sparse matrices","algorithm set;sparse features;density representations;BFS algorithm;pattern-based SpGEMM library;multicore architectures;many-core architectures;general sparse matrix-matrix multiplication;compressed sparse row format;parameter tuning;format-specific parallel SpGEMM algorithms;CSR format;arbitrary matrix pairs;mathematical library routines;AMG solver;hybrid deep learning;MatNet","",4.0,"",68.0,"IEEE","17 Jun 2021","","","IEEE","IEEE Journals"
"Scalable, Confidential and Survivable Software Updates","F. Magnanini; L. Ferretti; M. Colajanni","Department of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Modena, Italy; Department of Physics, Computer Science and Mathematics, University of Modena and Reggio Emilia, Modena, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy","IEEE Transactions on Parallel and Distributed Systems","5 Jul 2021",2022,33.0,1.0,176,191,"Software update systems must guarantee high availability, integrity and security even in presence of cyber attacks. We propose the first survivable software update framework for the secure distribution of confidential updates that is based on a distributed infrastructure with no single points of failure. Previous works guarantee either survivability or confidentiality of software updates but do not ensure both properties. Our proposal is based on an original application of a multi-authority attribute-based encryption scheme in the context of decentralized access control management that avoids single-point-of-vulnerability. We describe the original framework, propose the protocols to implement it, and demonstrate its feasibility through a security and performance evaluation.","1558-2183","","10.1109/TPDS.2021.3090330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459483","Software updates;survivability;transparency;proprietary software","Software;Proposals;Encryption;Servers;Authentication;Access control;Protocols","authorisation;cryptography;software performance evaluation","software update systems;security;cyber attacks;survivable software update framework;secure distribution;confidential updates;distributed infrastructure;multiauthority attribute-based encryption scheme;single-point-of-vulnerability;performance evaluation","",1.0,"",28.0,"IEEE","17 Jun 2021","","","IEEE","IEEE Journals"
"Communication-Efficient Federated Learning With Compensated Overlap-FedAvg","Y. Zhou; Q. Ye; J. Lv","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2021",2022,33.0,1.0,192,205,"While petabytes of data are generated each day by a number of independent computing devices, only a few of them can be finally collected and used for deep learning (DL) due to the apprehension of data security and privacy leakage, thus seriously retarding the extension of DL. In such a circumstance, federated learning (FL) was proposed to perform model training by multiple clients' combined data without the dataset sharing within the cluster. Nevertheless, federated learning with periodic model averaging (FedAvg) introduced massive communication overhead as the synchronized data in each iteration is about the same size as the model, and thereby leading to a low communication efficiency. Consequently, variant proposals focusing on the communication rounds reduction and data compression were proposed to decrease the communication overhead of FL. In this article, we propose Overlap-FedAvg, an innovative framework that loosed the chain-like constraint of federated learning and paralleled the model training phase with the model communication phase (i.e., uploading local models and downloading the global model), so that the latter phase could be totally covered by the former phase. Compared to vanilla FedAvg, Overlap-FedAvg was further developed with a hierarchical computing strategy, a data compensation mechanism, and a nesterov accelerated gradients (NAG) algorithm. In Particular, Overlap-FedAvg is orthogonal to many other compression methods so that they could be applied together to maximize the utilization of the cluster. Besides, the theoretical analysis is provided to prove the convergence of the proposed framework. Extensive experiments conducting on both image classification and natural language processing tasks with multiple models and datasets also demonstrate that the proposed framework substantially reduced the communication overhead and boosted the federated learning process.","1558-2183","","10.1109/TPDS.2021.3090331","National Key Research and Development Program of China(grant numbers:2017YFB1002201); National Natural Science Fund for Distinguished Young Scholar(grant numbers:61625204); National Natural Science Foundation of China(grant numbers:61836006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459540","Distributed computing;federated learning;overlap;efficient communication","Collaborative work;Data models;Training;Servers;Computational modeling;Convergence;Deep learning","data compression;data privacy;gradient methods;image classification;learning (artificial intelligence);mobile computing;natural language processing","global model;vanilla FedAvg;data compensation mechanism;federated learning process;communication-efficient federated learning;compensated Overlap-FedAvg;independent computing devices;deep learning;data security;privacy leakage;multiple clients;periodic model averaging;massive communication overhead;synchronized data;low communication efficiency;communication rounds reduction;data compression;model training phase;model communication phase;local models","",25.0,"",52.0,"IEEE","17 Jun 2021","","","IEEE","IEEE Journals"
"Energy-Efficient Cache-Aware Scheduling on Heterogeneous Multicore Systems","S. Z. Sheikh; M. A. Pasha","Department of Electrical Engineering, School of Science and Engineering (SSE), Lahore University of Management Sciences (LUMS), Lahore, Pakistan; Department of Electrical Engineering, School of Science and Engineering (SSE), Lahore University of Management Sciences (LUMS), Lahore, Pakistan","IEEE Transactions on Parallel and Distributed Systems","7 Jul 2021",2022,33.0,1.0,206,217,"The adoption of heterogeneous multicore architectures into deadline-constrained embedded systems has various benefits in terms of schedulability and energy-efficiency. Existing energy-efficient algorithms, in this domain, allocate tasks to their energy-favorable core-types while using dynamic voltage and frequency scaling to reduce energy consumption. However, the practicality of such algorithms is limited due to the underlying assumptions made to simplify the analysis. This article paves the way for more practical approaches to minimize the energy consumption on heterogeneous multicores. Specifically, we investigate the nonlinear impacts that core-frequency and cache-partitioning have on task-executions in a heterogeneous multicore environment. In doing so, we propose an algorithm that exploits this relationship to effectively allocate tasks to specific cores and core-types, and determine the number of cache-partitions for each core. Extensive simulations using real-world benchmarks show the proficiency of our approach by achieving an average and maximum energy savings of 14.9 and 20.4 percent, respectively for core-level energy consumption, and 20.2 and 60.4 percent, respectively for system-level energy consumption.","1558-2183","","10.1109/TPDS.2021.3090587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460819","Scheduling;multicores;heterogeneous;energy-efficiency","Multicore processing;Task analysis;Optimal scheduling;Energy consumption;Radio spectrum management;Resource management;Scheduling algorithms","cache storage;embedded systems;energy conservation;energy consumption;multiprocessing systems;power aware computing;scheduling","energy-efficient cache-aware scheduling;multicore architectures;energy-efficiency;frequency scaling;core-frequency;cache-partitioning;multicore environment;cache-partitions;energy savings;system-level energy consumption;schedulability","",2.0,"",26.0,"IEEE","18 Jun 2021","","","IEEE","IEEE Journals"
"PLVER: Joint Stable Allocation and Content Replication for Edge-Assisted Live Video Delivery","H. Wang; G. Tang; K. Wu; J. Wang","Department of Computer Science, University of Victoria, Victoria, BC, Canada; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Department of Computer Science, University of Victoria, Victoria, BC, Canada; Department of Computer Science, City University of Hong Kong, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","12 Jul 2021",2022,33.0,1.0,218,230,"Live streaming services have gained extreme popularity in recent years. Due to the spiky traffic patterns of live videos, utilizing distributed edge servers to improve viewers' quality of experience (QoE) has become a common practice nowadays. Nevertheless, the current client-driven content caching mechanism does not support pre-caching from the cloud to the edge, resulting in a considerable amount of cache misses in live video delivery. By jointly considering the features of live videos and edge servers, we propose PLVER, a proactive live video push scheme to address the cache miss problem in live video delivery. Specifically, PLVER first conducts a one-to-multiple stable allocation between edge clusters and user groups to balance the load of live traffic over the edge servers. It then adopts proactive video replication algorithms to speed up video replication among the edge servers. We conduct extensive trace-driven evaluation, covering 0.3 million Twitch viewers and more than 300 Twitch channels. The results demonstrate that with PLVER, edge servers can carry 28 and 82 percent more traffic than the auction-based replication (ABR) method and the caching on requested time (CORT) method, respectively.","1558-2183","","10.1109/TPDS.2021.3090784","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2018-03896); National Natural Science Foundation of China(grant numbers:61802421); China Postdoctoral Science Foundation(grant numbers:2019M663017); Hong Kong Research Grant Council(grant numbers:RIF R5060-19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462365","Live video streaming;stable one-to-multiple allocation;proactive video replication;edge computing","Streaming media;Servers;Resource management;Quality of experience;Social networking (online);Real-time systems;Clustering algorithms","bandwidth allocation;cache storage;client-server systems;distributed processing;multimedia Web sites;quality of experience;social networking (online);telecommunication traffic;video streaming","PLVER;joint stable allocation;content replication;edge-assisted live video delivery;live streaming services;distributed edge servers;client-driven content caching mechanism;proactive live video push scheme;cache miss problem;one-to-multiple stable allocation;proactive video replication;live video spiky traffic patterns;Twitch channel;viewers quality of experience;viewers QoE;Twitch viewers","",2.0,"",38.0,"IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"Taming System Dynamics on Resource Optimization for Data Processing Workflows: A Probabilistic Approach","A. C. Zhou; W. Xue; Y. Xiao; B. He; S. Ibrahim; R. Cheng","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; School of Computing, National University of Singapore, Singapore; Inria, Univ. Rennes, CNRS, IRISA, Rennes, France; Department of Computer Science, University of Hong Kong, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","12 Jul 2021",2022,33.0,1.0,231,248,"In many data-intensive applications, workflow is often used as an important model for organizing data processing tasks and resource provisioning is an important and challenging problem for improving the performance of workflows. Recently, system variations in the cloud and large-scale clusters, such as those in I/O and network performances and failure events, have been observed to greatly affect the performance of workflows. Traditional resource provisioning methods, which overlook these variations, can lead to suboptimal resource provisioning results. In this article, we provide a general solution for workflow performance optimizations considering system variations. Specifically, we model system dynamics as time-dependent random variables and take their probability distributions as optimization input. Despite its effectiveness, this solution involves heavy computation overhead. Thus, we propose three pruning techniques to simplify workflow structure and reduce the probability evaluation overhead. We implement our techniques in a runtime library, which allows users to incorporate efficient probabilistic optimization into existing resource provisioning methods. Experiments show that probabilistic solutions can improve the performance by up to 65 percent compared to state-of-the-art static solutions, and our pruning techniques can greatly reduce the overhead of our probabilistic approach.","1558-2183","","10.1109/TPDS.2021.3091400","National Natural Science Foundation of China(grant numbers:61802260); Shenzhen Science and Technology Foundation(grant numbers:JCYJ20180305125737520); Natural Science Foundation of SZU(grant numbers:000370); Microsoft Research Asia; ANR KerStream(grant numbers:ANR-16-CE25-0014-01); Research Grants Council of HK(grant numbers:17229116,106150091,17205115); University of HK(grant numbers:104004572,102009508,104004129); Innovation&Technology Commission of HK(grant numbers:MRP/029/18); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462122","Cloud dynamics;resource optimization;data processing workflows","Optimization;Task analysis;Probabilistic logic;Dynamic scheduling;Data processing;Cloud computing;Probability distribution","cloud computing;data analysis;optimisation;pattern clustering;resource allocation;statistical distributions","resource optimization;probabilistic approach;data-intensive applications;data processing tasks;cloud;large-scale clusters;network performances;failure events;suboptimal resource provisioning results;workflow performance optimizations;time-dependent random variables;optimization input;heavy computation overhead;pruning techniques;resource provisioning methods;taming system dynamics;data processing workflows;I/O performances","","","",51.0,"IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"iBalancer: Load-Aware in-Server Flow Scheduling for Sub-Millisecond Tail Latency","Q. Zhang; Y. Liu; T. Liu","Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China; Shandong Provincial Key Laboratory of Computer Networks, Shandong Computer Science Center (National Supercomputer Center in Jinan), Shandong Academy of Sciences, Qilu University of Technology, Jinan, Shandong, China","IEEE Transactions on Parallel and Distributed Systems","8 Dec 2021",2022,33.0,8.0,1761,1774,"Achieving microsecond-scale tail latency poses an extreme challenge to the conventional architecture of “NIC-OS-Application” in the face of high concurrent requests. Existing kernel-bypass network systems improve this situation significantly. Still, they cannot achieve load-aware in-server requests distribution, which in turn not only harms resource efficiency but, more importantly, beats the goal of squeezing tail latency. This paper proposes iBalancer, an in-server proactive load balancer for the kernel-bypass system, which aggressively handles NIC-side flow scheduling according to the load of threads on the processor-side. Furthermore, we propose a novel metric, “polling time interval (PTI),” to quantify the load of worker threads, which not only indicates utilization of the core bound to the worker thread but also reflects the differences in the processing time of different flows. By scheduling flows according to the metric PTI, iBalancer tends to average the queueing latencies of different flows, such as Set & Get operations for an in-memory key-value store. In addition, by decoupling flow scheduling from packet steering, iBalancer achieves a tail latency aware flow-to-core binding and preserves hardware-based request distribution among cores. The proposed system is evaluated and compared to mTCP and Shenango using two representative microsecond-scale network applications: Memcached KVS and a real-time deep-learning-based financial fraud identification application. Experimental results show that iBalancer can process up to 4.75$ \times $× and 1.55$ \times \ $× higher load over mTCP and Shenango under 500μs 99th percentile tail latency limit on Memcached. For the financial fraud identification application, iBalancer is able to process 4.56$ \times $× and 1.16$ \times $× higher load than mTCP and Shenango considering 900μs tail latency.","1558-2183","","10.1109/TPDS.2021.3120021","National Key Research and Development Program of China(grant numbers:2016YFB0200100); National Natural Science Foundation of China(grant numbers:61732002,62002186); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573311","Quality of service;parallel systems;performance measurements","Instruction sets;Servers;Dispatching;Measurement;Load management;Task analysis;Network systems","cache storage;deep learning (artificial intelligence);file servers;financial data processing;fraud;queueing theory;resource allocation;telecommunication scheduling;transport protocols","metric PTI;iBalancer;queueing latencies;flow-to-core binding;representative microsecond-scale network applications;financial fraud identification application;percentile tail;load-aware in-server flow scheduling;sub-millisecond tail;microsecond-scale tail;NIC-OS-Application;load-aware in-server requests distribution;in-server proactive load balancer;kernel-bypass system;hardware-based request distribution;kernel-bypass network systems;polling time interval;in-memory key-value store;tail latency aware flow-to-core binding;real-time deep-learning;Memcached;packet steering","","","",56.0,"IEEE","14 Oct 2021","","","IEEE","IEEE Journals"
"FedGraph: Federated Graph Learning With Intelligent Sampling","F. Chen; P. Li; T. Miyazaki; C. Wu","University of Aizu, Aizuwakamatsu, Japan; University of Aizu, Aizuwakamatsu, Japan; University of Aizu, Aizuwakamatsu, Japan; University of Electro-Communications, Chofu, Japan","IEEE Transactions on Parallel and Distributed Systems","8 Dec 2021",2022,33.0,8.0,1775,1786,"Federated learning has attracted much research attention due to its privacy protection in distributed machine learning. However, existing work of federated learning mainly focuses on Convolutional Neural Network (CNN), which cannot efficiently handle graph data that are popular in many applications. Graph Convolutional Network (GCN) has been proposed as one of the most promising techniques for graph learning, but its federated setting has been seldom explored. In this article, we propose FedGraph for federated graph learning among multiple computing clients, each of which holds a subgraph. FedGraph provides strong graph learning capability across clients by addressing two unique challenges. First, traditional GCN training needs feature data sharing among clients, leading to risk of privacy leakage. FedGraph solves this issue using a novel cross-client convolution operation. The second challenge is high GCN training overhead incurred by large graph size. We propose an intelligent graph sampling algorithm based on deep reinforcement learning, which can automatically converge to the optimal sampling policies that balance training speed and accuracy. We implement FedGraph based on PyTorch and deploy it on a testbed for performance evaluation. The experimental results of four popular datasets demonstrate that FedGraph significantly outperforms existing work by enabling faster convergence to higher accuracy.","1558-2183","","10.1109/TPDS.2021.3125565","Okawa Foundation for Information and Telecommunications; G-7 Scholarship Foundation; JSPS KAKENHI(grant numbers:21H03424,19K20258); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606516","Federated learning;graph learning;graph sampling;reinforcement learning","Training;Convolution;Collaborative work;Servers;Privacy;Computational modeling;Convolutional neural networks","convolutional neural nets;data privacy;deep learning (artificial intelligence);graph theory;reinforcement learning;sampling methods","intelligent graph sampling algorithm;deep reinforcement learning;FedGraph;federated graph learning;federated learning;distributed machine learning;convolutional neural network;graph data;graph convolutional network;federated setting;multiple computing clients;strong graph learning capability;traditional GCN training needs;graph size;cross-client convolution operation","",8.0,"",43.0,"IEEE","8 Nov 2021","","","IEEE","IEEE Journals"
"A Bifactor Approximation Algorithm for Cloudlet Placement in Edge Computing","D. Bhatta; L. Mashayekhy","Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA; Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA","IEEE Transactions on Parallel and Distributed Systems","8 Dec 2021",2022,33.0,8.0,1787,1798,"Emerging applications with low-latency requirements such as real-time analytics, immersive media applications, and intelligent virtual assistants have rendered Edge Computing as a critical computing infrastructure. Existing studies have explored the cloudlet placement problem in a homogeneous scenario with different goals such as latency minimization, load balancing, energy efficiency, and placement cost minimization. However, placing cloudlets in a highly heterogeneous deployment scenario considering the next-generation 5G networks and IoT applications is still an open challenge. The novel requirements of these applications indicate that there is still a gap in ensuring low-latency service guarantees when deploying cloudlets. Furthermore, deploying cloudlets in a cost-effective manner and ensuring full coverage for all users in edge computing are other critical conflicting issues. In this article, we address these issues by designing a bifactor approximation algorithm to solve the heterogeneous cloudlet placement problem to guarantee a bounded latency and placement cost, while fully mapping user applications to appropriate cloudlets. We first formulate the problem as a multi-objective integer programming model and show that it is a computationally NP-hard problem. We then propose a bifactor approximation algorithm, ACP, to tackle its intractability. We investigate the effectiveness of ACP by performing extensive theoretical analysis and experiments on multiple deployment scenarios based on New York City OpenData. We prove that ACP provides a (2,4)-approximation ratio for the latency and the placement cost. The experimental results show that ACP obtains near-optimal results in a polynomial running time making it suitable for both short-term and long-term cloudlet placement in heterogeneous deployment scenarios.","1558-2183","","10.1109/TPDS.2021.3126256","National Science Foundation(grant numbers:CNS-1755913); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609538","Edge computing;cloudlets;placement cost;latency;full coverage;approximation algorithm","Cloud computing;Costs;Approximation algorithms;Edge computing;Servers;Low latency communication;Internet of Things","approximation theory;cloud computing;computational complexity;integer programming;Internet of Things;quality of service;resource allocation","IoT applications;low-latency service guarantees;edge computing;critical conflicting issues;bifactor approximation algorithm;heterogeneous cloudlet placement problem;bounded latency;NP-hard problem;ACP;multiple deployment scenarios;approximation ratio;long-term cloudlet placement;heterogeneous deployment scenarios;low-latency requirements;real-time analytics;immersive media applications;intelligent virtual assistants;critical computing infrastructure;homogeneous scenario;latency minimization;placement cost minimization;next-generation 5G networks;heterogeneous deployment scenario;New York City OpenData;(2,4)-approximation ratio;polynomial running time","",6.0,"",44.0,"IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"LoomIO: Object-Level Coordination in Distributed File Systems","Y. Hua; X. Shi; K. He; H. Jin; W. Xie; L. He; Y. Chen","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; VMware Inc., Palo Alto, CA, USA; University of Warwick, Coventry, U.K; Texas Tech University, Lubbock, TX, USA","IEEE Transactions on Parallel and Distributed Systems","8 Dec 2021",2022,33.0,8.0,1799,1810,"Device-level interference is recognized as a major cause of the performance degradation in distributed file systems. Although the approaches of mitigating interference through coordination at application-level, middleware-level, and server-level have shown beneficial results in previous studies, we find their effectiveness is largely reduced since I/O requests are re-arranged by underlying object file systems. In this research study, we prove that object-level coordination is critical and often the key to address the interference issue, as the scheduling of object requests determines the device-level accesses and thus determines the actual I/O bandwidth and latency. This article proposes an object-level coordination system, LoomIO, which uses an OBOP (One-Broadcast-One-Propagate) method and a time-limited coordination process to deliver highly efficient coordination service. Specifically, LoomIO enables object requests to achieve an optimized scheduling decision within a few milliseconds and largely mitigates the device-level interference. We have implemented a LoomIO prototye and integrated it into Ceph file system. The evaluation results show that LoomIO achieved the considerable improvements in resource utilization (by up to 35%), in I/O throughput (by up to 31%), and in 99th percentile latency (by up to 54%) compared to the K-optimal method which uses the same scheduling algorithm as LoomIO but does not have the coordination support.","1558-2183","","10.1109/TPDS.2021.3126260","National Key Research and Development Program of China(grant numbers:2020AAA0108501); National Natural Science Foundation of China(grant numbers:61772218); Key R&D Program of Hubei(grant numbers:2020BAA020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609610","Distributed object file system;I/O coordination;performance;erasure-coding","Interference;Layout;Performance evaluation;Throughput;Redundancy;Receivers;Encoding","distributed databases;middleware;network operating systems;resource allocation;scheduling","LoomIO;distributed file systems;device-level interference;application-level;middleware-level;server-level;interference issue;device-level accesses;object-level coordination system;Ceph file system;coordination support;OBOP;object file systems;coordination service;one-broadcast-one-propagate;performance degradation;I/O requests;object requests scheduling;I/O bandwidth;time-limited coordination;optimized scheduling decision;resource utilization;I/O throughput","","","",36.0,"IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Efficient and Automated Deployment Architecture for OpenStack in TianHe SuperComputing Environment","B. Jiang; Z. Tang; X. Xiao; J. Yao; R. Cao; K. Li","College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China; College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China; College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China; College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China","IEEE Transactions on Parallel and Distributed Systems","8 Dec 2021",2022,33.0,8.0,1811,1824,"Recently, with the large-scale outbreak of the global financial crisis and public safety incidents (such as COVID-19), high-performance computing has been widely applied to risk prediction, vaccine development, and other fields. In scenarios where high-performance computing infrastructure responds to the instantaneous explosion of computing demands, a crucial issue is to provide large-scale flexible allocation and adjustment of computing capability by rapidly constructing computing clusters. Existing large-scale computing cluster deployment solutions usually utilize source code deployment or other deployment tools. The great challenge of existing deployment methods is to reduce excessive image distribution time and refrain from configuration defects. In this article, we design an intelligent distributed registry deployment (IDRD) architecture based on the OpenStack cloud platform, which adaptively places distributed image repositories using the containerized deployment of multiple registries. We propose a server load priority algorithm to solve multiple registries placement problems in IDRD. Furthermore, we devise a clustering algorithm based on demand density that can optimize the global performance of IDRD and improve large-scale cluster load balancing capabilities, which has been implemented in the TianHe Supercomputing environment. Extensive experimental results demonstrate that IDRD can effectively reduce $30\%$30%-$50\%$50% of the distribution time of component images and significantly improve the efficiency of large-scale cluster deployment.","1558-2183","","10.1109/TPDS.2021.3127128","National Key Research and Development Program of China(grant numbers:2018YFB1701400); National Natural Science Foundation of China(grant numbers:92055213,61873090,L1924056,62002114); Guangdong Province research and development plan project(grant numbers:2020B0101100001); China Knowledge Centre for Engineering Sciences and Technology(grant numbers:CKCEST-2021-2-7); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610176","Large-scale computing;supercomputing system;distributed containerized deployment;registries placement;OpenStack","Cloud computing;Containers;Computer architecture;Tools;Servers;Clustering algorithms;Resource management","cloud computing;parallel machines;parallel processing;pattern clustering;resource allocation","large-scale outbreak;global financial crisis;public safety incidents;COVID-19;risk prediction;vaccine development;high-performance computing infrastructure responds;instantaneous explosion;computing demands;large-scale flexible allocation;computing capability;computing clusters;large-scale computing cluster deployment solutions;source code deployment;deployment methods;excessive image distribution time;intelligent distributed registry deployment architecture;IDRD;OpenStack cloud platform;image repositories;containerized deployment;server load priority algorithm;multiple registries placement problems;clustering algorithm;large-scale cluster load balancing capabilities;large-scale cluster deployment;TianHe supercomputing environment","","","",37.0,"IEEE","10 Nov 2021","","","IEEE","IEEE Journals"
"Adaptive Resource Efficient Microservice Deployment in Cloud-Edge Continuum","K. Fu; W. Zhang; Q. Chen; D. Zeng; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science, China University of Geosciences, Wuhan, Hubei, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","8 Dec 2021",2022,33.0,8.0,1825,1840,"User-facing services are now evolving towards the microservice architecture where a service is built by connecting multiple microservice stages. Since the entire service is heavy, the microservice architecture shows the opportunity to only offload some microservice stages to the edge devices that are close to the end users. However, emerging techniques often result in the violation of Quality-of-Service (QoS) of microservice-based services in cloud-edge continuum, as they do not consider the communication overhead or the resource contention between microservices and external co-located tasks. We propose Nautilus, a runtime system that effectively deploys microservice-based user-facing services in cloud-edge continuum. Nautilus ensures the QoS of microservice-based user-facing services while minimizing the required computational resources, which is comprised of a communication-aware microservice mapper, a contention-aware resource manager and an IO-sensitive and load-aware microservice migration scheduler. The mapper divides the microservice graph into multiple partitions based on the communication overhead and maps the partitions to appropriate nodes. On each node, the resource manager determines the optimal resource allocation for its microservices based on reinforcement learning that may capture the complex contention behaviors. Once the microservices are suffered from external IO pressure, the IO-sensitive microservice scheduler migrates the critical one to idle nodes. Furthermore, when the load of microservices changes dynamically, the load-aware microservice scheduler migrates microservices from busy nodes to idle ones to ensure the QoS goal of the entire service. Our experimental results show that Nautilus can guarantee the required QoS target under external shared resources contention while the state-of-the-art suffers from QoS violations. Meanwhile, Nautilus reduces the computational resource usage by 23.9% and the network bandwidth usage by 53.4%, while achieving the required 99%-ile latency.","1558-2183","","10.1109/TPDS.2021.3128037","National Key Research and Development Program of China(grant numbers:2018YFB1004800); National Natural Science Foundation of China(grant numbers:62022057,61832006,61632017,61872240); Open Research Projects of Zhejiang Lab(grant numbers:2021KE0AB02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9615028","Cloud-edge continuum;QoS;microservice resources management","Quality of service;Cloud computing;Task analysis;Resource management;Computer architecture;Runtime;Bandwidth","quality of service;radio networks;reinforcement learning;resource allocation;telecommunication computing;telecommunication network performance;telecommunication scheduling","adaptive resource efficient microservice deployment;cloud-edge continuum;microservice architecture;multiple microservice stages;entire service;quality-of-service;microservice-based services;communication overhead;resource contention;microservice-based user-facing services;required computational resources;communication-aware microservice mapper;contention-aware resource manager;load-aware microservice migration scheduler;microservice graph;IO-sensitive microservice scheduler;microservices changes;load-aware microservice scheduler;external shared resources contention;user-facing services;Nautilus;runtime system;optimal resource allocation;reinforcement learning;external IO pressure;computational resource usage;network bandwidth usage","",13.0,"",57.0,"IEEE","15 Nov 2021","","","IEEE","IEEE Journals"
"Online Learning for Distributed Computation Offloading in Wireless Powered Mobile Edge Computing Networks","X. Wang; Z. Ning; L. Guo; S. Guo; X. Gao; G. Wang","School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Computational Intelligence, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Parallel and Distributed Systems","10 Dec 2021",2022,33.0,8.0,1841,1855,"A novel paradigm named Wireless Powered Mobile Edge Computing (WP-MEC) emerges recently, which integrates Mobile Edge Computing (MEC) and Wireless Power Transfer (WPT) technologies. It enables mobile clients to both extend their computing capacities by task offloading, and charge from edge servers via energy transmission. Existing studies generally focus on the centralized design of task scheduling and energy charging in WP-MEC networks. To meet the decentralization requirement of the near-coming 6G network, we propose an online learning algorithm for computation offloading in WP-MEC networks with a distributed execution manner. Specifically, we first define the delay minimization problem by considering task deadline and energy constraints. Then, we transform it into a primal-dual optimization problem based on the Bellman equation. After that, we design a novel neural model that learns both offloading and time division decisions in each time slot to solve the formulated optimization problem. To train and execute the designed algorithm distributivity, we form multiple learning models decentralized on edge servers and they work coordinately to achieve parameter synchronization. At last, both theoretical and performance analyses show that the designed algorithm has significant advantages in comparison with other representative schemes.","1558-2183","","10.1109/TPDS.2021.3129618","National Natural Science Foundation of China(grant numbers:61872310,61971084,62025105,62001073); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,15220320/20E); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:R2020A045); Natural Science Foundation of Chongqing(grant numbers:cstc2019jcyj-cxttX0002,cstc2021ycjh-bgzxm0013,cstc2021ycjh-bgzxm0039,cstc2021jcyj-msxmX0031); Chongqing Municipal Education Commission(grant numbers:HZ2021008,CXQT21019); Support Program for Overseas Students to Return to China for Entrepreneurship and Innovation(grant numbers:cx2021003,cx2021053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623474","Wireless powered mobile edge computing;delay minimization;distributed execution;online learning","Task analysis;Servers;Processor scheduling;Delays;Optimization;Resource management;Multi-access edge computing","6G mobile communication;minimisation;mobile computing;scheduling;synchronisation","distributed computation offloading;wireless powered mobile edge computing networks;mobile clients;edge servers;energy transmission;task scheduling;WP-MEC networks;online learning algorithm;energy constraints;primal-dual optimization problem;multiple learning models;WPT technologies;wireless power transfer technologies;decentralization requirement;delay minimization problem;6G network;Bellman equation;parameter synchronization","",12.0,"",31.0,"IEEE","22 Nov 2021","","","IEEE","IEEE Journals"
"Evaluating Data Redistribution in PaRSEC","Q. Cao; G. Bosilca; N. Losada; W. Wu; D. Zhong; J. Dongarra","Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA; Los Alamos National Laboratory, New Mexico, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA","IEEE Transactions on Parallel and Distributed Systems","13 Dec 2021",2022,33.0,8.0,1856,1872,"Data redistribution aims to reshuffle data to optimize some objective for an algorithm. The objective can be multi-dimensional, such as improving computational load balance or decreasing communication volume or cost, with the ultimate goal of increasing the efficiency and therefore reducing the time-to-solution for the algorithm. The classic redistribution problem focuses on optimally scheduling communications when reshuffling data between two regular, usually block-cyclic, data distributions. Besides distribution, data size is also a performance-critical parameter because it affects the reshuffling algorithm in terms of cache, communication efficiency, and potential parallelism. In addition, task-based runtime systems have gained popularity recently as a potential candidate to address the programming complexity on the way to exascale. In this scenario, it becomes paramount to develop a flexible redistribution algorithm for task-based runtime systems, which could support all types of regular and irregular data distributions and take data size into account. In this article, we detail a flexible redistribution algorithm and implement an efficient approach in a task-based runtime system, PaRSEC. Performance results show great capability compared to the theoretical bound and ScaLAPACK, and applications highlight an increased efficiency with little overhead in terms of data distribution, data size, and data format.","1558-2183","","10.1109/TPDS.2021.3131657","Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629320","Data redistribution;data distribution;data size;data format;task-based programming model;dynamic runtime system;high-performance computing","Task analysis;Runtime;Distributed databases;Programming;Parallel processing;Costs;Program processors","cache storage;data handling;parallel processing;resource allocation;scheduling","PaRSEC;data redistribution;computational load balance;data size;performance-critical parameter;communication efficiency;task-based runtime system;flexible redistribution algorithm;irregular data distributions;data format;communication volume;communication cost;time-to-solution;scheduling;data reshuffling;block-cyclic data distributions;cache;parallelism;programming complexity;ScaLAPACK","",2.0,"",68.0,"IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"CSEdge: Enabling Collaborative Edge Storage for Multi-Access Edge Computing Based on Blockchain","L. Yuan; Q. He; F. Chen; J. Zhang; L. Qi; X. Xu; Y. Xiang; Y. Yang","Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia; Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Computer Science, Qufu Normal University, Jining, Shandong, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China; Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia","IEEE Transactions on Parallel and Distributed Systems","10 Dec 2021",2022,33.0,8.0,1873,1887,"Multi-access Edge Computing (MEC), as an extension of cloud computing, provides storage resources at the network edge to enable low-latency data retrieval for users. Due to limited physical sizes and constrained storage resources, individual edge servers cannot store a large amount of data when operating independently. They often need to offload data to other edge servers to serve users collaboratively. Operated by different edge infrastructure providers, edge servers usually work in a distrusted environment. Incentive and trust are the two main challenges in facilitating collaborative edge storage. This article proposes CSEdge, a novel decentralized system that tackles these challenges to enable collaborative edge storage based on blockchain. On CSEdge, edge servers can submit data offloading requests for others to contend for. Winners are selected based on their reputations. They will store the offloaded data and receive rewards for successfully finishing data offloading tasks. Via a distributed consensus, their performance will be recorded on blockchain for future reputation evaluation. A prototype of CSEdge is built on Hyperledger Sawtooth and experimentally evaluated against a baseline system and two start-of-the-art systems in a simulated MEC environment. The results demonstrate that CSEdge can effectively and efficiently facilitate collaborative edge storage among edge servers.","1558-2183","","10.1109/TPDS.2021.3131680","Australian Research Council(grant numbers:DP180100212,DP200102491); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629346","Edge computing;cooperative edge computing;blockchain;distributed consensus;data offloading","Servers;Task analysis;Collaboration;Blockchains;Time factors;Cloud computing;Reliability","blockchains;cloud computing;groupware;information retrieval","CSEdge;blockchain;multiaccess edge computing;network edge;constrained storage resources;edge servers;edge infrastructure providers;offloaded data;data offloading tasks;collaborative edge storage;simulated MEC environment;distrusted environment;low-latency data retrieval","",15.0,"",56.0,"IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"Scaling Poisson Solvers on Many Cores via MMEwald","M. Wu; Y. Wu; H. Shang; Y. Liu; H. Cui; F. Li; X. Duan; Y. Zhang; X. Feng","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; National Supercomputer Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","16 Dec 2021",2022,33.0,8.0,1888,1901,"The Poisson solver for the calculation of the electrostatic potential is an essential primitive in quantum mechanics calculations. In this article, we adopt the Ewald method and propose a highly-optimized and scalable framework for Poisson solver, MMEwald, on the new generation Sunway supercomputer, capable of utilizing the collection of 390-core accelerators it uses. The MMEwald is based on a grid adapted cut-plane approach to partition the points into batches and distribute the batch to the processors. Furthermore, we propose a set of architecture-specific optimizations to efficiently utilize the memory bandwidth and computation capacity of the supercomputer. Experimental results demonstrate the efficiency of the MMEwald in providing strong and weak scaling performance.","1558-2183","","10.1109/TPDS.2021.3127138","National Natural Science Foundation of China(grant numbers:22003073); State Key Laboratory of Computer Architecture Foundation(grant numbers:CARCH 4205,CARCH 4411); National Natural Science Foundation of China(grant numbers:62090024,61872043,61802368); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9611019","Poisson solver;architecture-specific optimizations;many-core processor","Optimization;Bandwidth;Supercomputers;Electric potential;Boundary conditions;Electrostatics;Silicon","electrostatics;optimisation;Poisson equation;quantum theory","Poisson solver;MMEwald;generation Sunway supercomputer;390-core accelerators;electrostatic potential;quantum mechanics calculations;highly-optimized framework;scalable framework;grid adapted cut-plane approach;architecture-specific optimizations","","","",36.0,"IEEE","10 Nov 2021","","","IEEE","IEEE Journals"
"Construction of Dual-CISTs on an Infinite Class of Networks","X. -W. Qin; R. -X. Hao; J. Wu","Department of Mathematics, Beijing Jiaotong University, Beijing, P.R. China; Department of Mathematics, Beijing Jiaotong University, Beijing, P.R. China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA","IEEE Transactions on Parallel and Distributed Systems","20 Dec 2021",2022,33.0,8.0,1902,1910,"The main method to achieve fault-tolerant network systems is by exploiting and effectively utilizing the edge-disjoint and/or inner-vertex-disjoint paths between pairs of source and destination vertices. Completely independent spanning trees (CISTs for short) are powerful tools for reliable broadcasting/unicasting and secure message distribution. Particularly, it has been shown that two CISTs have an application on configuring a protection routing in IP networks, such as mobile ad hoc networks and relatively large (static) network topologies with scalability in [IEEE/ACM Trans. Netw., 27 (2019) 1112-1123]. Many results focus on CISTs in specific networks in the literature, however, few results are given on an infinite class of networks having common properties. In this article, we prove the existence of dual-CISTs in an infinite number of networks satisfying some Hamilton sufficient conditions. A unique algorithm to construct a CIST-partition is proposed, which can be applied to not only many kinds of networks, but our algorithm can also be implemented very easily in parallel or distributed systems satisfying the conditions. In addition, we make a comparative analysis between the proposed conditions and several known results on an infinite number of networks, the advantage of our result is significant. In particular, the bound in our conditions is sharp. The results will provide a powerful framework for the design of fault-tolerant network topologies and routing protocols for future networks.","1558-2183","","10.1109/TPDS.2021.3132412","National Natural Science Foundation of China(grant numbers:11971054,11731002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635685","Completely independent spanning trees;a protection routing;CIST-partition;Hamilton bipartition sufficient condition;constructive algorithm","Fault tolerant systems;Fault tolerance;Hypercubes;Tools;Routing;Network topology;Broadcasting","computer network reliability;computer network security;fault tolerant computing;IP networks;mobile ad hoc networks;routing protocols;telecommunication network topology;trees (mathematics)","Hamilton sufficient conditions;CIST-partition;parallel distributed systems;fault-tolerant network topologies;routing protocols;dual-CIST;infinite class;fault-tolerant network systems;edge-disjoint paths;destination vertices;completely independent spanning trees;secure message distribution;protection routing;IP networks;mobile ad hoc networks;inner-vertex-disjoint paths","",1.0,"",40.0,"IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"Adaptive and Efficient Resource Allocation in Cloud Datacenters Using Actor-Critic Deep Reinforcement Learning","Z. Chen; J. Hu; G. Min; C. Luo; T. El-Ghazawi","Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K; Department of Electrical and Computer Engineering, The George Washington University, Washington, DC, USA","IEEE Transactions on Parallel and Distributed Systems","16 Dec 2021",2022,33.0,8.0,1911,1923,"The ever-expanding scale of cloud datacenters necessitates automated resource provisioning to best meet the requirements of low latency and high energy-efficiency. However, due to the dynamic system states and various user demands, efficient resource allocation in cloud faces huge challenges. Most of the existing solutions for cloud resource allocation cannot effectively handle the dynamic cloud environments because they depend on the prior knowledge of a cloud system, which may lead to excessive energy consumption and degraded Quality-of-Service (QoS). To address this problem, we propose an adaptive and efficient cloud resource allocation scheme based on Actor-Critic Deep Reinforcement Learning (DRL). First, the actor parameterizes the policy (allocating resources) and chooses actions (scheduling jobs) based on the scores assessed by the critic (evaluating actions). Next, the resource allocation policy is updated by using gradient ascent while the variance of policy gradient is reduced with an advantage function, which improves the training efficiency of the proposed method. We conduct extensive simulation experiments using real-world data from Google cloud datacenters. The results show that our method can obtain the superior QoS in terms of latency and job dismissing rate with enhanced energy-efficiency, compared to two advanced DRL-based and five classic cloud resource allocation methods.","1558-2183","","10.1109/TPDS.2021.3132422","EU Horizon 2020 INITIATE(grant numbers:101008297); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635652","Cloud computing;datacenters;resource allocation;energy-efficiency;deep reinforcement learning","Resource management;Cloud computing;Quality of service;Training;Dynamic scheduling;Reinforcement learning;Energy consumption","cloud computing;computer centres;deep learning (artificial intelligence);quality of service;reinforcement learning;resource allocation;scheduling","actor-critic deep reinforcement learning;resource provisioning;energy consumption;Google cloud datacenters;energy efficiency;cloud resource allocation;quality of service;job scheduling;gradient ascent","",3.0,"",39.0,"IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"A GPU-Oriented Application Programming Interface for Digital Audio Workstations","D. Bianchi; F. Avanzini; A. Baratè; L. A. Ludovico; G. Presti","Computer Science Department, Università degli Studi di Milano, Milano, Italy; Computer Science Department, Università degli Studi di Milano, Milano, Italy; Computer Science Department, Università degli Studi di Milano, Milano, Italy; Computer Science Department, Università degli Studi di Milano, Milano, Italy; Computer Science Department, Università degli Studi di Milano, Milano, Italy","IEEE Transactions on Parallel and Distributed Systems","30 Dec 2021",2022,33.0,8.0,1924,1938,"A Digital Audio Workstation (DAW) is a hardware and/or software device aiming to ease those operations required for music production, such as arranging, recording, editing, mixing, and, more in general, modifying sounds creatively. A peculiarity of a DAW environment is that most of the work is highly parallelizable, since the basic architecture of a DAW consists in the simultaneous processing of different audio tracks, mainly independent from each other. In order to exploit such a feature, this paper proposes an interface that lets the DAW interact with the Graphics Processing Unit (GPU) in a standardized way. Despite some academic research and experimentation, the professional audio software industry almost never exploited GPUs when implementing entire DAWs, but only when realising very specific tools or third party extensions (plugins). This work also presents and discusses the outcomes of a number of tests conducted in order to choose the optimal architecture. As a result, a GPU-based approach turned to be a valid alternative to the use of CPUs in the computation of audio effects, such as the rendering of audio tracks after mixing and mastering operations, both in real time and offline.","1558-2183","","10.1109/TPDS.2021.3131659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629334","Graphic processing unit (GPU);audio;digital audio workstation (DAW);heterogeneous computing","Graphics processing units;Task analysis;Real-time systems;Software;Rendering (computer graphics);Parallel processing;Central Processing Unit","","","","","",50.0,"IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"Completely Independent Spanning Trees on BCCC Data Center Networks With an Application to Fault-Tolerant Routing","X. -Y. Li; W. Lin; X. Liu; C. -K. Lin; K. -J. Pai; J. -M. Chang","College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; Department of Industrial Engineering and Management, Ming Chi University of Technology, New Taipei, Taiwan; Institute of Information and Decision Sciences, National Taipei University of Business, Taipei, Taiwan","IEEE Transactions on Parallel and Distributed Systems","22 Dec 2021",2022,33.0,8.0,1939,1952,"A set of $k$k spanning trees in a graph $G$G are called completely independent spanning trees (CISTs for short) if the paths joining every pair of vertices $x$x and $y$y in any two trees have neither vertex nor edge in common, except for $x$x and $y$y. The existence of multiple CISTs in the underlying graph of a network has applications in fault-tolerant broadcasting and secure message distribution. In this paper, we investigate the construction of CISTs in a server-centric data center network called BCube connected crossbars (BCCC), which can provide good network performance using inexpensive commodity off-the-shelf switches and commodity servers with only two network interface card (NIC) ports. The significant advantages of BCCC are its good expandability, lower communication latency, and higher robustness in component failure. Based on the structure of compound graphs of BCCC, we provide efficient algorithms to construct $\lceil \frac{n}{4}\rceil$⌈n4⌉ CISTs in the logical graph of BCCC, denoted by $L$L-$BCCC(n,k)$BCCC(n,k), for $n\geqslant 5$n⩾5. As a by-product, we obtain a fault-tolerant routing that takes the constructed CISTs as its routing table. We then evaluate the performance of the fault-tolerant routing through simulation results.","1558-2183","","10.1109/TPDS.2021.3133595","National Natural Science Foundation of China(grant numbers:61872257,62002062,62072109); Ministry of Science and Technology, Taiwan(grant numbers:MOST-110-2221-E-141-004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645299","Completely independent spanning trees (CISTs);data center networks (DCNs);BCube connected crossbars (BCCC);server-centric DCNs;compound graphs","Servers;Routing;Fault tolerant systems;Fault tolerance;Data centers;Compounds;Ad hoc networks","computer centres;computer network fault tolerance;telecommunication network routing;trees (mathematics)","completely independent spanning trees;BCCC data center networks;fault-tolerant routing;graph GG;fault-tolerant broadcasting;server-centric data center network;network performance;inexpensive commodity off-the-shelf switches;commodity servers;network interface card ports;compound graphs;logical graph;LL-BCCC;constructed CIST;multiple CIST;BCube connected crossbars;secure message distribution","",13.0,"",45.0,"IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"TridentKV: A Read-Optimized LSM-Tree Based KV Store via Adaptive Indexing and Space-Efficient Partitioning","K. Lu; N. Zhao; J. Wan; C. Fei; W. Zhao; T. Deng","Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Shenzhen Huazhong University of Science and Technology Research Institute, Shenzhen, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; SenseTime Research, Shenzhen, China; SenseTime Research, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","30 Dec 2021",2022,33.0,8.0,1953,1966,"LSM-tree based key-value (KV) stores suffer severe read performance loss due to the leveled structure of the LSM-tree. Especially, when modern storage devices with high bandwidth and low latency are used, the read performance of KV store is seriously affected by inefficient file indexing. Besides, due to the deletion pattern of inserting tombstones, the KV stores based on LSM-tree are faced with the problem of read performance fluctuations that are caused by large-scale data deletion (also referred to as the Read-After-Delete problem). In this article, TridentKV is proposed to improve the read performance of KV stores. An adaptive learned index structure is first designed to speed up file indexing. Also, a space-efficient partition strategy is proposed to solve the Read-After-Delete problem. Besides, asynchronous reading design is adopted, and SPDK is supported for high concurrency and low latency. TridentKV is implemented on RocksDB and the evaluation results indicate that compared with RocksDB, the read performance of TridentKV is improved by 7× to 12× without loss of write performance and TridentKV provides stable read performance even if a large number of deletions or migrations occur. Instead of RocksDB, TridentKV is exploited to store metadata in Ceph, which improves the read performance of Ceph by 20%$\sim$∼60%.","1558-2183","","10.1109/TPDS.2021.3118599","National Natural Science Foundation of China(grant numbers:61821003); National Natural Science Foundation of China(grant numbers:62072196); Shenzhen basic Research(grant numbers:JCYJ20190809095001781); National Key Research and Development Program of China(grant numbers:2018YFB1004401); Beijing Natural Science Foundation(grant numbers:L192027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563237","Key-value store;read optimization;learned index;SPDK;partitioned store","Indexing;Performance evaluation;Optimization;Metadata;Training;Nonvolatile memory;Concurrent computing","","","",2.0,"",52.0,"IEEE","7 Oct 2021","","","IEEE","IEEE Journals"
"TensorOpt: Exploring the Tradeoffs in Distributed DNN Training With Auto-Parallelism","Z. Cai; X. Yan; K. Ma; Y. Wu; Y. Huang; J. Cheng; T. Su; F. Yu","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Huawei Technologies Co. Ltd., Shenzhen, Guangdong, China; Huawei Technologies Co. Ltd., Shenzhen, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","23 Dec 2021",2022,33.0,8.0,1967,1981,"Effective parallelization strategies are crucial for the performance of distributed deep neural network (DNN) training. Recently, several methods have been proposed to search parallelization strategies but they all optimize a single objective (e.g., execution time, memory consumption) and produce only one strategy. We propose Frontier Tracking (FT), an efficient algorithm that finds a set of Pareto-optimal parallelization strategies to explore the best trade-off among different objectives. FT can minimize the memory consumption when the number of devices is limited and fully utilize additional resources to reduce the execution time. Based on FT, we develop a user-friendly system, called TensorOpt, which allows users to run their distributed DNN training jobs without caring the details about searching and coding parallelization strategies. Experimental results show that TensorOpt is more flexible in adapting to resource availability compared with existing frameworks.","1558-2183","","10.1109/TPDS.2021.3132413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635821","Deep learning;distributed systems;large-scale model training","Training;Deep learning;Adaptation models;Memory management;Search problems;Encoding","","","",4.0,"",48.0,"IEEE","6 Dec 2021","","","IEEE","IEEE Journals"
"Cost-Effective Web Application Replication and Deployment in Multi-Cloud Environment","T. Shi; H. Ma; G. Chen; S. Hartmann","School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; Department of Informatics, Clausthal University of Technology, Clausthal-Zellerfeld, Germany","IEEE Transactions on Parallel and Distributed Systems","30 Dec 2021",2022,33.0,8.0,1982,1995,"Multi-cloud is becoming a popular cloud ecosystem because it allows enterprise users to share the workload across multiple cloud service providers to achieve high-quality services with lower operation cost and higher application resilience. In multi-cloud, cloud services are widely distributed at different locations with differentiated prices. Therefore, Web application providers face the challenge to select proper cloud services for application replication and deployment with the aim of minimizing the deployment cost. Meanwhile, the deployed application replicas must satisfy the constraint on request response time to maintain the quality of user experience. To meet the two major requirements, this article studies a new problem of Web application replication and deployment in multi-cloud (WARDMC) that jointly considers both the cost minimization and constraints on average response time, including particularly request processing time and network latency. To address the problem, we develop a new approach named MCApp. MCApp combines iterative mixed integer linear programming with domain-tailored large neighborhood search to optimize both application replicas deployment and user requests dispatching. Extensive experiments using the real-world datasets demonstrate that MCApp significantly outperforms several recently proposed approaches.","1558-2183","","10.1109/TPDS.2021.3133884","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645306","Multi-cloud;Web application deployment;service replication;cost optimization;mixed integer linear programming;large neighborhood search","Cloud computing;Costs;Time factors;Optimization;Upper bound;Pricing;User experience","","","",6.0,"",59.0,"IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"AUCTION: Automated and Quality-Aware Client Selection Framework for Efficient Federated Learning","Y. Deng; F. Lyu; J. Ren; H. Wu; Y. Zhou; Y. Zhang; X. Shen","Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, P.R. China; School of Computer Science and Engineering, Central South University, Changsha, P.R. China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, P.R. China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, P.R. China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, P.R. China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada","IEEE Transactions on Parallel and Distributed Systems","30 Dec 2021",2022,33.0,8.0,1996,2009,"The emergency of federated learning (FL) enables distributed data owners to collaboratively build a global model without sharing their raw data, which creates a new business chance for building data market. However, in practical FL scenarios, the hardware conditions and data resources of the participant clients can vary significantly, leading to different positive/negative effects on the FL performance, where the client selection problem becomes crucial. To this end, we propose AUCTION, an Automated and qUality-aware Client selecTION framework for efficient FL, which can evaluate the learning quality of clients and select them automatically with quality-awareness for a given FL task within a limited budget. To design AUCTION, multiple factors such as data size, data quality, and learning budget that can affect the learning performance should be properly balanced. It is nontrivial since their impacts on the FL model are intricate and unquantifiable. Therefore, AUCTION is designed to encode the client selection policy into a neural network and employ reinforcement learning to automatically learn client selection policies based on the observed client status and feedback rewards quantified by the federated learning performance. In particular, the policy network is built upon an encoder-decoder deep neural network with an attention mechanism, which can adapt to dynamic changes of the number of candidate clients and make sequential client selection actions to reduce the learning space significantly. Extensive experiments are carried out based on real-world datasets and well-known learning models to demonstrate the efficiency, robustness, and scalability of AUCTION.","1558-2183","","10.1109/TPDS.2021.3134647","National Natural Science Foundation of China(grant numbers:62002389,62122095,62072472,U19A2067); Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010137005); National Key Research and Development Program of China(grant numbers:2019YFA0706403); Natural Science Foundation of Hainan Province(grant numbers:2020JJ2050,2021JJ20079); Higher Education Discipline Innovation Project(grant numbers:B18059); Young Elite Scientists Sponsorship Program by Tianjin(grant numbers:YESS20200238); Young Talents Plan of Hunan Province of China(grant numbers:2019RS2001,2021RC3004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647925","Federated learning;distributed system;client selection;data quality;reinforcement learning","Data models;Training;Distributed databases;Task analysis;Data integrity;Collaborative work;Data privacy","","","",16.0,"",50.0,"IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"Power Log’n’Roll: Power-Efficient Localized Rollback for MPI Applications Using Message Logging Protocols","K. Dichev; D. De Sensi; D. S. Nikolopoulos; K. W. Cameron; I. Spence","Electronics, Electrical Engineering and Computer Science, Queen's University Belfast, Belfast, United Kingdom of Great Britain and Northern Ireland; University of Pisa, Pisa, Italy; Computer Science, Virginia Tech, Blacksburg, Virginia, USA; Computer Science, Virginia Tech, Blacksburg, Virginia, USA; Electronics, Electrical Engineering and Computer Science, Queen's University Belfast, Belfast, United Kingdom of Great Britain and Northern Ireland","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1276,1288,"In fault tolerance for parallel and distributed systems, message logging protocols have played a prominent role in the last three decades. Such protocols enable local rollback to provide recovery from fail-stop errors. Global rollback techniques can be straightforward to implement but at times lead to slower recovery than local rollback. Local rollback is more complicated but can offer faster recovery times. In this work, we study the power and energy efficiency implications of global and local rollback. We propose a power-efficient version of local rollback to reduce power consumption for non-critical, blocked processes, using Dynamic Voltage and Frequency Scaling (DVFS) and clock modulation (CM). Our results for 3 different MPI codes on 2 parallel systems show that power-efficient local rollback reduces CPU energy waste up to 50% during the recovery phase, compared to existing global and local rollback techniques, without introducing significant overheads. Furthermore, we show that savings manifest for all blocked processes, which grow linearly with the process count. We estimate that for settings with high recovery overheads the total energy waste of parallel codes is reduced with the proposed local rollback.","1558-2183","","10.1109/TPDS.2021.3107745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524502","Fault tolerance;local rollback;fail-stop errors;MPI;message logging;power/energy savings","Protocols;Fault tolerant systems;Fault tolerance;Runtime;Resilience;Payloads;Topology","application program interfaces;checkpointing;energy conservation;fault tolerant computing;message passing;power aware computing;power consumption;protocols","message logging protocols;power-efficient local rollback;parallel codes;CPU energy;dynamic voltage and frequency scaling;DVFS;clock modulation;CM;MPI codes;parallel systems;distributed systems;fault tolerance","",1.0,"",53.0,"IEEE","27 Aug 2021","","","IEEE","IEEE Journals"
"DS-ADMM++: A Novel Distributed Quantized ADMM to Speed up Differentially Private Matrix Factorization","F. Zhang; E. Xue; R. Guo; G. Qu; G. Zhao; A. Y. Zomaya","School of Computer Science, China University of Geoscience, Wuhan, Hubei, China; School of Computer Science, China University of Geoscience, Wuhan, Hubei, China; School of Computer Science, China University of Geoscience, Wuhan, Hubei, China; Department of Engineering and Computer Science, Oakland University, Rochester, MI, USA; School of Computer Science, South China Normal University, Guangzhou, Guangdong, China; School of Computer Science, University of Sydney, Camperdown, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","8 Nov 2021",2022,33.0,6.0,1289,1302,"Matrix factorization is a powerful method to implement collaborative filtering recommender systems. This article addresses two major challenges, privacy and efficiency, which matrix factorization is facing. We based our work on DS-ADMM, a distributed matrix factorization algorithm with decent efficiency, to achieve the following two pieces of work: (1) Integrated local differential privacy paradigm into DS-ADMM to provide the privacy-preserving property; (2) Introduced a stochastic quantized function to reduce transmission overheads in ADMM to further improve efficiency. We named our work DS-ADMM++, in which one ’+’ refers to differential privacy, and the other ’+’ refers to quantized techniques. DS-ADMM++ is the first to perform efficient and private matrix factorization under the scenarios of differential privacy and DS-ADMM. We conducted experiments with benchmark data sets to demonstrate that our approach provides differential privacy and excellent scalability with a decent loss of accuracy.","1558-2183","","10.1109/TPDS.2021.3110104","National Key Research and Development Program of China(grant numbers:2018YFB1404402); Overseas Scientific and Technological Cooperation(grant numbers:2020197001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529010","Matrix factorization;parallel and distributed computing;differential privacy;ADMM;quantization","Differential privacy;Quantization (signal);Matrix decomposition;Recommender systems;Charge coupled devices;Machine learning;Costs","collaborative filtering;data privacy;information filtering;matrix decomposition;recommender systems","novel distributed quantized ADMM;differentially private matrix factorization;distributed matrix factorization algorithm;work DS-ADMM;efficient matrix factorization","",4.0,"",45.0,"IEEE","3 Sep 2021","","","IEEE","IEEE Journals"
"Taskflow: A Lightweight Parallel and Heterogeneous Task Graph Computing System","T. -W. Huang; D. -L. Lin; C. -X. Lin; Y. Lin","Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA; Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA; MathWorks, Natick, MA, USA; Department of Computer Science, Peking University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1303,1320,"Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5× less memory, and 1.9× higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community.","1558-2183","","10.1109/TPDS.2021.3104255","Defense Advanced Research Projects Agency(grant numbers:FA 8650-18-2-7843); National Science Foundation(grant numbers:CCF-2126672); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511796","Parallel programming;task parallelism;high-performance computing;modern C++ programming","Task analysis;Parallel processing;Graphics processing units;Computational modeling;Programming;Solid modeling;Runtime","graph theory;learning (artificial intelligence);parallel programming;scheduling;task analysis","Taskflow;parallel applications;heterogeneous applications;lightweight task graph-based approach;expressive task graph;parallel decomposition strategies;heterogeneous decomposition strategies;heterogeneous computing platform;programming model;task graph parallelism;in-graph control flow;end-to-end parallel optimization","",11.0,"",57.0,"IEEE","11 Aug 2021","","","IEEE","IEEE Journals"
"Distributed Graph Realizations","J. Augustine; K. Choudhary; A. Cohen; D. Peleg; S. Sivasubramaniam; S. Sourav","Indian Institute of Technology Madras, Chennai, Tamil Nadu, India; Tel Aviv University, Tel Aviv-Yafo, Israel; Weizmann Institute of Science, Rehovot, Israel; Weizmann Institute of Science, Rehovot, Israel; Indian Institute of Technology Madras, Chennai, Tamil Nadu, India; Advanced Digital Sciences Center, Singapore","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1321,1337,"We study graph realization problems for the first time from a distributed perspective. Graph realization problems are encountered in distributed construction of overlay networks that must satisfy certain degree or connectivity properties. We study them in the node capacitated clique (NCC) model of distributed computing, recently introduced for representing peer-to-peer overlay networks. We focus on two central variants, degree-sequence realization and minimum threshold-connectivity realization. In the degree sequence problem, each node $v$v is associated with a degree $d(v)$d(v), and the resulting degree sequence is realizable if it is possible to construct an overlay network in which the degree of each node $v$v is $d(v)$d(v). The minimum threshold-connectivity problem requires us to construct an overlay network that satisfies connectivity constraints specified between every pair of nodes. Overlay network realizations can be either explicit or implicit. Explicit realizations require both endpoints of any edge in the realized graph to be aware of the edge. In implicit realizations, on the other hand, at least one endpoint of each edge of the realized graph needs to be aware of the edge. The main realization algorithms we present are the following. (Note that all our algorithms are randomized Las Vegas algorithms unless specified otherwise. The stated running times hold with high probability.) 1) An $\tilde{O}(\min \lbrace \sqrt{m},\Delta \rbrace)$O˜(min{m,Δ}) time algorithm for implicit realization of a degree sequence. Here, $\Delta = \max _v d(v)$Δ=maxvd(v) is the maximum degree and $m = (1/2) \sum _v d(v)$m=(1/2)∑vd(v) is the number of edges in the final realization. 2) $\tilde{O}(\Delta)$O˜(Δ) time algorithm for an explicit realization of a degree sequence. We first compute an implicit realization and then transform it into an explicit one in $\tilde{O}(\Delta)$O˜(Δ) additional rounds. 3) An $\tilde{O}(\Delta)$O˜(Δ) time algorithm for the threshold connectivity problem that obtains an explicit solution and an improved $\tilde{O}(1)$O˜(1) algorithm for implicit realization when all nodes know each other’s IDs. These algorithms yield 2-approximations w.r.t. the number of edges. We complement our upper bounds with lower bounds to show that the above algorithms are tight up to factors of $\log n$logn. Additionally, we provide algorithms for realizing trees (including a procedure for obtaining a tree with a minimal diameter), an $\tilde{O}(1)$O˜(1) round algorithm for approximate degree sequence realization and finally an $O(\log ^2 n)$O(log2n) algorithm for degree sequence realization in the non-preassigned case namely, where the input degree sequence may be permuted among the nodes.","1558-2183","","10.1109/TPDS.2021.3104239","DST/SERB Extra Mural(grant numbers:EMR/2016/00301); DST/SERB MATRICS(grant numbers:MTR/2018/001198); Indian Institute of Technology Madras; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511805","Peer-to-peer overlay networks;node capacitated clique model;graph realization;degree realization;connectivity realization","Peer-to-peer computing;Overlay networks;Graphics;Approximation algorithms;Computational modeling;Adaptation models;Upper bound","approximation theory;computational complexity;deterministic algorithms;directed graphs;peer-to-peer computing;probability;randomised algorithms","distributed graph realizations;graph realization problems;distributed construction;overlay networks;node capacitated clique;NCC model;distributed computing;peer-to-peer overlay networks;degree-sequence realization;minimum threshold-connectivity realization;degree sequence problem;threshold connectivity problem;input degree sequence","",2.0,"",43.0,"IEEE","11 Aug 2021","","","IEEE","IEEE Journals"
"On Mixing Eventual and Strong Consistency: Acute Cloud Types","M. Kokociński; T. Kobus; P. T. Wojciechowski","Institute of Computing Science, Poznan University of Technology, Poznań, Poland; Institute of Computing Science, Poznan University of Technology, Poznań, Poland; Institute of Computing Science, Poznan University of Technology, Poznań, Poland","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1338,1356,"In this article we study the properties of distributed systems that mix eventual and strong consistency. We formalize such systems through acute cloud types (ACTs), abstractions similar to conflict-free replicated data types (CRDTs), which by default work in a highly available, eventually consistent fashion, but which also feature strongly consistent operations for tasks which require global agreement. Unlike other mixed-consistency solutions, ACTs can rely on efficient quorum-based protocols, such as Paxos. Hence, ACTs gracefully tolerate machine and network failures also for the strongly consistent operations. We formally study ACTs and demonstrate phenomena which are neither present in purely eventually consistent nor strongly consistent systems. In particular, we identify temporary operation reordering, which implies interim disagreement between replicas on the relative order in which the client requests were executed. When not handled carefully, this phenomenon may lead to undesired anomalies, including circular causality. We prove an impossibility result which states that temporary operation reordering is unavoidable in mixed-consistency systems with sufficiently complex semantics. Our result is startling, because it shows that apparent strengthening of the semantics of a system (by introducing strongly consistent operations to an eventually consistent system) results in the weakening of the guarantees on the eventually consistent operations.","1558-2183","","10.1109/TPDS.2021.3090318","Fundacja na rzecz Nauki Polskiej; European Commission; European Regional Development Fund(grant numbers:POIR.04.04.00-00-5C5B/17-00); Narodowe Centrum Nauki(grant numbers:DEC-2012/07/B/ST6/01230); Politechnika Poznańska; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459528","Eventual consistency;mixed consistency;fault-tolerance;acute cloud types;ACT","Semantics;Protocols;Synchronization;Data structures;Reactive power;Servers;Scalability","cloud computing;concurrency control;software fault tolerance","strong consistency;acute cloud types;distributed systems;conflict-free replicated data types;highly available fashion;strongly consistent operations;mixed-consistency solutions;network failures;temporary operation reordering;mixed-consistency systems;eventually consistent system;eventually consistent operations;eventual consistency","",1.0,"",79.0,"IEEE","17 Jun 2021","","","IEEE","IEEE Journals"
"Benchmarking 50-Photon Gaussian Boson Sampling on the Sunway TaihuLight","Y. Li; L. Gan; M. Chen; Y. Chen; H. Lu; C. Lu; J. Pan; H. Fu; G. Yang","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Shanghai Branch, CAS Center for Excellence in Quantum Information and Quantum Physics, University of Science and Technology of China, Shanghai, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; National Supercomputing Center, Wuxi, China; Shanghai Branch, CAS Center for Excellence in Quantum Information and Quantum Physics, University of Science and Technology of China, Shanghai, China; Shanghai Branch, CAS Center for Excellence in Quantum Information and Quantum Physics, University of Science and Technology of China, Shanghai, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1357,1372,"Boson sampling is expected to be an important milestone that will demonstrate quantum computational advantage (or quantum supremacy). This work establishes the benchmarking of Gaussian boson sampling (GBS) with threshold detection based on the Sunway TaihuLight supercomputer. To achieve the best performance and provide a competitive scenario for future quantum computing studies, the selected simulation algorithm is fully optimized based on a set of innovative approaches, including a parallel framework with almost perfect load balance and an instruction-level optimizing scheme based on a shortest-path-based instruction scheduling. In addition, data precision is carefully processed by an integer-instruction-based and multiple-precision fixed-point implementation, including 128- and 256-bit precison mode, which can be appropriately selected based on an adaptive precision optimizing scheme. Based on these methods, a highly efficient parallel quantum sampling algorithm is designed. The largest run enables us to obtain one Torontonian function of a $100\times 100$100×100 submatrix from 50-photon GBS within 20 hours in 128-bit precision and 2 days in 256-bit precision. To our knowledge, this was the largest quantum computing simulation based on Boson Sampling by using modern supercomputers.","1558-2183","","10.1109/TPDS.2021.3111185","National Key Research and Development Program of China(grant numbers:2020YFB0204700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534667","Boson sampling simulation;quantum computation;parallel computing;sunway TaihuLight supercomputer","Quantum computing;Photonics;Benchmark testing;Supercomputers;Detectors;Computational modeling;Standards","boson systems;fixed point arithmetic;Gaussian processes;optimisation;parallel algorithms;quantum computing","Torontonian function;shortest-path-based instruction scheduling;instruction-level optimizing scheme;perfect load balance;parallel framework;selected simulation algorithm;future quantum computing studies;competitive scenario;Sunway TaihuLight supercomputer;threshold detection;quantum supremacy;quantum computational advantage;important milestone;benchmarking 50-photon Gaussian Boson Sampling;largest quantum computing simulation;128-bit precision;50-photon GBS;highly efficient parallel quantum sampling algorithm;adaptive precision optimizing scheme;256-bit precison mode;multiple-precision fixed-point implementation;integer-instruction-based;data precision;time 2.0 d;time 20.0 hour","",3.0,"",72.0,"IEEE","9 Sep 2021","","","IEEE","IEEE Journals"
"Customer Adaptive Resource Provisioning for Long-Term Cloud Profit Maximization under Constrained Budget","P. Cong; Z. Zhang; J. Zhou; X. Liu; Y. Liu; T. Wei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Software Engineering Institute, East China Normal University, Shanghai, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; National Research Centre of Parallel Computer Engineering and Technology, Wuxi, China; School of Data Science and Engineering, East China Normal University, Shanghai, China; School of Computer Science and Technology, Shanghai Key Laboratory of Trustworthy Computing, East China Normal University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1373,1392,"As an efficient commercial information technology, cloud computing has attracted more and more users and enterprises to use it. Faced with such a large number and variety of customers, it is necessary for cloud providers (CPs) with limited budget to provide satisfactory customized pricing services, profitable customer and system investments, and flexible system resource provisioning strategies to improve both customer experience and long-term profit. Existing profit optimization research rarely considers customer diversity and dynamics, which may have a negative impact on long-term profit growth due to poor management of customer relations. In this article, we implement customer relationship management by considering both customer diversity and dynamics, and propose a customer adaptive resource provisioning scheme to maximize long-term profit under constrained budget. We consider four customer types (i.e., loyal, old, new, and lost) that can transition to each other during the customer's lifetime of interaction with the CP. The CP builds multiple cloud service sub-platforms, each of which contains multiple multiserver systems and serves the same type of customers. For the cloud service platform, we first analyze single multiserver system using an analytical method to obtain its optimal profit, invested funding, and system configuration. In particular, for systems serving new and lost customers, we develop a novel customer lifetime value (CLV)-based customer investment scheme that selects valuable customers for investment under limited marketing budget. Based on the above analysis, we then present a customer retention rate (CRR)-driven three-stage heuristic scheme that prioritizes investment in multiserver systems with endangered customers under limited infrastructure budget for reducing customer churn and promoting long-term profit growth. We conduct extensive simulation experiments to validate the effectiveness of our method. Simulation results show that compared with the benchmark algorithms, our method can improve the long-term profit and CRR by up to 3.4x and 7.8x, respectively.","1558-2183","","10.1109/TPDS.2021.3112562","National Key Research and Development Program of China(grant numbers:2018YFB2101300); National Natural Science Foundation of China(grant numbers:62172224,61802185); China Postdoctoral Science Foundation(grant numbers:BX2021128,2021T140327,2020M680068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537613","Cloud pricing;profit maximization;budget allocation;multiservers;customer lifetime value;customer retention rate","Pricing;Cloud computing;Biological system modeling;Quality of service;Investment;Optimization;Costs","cloud computing;customer services;investment;pricing;profitability;resource allocation","customer adaptive resource provisioning;long-term cloud profit maximization;constrained budget;efficient commercial information technology;cloud computing;cloud providers;satisfactory customized pricing services;profitable customer;system investments;flexible system resource;customer experience;profit optimization research;customer diversity;customer relationship management;multiple cloud service sub-platforms;multiple multiserver systems;single multiserver system;optimal profit;customer lifetime value-based customer investment scheme;customer retention rate-driven three-stage heuristic scheme;long-term profit growth promotion;limited marketing budget","",6.0,"",41.0,"IEEE","14 Sep 2021","","","IEEE","IEEE Journals"
"VQL: Efficient and Verifiable Cloud Query Services for Blockchain Systems","H. Wu; Z. Peng; S. Guo; Y. Yang; B. Xiao","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Hong Kong; College of Computer Science, Chongqing University, Chongqing, China; Department of Computer Engineering and Computer Science, Stony Brook University, Stony Brook, NY, USA; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1393,1406,"Despite increasingly emerging applications, a primary concern for blockchain to be fully practical is the inefficiency of data query. Direct queries on the blockchain take much time by searching every block, while indirect queries on a blockchain database greatly degrade the authenticity of query results. To conquer the authenticity problem, we propose a Verifiable Query Layer (VQL) that can be deployed in the cloud to provide both efficient and verifiable data query services for blockchain systems. The middleware layer extracts data from the underlying blockchain system and efficiently reorganizes them in databases. To prevent falsified data from being stored in the middleware, a cryptographic fingerprint is calculated based on each constructed database. The database fingerprint will be first verified by miners and then written into the blockchain. Moreover, public users can verify the entire databases or several databases that interest them in the middleware layer. We implement VQL together with the verification schemes and conduct extensive experiments based on a practical blockchain system. The evaluation results demonstrate that VQL can efficiently support various data query services and guarantee the authenticity of query results for blockchain systems.","1558-2183","","10.1109/TPDS.2021.3113873","HK RGC GRF(grant numbers:PolyU 15217321,PolyU 15216220); HK ITF(grant numbers:ITS/081/18); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515111070); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9541060","Cloud query service;verifiable query;data authenticity;blockchain systems","Blockchains;Peer-to-peer computing;Middleware;Bitcoin;Data mining;Distributed ledger;Costs","blockchains;cloud computing;cryptography;fingerprint identification;middleware;query processing","support various data query services;cryptographic fingerprint;verifiable cloud query services;practical blockchain system;entire databases;database fingerprint;underlying blockchain system;middleware layer;verifiable data query services;verifiable query layer;authenticity problem;blockchain database;indirect queries;direct queries;blockchain systems;VQL","",16.0,"",33.0,"IEEE","20 Sep 2021","","","IEEE","IEEE Journals"
"Network Cost-Aware Geo-Distributed Data Analytics System","K. Oh; M. Zhang; A. Chandra; J. Weissman","Department of Computer Science, University of Nebraska Omaha, Omaha, NE, USA; Department of Computer Science, University of Nebraska Omaha, Omaha, NE, USA; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, MN, USA; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, MN, USA","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1407,1420,"Many geo-distributed data analytics (GDA) systems have focused on the network performance-bottleneck: inter-data center network bandwidth to improve performance. Unfortunately, these systems may encounter a cost-bottleneck (${\$}$$) because they have not considered data transfer cost (${\$}$$), one of the most expensive and heterogeneous resources in a multi-cloud environment. In this article, we present Kimchi, a network cost-aware GDA system to meet the cost-performance tradeoff by exploiting data transfer cost heterogeneity to avoid the cost-bottleneck. Kimchi determines cost-aware task placement decisions for scheduling tasks given inputs including data transfer cost, network bandwidth, input data size and locations, and desired cost-performance tradeoff preference. In addition, Kimchi is also mindful of data transfer cost in the presence of dynamics. Kimchi has been applied to two common GDA MapReduce models: synchronous barrier and asynchronous push-based shuffle. A Kimchi prototype has been implemented on Spark, and experiments show that it reduces cost by 5% $\scriptstyle \sim$∼ 24% without impacting performance and reduces query execution time by 45% $\scriptstyle \sim$∼ 70% without impacting cost compared to other baseline approaches centralized, vanilla Spark, and bandwidth-aware (e.g., Iridium). More importantly, Kimchi allows applications to explore a much richer cost-performance tradeoff space in a multi-cloud environment.","1558-2183","","10.1109/TPDS.2021.3108893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9527073","Geo-distributed data;multi-DCs;multi cloud providers;data analytics system","Task analysis;Data transfer;Bandwidth;Wide area networks;Sparks;Iridium;Distributed databases","cloud computing;computer centres;data analysis;parallel processing;query processing;scheduling","Kimchi;network cost-aware geo-distributed data analytics system;network performance-bottleneck;inter-data center network bandwidth;cost-bottleneck;network cost-aware GDA system;data transfer cost heterogeneity;cost-aware task placement decisions;input data size;cost-performance tradeoff preference;multicloud environment;GDA MapReduce models;synchronous barrier;asynchronous push-based shuffle;scheduling tasks;query execution time","",1.0,"",49.0,"IEEE","1 Sep 2021","","","IEEE","IEEE Journals"
"HSA-Net: Hidden-State-Aware Networks for High-Precision QoS Prediction","Z. Wang; X. Zhang; M. Yan; L. Xu; D. Yang","School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; PengCheng Laboratory, Chongqing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China","IEEE Transactions on Parallel and Distributed Systems","28 Oct 2021",2022,33.0,6.0,1421,1435,"The high-precision QoS (quality of service) prediction is based on the comprehensive perception of state information of users and services. However, the current QoS prediction approaches have limited accuracy, for most state information of users and services (i.e., network speed, latency, network type, and more) are hidden due to privacy protection. Therefore, this article proposes a hidden-state-aware network (HSA-Net) that includes three steps called hidden state initialization, hidden state perception, and QoS prediction. A hidden state initialization approach is developed first based on the latent dirichlet allocation (LDA). After that, a hidden-state perception approach is proposed to abstract the initialized hidden state by fusing the known information (e.g., service ID and user location). The perception approach consists of four hidden-state perception (HSP) modes (i.e., known mode, object mode, hybrid mode and overall mode) implemented to generate explainable and fused features through four adaptive convolutional kernels. Finally, the relationship between the fused features and the QoS is discovered through a fully connected network to complete the high-precision QoS prediction process. The proposed HSA-Net is evaluated on two real-world datasets. According to the results, the HSA-Net's mean absolute error (MAE) index reduced by 3.67% and 28.84%, whereas the root mean squared error (RMSE) index decreased by 3.07% and 7.14% compared with ten baselines on average in the two datasets.","1558-2183","","10.1109/TPDS.2021.3111810","National Key Research and Development Program of China(grant numbers:2018YFB2101200); Special Funds for the Central Government(grant numbers:YDZX20195000004725); National Natural Science Foundation of China(grant numbers:61772093); Key Project of Technology Innovation and Application Development of Chongqing(grant numbers:cstc2019jscx-mbdxX0020); Chongqing Science and Technology Plan Project(grant numbers:cstc2018jszx-cyztzxX0037); Chongqing Technology Innovation and Application Development Project(grant numbers:cstc2019jszx- mbdxX0064); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535236","Service recommendation;convolutional neural network;QoS prediction;hidden states;feature fusion","Quality of service;Time factors;Deep learning;Predictive models;Kernel;Collaboration;Web services","convolutional neural nets;data fusion;mean square error methods;quality of service;Web services","hidden-state-aware network;comprehensive perception;state information;network speed;network type;hidden state initialization approach;hidden-state perception approach;user location;fully connected network;high-precision QoS prediction process;HSA-Net mean absolute error index;root mean squared error index;latent dirichlet allocation;adaptive convolutional kernels;privacy protection;feature fusion","",2.0,"",66.0,"IEEE","10 Sep 2021","","","IEEE","IEEE Journals"
"Modeling Speedup in Multi-OS Environments","B. R. Tauro; C. Liu; K. C. Hale","Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1436,1450,"For workloads that place strenuous demands on system software, novel operating system designs like unikernels, library OSes, and hybrid runtimes offer a promising path forward. However, while these systems can outperform general-purpose OSes, they have limited ability to support legacy applications. Multi-OS environments, where the application’s execution is split between a control plane and a data plane operating system, can address this challenge, but reasoning about the performance of applications that run in such a split execution environment is currently guided only by expert intuition and empirical analysis. As the level of specialization in system software and hardware continues to increase, there is both a pressing need and ripe opportunity for investigating analytical models that can predict application performance and guide programmers’ intuition when considering multi-OS environments. In this paper we present such a model to place bounds on application speedup, beginning with a simple, intuitive formulation, and progressing to a more refined model. We present an analysis of the model for a diverse set of benchmarks, as well as a prototype tool to project multi-OS speedups for applications on existing systems. Finally, we validate our model on state-of-the-art multi-OS systems, demonstrating that it reliably predicts speedup with 96% average accuracy.","1558-2183","","10.1109/TPDS.2021.3114984","National Science Foundation(grant numbers:CNS-1718252,CNS-1730689,>CNS-1763612,CCF-2028958,CCF-2029014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547731","Operating systems;multi-kernels;speedup models;performance modeling","Kernel;Computational modeling;Hardware;Analytical models;Tools;Linux;Benchmark testing","operating system kernels;software libraries;software performance evaluation","unikernels;multiOS systems;intuitive formulation;application speedup;split execution environment;data plane operating system;control plane;legacy applications;general-purpose OSes;hybrid runtimes;library OSes;system software;multiOS speedups","","","",54.0,"IEEE","24 Sep 2021","","","IEEE","IEEE Journals"
"A Survey of GPU Multitasking Methods Supported by Hardware Architecture","C. Zhao; W. Gao; F. Nie; H. Zhou","School of Computer, Northwestern Polytechnical University, Xi'an, China; School of Computer, Northwestern Polytechnical University, Xi'an, China; School of Computer, Northwestern Polytechnical University, Xi'an, China; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, USA","IEEE Transactions on Parallel and Distributed Systems","28 Oct 2021",2022,33.0,6.0,1451,1463,"The ability to support multitasking becomes more and more important in the development of graphic processing unit (GPU). GPU multitasking methods are classified into three types: temporal multitasking, spatial multitasking, and simultaneous multitasking (SMK). This article first introduces the features of some commercial GPU architectures to support multitasking and the common metrics used for evaluating the performance of GPU multitasking methods, and then reviews the GPU multitasking methods supported by hardware architecture (i.e., hardware GPU multitasking methods). The main problems of each type of hardware GPU multitasking methods to be solved are illustrated. Meanwhile, the key idea of each previous hardware GPU multitasking method is introduced. In addition, the characteristics of hardware GPU multitasking methods belonging to the same type are compared. This article also gives some valuable suggestions for the future research. An enhanced GPU simulator is needed to bridge the gap between academia and industry. In addition, it is promising to expand the research space with machine learning technologies, advanced GPU architectural innovations, 3D stacked memory, etc. Because most previous GPU multitasking methods are based on NVIDIA GPUs, this article focuses on NVIDIA GPU architecture, and uses NVIDIA's terminology. To our knowledge, this article is the first survey about hardware GPU multitasking methods. We believe that our survey can help the readers gain insights into the research field of hardware GPU multitasking methods.","1558-2183","","10.1109/TPDS.2021.3115630","National Natural Science Foundation of China(grant numbers:11875221); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548839","GPU multitasking;survey;hardware architecture;temporal multitasking;spatial multitasking;simultaneous multitasking (SMK)","Graphics processing units;Multitasking;Kernel;Hardware;Computer architecture;Registers;Task analysis","graphics processing units;learning (artificial intelligence);multiprogramming;parallel architectures","hardware GPU multitasking methods;NVIDIA GPU architecture;hardware architecture;temporal multitasking multitasking;spatial multitasking multitasking;simultaneous multitasking;machine learning technologies;graphic processing unit","",3.0,"",111.0,"IEEE","27 Sep 2021","","","IEEE","IEEE Journals"
"EnosLib: A Library for Experiment-Driven Research in Distributed Computing","R. -A. Cherrueau; M. Delavergne; A. van Kempen; A. Lebre; D. Pertin; J. R. Balderrama; A. Simonet; M. Simonin","Inria, LS2N, IMT Atlantique, Nantes, France; Inria, LS2N, IMT Atlantique, Nantes, France; Inria, LS2N, IMT Atlantique, Nantes, France; Inria, LS2N, IMT Atlantique, Nantes, France; Inria, LS2N, IMT Atlantique, Nantes, France; Inria, LS2N, IMT Atlantique, Nantes, France; iExec Blockchain Tech, Lyon, Auvergne-Rhône-Alpes, France; Inria Rennes - Bretagne Atlantique, Rennes, France","IEEE Transactions on Parallel and Distributed Systems","28 Oct 2021",2022,33.0,6.0,1464,1477,"Despite the importance of experiment-driven research in the distributed computing community, there has been little progress in helping researchers conduct their experiments. In most cases, they have to achieve tedious and time-consuming development and instrumentation activities to deal with the specifics of testbeds and the system under study. In order to relieve researchers of the burden of those efforts, we have developed EnosLib: a Python library that takes into account best experimentation practices and leverages modern toolkits on automatic deployment and configuration systems. EnosLib helps researchers not only in the process of developing their experimental artifacts, but also in running them over different infrastructures. To demonstrate the relevance of our library, we discuss three experimental engines built on top of EnosLib, and used to conduct empirical studies on complex software stacks between 2016 and 2019 (database systems, communication buses and OpenStack). By introducing EnosLib, our goal is to gather academic and industrial actors of our community around a library that aggregates everyday experiment-driven research operations. A library that has been already adopted by open-source projects and members of the scientific community thanks to its ease of use and extension.","1558-2183","","10.1109/TPDS.2021.3111159","Inria and Orange Labs; Institut national de recherche en informatique et en automatique; CNRS; RENATER; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534688","Experiment-driven research;performance evaluation;distributed computing experimentation library","Libraries;Software;Task analysis;Tools;Protocols;Codes;Benchmark testing","cloud computing;public domain software;software libraries","EnosLib;distributed computing community;time-consuming development;instrumentation activities;Python library;experimentation practices;leverages modern toolkits;automatic deployment;configuration systems;experimental artifacts;experimental engines;aggregates everyday experiment-driven research operations;complex software stacks;database systems","",2.0,"",44.0,"IEEE","9 Sep 2021","","","IEEE","IEEE Journals"
"Coarse Grained FPGA Overlay for Rapid Just-In-Time Accelerator Compilation","A. K. Jain; D. L. Maskell; S. A. Fahmy","Xilinx Inc., San Jose, CA, USA; School of Computer Science and Engineering, Nanyang Technological University, Singapore; King Abdullah University of Science and Technology, Thuwal, Saudi Arabia","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1478,1490,"Coarse-grained FPGA overlays built around the runtime programmable DSP blocks in modern FPGAs can achieve high throughput and improved scalability compared to traditional overlays built without detailed consideration of FPGA architecture. These overlays can be mapped to using higher level compilers, achieving fast compilation, software-like programmability and run-time management, and high-level design abstraction. OpenCL allows programs running on a host computer to launch accelerator kernels which can be compiled at run-time for a specific architecture, thus enabling portability. However, prohibitive hardware compilation times in traditional design flows mean that the tools cannot effectively use just-in-time (JIT) compilation or runtime performance scaling on FPGAs. We present a methodology for runtime compilation of dataflow graphs expressed as OpenCL kernels onto coarse-grained overlays. The methodology benefits from the high level of abstraction afforded by using the OpenCL programming model, while the mapping to the overlay significantly reduces compilation and load times. Key characteristics of this work include highly performant DSP-optimized functional units that scale to large overlays on modern devices and the ability to perform automatic resource-aware kernel replication up to the size of the overlay. We demonstrate place and route times orders of magnitude better than traditional HLS flows, even when running on an embedded processor in the Xilinx Zynq.","1558-2183","","10.1109/TPDS.2021.3116859","Ministry of Education(grant numbers:MOE2017-T2-1-002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555373","Field programmable gate arrays;parallel processing;hardware accelerators","Field programmable gate arrays;Kernel;Computer architecture;Hardware;Runtime;Performance evaluation;Throughput","digital signal processing chips;field programmable gate arrays;high level synthesis;just-in-time;logic design;program compilers;reconfigurable architectures","coarse grained FPGA overlay;Just-In-Time Accelerator Compilation;coarse-grained FPGA overlays;runtime programmable DSP blocks;modern FPGAs;traditional overlays;FPGA architecture;higher level compilers;software-like programmability;run-time management;high-level design abstraction;accelerator kernels;prohibitive hardware compilation times;traditional design flows;just-in-time compilation;runtime performance;runtime compilation;OpenCL kernels;coarse-grained overlays;OpenCL programming model;load times;highly performant DSP-optimized","",1.0,"",60.0,"CCBYNCND","30 Sep 2021","","","IEEE","IEEE Journals"
"Deep Reinforcement Learning for Load-Balancing Aware Network Control in IoT Edge Systems","Q. Liu; T. Xia; L. Cheng; M. van Eijk; T. Ozcelebi; Y. Mao","Interconnected Resource-aware Intelligent Systems Group, Eindhoven University of Technology, Eindhoven, The Netherlands; Interconnected Resource-aware Intelligent Systems Group, Eindhoven University of Technology, Eindhoven, The Netherlands; School of Control and Computer Engineering, North China Electric Power University, Beijing, China; Prodrive Technologies, Eindhoven, The Netherlands; Interconnected Resource-aware Intelligent Systems Group, Eindhoven University of Technology, Eindhoven, The Netherlands; Fordham University in New York City, Bronx, NY, USA","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2021",2022,33.0,6.0,1491,1502,"Load balancing is directly associated with the overall performance of a parallel and distributed computing system. Although the relevant problems in communication and computation have been well studied in data center environments, few works have considered the issues in an Internet of Things (IoT) edge scenario. In fact, processing data in a load balancing way for the latter case is more challenging. The main reason is that, unlike a data center, both the data sources and the network infrastructure in an IoT edge system can be dynamic. Moreover, with different performance requirements from IoT networks and edge servers, it will be hard to characterize the performance model and to perform runtime optimization for the whole system. To tackle this problem, in this work, we propose a load-balancing aware networking approach for efficient data processing in IoT edge systems. Specifically, we introduce an IoT network dynamic clustering solution using the emerging deep reinforcement learning (DRL), which can both fulfill the communication balancing requirements from IoT networks and the computation balancing requirements from edge servers. Moreover, we implement our system with a long short term memory (LSTM) based Dueling Double Deep Q-Learning Network (D3QN) model, and our experiments with real-world datasets collected from an autopilot vehicle demonstrate that our proposed method can achieve significant performance improvement compared to benchmark solutions.","1558-2183","","10.1109/TPDS.2021.3116863","Fundamental Research Funds for the Central Universities(grant numbers:2021MS017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555233","Load balancing;Internet of Things;network;edge computing;distributed systems;deep reinforcement learning;LSTM","Servers;Internet of Things;Image edge detection;Computational modeling;Sensors;Data models;Long short term memory","computer centres;deep learning (artificial intelligence);Internet of Things;optimisation;pattern clustering;recurrent neural nets;reinforcement learning;resource allocation","IoT edge system;distributed computing system;data center environments;data sources;network infrastructure;performance requirements;IoT networks;edge servers;load-balancing aware networking approach;IoT network dynamic clustering solution;communication balancing requirements;computation balancing requirements;deep reinforcement learning;load-balancing aware network control;parallel computing system;Internet of Things edge scenario;runtime optimization;data processing;long short term memory;dueling double deep q-learning network model","",8.0,"",44.0,"IEEE","30 Sep 2021","","","IEEE","IEEE Journals"
"A Potential Game Theoretic Approach to Computation Offloading Strategy Optimization in End-Edge-Cloud Computing","Y. Ding; K. Li; C. Liu; K. Li","National Supercomputing Center in Changsha, Changsha, Hunan, China; National Supercomputing Center in Changsha, Changsha, Hunan, China; National Supercomputing Center in Changsha, Changsha, Hunan, China; Department of Computer Science, State University of New York, New Paltz, NY, USA","IEEE Transactions on Parallel and Distributed Systems","28 Oct 2021",2022,33.0,6.0,1503,1519,"Integrating user ends (UEs), edge servers (ESs), and the cloud into end-edge-cloud computing (EECC) can enhance the utilization of resources and improve quality of experience (QoE). However, the performance of EECC is significantly affected by its architecture. In this article, we classify EECC into two computing architectures types according to the visibility and accessibility of the cloud to UEs, i.e., hierarchical end-edge-cloud computing (Hi-EECC) and horizontal end-edge-cloud computing (Ho-EECC). In Hi-EECC, UEs can offload their tasks only to ESs. When the resources of ESs are exhausted, the ESs request the cloud to provide resources to UEs. In Ho-EECC, UEs can offload their tasks directly to ESs and the cloud. In this article, we construct a potential game for the EECC environment, in which each UE selfishly minimizes its payoff, study the computation offloading strategy optimization problems, and develop two potential game-based algorithms in Hi-EECC and Ho-EECC. Extensive experiments with real-world data are conducted to demonstrate the performance of the proposed algorithms. Moreover, the scalability and applicability of the two computing architectures are comprehensively analyzed. The conclusions of our work can provide useful suggestions for choosing specific computing architectures under different application environments to improve the performance of EECC and QoE.","1558-2183","","10.1109/TPDS.2021.3112604","National Natural Science Foundation of China(grant numbers:61625202); National Key Research and Development Program of China(grant numbers:2018YFB1701403); National Natural Science Foundation of China(grant numbers:61876061,62072165,U19A2058,61702170); Postgraduate Scientific Research Innovation Project of Hunan Province(grant numbers:CX20200435); Zhejiang Lab(grant numbers:2020KE0AB01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537304","Computation offloading;end-edge-cloud computing (EECC);hierarchical EECC;horizontal EECC;potential game","Task analysis;Computer architecture;Optimization;Delays;Servers;Costs;Quality of experience","cloud computing;game theory;mobile computing;optimisation","ESs;Hi-EECC;horizontal end-edge-cloud computing;Ho-EECC;UE;computation offloading strategy optimization problems;potential game theoretic approach;user ends;edge servers","",15.0,"",41.0,"IEEE","14 Sep 2021","","","IEEE","IEEE Journals"
"Decentralized Application Placement in Fog Computing","Z. Á. Mann","University of Amsterdam, Amsterdam, WX, The Netherlands","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2022",2022,33.0,12.0,3262,3273,"In recent years, cloud computing concepts have been extended towards the network edge, leading to paradigms like fog and edge computing. As a result, applications can be placed on a variety of resources, including fog nodes and cloud data centers. Application placement has significant impact on important metrics like latency. Finding an optimal application placement is computationally challenging, particularly because of the potentially huge number of infrastructure nodes and application components. To overcome the limited scalability of application placement algorithms, optimization can be decentralized, i.e., performed separately for different parts of the infrastructure. The infrastructure can be split into fog colonies, where a fog colony consists of the computational resources in a given geographical region. Application placement can then be performed for the individual fog colonies, thus mitigating the scalability problem. However, independent optimization of application placement in different fog colonies may lead to missed synergies and thus to sub-optimal overall results. Hence, some kind of coordination between fog colonies may be beneficial. In this article, we analyze the effects of decentralization and coordination on the optimization results. In particular, we compare empirically four different approaches: (i) centralized decision-making, where decisions are made in one go for the entire infrastructure, (ii) independent fog colonies, where optimization is carried out in each fog colony independently from each other, (iii) fog colonies with communication, where excess application components in one fog colony can be sent to a neighboring fog colony, and (iv) fog colonies with overlaps, where shared resources may be dynamically distributed between neighboring fog colonies. Our experiments show that, for large problem instances, decentralization combined with coordination leads to the best results.","1558-2183","","10.1109/TPDS.2022.3148985","EU’s Horizon 2020 Research and Innovation programme(grant numbers:871525 (FogProtect)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706284","Fog computing;edge computing;application placement;distributed algorithms;fog colonies","Peer-to-peer computing;Cloud computing;Optimization;Scalability;Edge computing;Task analysis;Bandwidth","decision making;distributed processing;optimisation","application placement algorithms;independent fog colonies;decentralized application placement;fog computing;edge computing;cloud computing;infrastructure nodes;application components;independent optimization;centralized decision-making;overlapping fog colonies","",6.0,"",38.0,"IEEE","7 Feb 2022","","","IEEE","IEEE Journals"
"DHash: Dynamic Hash Tables With Non-Blocking Regular Operations","J. Wang; D. Liu; X. Fu; F. Xiao; C. Tian","School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2022",2022,33.0,12.0,3274,3290,"Once started, existing hash tables cannot change their pre-defined hash functions, even if the incoming data cannot be evenly distributed to the hash table buckets. In this paper, we present DHash, a type of hash table for shared memory systems, that can change its hash function and rebuild the hash table on the fly, without noticeably degrading its service. The major technical novelty of DHash stems from an efficient distributing mechanism that can atomically distribute every node when rebuilding, without locking the corresponding hash table buckets. This not only enables non-blocking lookup, insert, and delete operations, but more importantly, makes DHash independent of the implementation of hash table buckets, such that DHash allows programmers to select the set algorithms that meet their requirements best from a variety of existing lock-free and wait-free set algorithms. Evaluations show that DHash can efficiently change its hash function on the fly. Moreover, when rebuilding, DHash consistently outperforms the state-of-the-art hash tables in terms of throughput and response time of concurrent operations, at different concurrency levels, and with different operation mixes and average load factors.","1558-2183","","10.1109/TPDS.2022.3151499","National Science Fund for Distinguished Young Scholars(grant numbers:62125203); National Natural Science Foundation of China(grant numbers:61932013,62072228); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714033","Concurrent programming;parallelism and concurrency;dynamic hash tables","Hash functions;Heuristic algorithms;Robustness;Kernel;Synchronization;Linux;Time factors","concurrency (computers);distributed algorithms;distributed programming;file organisation;shared memory systems;table lookup","concurrent operations;lock-free set algortithms;wait-free set algorithms;insert operations;delete operations;nonblocking lookup;distributing mechanism;shared memory systems;nonblocking regular operations;dynamic hash tables;DHash;hash table buckets;hash function","","","",42.0,"IEEE","14 Feb 2022","","","IEEE","IEEE Journals"
"Towards Efficient and Stable K-Asynchronous Federated Learning With Unbounded Stale Gradients on Non-IID Data","Z. Zhou; Y. Li; X. Ren; S. Yang","National Engineering Laboratory for Big Data Analytics, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, Shaanxi, China; National Engineering Laboratory for Big Data Analytics, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, Shaanxi, China; National Engineering Laboratory for Big Data Analytics, School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, Shaanxi, China; National Engineering Laboratory for Big Data Analytics, the Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, Shaanxi, China","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2022",2022,33.0,12.0,3291,3305,"Federated learning (FL) is an emerging privacy-preserving paradigm that enables multiple participants collaboratively to train a global model without uploading raw data. Considering heterogeneous computing and communication capabilities of different participants, asynchronous FL can avoid the stragglers effect in synchronous FL and adapts to scenarios with vast participants. Both staleness and non-IID data in asynchronous FL would reduce the model utility. However, there exists an inherent contradiction between the solutions to the two problems. That is, mitigating the staleness requires to select less but consistent gradients while coping with non-IID data demands more comprehensive gradients. To address the dilemma, this paper proposes a two-stage weighted $K$K asynchronous FL with adaptive learning rate (WKAFL). By selecting consistent gradients and adjusting learning rate adaptively, WKAFL utilizes stale gradients and mitigates the impact of non-IID data, which can achieve multifaceted enhancement in training speed, prediction accuracy and training stability. We also present the convergence analysis for WKAFL under the assumption of unbounded staleness to understand the impact of staleness and non-IID data. Experiments implemented on both benchmark and synthetic FL datasets show that WKAFL has better overall performance compared to existing algorithms.","1558-2183","","10.1109/TPDS.2022.3150579","National Key Research and Development Program of China(grant numbers:2020YFA0713900); National Natural Science Foundation of China(grant numbers:62172329,61802298,61772410,U21A6005,U1811461,11690011); China Postdoctoral Science Foundation(grant numbers:2020T130513,2019M663726); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712243","Federated learning;asynchronous learning;data heterogeneity;prediction accuracy;training stability","Training;Servers;Computational modeling;Data models;Convergence;Distributed databases;Stability analysis","data privacy;gradient methods;learning (artificial intelligence)","k-asynchronous federated learning;unbounded stale gradients;raw data;consistent gradients;adaptive learning rate;WKAFL;learning rate;privacy-preserving paradigm;nonIID data;two-stage weighted K asynchronous FL","",1.0,"",56.0,"IEEE","11 Feb 2022","","","IEEE","IEEE Journals"
"A Bayesian Approach To Distributed Anomaly Detection In Edge AI Networks","M. Odiathevar; W. K. G. Seah; M. Frean","School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2022",2022,33.0,12.0,3306,3320,"The challenge of anomaly detection is to obtain an accurate understanding of expected behaviour which is intensified when the data are distributed heterogeneously. Transmitting raw data to a central site incurs high communication overhead and raises privacy issues. The concept of Edge AI allows computation to be performed at the edge site allowing for quick decision making in mission critical scenarios such as self-driving cars. A model is learnt locally and its parameters are transmitted and aggregated. However, existing methods of aggregation do not account for variance and heterogeneous distribution of data. They also do not consider edge constraints such as limited computational, memory and communication capabilities of edge devices. In this work, a fully Bayesian approach is employed by means of a Bayesian Random Vector Functional Link AutoEncoder being incorporated with Expectation Propagation for distributed training. Our anomaly detection system operates without any transmission of raw data, is robust under inhomogeneous network densities and under uneven and biased data distributions. It allows for asynchronous updates to converge in a few iterations and is a relatively simple neural network addressing edge constraints without compromising on performance as compared to existing more complex models.","1558-2183","","10.1109/TPDS.2022.3151853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714821","Anomaly detection;Bayesian;random vector functional link;expectation propagation;edge AI;single layer feed-forward neural network","Bayes methods;Anomaly detection;Training;Image edge detection;Distributed databases;Data models;Computational modeling","Bayes methods;data handling;data privacy;decision making;neural nets","distributed anomaly detection;raw data;central site;high communication overhead;privacy issues;edge site;quick decision;mission critical scenarios;self-driving cars;heterogeneous distribution;edge constraints;communication capabilities;edge devices;expectation propagation;distributed training;anomaly detection system;inhomogeneous network densities;uneven data distributions;biased data distributions;neural network;Bayesian random vector functional link autoencoder;edge AI networks","",1.0,"",61.0,"IEEE","16 Feb 2022","","","IEEE","IEEE Journals"
"GraphOpt: Constrained-Optimization-Based Parallelization of Irregular Graphs","N. Shah; W. Meert; M. Verhelst","Department of Electrical Engineering - MICAS, KU Leuven, Leuven, Belgium; Department of Computer Science - DTAI, KU Leuven, Leuven, Belgium; Department of Electrical Engineering - MICAS, KU Leuven, Leuven, Belgium","IEEE Transactions on Parallel and Distributed Systems","13 Jun 2022",2022,33.0,12.0,3321,3332,"Sparse, irregular graphs show up in various applications like linear algebra, machine learning, engineering simulations, robotic control, etc. These graphs have a high degree of parallelism, but their execution on parallel threads of modern platforms remains challenging due to the irregular data dependencies. The execution performance can be improved by efficiently partitioning the graphs such that the communication and thread synchronization overheads are minimized without hurting the utilization of the threads. To achieve this, this article proposes GraphOpt, a tool that models the graph parallelization as a constrained optimization problem and uses the open Google OR-Tools solver to find good partitions. Several scalability techniques are developed to handle large real-world graphs with millions of nodes and edges. Extensive experiments are performed on the graphs of sparse matrix triangular solves (linear algebra) and sum-product networks (machine learning), respectively, showing a mean speedup of 2.0× and 1.8× over previous state-of-the-art libraries, demonstrating the effectiveness of the constrained-optimization-based graph parallelization.","1558-2183","","10.1109/TPDS.2022.3151194","H2020 European Research Council(grant numbers:ERC-2016-STG-715037); Intel; Huawei; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9713729","Graph parallelization;partitioning;constrained optimization;sparse matrix triangular solves;CPU multithreading","Parallel processing;Task analysis;Instruction sets;Synchronization;Hardware;Optimization;Scalability","graph theory;learning (artificial intelligence);linear algebra;mathematics computing;optimisation;parallel processing","GraphOpt;constrained-optimization-based parallelization;irregular graphs;linear algebra;machine learning;parallel threads;modern platforms;irregular data dependencies;execution performance;thread synchronization overheads;constrained optimization problem;real-world graphs;sparse matrix triangular solves;sum-product networks;constrained-optimization-based graph parallelization;graph partitioning;sparse graph;open Google OR-Tools solver;scalability technique","","","",48.0,"IEEE","14 Feb 2022","","","IEEE","IEEE Journals"
"Bandwidth-Aware Scheduling Repair Techniques in Erasure-Coded Clusters: Design and Analysis","H. Zhou; D. Feng; Y. Hu","Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education of China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education of China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2022",2022,33.0,12.0,3333,3348,"Erasure codes offer a storage-efficient redundancy mechanism for maintaining data availability guarantees in storage clusters, yet also incur high network traffic consumption and recovery time in failure repair. Extensive research has been carried out to reduce the recovery time. However, previous works either target specific erasure code constructions which are not commonly used in today’s distributed storage clusters or neglect the heterogeneous bandwidth property in real network environments. Since erasure-coded clusters are typically composed of multi-node with heterogeneous bandwidth and accessed in parallel, the whole recovery time is mainly restricted by the low-bandwidth links. In this article, we propose SMFRepair, a single-node multi-level forwarding repair technique that is designed to improve the performance in heterogeneous networks based on Reed-Solomon codes for general fault tolerance. SMFRepair carefully selects the helper nodes and uses idle nodes to bypass low-bandwidth links. Idle nodes have sufficient and unused network bandwidth. It also pipelines the repair links that are optimized by idle nodes. Furthermore, a multi-node scheduling repair technique, called MSRepair, is proposed. MSRepair carefully schedules the multi-node repair link to saturate the most unoccupied bandwidth and transfers data from as large-bandwidth links as possible, with the primary objective of minimizing the recovery time. Large-scale simulation and Amazon EC2 real experiments show that compared to state-of-the-art repair techniques, SMFRepair can accelerate the single-node recovery by up to 47.69%, and MSRepair can reduce the multi-node recovery time by 33.78%$\sim$∼67.53%.","1558-2183","","10.1109/TPDS.2022.3153061","National Key R&D Program of China(grant numbers:2018YFB1003305); National Natural Science Foundation of China(grant numbers:61872414); Key Laboratory of Information Storage System Ministry of Education of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720121","Erasure coding;recovery time;heterogeneous network;repair link","Maintenance engineering;Bandwidth;Codes;Encoding;Task analysis;Heterogeneous networks;Data centers","fault tolerance;redundancy;Reed-Solomon codes;storage management;telecommunication scheduling;telecommunication traffic","bandwidth-aware scheduling repair techniques;erasure-coded clusters;storage-efficient redundancy mechanism;data availability guarantees;high network traffic consumption;failure repair;specific erasure code constructions;distributed storage clusters;heterogeneous bandwidth property;network environments;low-bandwidth links;SMFRepair;single-node multilevel forwarding repair technique;heterogeneous networks;Reed-Solomon codes;helper nodes;idle nodes;unused network bandwidth;repair links;multinode scheduling repair technique;multinode repair link;unoccupied bandwidth;large-bandwidth links;single-node recovery;multinode recovery time","",1.0,"",62.0,"IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"Coordinating Fast Concurrency Adapting With Autoscaling for SLO-Oriented Web Applications","J. Liu; S. Zhang; Q. Wang; J. Wei","Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, LA, USA; School of Cyber and Computer Sciences, Augusta University, Augusta, GA, USA; Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, LA, USA; Department of Software and Information Systems, University of North Carolina at Charlotte, Charlotte, NC, USA","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2022",2022,33.0,12.0,3349,3362,"Cloud providers tend to support dynamic computing resources reallocation (e.g., Autoscaling) to handle the bursty workload for web applications (e.g., e-commerce) in the cloud environment. Nevertheless, we demonstrate that directly scaling a bottleneck server without quickly adjusting its soft resources (e.g., server threads and database connections) can cause significant response time fluctuations of the target web application. Since soft resources determine the request processing concurrency of each server in the system, simply scaling out/in the bottleneck service can unintentionally change the concurrency level of related services, inducing either under- or over-utilization of the critical hardware resource. In this paper, we propose the Scatter-Concurrency-Throughput (SCT) model, which can rapidly identify the near-optimal soft resource allocation of each server in the system using the measurement of each server’s real-time throughput and concurrency. Furthermore, we implement a Concurrency-aware autoScaling (ConScale) framework that integrates the SCT model to quickly reallocate the soft resources of the key servers in the system to best utilize the new hardware resource capacity after the system scaling. Based on extensive experimental comparisons with two widely used hardware-only scaling mechanisms for web applications: EC2-AutoScaling (VM-based autoscaler) and Kubernetes HPA (container-based autoscaler), we show that ConScale can successfully mitigate the response time fluctuations over the system scaling phase in both VM-based and container-based environments.","1558-2183","","10.1109/TPDS.2022.3151512","National Science Foundation(grant numbers:CNS-2000681); Office of Naval Research(grant numbers:N00014-21-1-2171/N00014-19-1-2371); Army Research Office(grant numbers:W911NF-17-1-0437); Fujitsu; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714008","Scalability;auto-scaling;soft resource;cloud-based applications","Concurrent computing;Servers;Hardware;Throughput;Time factors;Resource management;Topology","cloud computing;Internet;network servers;resource allocation;virtual machines","key servers;hardware resource capacity;EC2-AutoScaling;system scaling phase;coordinating fast Concurrency adapting;SLO-oriented web applications;cloud providers;dynamic computing resources reallocation;cloud environment;bottleneck server;soft resources;server threads;significant response time fluctuations;target web application;request processing concurrency;bottleneck service;concurrency level;inducing;critical hardware resource;Scatter-Concurrency-Throughput model;near-optimal soft resource allocation;real-time throughput;Concurrency-aware autoScaling framework","","","",53.0,"IEEE","14 Feb 2022","","","IEEE","IEEE Journals"
"A Native Tensor–Vector Multiplication Algorithm for High Performance Computing","P. J. Martinez-Ferrer; A. N. Yzelman; V. Beltran","Universitat Politècnica de Catalunya (UPC), Barcelona, Spain; Computing Systems Laboratory, Huawei Technologies Switzerland, Zürich, Switzerland; Barcelona Supercomputing Center (BSC), Barcelona, Spain","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2022",2022,33.0,12.0,3363,3374,"Tensor computations are important mathematical operations for applications that rely on multidimensional data. The tensor–vector multiplication (TVM) is the most memory-bound tensor contraction in this class of operations. This article proposes an open-source TVM algorithm which is much simpler and efficient than previous approaches, making it suitable for integration in the most popular BLAS libraries available today. Our algorithm has been written from scratch and features unit-stride memory accesses, cache awareness, mode obliviousness, full vectorization and multi-threading as well as NUMA awareness for non-hierarchically stored dense tensors. Numerical experiments are carried out on tensors up to order 10 and various compilers and hardware architectures equipped with traditional DDR and high bandwidth memory (HBM). For large tensors the average performance of the TVM ranges between 62% and 76% of the theoretical bandwidth for NUMA systems with DDR memory and remains independent of the contraction mode. On NUMA systems with HBM the TVM exhibits some mode dependency but manages to reach performance figures close to peak values. Finally, the higher-order power method is benchmarked with the proposed TVM kernel and delivers on average between 58% and 69% of the theoretical bandwidth for large tensors.","1558-2183","","10.1109/TPDS.2022.3153113","HPC Technology Innovation Lab; Barcelona Supercomputing Center; Huawei Research Cooperation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720217","Parallel algorithms;shared memory;tensor computations;high bandwidth memory;NUMA","Tensors;Kernel;Libraries;Bandwidth;Virtual machine monitors;Layout;Benchmark testing","cache storage;DRAM chips;mathematics computing;matrix multiplication;multiprocessing systems;multi-threading;parallel architectures;parallel processing;shared memory systems;tensors","memory-bound tensor contraction;open-source TVM algorithm;popular BLAS libraries available today;features unit-stride memory accesses;cache awareness;mode obliviousness;vectorization;NUMA awareness;dense tensors;various compilers;hardware architectures;high bandwidth memory;theoretical bandwidth;NUMA systems;DDR memory;contraction mode;performance figures;TVM kernel;native tensor-vector multiplication algorithm;high performance computing;tensor computations;important mathematical operations;multidimensional data","","","",19.0,"IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"FenceKV: Enabling Efficient Range Query for Key-Value Separation","C. Tang; J. Wan; C. Xie","Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2022",2022,33.0,12.0,3375,3386,"LSM-tree is widely used in key-value stores for big data storage, but it suffers from write amplification brought by frequent compaction operations. An effective solution for this problem is key-value separation, which decouples values from the LSM-tree and stores them in a separate value log. However, existing key-value separation schemes achieve poor range query performance, especially for small key-value pairs, because they focus on mitigating write amplification but neglect access characteristics of the SSD. In this article, we propose FenceKV, which aims to achieve better range query performance while maintaining reasonable update performance for update-intensive workloads. FenceKV employs a new partition method to map values to the storage space based on the key-range to achieve efficient update and range query. Moreover, it adopts a key-range garbage collection policy to mitigate the garbage collection overhead and maintain sequential access for range queries. We compare FenceKV with modern key-value stores with various workloads, and results show that FenceKV can improve the range query performance significantly, while maintaining reasonable update performance compared to the existing designs of key-value separation.","1558-2183","","10.1109/TPDS.2022.3149003","National Natural Science Foundation of China(grant numbers:62072196); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2021B0101400003); National Natural Science Foundation of China(grant numbers:61821003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706279","LSM-tree;key-value store;key-value separation;range query;SSD","Compaction;Performance evaluation;Throughput;Metadata;Space exploration;Optimization;Machine learning algorithms","distributed databases;flash memories;query processing;storage management;tree data structures","map values;efficient update;key-range garbage collection policy;range queries;FenceKV;modern key-value stores;reasonable update performance;enabling efficient range query;LSM-tree;write amplification;separate value log;key-value separation schemes;poor range query performance;key-value pairs","",1.0,"",45.0,"IEEE","7 Feb 2022","","","IEEE","IEEE Journals"
"Accelerating Bayesian Neural Networks via Algorithmic and Hardware Optimizations","H. Fan; M. Ferianc; Z. Que; X. Niu; M. Rodrigues; W. Luk","Department of Computing, Imperial College London, London, U.K.; Department of Electronic and Electrical Engineering, University College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.; Corerain Technologies Ltd., Shenzhen, Guangdong, China; Department of Electronic and Electrical Engineering, University College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2022",2022,33.0,12.0,3387,3399,"Bayesian neural networks (BayesNNs) have demonstrated their advantages in various safety-critical applications, such as autonomous driving or healthcare, due to their ability to capture and represent model uncertainty. However, standard BayesNNs require to be repeatedly run because of Monte Carlo sampling to quantify their uncertainty, which puts a burden on their real-world hardware performance. To address this performance issue, this article systematically exploits the extensive structured sparsity and redundant computation in BayesNNs. Different from the unstructured or structured sparsity in standard convolutional NNs, the structured sparsity of BayesNNs is introduced by Monte Carlo Dropout and its associated sampling required during uncertainty estimation and prediction, which can be exploited through both algorithmic and hardware optimizations. We first classify the observed sparsity patterns into three categories: channel sparsity, layer sparsity and sample sparsity. On the algorithmic side, a framework is proposed to automatically explore these three sparsity categories without sacrificing algorithmic performance. We demonstrated that structured sparsity can be exploited to accelerate CPU designs by up to 49 times, and GPU designs by up to 40 times. On the hardware side, a novel hardware architecture is proposed to accelerate BayesNNs, which achieves a high hardware performance using the runtime adaptable hardware engines and the intelligent skipping support. Upon implementing the proposed hardware design on an FPGA, our experiments demonstrated that the algorithm-optimized BayesNNs can achieve up to 56 times speedup when compared with unoptimized Bayesian nets. Comparing with the optimized GPU implementation, our FPGA design achieved up to 7.6 times speedup and up to 39.3 times higher energy efficiency.","1558-2183","","10.1109/TPDS.2022.3153682","U.K. EPSRC(grant numbers:EP/L016796/1,EP/N031768/1,EP/P010040/1,EP/V028251/1,EP/S030069/1); Corerain; Maxeler; Intel(grant numbers:Xilinx); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720069","Bayesian neural network (BayesNN);structured sparsity;field-programmable gate array (FPGA);deep learning","Hardware;Artificial neural networks;Uncertainty;Bayes methods;Standards;Estimation;Prediction algorithms","Bayes methods;belief networks;field programmable gate arrays;graphics processing units;logic design;Monte Carlo methods;multiprocessing systems;neural nets","Bayesian neural networks;safety-critical applications;autonomous driving;model uncertainty;standard BayesNNs;Monte Carlo sampling;real-world hardware performance;performance issue;extensive structured sparsity;unstructured sparsity;standard convolutional NNs;Monte Carlo Dropout;associated sampling;uncertainty estimation;algorithmic hardware optimizations;observed sparsity patterns;channel sparsity;layer sparsity;sample sparsity;sparsity categories;sacrificing algorithmic performance;novel hardware architecture;high hardware performance;runtime adaptable hardware engines;hardware design;algorithm-optimized BayesNNs;56 times speedup;unoptimized Bayesian nets;optimized GPU implementation;7.6 times speedup","","","",49.0,"IEEE","23 Feb 2022","","","IEEE","IEEE Journals"
"Fast Proactive Repair in Erasure-Coded Storage: Analysis, Design, and Implementation","X. Li; K. Cheng; Z. Shen; P. P. C. Lee","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong; School of Informatics, Xiamen University, Xiamen, Fujian, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","13 Jun 2022",2022,33.0,12.0,3400,3414,"Erasure coding offers a storage-efficient redundancy mechanism for maintaining data availability guarantees in large-scale storage clusters, yet it also incurs high performance overhead in failure repair. Recent developments in accurate disk failure prediction allow soon-to-fail (STF) nodes to be repaired in advance, thereby opening new opportunities for accelerating failure repair in erasure-coded storage. To this end, we present a fast proactive repair solution called ${{\sf FastPR}}$FastPR, which carefully couples two repair methods, namely migration (i.e., relocating the chunks of an STF node) and reconstruction (i.e., decoding the chunks of an STF node through erasure coding), so as to fully parallelize the repair operation across the storage cluster. ${{\sf FastPR}}$FastPR solves a bipartite maximum matching problem and schedules both migration and reconstruction in a parallel fashion. We show that ${{\sf FastPR}}$FastPR significantly reduces the repair time over the baseline repair approaches for both Reed-Solomon codes and Azure's Local Reconstruction Codes via mathematical analysis, large-scale simulation, and Amazon EC2 experiments.","1558-2183","","10.1109/TPDS.2022.3152817","National Key R&D Program of China(grant numbers:2021YFF0704001); National Natural Science Foundation of China(grant numbers:62072381); CCF-Huawei Innovation Research Plan(grant numbers:CCF2021-admin-270-202102); Xiamen Youth Innovation Fund(grant numbers:3502Z20206052); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721141","Erasure coding;repair;distributed storage","Maintenance engineering;Codes;Encoding;Redundancy;Production;Bandwidth;Schedules","digital storage;disc storage;error correction codes;parallel processing;Reed-Solomon codes;storage management","FastPR;Reed-Solomon codes;Azure local reconstruction codes;erasure-coded storage;storage-efficient redundancy mechanism;data availability guarantees;large-scale storage clusters;high performance overhead;failure repair;disk failure prediction;fast proactive repair solution;repair methods;STF node;repair operation;storage cluster","","","",54.0,"IEEE","24 Feb 2022","","","IEEE","IEEE Journals"
"OSM: Off-Chip Shared Memory for GPUs","S. Darabi; E. Yousefzadeh-Asl-Miandoab; N. Akbarzadeh; H. Falahati; P. Lotfi-Kamran; M. Sadrosadati; H. Sarbazi-Azad","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; School of Computer Science, Institute for Researches in Fundamental Sciences (IPM), Tehran, Iran; School of Computer Science, Institute for Researches in Fundamental Sciences (IPM), Tehran, Iran; School of Computer Science, Institute for Researches in Fundamental Sciences (IPM), Tehran, Iran; School of Computer Science, Institute for Researches in Fundamental Sciences (IPM), Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","13 Jun 2022",2022,33.0,12.0,3415,3429,"Graphics Processing Units (GPUs) employ a shared memory, a software-managed cache for programmers, in each streaming multiprocessor to accelerate data sharing among the threads in a thread block. Although 60% of the shared memory space is underutilized, on average, there are some workloads that demand higher shared memory capacities. Therefore, improving shared memory utilization while satisfying the needs of shared memory intensive workloads is challenging. We make a key observation that the lifetime of each shared memory address is significantly shorter than the execution time of a thread block. In this paper, we first propose Off-Chip Shared Memory (OSM) that allocates shared memory space in the off-chip memory, and accelerates accesses to it via a small on-chip cache. Using an 8 KB cache for shared memory addresses, OSM provides almost the same performance as the baseline GPU that uses 96 KB on-chip shared memory. OSM improves GPU performance in two ways. First, it allocates higher shared memory capacities in the off-chip memory, and improves thread-level parallelism (TLP). Second, it designs a unified cache for shared memory and global address spaces, providing more caching space for global memory address space even for the workloads with high shared memory utilization. Our experimental results show an average 21% and 18% IPC improvement compared to the baseline and the state-of-the-art architectures.","1558-2183","","10.1109/TPDS.2022.3154315","Iran National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721067","Cache memory;GPUs;lifetime;off-chip memory;shared memory","Instruction sets;Graphics processing units;System-on-chip;Memory management;Proposals;Bandwidth;Registers","cache storage;coprocessors;graphics processing units;multiprocessing systems;shared memory systems","OSM;data sharing;thread block;shared memory space;shared memory utilization;shared memory intensive workloads;shared memory address;on-chip cache;on-chip shared memory;global memory address space;off-chip shared memory;graphics processing units;software-managed cache;multiprocessor streaming","",1.0,"",76.0,"IEEE","24 Feb 2022","","","IEEE","IEEE Journals"
"Efficient Function Queryable and Privacy Preserving Data Aggregation Scheme in Smart Grid","Y. Zhan; L. Zhou; B. Wang; P. Duan; B. Zhang","State Key Laboratory of Integrated Service Networks and the Cryptographic Research Center, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks and the Cryptographic Research Center, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks and the Cryptographic Research Center, Xidian University, Xi’an, China; Secure Collaborative Intelligence Laboratory, Ant Group, Hangzhou, China; Secure Collaborative Intelligence Laboratory, Ant Group, Hangzhou, China","IEEE Transactions on Parallel and Distributed Systems","13 Jun 2022",2022,33.0,12.0,3430,3441,"The collection of users’ near-real-time electricity consumption data brings advantages to the operation of smart grids, while raising some security and privacy issues. Multiple privacy preserving data aggregation schemes have been proposed to address these problems. However, most schemes only focus on the aggregation of electricity consumption data without considering the data availability. In addition, although a data aggregation scheme that supports function queries on encrypted data has also been developed, its efficiency is insufficient. In this paper, we first propose an EC-ElGamal encryption algorithm with a double trapdoor decryption mechanism. Through employing the proposed algorithm and the elliptic curve Schnorr signature scheme, an efficient data aggregation scheme supporting privacy protection and function query is proposed for smart grids. This solution allows the control center and users to initiate various function queries on encrypted data. In order to lighten the calculation burden of the control center, we propose another cryptosystem named ElGamal-OU to improve the decryption efficiency, which also supports two independent decryption methods. Finally, the security analysis and performance comparison with related work show that our schemes have advantages in terms of computational, communication and storage overhead.","1558-2183","","10.1109/TPDS.2022.3153930","National Natural Science Foundation of China(grant numbers:U19B2021,61972457); Key Research and Development Program of Shaanxi(grant numbers:2020ZDLGY08-04); Innovation Scientists and Technicians Troop Construction Projects of Henan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720954","Fog computing;function queryable;homomorphic encryption;privacy-preserving","Data aggregation;Smart grids;Smart meters;Data privacy;Costs;Real-time systems;Privacy","data privacy;digital signatures;power engineering computing;public key cryptography;query processing;smart power grids","privacy preserving data aggregation scheme;smart grid;near-real-time electricity consumption data;security issues;privacy issues;data availability;encrypted data;EC-ElGamal encryption algorithm;double trapdoor decryption mechanism;elliptic curve Schnorr signature scheme;data aggregation;privacy protection;function query;control center;decryption efficiency","",2.0,"",35.0,"IEEE","24 Feb 2022","","","IEEE","IEEE Journals"
"Efficient Flow-Based Scheduling for Geo-Distributed Simulation Tasks in Collaborative Edge and Cloud Environments","Z. Miao; P. Yong; Z. Jiancheng; Y. Quanjun","College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, China; College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, China; College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, China; College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","13 Jun 2022",2022,33.0,12.0,3442,3459,"Edge computing is a good complement to cloud computing for deploying large-scale geo-distributed simulation applications, which are very sensitive to the communication delay among different simulation components (also called tasks in this paper) and users. We mainly focus on the efficient scheduling of simulation components in collaborative edge and cloud environments. As components should be deployed jointly with the consideration of capacity constraints of hosts, it is actually an NP-complete multi-dimensional bin packing problem. Meanwhile, dynamic changes of component and host states require the low deployment latency of scheduling algorithms. Unfortunately, most of the existing schedulers for modern clusters are queue-based, in which tasks are scheduled sequentially, thus lacking the ability to process tightly coupled tasks jointly. Other batching-based placement algorithms are usually time-consuming. This paper describes Pond, a novel flow-based scheduler with the awareness of interactions among tasks and users as well as heterogeneous multi-dimensional resources. First, characteristics of distributed simulation tasks are analysed and the scheduling problem is formulated as a min-cost max-flow (MCMF) problem over the flow network by mapping the communication overhead among tasks and users to the costs of arcs in the network. Considering the inherent defects of existing flow-based schedulers in dealing with multi-dimensional resources, a new method based on dominant resource is proposed and some problem specific heuristics are also designed. Extensive simulation experiments based on Alibaba production trace and some random synthetic parameters are conducted. Results show that Pond can reduce the average communication cost for each task significantly in a quite low deployment latency compared with some baselines.","1558-2183","","10.1109/TPDS.2022.3155713","National Natural Science Foundation of China(grant numbers:62103425,62103428); Natural Science Foundation of Hunan Province(grant numbers:2021JJ40697); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9726854","Task scheduling;distributed simulation;minimum cost maximum flow;cloud and edge environment","Task analysis;Computational modeling;Data models;Costs;Cloud computing;Optimization;Clustering algorithms","bin packing;cloud computing;computational complexity;geographic information systems;resource allocation;scheduling","problem specific heuristics;extensive simulation experiments;average communication cost;efficient flow-based scheduling;geo-distributed simulation tasks;collaborative edge;cloud environments;edge computing;cloud computing;large-scale geo-distributed simulation applications;communication delay;simulation components;NP-complete multidimensional bin packing problem;host states;scheduling algorithms;tightly coupled tasks;batching-based placement algorithms;heterogeneous multidimensional resources;scheduling problem;min-cost max-flow problem;flow network;flow-based scheduler;flow-based schedulers;MCMF","","","",66.0,"IEEE","3 Mar 2022","","","IEEE","IEEE Journals"
"OCTOPUS: Overcoming Performance and Privatization Bottlenecks in Distributed Learning","S. Wang; S. Nepal; K. Moore; M. Grobler; C. Rudolph; A. Abuadbba","CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia; CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia; CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia; CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia","IEEE Transactions on Parallel and Distributed Systems","13 Jun 2022",2022,33.0,12.0,3460,3477,"The diversity and quantity of data warehouses, gathering data from distributed devices such as mobile devices, can enhance the success and robustness of machine learning algorithms. Federated learning enables distributed participants to collaboratively learn a commonly shared model while holding data locally. However, it is also faced with expensive communication and limitations due to the heterogeneity of distributed data sources and lack of access to global data. In this paper, we investigate a practical distributed learning scenario where multiple downstream tasks (e.g., classifiers) could be efficiently learned from dynamically updated and non-iid distributed data sources while providing local data privatization. We introduce a new distributed/collaborative learning scheme to address communication overhead via latent compression, leveraging global data while providing privatization of local data without additional cost due to encryption or perturbation. This scheme divides learning into (1) informative feature encoding, and transmitting the latent representation of local data to address communication overhead; (2) downstream tasks centralized at the server using the encoded codes gathered from each node to address computing overhead. Besides, a disentanglement strategy is applied to address the privatization of sensitive components of local data. Extensive experiments are conducted on image and speech datasets. The results demonstrate that downstream tasks with the compact latent representations with the privatization of local data can achieve comparable accuracy to centralized learning.","1558-2183","","10.1109/TPDS.2022.3157258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729632","Distributed learning;data collection;representation learning;disentanglement;privatization","Distributed databases;Task analysis;Servers;Privatization;Data models;Dictionaries;Training","cryptography;data compression;data warehouses;distributed processing;groupware;learning (artificial intelligence);pattern classification","global data;communication overhead;centralized learning;data warehouses;distributed devices;machine learning;federated learning;distributed learning;noniid distributed data sources;local data privatization;OCTOPUS;mobile devices;classifiers;collaborative learning;latent compression;encryption;informative feature encoding;local data latent representation;computing overhead;disentanglement strategy","","","",70.0,"IEEE","7 Mar 2022","","","IEEE","IEEE Journals"
"Solving Consensus in True Partial Synchrony","S. Srinivasan; R. Kandukoori","Department of Computer Science and Engineering, National Institute of Technology Warangal, Hanamkonda, Telangana, India; Department of Computer Science and Engineering, National Institute of Technology Warangal, Hanamkonda, Telangana, India","IEEE Transactions on Parallel and Distributed Systems","15 Jun 2022",2022,33.0,12.0,3478,3490,"The notion of partial synchrony has been introduced to circumvent the FLP impossibility result for solvability of consensus in fault tolerant distributed systems. This notion helps us to evaluate the efficiency of algorithms that needs to solve consensus in the given time interval $ T[\tau _{1} \leq T_{S} \leq \tau _{2}]$T[τ1≤TS≤τ2] where $ T_{S}$TS is the stable time when the system becomes synchronous after a transient period of asynchrony. It has been shown that consensus can be solved in constant time after system enters $ T_{S}$TS with an upper bound of $ T_{S}+17\delta$TS+17δ, and this was later reduced to $ T_{S}+11\delta$TS+11δ, where $ \delta$δ is the upper bound on message delivery. The above result is not trivial as they allowed processes that failed before $ T_{S}$TS to recover after $ T_{S}$TS and participate in consensus. But these algorithms assume that the upper bound for message delivery $\delta$δ holds even for messages sent before $ T_{S}$TS (i.e.,) when the system is in asynchrony. This assumption is limiting as messages can get delayed beyond expected timings in real time distributed systems due to different reasons like network congestion, packet re-transmission, e.t.c. In this work, we overcome this limitation by assuming messages sent before $ T_{S}$TS has no upper bound while also allowing process restarts after $ T_{S}$TS and show that consensus can be solved in constant time after $ T_{S}$TS. Our proposed algorithm solves consensus in $ T_{S}+10\delta$TS+10δ time, which is more efficient than the current upper bound and without the limiting assumption of bounded message delay before $ T_{S}$TS.","1558-2183","","10.1109/TPDS.2022.3156925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729476","Paxos;consensus;agreement problem;fault tolerance;partial synchronous systems","Upper bound;Fault tolerant systems;Fault tolerance;Transient analysis;Synchronization;Real-time systems;Limiting","delays;distributed algorithms;fault tolerance;message passing;system recovery","message delivery;constant time;bounded message delay;partial synchrony;FLP impossibility result;fault tolerant distributed systems","",1.0,"",12.0,"IEEE","7 Mar 2022","","","IEEE","IEEE Journals"
"Jdebug: A Fast, Non-Intrusive and Scalable Fault Locating Tool for Ten-Million-Scale Parallel Applications","D. Peng; Y. Feng; Y. Liu; X. Liu; W. Xue; D. Chen; J. Song; Z. Chen","National Supercomputer Center in Wuxi, Wuxi, China; National Supercomputer Center in Wuxi, Wuxi, China; National Research Centre of Parallel Computer Engineering and Technology, Beijing, China; National Research Centre of Parallel Computer Engineering and Technology, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; National Supercomputer Center in Wuxi, Wuxi, China; National Research Centre of Parallel Computer Engineering and Technology, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","17 Jun 2022",2022,33.0,12.0,3491,3504,"This article presents Jdebug, a fast, non-intrusive and scalable fault locating tool for extreme-scale parallel applications. Large-scale debugging has drawn more attention with the increasing scale of supercomputers and applications. To eliminate program intrusion caused by traditional instrumentation or interception during debugging information acquisition, we introduce the out-of-band management into large-scale debugging. We propose a rapid information gathering scheme that separates user and debugging traffic to solve scalability problem and to eliminate program interference during merging data. Observations of Program Counters (PC) and performance characteristics in suspended applications find abnormalities and help locate abnormal threads caused by software errors or hardware failures effectively. Evaluation shows that Jdebug collects PCs of over 20 million cores on the new Sunway supercomputer within 1.97 seconds, and can locate the abnormal threads in 1.4 seconds with an accuracy of 92.5%. In the running test of three fundamental benchmarks (HPL, HPCG, Graph500) and seventeen real-world applications, Jdebug quickly and accurately locates abnormal threads to help find scalability errors and hardware failures including memory access failures, communication failures, and execution component failures, which validates its effectiveness.","1558-2183","","10.1109/TPDS.2022.3157690","National Key Research and Development Program of China(grant numbers:2017YFB0202001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730097","Distributed debugging;software engineering;software/software engineering;testing and debugging","Debugging;Servers;Supercomputers;Hardware;Maintenance engineering;Instruction sets;Task analysis","fault location;parallel machines;program debugging","Sunway supercomputer;program intrusion;large-scale debugging;rapid information gathering scheme;debugging traffic;scalability problem;performance characteristics;Jdebug;scalability errors;scalable fault locating tool;ten-million-scale parallel applications;extreme-scale parallel applications;program counters;abnormal threads","",1.0,"",52.0,"IEEE","8 Mar 2022","","","IEEE","IEEE Journals"
"OptZConfig: Efficient Parallel Optimization of Lossy Compression Configuration","R. Underwood; J. C. Calhoun; S. Di; A. Apon; F. Cappello","Argonne National Laboratory Mathematics and Computer Science Division, Lemont, IL, USA; Clemson University, Clemson, SC, USA; Argonne National Laboratory Mathematics and Computer Science Division, Lemont, IL, USA; Clemson University, Clemson, SC, USA; Argonne National Laboratory Mathematics and Computer Science Division, Lemont, IL, USA","IEEE Transactions on Parallel and Distributed Systems","13 Jun 2022",2022,33.0,12.0,3505,3519,"Lossless compressors have very low compression ratios that do not meet the needs of today’s large-scale scientific applications that produce vast volumes of data. Error-bounded lossy compression (EBLC) is considered a critical technique for the success of scientific research. Although EBLC allows users to set an error bound for the compression, users have been unable to specify the requirements on the compression quality, limiting practical use. Our contributions are: (1) We formulate the problem of configuring EBLC to preserve a user-defined metric as an optimization problem. This allows many classes of new metrics to be preserved, which improves over current practices. (2) We present a framework, OptZConfig, that can adapt to improvements in the search algorithm, compressor, and metrics with minimal changes, enabling future advancements in this area. (3) We demonstrate the advantages of our approach against the leading methods to configure compressors to preserve specific metrics. Our approach improves compression ratios against a specialized compressor by up to $3\times$3×, has a 56× speedup over FRaZ, 1000× speedup over MGARD-QOI post tuning, and 110× speedup over systematic approaches which had not been bounded by compressors before.","1558-2183","","10.1109/TPDS.2022.3154096","National Science Foundation(grant numbers:NRTDESE 1633608,1619253,1910197,OAC-2003709,OAC-2104023); Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration(grant numbers:DE-AC02-06CH11357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730115","Error bounded lossy compression;LibPressio;non-linear optimization;parallel computing","Measurement;Compressors;Optimization;Distortion;Meteorology;Quantization (signal);PSNR","data compression;optimisation","parallel optimization;lossy compression configuration;error-bounded lossy compression;EBLC;OptZConfig;user-defined metric","",2.0,"",38.0,"IEEE","8 Mar 2022","","","IEEE","IEEE Journals"
"Max-Tree Computation on GPUs","N. Blin; E. Carlinet; F. Lemaitre; L. Lacassagne; T. Géraud","EPITA Research and Development Laboratory (LRDE), Paris, France; EPITA Research and Development Laboratory (LRDE), Paris, France; LIP6, ALSOC team, Sorbonne University, CNRS, Paris, France; LIP6, ALSOC team, Sorbonne University, CNRS, Paris, France; EPITA Research and Development Laboratory (LRDE), Paris, France","IEEE Transactions on Parallel and Distributed Systems","15 Jun 2022",2022,33.0,12.0,3520,3531,"In Mathematical Morphology, the max-tree is a region-based representation that encodes the inclusion relationship of the threshold sets of an image. This tree has proved useful in numerous image processing applications. For the last decade, work has led to improving the construction time of this structure; mixing algorithmic optimizations, parallel and distributed computing. Nevertheless, there is still no algorithm that benefits from the computing power of the massively parallel architectures. In this work, we propose the first GPU algorithm to compute the max-tree. The proposed approach leads to significant speed-ups, and is up to one order of magnitude faster than the current State-of-the-Art parallel CPU algorithms. This work paves the way for a max-tree integration in image processing GPU pipelines and real-time image processing based on Mathematical Morphology. It is also a foundation for porting other image representations from Mathematical Morphology on GPUs.","1558-2183","","10.1109/TPDS.2022.3158488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732653","Mathematical morphology;hierarchical image representation;component-trees;max-tree;graph algorithms","Image processing;Pipelines;Morphology;Optimization;Shape;Level set;Encoding","graphics processing units;image processing;image representation;mathematical morphology;multiprocessing systems;parallel architectures;trees (mathematics)","region-based representation;inclusion relationship;threshold sets;numerous image processing applications;construction time;algorithmic optimizations;computing power;massively parallel architectures;GPU algorithm;current State-of-the-Art parallel CPU algorithms;max-tree integration;image processing GPU pipelines;real-time image processing;Mathematical Morphology;image representations;GPUs;max-tree computation","",1.0,"",49.0,"IEEE","10 Mar 2022","","","IEEE","IEEE Journals"
"Optimal Convex Hull Formation on a Grid by Asynchronous Robots With Lights","R. Hector; R. Vaidyanathan; G. Sharma; J. L. Trahan","Division of Electrical & Computer Engineering Louisiana State University, Baton Rouge, LA, USA; Division of Electrical & Computer Engineering Louisiana State University, Baton Rouge, LA, USA; Department of Computer Science, Kent State University, Kent, OH, USA; Division of Electrical & Computer Engineering Louisiana State University, Baton Rouge, LA, USA","IEEE Transactions on Parallel and Distributed Systems","13 Jun 2022",2022,33.0,12.0,3532,3545,"We consider the distributed setting of $n$n autonomous mobile robots that operate in Look-Compute-Move cycles and communicate with other robots using a constant number of colored lights (the robots with lights model). We assume obstructed visibility where collinear robots do not see each other. In addition, we consider a grid-based terrain embedded in the 2-dimensional euclidean plane. The Convex Hull Formation problem is to relocate the $n$n robots (starting at arbitrary, but distinct, initial positions) so that each robot is positioned on a vertex of a convex hull. In this article, we provide a framework for solving Convex Hull Formation. We then provide four asynchronous algorithms under this framework. Key measures of the algorithms’ performance include the time taken and the space occupied. The presented algorithms are randomized and their time bounds hold with high probability. The first $O(\max \lbrace n^{2},D\rbrace)$O(max{n2,D})-time, $O({n^{2}})$O(n2)-perimeter, and $O({n^{3}})$O(n3)-area algorithm serves to introduce key ideas, where $D$D is the diameter of the initial configuration. The subsequent algorithms, differing in computational requirements, run in $O(\max \lbrace n^{\frac{3}{2}},D\rbrace)$O(max{n32,D}) time with a perimeter of $O(n^{\frac{3}{2}})$O(n32) and area of $O(n^{3})$O(n3). We also prove lower bounds of $\Omega (n^{\frac{3}{2}})$Ω(n32) for time and perimeter and $\Omega (n^{3})$Ω(n3) for area, for any Convex Hull Formation algorithm; i.e., our $O(\max \lbrace n^{\frac{3}{2}},D\rbrace)-$O(max{n32,D})-time algorithm is optimal in time, perimeter, and area.","1558-2183","","10.1109/TPDS.2022.3158202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732663","Autonomous robots;robots with lights;complete visibility;convex hull;grid;area;perimeter;time;colors","Robots;Robot kinematics;Robot sensing systems;Color;Computational modeling;Service robots;Voting","collision avoidance;computational complexity;computational geometry;mobile robots;probability","constant number;colored lights;lights model;collinear robots;grid-based terrain;2-dimensional euclidean plane;Convex Hull Formation problem;n robots;distinct positions;initial positions;asynchronous algorithms;key measures;time bounds;subsequent algorithms;computational requirements;perimeter;Convex Hull Formation algorithm;optimal Convex Hull Formation;asynchronous robots;distributed setting;n autonomous mobile robots","","","",30.0,"IEEE","10 Mar 2022","","","IEEE","IEEE Journals"
"Reliable Wide-Area Data Transfers for Streaming Workflows","H. Sapkota; E. Arslan","Department of Computer Science & Engineering, University of Nevada, Reno, NV, USA; Department of Computer Science & Engineering, University of Nevada, Reno, NV, USA","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3546,3557,"Many large science projects rely on remote clusters for (near) real-time data processing, thus they demand reliable wide-area data transfer performance for smooth end-to-end workflow executions. However, data transfers are often exposed to performance variations due to the changing network (e.g., background traffic) and dataset (e.g., average file size) conditions, necessitating adaptive solutions to meet stringent performance requirements of delay-sensitive streaming workflows. In this article, we propose FStream++ to provide reliable transfer performance for large streaming science applications by dynamically adjusting transfer settings to adapt to changing transfer conditions. FStream++ combines three optimization methods as dynamic tuning, online profiling, and historical analysis to swiftly and accurately discover optimal transfer settings that can meet workflow requirements. Dynamic tuning uses a heuristic model to predict the values of transfer parameters based on dataset characteristics and network settings. Since heuristic models fall short to incorporate many important factors such as I/O throughput and resource interference, we complement it with online profiling to execute a real-time search for a subset of transfer settings. Finally, historical analysis takes advantage of the long-running nature of streaming workflows by storing and analyzing previous performance observations to shorten the execution time of online profiling. We evaluate the performance of FStream++ by transferring several synthetic and real-world workloads in high-performance production networks and show that it offers up to $3.6x$3.6x performance improvement over legacy transfer applications and up to 24% over our previous work FStream.","1558-2183","","10.1109/TPDS.2022.3158673","National Science Foundation(grant numbers:1850353,2007789,2019164); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733263","Distributed workflows;high-speed networks;online profiling;streaming science applications;throughput optimization","Throughput;Reliability;Real-time systems;Data transfer;Pipeline processing;Concurrent computing;Optimization","C++ language;data analysis;input-output programs","end-to-end workflow executions;FStream++;dataset characteristics;streaming workflows;data processing;wide-area data transfers;high-performance production networks;historical analysis;online profiling;network settings;transfer parameters;dynamic tuning","","","",40.0,"IEEE","11 Mar 2022","","","IEEE","IEEE Journals"
"Leveraging Code Snippets to Detect Variations in the Performance of HPC Systems","J. Zhai; L. Zheng; J. Sun; F. Zhang; X. Tang; X. Qian; B. He; W. Xue; W. Chen; W. Zheng","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science, University of Illinois Urbana-Champaign, Champaign, IL, USA; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA; School of Computing, National University of Singapore, Singapore; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3558,3574,"Variations in the performance of parallel and distributed systems are becoming increasingly challenging. The runtimes of different executions can vary greatly even with a fixed number of computing nodes. Many HPC applications on supercomputers exhibit such variance. This not only leads to unpredictable execution times, but also renders the system’s behavior unintuitive. The efficient online detection of variations in performance is an open problem in HPC research. To solve it, we propose an approach, called vSensor, to detect variations in the performance of systems. The key finding of this study is that the source code of programs can better represent performance at runtime than an external detector. Specifically, many HPC applications contain code snippets that are fixed workload patterns of execution, e.g., the workload of an invariant quantity and a linearly growing workload. This observation allows us to automatically identify these snippets of workload-related code and use them to detect variations in performance. We evaluate vSensor on the Tianhe-2A system with a large number of parallel applications, and the results indicate that it can efficiently identify variations in system performance. The average overhead of 4,096 processes is less than 6% for fixed-workload v-sensors. We identify a problematic node with slow memory by using vSensor that degrades the performance of the program by 21%. A serious issue with network performance is also detected that slows down the Tianhe-2A system by 3.37 times for an HPC kernel.","1558-2183","","10.1109/TPDS.2022.3158742","National Key R&D Program of China(grant numbers:2017YFA0604500); National Natural Science Foundation of China(grant numbers:U20A20226); Beijing Natural Science Foundation(grant numbers:4202031,L192027); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program(grant numbers:20191080594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9735307","Parallel computing;performance variance;HPC;performance analysis;performance optimization","Codes;Runtime;Sensors;Benchmark testing;Instruments;Performance analysis;Supercomputers","application program interfaces;multiprocessing systems;parallel architectures;parallel machines;source code (software)","code snippets;workload patterns;workload-related code;vSensor;HPC kernel;HPC systems;parallel systems;distributed systems;supercomputer;online variation detection;source code;fixed-workload V-sensor","","","",47.0,"IEEE","15 Mar 2022","","","IEEE","IEEE Journals"
"SciSpot: Scientific Computing On Temporally Constrained Cloud Preemptible VMs","J. Kadupitiya; V. Jadhao; P. Sharma","Department of Intelligent Systems Engineering, Indiana University, Bloomington, IN, USA; Department of Intelligent Systems Engineering, Indiana University, Bloomington, IN, USA; Department of Intelligent Systems Engineering, Indiana University, Bloomington, IN, USA","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3575,3588,"Scientific computing applications are being increasingly deployed on cloud computing platforms. Transient servers such as EC2 spot instances and Google Preemptible VMs, can be used to lower the costs of running applications on the cloud by up to $10\times$10×. However, the frequent preemptions and resource heterogeneity of these transient servers introduces many challenges in their effective and efficient use. In this paper, we develop techniques for modeling and mitigating preemptions of transient servers, and present SciSpot, a software framework that enables low-cost scientific computing on the cloud. SciSpot deploys applications on Google Cloud Preemptible Virtual Machines that exhibit temporally constrained preemptions: VMs are always preempted in a 24 hour interval. Our empirical analysis shows that the preemption rate is generally bathtub shaped, which raises multiple fundamental challenges in performance modeling and policy design. We develop a new reliability model for temporally constrained preemptions, and use statistical mechanics to show why the bathtub shape is generally exhibited. SciSpot’s design is guided by our observation that many emerging scientific computing applications that integrate machine learning with simulations, can be deployed as “bags” of jobs, which represent multiple instantiations of the same computation with different physical model parameters. For a bag of jobs, SciSpot finds the optimal transient server on-the-fly, by taking into account the price, performance, and preemption rates of different servers. SciSpot reduces costs by $5\times$5× compared to conventional cloud deployments, and reduces makespans by up to $10\times$10× compared to conventional high performance computing clusters.","1558-2183","","10.1109/TPDS.2022.3157272","Google; U.S. Department of Energy(grant numbers:DE-SC0021418); National Science Foundation(grant numbers:OAC-2112606); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739004","Distributed systems;cloud computing;scientific computing","Cloud computing;Computational modeling;Transient analysis;Internet;Servers;Costs;Analytical models","cloud computing;data handling;learning (artificial intelligence);virtual machines","temporally constrained cloud preemptible VMs;machine learning;statistical mechanics;Google cloud preemptible virtual machines;low-cost scientific computing;resource heterogeneity;Google Preemptible VMs;EC2 spot instances;cloud computing platforms;high performance computing clusters;cloud deployments;optimal transient server on-the-fly;physical model parameters;scientific computing applications;SciSpot's design;bathtub shape;temporally constrained preemptions;reliability model;policy design;multiple fundamental challenges","","","",65.0,"IEEE","21 Mar 2022","","","IEEE","IEEE Journals"
"Storage-Heterogeneity Aware Task-based Programming Models to Optimize I/O Intensive Applications","H. Elshazly; J. Ejarque; R. M. Badia","Barcelona Supercomputing Center (BSC), Barcelona, Spain; Barcelona Supercomputing Center (BSC), Barcelona, Spain; Barcelona Supercomputing Center (BSC), Barcelona, Spain","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3589,3599,"Task-based programming models have enabled the optimized execution of the computation workloads of applications. These programming models can take advantage of large-scale distributed infrastructures by allowing the parallel and distributed execution of applications in high-level work components called tasks. Nevertheless, in the era of Big Data and Exascale, the amount of data produced by modern scientific applications has already surpassed terabytes and is rapidly increasing. Hence, I/O performance became the bottleneck to overcome in order to achieve more total performance improvement. New storage technologies offer higher bandwidth and faster solutions than traditional Parallel File Systems (PFS). Such storage devices are deployed in modern day infrastructures to boost I/O performance by offering a fast layer that absorbs the generated data. Therefore, it is necessary for any programming model targeting more performance to manage this heterogeneity and take advantage of it to improve the I/O performance of applications. Towards this goal, we propose in this article a set of programming model capabilities that we refer to as Storage-Heterogeneity Awareness. Such capabilities include: (i) abstracting the heterogeneity of storage systems, and (ii) optimizing I/O performance by supporting dedicated I/O schedulers and an automatic data flushing technique. The evaluation section of this article presents the performance results of different applications on the MareNostrum CTE-Power heterogeneous storage cluster. Our experiments demonstrate that a storage-heterogeneity aware programming model can achieve up to almost 5x I/O performance speedup and 48% total time improvement compared to the reference PFS-based usage of the execution infrastructure.","1558-2183","","10.1109/TPDS.2022.3161123","European Commission(grant numbers:721865); Spanish Government(grant numbers:PID2019-107255GB); Generalitat de Catalunya(grant numbers:2014-SGR-1051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739916","Heterogeneous storage systems;task-based programming models;I/O intensive applications;I/O scheduling;task scheduling;automatic data movement;heterogeneity abstraction;resource pooling;checkpointing","Task analysis;Programming;Performance evaluation;Computational modeling;Bandwidth;Proposals;Random access memory","Big Data;optimisation;parallel processing;power aware computing;resource allocation;scheduling;storage management","reference PFS-based usage;execution infrastructure;optimized execution;large-scale distributed infrastructures;high-level work components;Big Data;modern scientific applications;total performance improvement;traditional parallel file systems;storage devices;programming model capabilities;storage systems;automatic data flushing technique;MareNostrum CTE-Power heterogeneous storage cluster;storage-heterogeneity aware task-based programming models;Exascale","",2.0,"",27.0,"IEEE","22 Mar 2022","","","IEEE","IEEE Journals"
"Batch Crowdsourcing for Complex Tasks Based on Distributed Team Formation in E-Markets","J. Jiang; K. Di; B. An; Y. Jiang; Z. Bu; J. Cao","Jiangsu Provincial Key Laboratory of E-Business, School of Information Engineering, Nanjing University of Finance and Economics, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Southeast University, Nanjing, China; Jiangsu Provincial Key Laboratory of E-Business, School of Information Engineering, Nanjing University of Finance and Economics, Nanjing, China; Jiangsu Provincial Key Laboratory of E-Business, School of Information Engineering, Nanjing University of Finance and Economics, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3600,3615,"Team formation has been extensively studied for complex task crowdsourcing in E-markets, in which a set of workers are hired to form a team to complete a complex task collaboratively. However, existing studies have two typical drawbacks: 1) each team is created for only one task, which may be costly and cannot accommodate crowdsourcing markets with a large number of tasks; and 2) most existing studies form teams in a centralized manner by the requesters, which may place a heavy burden on requesters. In fact, we observe that many complex tasks at real-world crowdsourcing platforms have similar skill requirements and workers are often connected through social networks. Therefore, this paper explores distributed team formation-based batch crowdsourcing for complex tasks to address the drawbacks in existing studies, in which similar tasks can be addressed in a batch to reduce computational costs and workers can self-organize through their social networks to form teams. To solve such an NP-hard problem, this paper presents two approaches: one is to form a fixed team for all tasks in the batch; the other is to form a basic team that can be dynamically adjusted for each task in the batch. In comparison, the former approach has lower computational complexity but the latter approach performs better in reducing the total payments by requesters. With the experiments on a real-world dataset comparing with previous benchmark approaches, it is shown that the presented approaches have better performance in saving the costs of forming teams, payments by requesters, and communication among team members; moreover, the presented approaches have higher success rate of tasks and much better scalability.","1558-2183","","10.1109/TPDS.2022.3161019","National Key Research and Development Program of China(grant numbers:2019YFB1405000); National Natural Science Foundation of China(grant numbers:62076060,61932007,92046026,71871109); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201394); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739859","Batch crowdsourcing;complex tasks;team formation;social network;distributed manner","Task analysis;Crowdsourcing;Social networking (online);Costs;Training;NP-hard problem;Reliability","computational complexity;groupware;social networking (online);team working","distributed team formation;E-markets;complex task crowdsourcing;complex task collaboratively;requesters;real-world crowdsourcing platforms;similar skill requirements;social networks;team formation-based batch crowdsourcing;fixed team;basic team;presented approaches;team members","",1.0,"",41.0,"IEEE","22 Mar 2022","","","IEEE","IEEE Journals"
"Deep Neural Network Training With Distributed K-FAC","J. G. Pauloski; L. Huang; W. Xu; K. Chard; I. T. Foster; Z. Zhang","Department of Computer Science, University of Chicago, Chicago, IL, USA; Texas Advanced Computing Center, Austin, TX, USA; Texas Advanced Computing Center, Austin, TX, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Texas Advanced Computing Center, Austin, TX, USA","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3616,3627,"Scaling deep neural network training to more processors and larger batch sizes is key to reducing end-to-end training time; yet, maintaining comparable convergence and hardware utilization at larger scales is challenging. Increases in training scales have enabled natural gradient optimization methods as a reasonable alternative to stochastic gradient descent and variants thereof. Kronecker-factored Approximate Curvature (K-FAC), a natural gradient method, preconditions gradients with an efficient approximation of the Fisher Information Matrix to improve per-iteration progress when optimizing an objective function. Here we propose a scalable K-FAC algorithm and investigate K-FAC’s applicability in large-scale deep neural network training. Specifically, we explore layer-wise distribution strategies, inverse-free second-order gradient evaluation, and dynamic K-FAC update decoupling, with the goal of preserving convergence while minimizing training time. We evaluate the convergence and scaling properties of our K-FAC gradient preconditioner, for image classification, object detection, and language modeling applications. In all applications, our implementation converges to baseline performance targets in 9–25% less time than the standard first-order optimizers on GPU clusters across a variety of scales.","1558-2183","","10.1109/TPDS.2022.3161187","U.S. Department of Energy; Office of Science, Advanced Scientific Computing Research(grant numbers:DE-AC02-06CH11357); Exascale Computing Project(grant numbers:17-SC-20-SC); National Science Foundation(grant numbers:OAC-1931354,OAC-1818253,OAC-2106661,OAC-2107511); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739867","Optimization methods;neural networks;scalability;high-performance computing","Training;Parallel processing;Program processors;Convergence;Computational modeling;Data models;Deep learning","convergence of numerical methods;deep learning (artificial intelligence);gradient methods;graphics processing units;image classification;iterative methods;matrix algebra;object detection;optimisation","dynamic K-FAC update decoupling;scaling properties;K-FAC gradient preconditioner;distributed K-FAC;end-to-end training time;stochastic gradient descent;Kronecker-factored approximate curvature;large-scale deep neural network training;layer-wise distribution strategies;natural gradient optimization method;Fisher information matrix;inverse-free second-order gradient evaluation;image classification;object detection;language modeling application;standard first-order optimizer;GPU cluster","","","",55.0,"IEEE","22 Mar 2022","","","IEEE","IEEE Journals"
"Meet: Rack-Level Pooling Based Load Balancing in Datacenter Networks","J. Dong; L. Tan; C. Tian; Y. Zhou; Y. Wang; W. Dou; G. Chen","State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Pengcheng Lab, Shenzhen, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3628,3639,"Datacenter networks enable multiple paths between hosts to provide large bisection bandwidth. It requires load balancers to cope with network uncertainties such as traffic dynamics and topology asymmetry. Existing edge-based load balancing schemes are usually faced with the problem of limited network visibility. This article proposes Meet, a rack-level pooling based load-balancer deployed at the edge that can handle the aformentioned uncertainties. Meet utilizes both passive information as well as active probing to comprehensively sense the network conditions with relatively low cost. Meet dynamically reroutes flows effectively based on the visibility of the network condition. Meet has been tested with extensive flow-level simulations against state-of-the-art load balancers. It outperforms Hermes by up to 10% in the experiments, and outperforms others solutions such as DRILL by up to 50%. Meet requires no modifications to the switches and is feasible to deploy at the edge.","1558-2183","","10.1109/TPDS.2022.3162297","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101390001); National Natural Science Foundation of China(grant numbers:62072228,61971382); Fundamental Research Funds for the Central Universities; Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9743293","Load balancing;datacenter network","Load management;Bandwidth;Uncertainty;Topology;Routing;Software;Market research","computer centres;computer networks;resource allocation;telecommunication traffic","network visibility;rack-level pooling based load-balancer;Meet;network condition;extensive flow-level simulations;datacenter networks;network uncertainties;existing edge-based load balancing schemes;load balancers;traffic dynamics;topology asymmetry","","","",46.0,"IEEE","25 Mar 2022","","","IEEE","IEEE Journals"
"Ethernet for High-Throughput Computing at CERN","R. Krawczyk; T. Colombo; N. Neufeld; F. Pisani; S. Valat","Faculty of Electronics and Information Technology, Warsaw University of Technology, Warsaw, Poland; LHCb, EP-LBC, CERN, Meyrin, Switzerland; LHCb, EP-LBC, CERN, Meyrin, Switzerland; LHCb, EP-LBC, CERN, Meyrin, Switzerland; Atos, Bezons, France","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3640,3650,"When high throughput and utilization of fabric at close-to-the-link capacity are most needed in a cluster, Ethernet is a potential candidate, rivaling traditional HPC interconnects. The distributed real-time data acquisition at particle physics experiments presents an interesting use case. This article evaluates possible Ethernet-based solutions for aggregating data from hundreds of data sources at a throughput of dozens of Tb/s. This leads us to many-to-one data exchanges where we strive for a cost-optimized setup sustaining more than 80 % of the theoretical link-load. We investigate possible Ethernet-based traffic patterns to handle data acquisition on large multi-source apparatuses. Different numbers of producers and receivers and different link speeds are allowed in a large-scale network. Performance tests were conducted using customized benchmarks and evaluation test benches. The article presents tested scenarios and problems encountered in practice. We describe how our findings influenced the design of a large production system at CERN. We also present relevant general conclusions for a broader range of applications of Ethernet in HPC.","1558-2183","","10.1109/TPDS.2022.3163472","CERN; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745357","Computer system implementation;distributed architectures;ethernet;high energy physics;high-speed;network protocols;network topology;parallel architectures;real-time distributed","Ethernet;Filtering;Throughput;Real-time systems;Servers;Protocols;Large Hadron Collider","data acquisition;high energy physics instrumentation computing;local area networks","CERN;close-to-the-link capacity;traditional HPC interconnects;real-time data acquisition;particle physics experiments;possible Ethernet-based solutions;data sources;data exchanges;cost-optimized setup;theoretical link-load;possible Ethernet-based traffic patterns;multisource apparatuses;link speeds;performance tests;evaluation test benches;high-throughput computing","","","",48.0,"IEEE","30 Mar 2022","","","IEEE","IEEE Journals"
"High Performance Evaluation of Helmholtz Potentials Using the Multi-Level Fast Multipole Algorithm","M. P. Lingg; S. M. Hughey; B. Shanker; H. M. Aktulga","Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, USA; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3651,3666,"Evaluation of pair potentials is critical in a number of areas of physics. The classical $N$N-body problem has its root in evaluating the Laplace potential, and has spawned tree-algorithms, the fast multipole method (FMM), as well as kernel independent approaches. Over the years, FMM for Laplace potential has had a profound impact on a number of disciplines as it has been possible to develop highly scalable parallel versions of these algorithms. This is in stark contrast to parallel algorithms for oscillatory potentials such as the Helmholtz potential. The principal bottlenecks to scalable parallelism are the computation and communication costs of operations necessary to traverse up, across, and down the tree. In this article, we analyze asymptotic costs for both computation and communication in a parallel implementation, and describe techniques to overcome bottlenecks and achieve high performance evaluation of the Helmholtz potential for different distributions of particles. We demonstrate that the resulting implementation has a load balancing effect that significantly reduces the time-to-solution and enhances the scale of problems that can be treated using full wave physics.","1558-2183","","10.1109/TPDS.2022.3165649","National Science Foundation(grant numbers:CCF-1822932); National Energy Research Scientific Computing Center; U.S. Department of Energy(grant numbers:DE-AC02-05CH11231); High Performance Computing Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751386","Helmholtz equation;multilevel fast multipole method;tree algorithm;electromagnetic interaction;global interpolation","Costs;Parallel algorithms;Electric potential;Mathematical models;Octrees;Machine-to-machine communications;Kernel","computational electromagnetics;electromagnetic wave scattering","Laplace potential;spawned tree-algorithms;fast multipole method;oscillatory potentials;Helmholtz potential;parallel implementation;high performance evaluation;multilevel fast multipole algorithm;full wave physics","","","",60.0,"IEEE","7 Apr 2022","","","IEEE","IEEE Journals"
"Efficient Data Redistribution Algorithms From Irregular to Block Cyclic Data Distribution","S. Li; H. Jiang; D. Dong; C. Huang; J. Liu; X. Liao; X. Chen","College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; Science and Technology on Parallel and Distributed Processing Laboratory, Laboratory of Software Engineering for Complex Systems, College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3667,3677,"In this paper, we propose some efficient data redistribution algorithms for redistributing matrices from 1D or 2D irregular format to block cyclic data distribution (BCDD) format, which can be much faster than the BLACS routine PXGEMR2D. These algorithms can be used to combine direct methods with iterative methods. The proposed algorithms divide the communication into two phases: one for processes in the same column and the other for processes in the same row, and the whole data redistribution task is divided into several independent sub-communications. The communication time can be reduced a lot compared with BLACS. Performance results show that our algorithms can be $2\times$2×–$5\times$5× faster than the BLACS routine PXGEMR2D when using 4096 processes and the experiments are performed on Tianhe-2A supercomputer.","1558-2183","","10.1109/TPDS.2022.3166484","National Key R&D Program of China(grant numbers:2021YFB0300101); National Natural Science Foundation of China(grant numbers:61902411,62032023,12002382,11275269,42104078); 173 Program of China(grant numbers:2020-JCJQ-ZD-029); Open Research Fund; State Key Laboratory of High Performance Computing of China(grant numbers:202101-01); Excellent Youth Foundation of Hunan Province(grant numbers:2021JJ10050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756281","BLACS;data redistribution;irregular data;scalapack;BCDD;exascale computing","Arrays;Program processors;Processor scheduling;Iterative methods;Heuristic algorithms;Eigenvalues and eigenfunctions;Task analysis","data handling;dynamic scheduling;eigenvalues and eigenfunctions;iterative methods;matrix algebra;parallel algorithms","efficient data redistribution algorithms;BLACS routine PXGEMR2D;block cyclic data distribution format;2D irregular format;Tianhe-2A supercomputer;redistributing matrices;iterative methods;BCDD;1D irregular format","",1.0,"",35.0,"IEEE","12 Apr 2022","","","IEEE","IEEE Journals"
"Communicational and Computational Efficient Federated Domain Adaptation","H. Kang; Z. Li; Q. Zhang","Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China; ByteDance Inc., Hangzhou, Zhejiang, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3678,3689,"The emerging paradigm of Federated Learning enables mobile users to collaboratively train a model without disclosing their privacy-sensitive data. Nevertheless, data collected from different mobile users may not be independent and identically distributed. Thus directly applying the trained model to a new mobile user usually leads to performance degradation due to the so-called domain shift. Unsupervised Domain Adaptation is an effective technique to mitigate domain shift and transfer knowledge from labeled source domains to the unlabeled target domain. In this article, we design a Federated Domain Adaptation framework that extends Domain Adaptation with the constraints of Federated Learning to train a model for the target domain and preserve the data privacy of all the source and target domains. As mobile devices usually have limited computation and communication capabilities, we design a set of optimization methods that significantly enhance our framework’s computation and communication efficiency, making it more friendly to resource-constrained edge devices. Evaluation results on three datasets show that our framework has comparable performance with the standard centralized training approach, and the optimization methods can reduce the computation and communication overheads by up to two orders of magnitude.","1558-2183","","10.1109/TPDS.2022.3167457","RGC(grant numbers:CERG 16204418,16203719,16204820,R8015); Natural Science Foundation of Guangdong Province(grant numbers:2017A030312008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757821","Federated learning;domain adaptation;communicational efficient","Feature extraction;Training;Adaptation models;Computational modeling;Data models;Optimization methods;Transfer learning","data privacy;learning (artificial intelligence);mobile computing;optimisation","standard centralized training approach;communication overheads;Federated Learning;mobile user;privacy-sensitive data;different mobile users;trained model;domain shift;Unsupervised Domain Adaptation;unlabeled target domain;Federated Domain Adaptation framework;data privacy;target domains;mobile devices;communication capabilities;communication efficiency","","","",38.0,"IEEE","14 Apr 2022","","","IEEE","IEEE Journals"
"Differentially Private Byzantine-Robust Federated Learning","X. Ma; X. Sun; Y. Wu; Z. Liu; X. Chen; C. Dong","State Key Laboratory of Cryptology, Beijing, China; School of Cyber Science and Engineering, Qufu Normal University, Qufu, China; College of Cyber Science and College of Computer Science, Nankai University, Tianjin, China; College of Cyber Science and College of Computer Science, Nankai University, Tianjin, China; State Key Laboratory of Integrated Service Networks (ISN), Xidian University, Xi’an, China; School of Computing, Newcastle University, Newcastle Upon Tyne, U.K.","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3690,3701,"Federated learning is a collaborative machine learning framework where a global model is trained by different organizations under the privacy restrictions. Promising as it is, privacy and robustness issues emerge when an adversary attempts to infer the private information from the exchanged parameters or compromise the global model. Various protocols have been proposed to counter the security risks, however, it becomes challenging when one wants to make federated learning protocols robust against Byzantine adversaries while preserving the privacy of the individual participant. In this article, we propose a differentially private Byzantine-robust federated learning scheme (DPBFL) with high computation and communication efficiency. The proposed scheme is effective in preventing adversarial attacks launched by the Byzantine participants and achieves differential privacy through a novel aggregation protocol in the shuffle model. The theoretical analysis indicates that the proposed scheme converges to the approximate optimal solution with the learning error dependent on the differential privacy budget and the number of Byzantine participants. Experimental results on MNIST, FashionMNIST and CIFAR10 demonstrate that the proposed scheme is effective and efficient.","1558-2183","","10.1109/TPDS.2022.3167434","National Natural Science Foundation of China(grant numbers:62072132,61960206014,62032012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757841","Federated learning;differential privacy;byzantine-robust","Collaborative work;Privacy;Servers;Differential privacy;Computational modeling;Training;Data models","computer crime;data aggregation;data privacy;learning (artificial intelligence);protocols","Byzantine adversarial attacks;aggregation protocol;shuffle model;collaborative machine learning framework;CIFAR10;FashionMNIST;MNIST;differentially private Byzantine-robust federated learning scheme;DPBFL;federated learning protocols;robustness issues;privacy restrictions;Byzantine participants;differential privacy budget;learning error","",5.0,"",44.0,"IEEE","14 Apr 2022","","","IEEE","IEEE Journals"
"zMesh: Theories and Methods to Exploring Application Characteristics to Improve Lossy Compression Ratio for Adaptive Mesh Refinement","H. Luo; J. Wang; Q. Liu; J. Chen; S. Klasky; N. Podhorszki","Department of Electrical and Computer Engineering, Hunan University, Changsha, Hunan, China; Department of Mathematics and Computer Science, Rutgers University, Newark, NJ, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3702,3717,"Scientific simulations on high-performance computing systems produce vast amounts of data that need to be stored and analyzed efficiently. Lossy compression significantly reduces the data volume by trading accuracy for performance. Despite the recent success of lossy compressions, such as ZFP and SZ, the compression performance is still far from being able to keep up with the exponential growth of data. This article aims to further take advantage of application characteristics, an area that is often under-explored, to improve the compression ratios of adaptive mesh refinement (AMR) - a widely used numerical solver that allows for an improved resolution in limited regions. We propose a level reordering technique zMeshto reduce the storage footprint of AMR applications. In particular, we group the data points that are mapped to the same or adjacent geometric coordinates such that the dataset is smoother and more compressible. Unlike the prior work where the compression performance is affected by the overhead of metadata, this work re-generates the restore recipe using a chained tree structure, thus involving no extra storage overhead for compressed data, which substantially improves the compression ratios. We further derive a mathematical proof that lays the foundation for our method. The results demonstrate that zMesh can improve the smoothness of data by 67.9% and 71.3% for Z-ordering and Hilbert, respectively. Overall, zMesh improves the compression ratios by up to 16.5% and 133.7% for ZFP and SZ, respectively. Despite that zMesh involves additional compute overhead for tree and restore recipe construction, we show that the cost can be amortized as the number of quantities to be compressed increases.","1558-2183","","10.1109/TPDS.2022.3168386","Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101190004); National Natural Science Foundation of China(grant numbers:62102141); National Science Foundation(grant numbers:CCF-1718297,CCF-1812861); Oak Ridge National Laboratory; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760099","High-performance computing;adaptive mesh refinement (AMR);data reduction;lossy compression","Data models;1/f noise;Layout;Solid modeling;Data compression;Computational modeling;Compressors","computational geometry;data compression;mesh generation;meta data;parallel processing;trees (mathematics)","zMesh;exploring application characteristics;lossy compression ratio;adaptive mesh refinement;high performance computing systems;data volume;compression ratios;AMR applications;data points;compressed data;level reordering technique;meta data","",1.0,"",59.0,"IEEE","19 Apr 2022","","","IEEE","IEEE Journals"
"Distributed Evolution Strategies for Black-Box Stochastic Optimization","X. He; Z. Zheng; C. Chen; Y. Zhou; C. Luo; Q. Lin","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Software, Beihang University, Beijing, China; Microsoft Research, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Jul 2022",2022,33.0,12.0,3718,3731,"This work concerns the evolutionary approaches to distributed stochastic black-box optimization, in which each worker can individually solve an approximation of the problem with nature-inspired algorithms. We propose a distributed evolution strategy (DES) algorithm grounded on a proper modification to evolution strategies, a family of classic evolutionary algorithms, as well as a careful combination with existing distributed frameworks. On smooth and nonconvex landscapes, DES has a convergence rate competitive to existing zeroth-order methods, and can exploit the sparsity, if applicable, to match the rate of first-order methods. The DES method uses a Gaussian probability model to guide the search and avoids the numerical issue resulted from finite-difference techniques in existing zeroth-order methods. The DES method is also fully adaptive to the problem landscape, as its convergence is guaranteed with any parameter setting. We further propose two alternative sampling schemes which significantly improve the sampling efficiency while leading to similar performance. Simulation studies on several machine learning problems suggest that the proposed methods show much promise in reducing the convergence time and improving the robustness to parameter settings.","1558-2183","","10.1109/TPDS.2022.3168873","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010165003); National Natural Science Foundation of China(grant numbers:62006252,62176269); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515011840); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762038","Evolution strategies;distributed optimization;black-box optimization;stochastic optimization;zeroth-order methods","Smoothing methods;Stochastic processes;Convergence;Optimization methods;Machine learning;Linear programming;Distributed databases","evolutionary computation;learning (artificial intelligence);probability;stochastic processes;stochastic programming","machine learning;Gaussian probability model;DES method;zeroth-order methods;convergence rate;nonconvex landscapes;evolutionary algorithms;nature-inspired algorithms;distributed stochastic black-box optimization;distributed evolution strategies","",1.0,"",55.0,"IEEE","22 Apr 2022","","","IEEE","IEEE Journals"
"TaiChi: A Hybrid Compression Format for Binary Sparse Matrix-Vector Multiplication on GPU","J. Gao; W. Ji; Z. Tan; Y. Wang; F. Shi","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","13 Jul 2022",2022,33.0,12.0,3732,3745,"Binary Sparse Matrix-Vector Multiplication (SpMV) is a heavy computational kernel in weblink analysis, integer factorization, compressed sensing, spectral graph theory, and other domains. Testing several popular GPU-based SpMV implementations on 400 sparse matrices, we observed that data transfer to GPU memory accounts for a large part of the total computation time. The transfer of constant value “1”s can be easily eliminated for binary sparse matrices. However, compressing index arrays has always been a great challenge. This article proposes a new compression format TaiChi to further reduce index data copies and improve the performance of SpMV, especially for diagonally dominant binary sparse matrices. Input matrices are first partitioned into relatively dense and ultra-sparse areas. Then the dense areas are encoded inversely by marking “0”s, while the ultra-sparse area is encoded by marking “1”s. We also designed a new SpMV algorithm only using addition and subtraction for binary matrices based on our partition and encoding format. Evaluation results on real-world binary sparse matrices show that our hybrid encoding for binary matrix significantly reduces the data transfer and speeds up the kernel execution. It achieves the highest transfer and kernel execution speedups of 5.63x and 3.84x on GTX 1080 Ti, 3.39x and 3.91x on Tesla V100.","1558-2183","","10.1109/TPDS.2022.3170501","National Natural Science Foundation of China(grant numbers:61972033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763312","Binary sparse matrix;SpMV;GPU;parallel computing;mathematical morphology","Sparse matrices;Kernel;Graphics processing units;Indexes;Arrays;Encoding;Partitioning algorithms","data compression;electronic data interchange;encoding;graphics processing units;matrix multiplication;parallel processing;sparse matrices;vectors","index array compression;partition format;dense area encoding;binary sparse matrix-vector multiplication;encoding format;ultra-sparse area;diagonally dominant binary sparse matrices;index data copies;total computation time;GPU memory;popular GPU-based SpMV implementations;heavy computational kernel;hybrid compression format;TaiChi","","","",67.0,"IEEE","26 Apr 2022","","","IEEE","IEEE Journals"
"Online Thread Auto-Tuning for Performance Improvement and Resource Saving","G. Luan; P. Pang; Q. Chen; S. Xue; Z. Song; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Alibaba Cloud, Hangzhou, Zhejiang, China; Alibaba Cloud, Hangzhou, Zhejiang, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","14 Jul 2022",2022,33.0,12.0,3746,3759,"Multi-threading is a common way for programs to benefit from the multi/many-core design. However, the performance of some parallel programs does not increase/even decrease as the number of cores/threads increases. Our study shows that the performance of a parallel program is impacted by the number of cores/threads, the thread placement, the inputs of the program. It is nontrivial to identify the optimal number of cores and the corresponding thread placement to maximize the performance, when the input of a program is determined online and the workload of different iterations may not be identical. To resolve the above problem, we propose Otter, a thread auto-tuning system at runtime for iterative parallel programs. Otter collects the runtime information in the first few iterations and makes decisions on the number of threads and thread placement policy to achieve the goal of improving performance or saving resources. It considers the characteristics of dynamic workload in the iteration process and reduces the time overhead through a migration method. Experiments on a 96-core machine show that Otter improves the performance of the benchmarks by 20.7% and reduces core hours by 51.3% on average compared to the case of running them with all the CPU cores.","1558-2183","","10.1109/TPDS.2022.3169410","National Natural Science Foundation of China(grant numbers:62022057,61832006,61872240); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762963","Iteration;runtime;thread placement;thread scalability;parallel","Optimization;Instruction sets;Scalability;Market research;Benchmark testing;Tuning;Runtime","iterative methods;multiprocessing systems;multi-threading;parallel processing;resource allocation","online thread auto-tuning;performance improvement;multithreading;Otter;iterative parallel programs;thread placement policy;resource saving","","","",44.0,"IEEE","26 Apr 2022","","","IEEE","IEEE Journals"
"Joint Coverage-Reliability for Budgeted Edge Application Deployment in Mobile Edge Computing Environment","L. Zhao; B. Li; W. Tan; G. Cui; Q. He; X. Xu; L. Xu; Y. Yang","School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China; Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; Shanghai Polytechnic University, Shanghai, China; Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; State Key Laboratory Novel Software Technology, Nanjing University, Nanjing, China; Department of Information Technology and Decision Sciences, Old Dominion University, Norfolk, VA, USA; Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3760,3771,"Mobile edge computing (MEC), as an emerging technology, allows application vendors to deploy application instances on edge servers to deliver low-latency services to nearby end-users. However, due to hardware faults, software exceptions, or cyberattacks, edge servers are prone to failures in the highly distributed and dynamic MEC environment. Hence service reliability must be ensured when failures occur. This raises a critical and open problem - improving service reliability when deploying application instances in the MEC environment. In this article, we jointly consider both user coverage and service reliability when deploying application instances on edge servers with a given application deployment budget $\mathcal {K}$K. We formally define this joint Coverage-Reliability for $\mathcal {K}$K-Budgeted Edge Application Deployment (CR-BEAD) problem and model it as a constrained optimization problem. Next, we propose an optimal approach (named BEAD-O) based on integer programming to find optimal solutions to small-scale CR-BEAD problems. We also propose a greedy approach named BEAD-G with a constant approximation ratio of $1 - 1/e$1-1/e to solve large-scale CR-BEAD problems efficiently. Extensive experimental evaluation against three representative approaches illustrates the effectiveness and efficiency of our approaches.","1558-2183","","10.1109/TPDS.2022.3166163","National Natural Science Foundation of China(grant numbers:61672022,U1904186); Key Disciplines of Computer Science and Technology of Shanghai Polytechnic University(grant numbers:XXKZD1604); Australian Research Council Discovery(grant numbers:DP180100212,DP200102491); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763314","Mobile edge computing;service reliability;user coverage;application deployment;approximation approach","Servers;Reliability;Social networking (online);Cloud computing;Software;Computer crashes;Multi-access edge computing","computational complexity;greedy algorithms;integer programming;mobile computing;telecommunication network reliability","budgeted edge application deployment problem;highly distributed MEC environment;nearby end-users;application vendors;mobile edge computing environment;large-scale CR-BEAD problems;greedy approach named BEAD-G;small-scale CR-BEAD problems;constrained optimization problem;joint coverage-reliability;given application deployment budget;edge servers;user coverage;deploying application instances;open problem;critical problem;service reliability;dynamic MEC environment","",3.0,"",39.0,"IEEE","26 Apr 2022","","","IEEE","IEEE Journals"
"Deadline and Reliability Aware Multiserver Configuration Optimization for Maximizing Profit","T. Wang; J. Zhou; L. Li; G. Zhang; K. Li; X. S. Hu","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; National Trusted Embedded Software Engineering Technology Research Center, East China Normal University, Shanghai, China; Department of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Department of Computer Science, State University of New York, New York, NY, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3772,3786,"Maximizing profit is a key goal for cloud service providers in the modern cloud business market. Service revenue and business cost are two major factors in determining profit and highly depend on multiserver configuration. Understanding the relationship between multiserver configuration and profit is important to service providers. Although existing articles have explored this issue, few of them consider deadline miss rate and soft error reliability of cloud services in multiserver configuration for profit maximization. Since deadline misses violate cloud services’ real-time requirements and soft error prevents successful processing of cloud services, it is necessary to consider the impact of deadline miss rate and soft error reliability on service providers’ profits when configuring the multiserver. This article introduces a deadline miss rate and soft error reliability aware multiserver configuration scheme for maximizing cloud service providers’ profit. Specifically, we derive the deadline miss rate considering the heterogeneity of cloud service requests, and propose an analytical method to compute the soft error reliability of multiserver systems. Based on the new deadline miss rate and soft error reliability models, we formulate the multiserver configuration optimization problem and introduce an augmented Lagrange multiplier-based iterative method to find the optimal multiserver configuration. Extensive experiments evaluate the efficacy of the proposed multiserver configuration approach. Compared with the two state-of-the-art methods, the profit gained by our scheme can be up to 11.92% higher.","1558-2183","","10.1109/TPDS.2022.3170305","National Natural Science Foundation of China(grant numbers:62172224,61802185); China Postdoctoral Science Foundation(grant numbers:BX2021128,2021T140327,2020M680068); Postdoctoral Science Foundation of Jiangsu Province(grant numbers:2021K066A); Open Research Fund of the State Key Laboratory of Computer Architecture; Institute of Computing Technology, Chinese Academy of Sciences(grant numbers:CARCHA202105); Future Network Scientific Research Fund(grant numbers:FNSRFP-2021-YB-6); Open Research Fund of National Trusted Embedded Software Engineering Technology Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763428","Cloud service;deadline miss rate;multiserver configuration;profit maximization;soft error reliability","Cloud computing;Servers;Reliability;Business;Computer architecture;Real-time systems;Processor scheduling","cloud computing;iterative methods;optimisation;profitability","deadline misses;cloud services;deadline miss rate;soft error reliability aware multiserver configuration scheme;cloud service providers;cloud service requests;multiserver systems;soft error reliability models;multiserver configuration optimization problem;optimal multiserver configuration;multiserver configuration approach;reliability aware multiserver configuration optimization;maximizing profit;modern cloud business market;business cost;profit maximization","",1.0,"",47.0,"IEEE","26 Apr 2022","","","IEEE","IEEE Journals"
"State Space Model and Queuing Network Based Cloud Resource Provisioning for Meshed Web Systems","Y. Lei; Z. Cai; X. Li; R. Buyya","Southeast University, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, The University of Melbourne, Melbourne, Australia","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3787,3799,"Functions provided by Web applications are increasingly diverse which make their structures complicated and meshed. Cloud computing platforms provide elastic computing capacities for these meshed Web systems to guarantee Service Level Agreement (SLA). Though workloads of meshed Web systems usually change steadily and periodically in total, sometimes there are sudden fluctuations. In this paper, a hybrid State-space-model-and-Queuing-network based Feedback control method (SQF) is developed for auto-scaling Virtual Machines (VMs) allocated to each tier of meshed Web systems. For the case with workloads changing steadily, a State-space-model based static Feedback Control method (SFC) is proposed in SQF to stabilize request response times near the reference time. For unsteadily changing workloads, a Queuing-network based multi-tier collaborative Feedback Control method (QFC) is proposed for effectively eliminating bottlenecks. QFC builds a control system for each tier individually and uses the queuing network to measure the interaction relationships among different tiers. Experimental results show that QFC is able to improve the efficiency of eliminating bottlenecks (decreasing upper-limit SLA violation ratios by 31.99%$\sim$∼56.52%) with similar or a little bit high VM rental costs compared to existing methods while SFC obtains more stable response times for requests with reasonable additional costs.","1558-2183","","10.1109/TPDS.2022.3170834","National Key Research and Development Program of China(grant numbers:2018YFB1402500); National Natural Science Foundation of China(grant numbers:61972202,61872186,61872077,61832004); Fundamental Research Funds for the Central Universities(grant numbers:30919011235); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9764612","Bottleneck eliminating;cloud computing;feedback control;resource provisioning;state-space model","Queueing analysis;Feedback control;Control theory;Time factors;Cloud computing;Service level agreements;Predictive models","cloud computing;queueing theory;resource allocation;state feedback;virtual machines","control system;queuing network;meshed Web systems;Web applications;state-space-model based static feedback control method;multitier collaborative feedback control method;service level agreement;SLA;cloud computing platforms;hybrid state-space-model-and-queuing-network based feedback control method;SQF;virtual machines;VM rental costs;SFC","","","",46.0,"IEEE","27 Apr 2022","","","IEEE","IEEE Journals"
"Design and Implementation of 2D Convolution on x86/x64 Processors","V. Kelefouras; G. Keramidas","School of Engineering, Computing and Mathematics, University of Plymouth, Plymouth, U.K.; School of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3800,3815,"In this paper, a new method for accelerating the 2D direct Convolution operation on x86/x64 processors is presented. It includes efficient vectorization by using SIMD intrinsics, bit-twiddling optimizations, the optimization of the division operation, multi-threading using OpenMP, register blocking and the shortest possible bit-width value of the intermediate results. The proposed method, which is provided as open-source, is general and can be applied to other processor families too, e.g., Arm. The proposed method has been evaluated on two different multi-core Intel CPUs, by using twenty different image sizes, 8-bit integer computations and the most commonly used kernel sizes (3x3, 5x5, 7x7, 9x9). It achieves from $2.8\times$2.8× to $40\times$40× speedup over the Intel IPP library (OpenCV GaussianBlur and Filter2D routines), from $105 \times$105× to $400 \times$400× speedup over the gemm-based convolution method (by using Intel MKL int8 matrix multiplication routine), and from $8.5\times$8.5× to $618\times$618× speedup over the vslsConvExec Intel MKL direct convolution routine. The proposed method is superior as it achieves far fewer arithmetical and load/store instructions.","1558-2183","","10.1109/TPDS.2022.3171471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765671","Convolution;gaussian blur;code optimization;vectorization;AVX;OpenMP;OpenCV;Intel MKL;Intel IPP;high performance computing (HPC);image processing","Convolution;Kernel;Libraries;Optimization;Codes;Registers;Quantization (signal)","computational complexity;convolution;digital arithmetic;fast Fourier transforms;image processing;matrix multiplication;microprocessor chips;multiprocessing systems;multi-threading;optimisation;parallel algorithms;parallel architectures;parallel processing;performance evaluation","load-store instructions;arithmetical instructions;OpenMP;bit-width value;2D direct convolution operation;SIMD intrinsics;efficient vectorization;vslsConvExec Intel MKL direct convolution routine;int8 matrix multiplication routine;gemm-based convolution method;Filter2D routines;Intel IPP library;commonly used kernel;8-bit integer computations;image sizes;multicore Intel CPU;processor families;open-source;register blocking;multithreading;division operation;bit-twiddling optimizations","",1.0,"",34.0,"CCBY","29 Apr 2022","","","IEEE","IEEE Journals"
"QoS-Aware Scheduling of Remote Rendering for Interactive Multimedia Applications in Edge Computing","R. Xie; J. Fang; J. Yao; K. Liu; X. Jia; K. Wu","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science, Chongqing University, Chongqing, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China; Peng Cheng Laboratory, PCL Research Center of Networks and Communications, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3816,3832,"Leveraging emerging edge computing and 5G networks, researchers proposed to offload the 3D rendering of interactive multimedia applications (e.g., virtual reality and cloud gaming) onto edge servers. For high resource utilization, multiple rendering tasks run in the same GPU server and compete against each other for the computation resource. Each task has its requirement for performance, i.e., QoS target. A significant problem is how to schedule tasks so that each preset QoS is met and the performance of all tasks are maximized. We make the following contributions. First, we formulate the problem into a QoS constrained max-min utility problem. Second, we find that using the common natural logarithm as a utility function overly promotes one performance but demotes another. To avoid this phenomenon, we design a special utility function. Third, we propose an efficient scheduling algorithm, consisting of a resolution adjustment algorithm and a frame rate fair scheduling algorithm, both of which interact with each other. The former selects resolutions for tasks and the latter decides which task to process. We evaluate our method with actual rendering data, and the simulations demonstrate that our method can effectively improve task performance as well as satisfy QoS simultaneously.","1558-2183","","10.1109/TPDS.2022.3172121","China NSFC(grant numbers:61802263,62072317,62172064); Research Grants Council of Hong Kong(grant numbers:CityU 11202419); Faculty Research Fund of Shenzhen University(grant numbers:860/000002110325); China NSFC(grant numbers:U2001207,61872248); Guangdong NSF(grant numbers:2017A030312008); Shenzhen Science and Technology Foundation(grant numbers:ZDSYS20190902092853047,R2020A045); Department of Education of Guangdong Province(grant numbers:2019KCXTD005,2021ZDZX1068); Guangdong(grant numbers:2019ZT08X603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9767696","Edge computing;task scheduling;remote rendering","Rendering (computer graphics);Task analysis;Servers;Quality of service;Streaming media;Graphics processing units;Three-dimensional displays","quality of service;rendering (computer graphics);scheduling;virtual reality","remote rendering;interactive multimedia applications;edge computing;virtual reality;cloud gaming;edge servers;high resource utilization;multiple rendering tasks;GPU server;computation resource;QoS target;preset QoS;QoS constrained max-min utility problem;common natural logarithm;special utility function;efficient scheduling algorithm;resolution adjustment algorithm;frame rate fair scheduling algorithm;actual rendering data;task performance;satisfy QoS;QoS-aware scheduling","","","",37.0,"IEEE","3 May 2022","","","IEEE","IEEE Journals"
"Astrea: Auto-Serverless Analytics Towards Cost-Efficiency and QoS-Awareness","J. Jarachanthan; L. Chen; F. Xu; B. Li","School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Computer Science and Technology, East China Normal University, Shanghai, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3833,3849,"With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of Astrea, which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. Astrea relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain the optimal job execution. We deploy Astrea in the AWS Lambda platform and conduct real-world experiments over representative benchmarks, including Big Data analytics and machine learning workloads, at different scales. Extensive results demonstrate that Astrea can achieve the optimal execution decision for serverless data analytics, in comparison with various provisioning and deployment baselines. For example, when compared with three provisioning baselines, Astrea manages to reduce the job completion time by 21% to 69% under a given budget constraint, while saving cost by 20% to 84% without violating performance requirements.","1558-2183","","10.1109/TPDS.2022.3172069","National Science Foundation(grant numbers:OIA-2019511); Louisiana Board of Regents(grant numbers:LEQSF(2019-22)-RD-A-21); National Natural Science Foundation of China(grant numbers:61972158); Science and Technology Commission of Shanghai Municipality(grant numbers:20511102802,18DZ2270800); RGC RIF(grant numbers:R6021-20); RGC GRF(grant numbers:16209120,16200221); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9767624","Cloud computing;serverless computing;resource provisioning;modeling;optimization","Costs;Serverless computing;Data analysis;Quality of service;Optimization;Machine learning;Parallel processing","Big Data;cloud computing;cost reduction;data analysis;graph theory;learning (artificial intelligence);optimisation;quality of service;resource allocation","code deployment;QoS-awareness;cost-efficiency;auto-serverless analytics;performance requirements;deployment baselines;serverless data analytics;optimal execution decision;machine learning workloads;conduct real-world experiments;optimal job execution;user-specific requirements;account flexibly-specified user requirements;serverless analytics jobs;Astrea;configuration space;provisioning resources;serverless context;adapting data-intensive analytics applications;serverless computing;lightweight execution;one-click upload","","","",54.0,"IEEE","3 May 2022","","","IEEE","IEEE Journals"
"The State of the Art of Metadata Managements in Large-Scale Distributed File Systems — Scalability, Performance and Availability","H. Dai; Y. Wang; K. B. Kent; L. Zeng; C. Xu","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Faculty of Computer Science, University of New Brunswick, Fredericton, NB, Canada; Zhejiang Lab, ZJ Lab-Enflame Joint Innovation Research Center, Hangzhou, Zhejiang, China; Faculty of Science and Technology, University of Macau, Macau, China","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3850,3869,"File system metadata is the data in charge of maintaining namespace, permission semantics and location of file data blocks. Operations on the metadata can account for up to 80% of total file system operations. As such, the performance of metadata services significantly impacts the overall performance of file systems. A large-scale distributed file system (DFS) is a storage system that is composed of multiple storage devices spreading across different sites to accommodate data files, and in most cases, to provide users with location independent access interfaces. Large-scale DFSs have been widely deployed as a substrate to a plethora of computing systems, and thus their metadata management efficiency is crucial to a massive number of applications, especially with the advent of the Big Data age, which poses tremendous pressure on underlying storage systems. This paper reports the state-of-the-art research on metadata services in large-scale distributed file systems, which is conducted from three indicative perspectives that are always used to characterize DFSs: high-scalability, high-performance, and high-availability, with special focus on their respective major challenges as well as their developed mainstream technologies. Additionally, the paper also identifies and analyzes several existing problems in the research, which could be used as a reference for related studies.","1558-2183","","10.1109/TPDS.2022.3170574","Third Xinjiang Scientific Expedition Program(grant numbers:2021xjkk1300); National Natural Science Foundation of China(grant numbers:61672513); Zhejiang provincial(grant numbers:2021R52007); Center-initiated Research Project of Zhejiang Lab(grant numbers:2021DA0AM01); Natural Sciences and Engineering Research Council of Canada; Lockheed-Martin Cybersecurity Fund(grant numbers:LMCRF2020-02); Mitacs(grant numbers:IT24602); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9768784","High-availability;high-performance;high-scalability;large-scale distributed file system;metadata management","Metadata;Servers;Distributed databases;Scalability;Computer architecture;Performance evaluation;Memory","Big Data;distributed databases;file organisation;Internet;meta data;network operating systems;storage management","storage system;data files;computing systems;metadata management efficiency;metadata services;large-scale distributed file system;metadata managements;file system metadata;file data blocks;total file system operations;file systems;DFS;big data age","",2.0,"",156.0,"IEEE","4 May 2022","","","IEEE","IEEE Journals"
"Efficient Renaming in Sequence CRDTs","M. Nicolas; G. Oster; O. Perrin","CNRS, Inria, LORIA, Université de Lorraine, Nancy, France; CNRS, Inria, LORIA, Université de Lorraine, Nancy, France; CNRS, Inria, LORIA, Université de Lorraine, Nancy, France","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3870,3885,"To achieve high availability, large-scale distributed systems have to replicate data and to minimise coordination between nodes. For these purposes, literature and industry increasingly adopt Conflict-free Replicated Data Types (CRDTs) to design such systems. Conflict-free Replicated Data Types (CRDTs) are new specifications of existing data types, e.g., Set or Sequence. While CRDTs have the same behaviour as previous specifications in sequential executions, they actually shine in distributed settings as they natively support concurrent updates. To this end, CRDTs embed in their specification conflict resolution mechanisms. These mechanisms usually rely on identifiers attached to elements of the data structure to resolve conflicts in a deterministic and coordination-free manner. Identifiers have to comply with several constraints, such as being unique or belonging to a dense total order. These constraints may hinder the identifier size from being bounded. Identifiers hence tend to grow as the system progresses, which increases the overhead of CRDTs over time and leads to performance issues. To address this issue, we propose a novel Sequence CRDT which embeds a renaming mechanism. It enables nodes to reassign shorter identifiers to elements in an uncoordinated manner. Experimental results demonstrate that this mechanism decreases the overhead of the replicated data structure and eventually minimises it.","1558-2183","","10.1109/TPDS.2022.3172570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770022","CRDTs;replication;real-time collaborative editing;eventual consistency;memory-wise optimisation;performance","Peer-to-peer computing;Data structures;Metadata;Collaboration;Semantics;Real-time systems;Distributed databases","data structures;distributed processing;replicated databases","renaming mechanism;conflict-free replicated data types;sequence CRDT;large-scale distributed systems;replicated data structure;deterministic coordination-free manner;specification conflict resolution mechanisms;distributed settings","","","",41.0,"IEEE","5 May 2022","","","IEEE","IEEE Journals"
"A Resource-Efficient Predictive Resource Provisioning System in Cloud Systems","H. Shen; L. Chen","Department of Computer Science, University of Virginia, Charlottesville, VA, USA; VMware World Headquarters, Palo Alto, CA, USA","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3886,3900,"In cloud systems, demand-prediction based resource provisioning schemes help assure the SLOs (service level objectives) of cloud tenants. We notice that if a provisioning scheme does not exclude bursts from historical resource demands in normal demand prediction or always uses a large padding to correct under-prediction, it will lead to resource over-provisioning and low resource utilization. To improve the previous schemes, in this paper, we present a Resource-efficient Predictive Resource Provisioning system in cloud systems (RPRP) that excludes bursts in demand prediction and has algorithms to specifically handle bursts to avoid resource over-provisioning. Rather than setting padding to a possibly high value, RPRP has a load-dependent padding algorithm that adaptively determines padding based on predicted demands. To handle bursts, RPRP has a burst-resilient shared padding algorithm that reserves resource shared by multiple co-located VMs rather than for individual VMs. It also embodies a responsive padding algorithm that adaptively adjusts padding to recover from both under-provisioning and over-provisioning. We implemented RPRP on top of Xen and conducted both trace-driven simulation and real-world testbed experiments. The experimental results show that RPRP achieves higher resource utilization, more accurate demand predictions, and fewer SLO violations than previous schemes.","1558-2183","","10.1109/TPDS.2022.3172493","National Science Foundation(grant numbers:ACI-1719397,CNS-1733596,NSF-1827674,CCF-1822965); Microsoft Research Faculty Fellowship(grant numbers:8300751); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9769930","Cloud;resource management;resource provisioning","Resource management;Prediction algorithms;Time series analysis;Cloud computing;Heuristic algorithms;System analysis and design;Monitoring","cloud computing;computer centres;resource allocation;virtual machines","resource-efficient predictive resource provisioning system;accurate demand predictions;higher resource utilization;burst-resilient shared padding algorithm;predicted demands;RPRP;low resource utilization;resource over-provisioning;normal demand prediction;historical resource demands;provisioning scheme;demand-prediction based resource provisioning schemes;cloud systems","",1.0,"",57.0,"IEEE","5 May 2022","","","IEEE","IEEE Journals"
"An In-Depth Study of Microservice Call Graph and Runtime Performance","S. Luo; H. Xu; C. Lu; K. Ye; G. Xu; L. Zhang; J. He; C. Xu","Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Guangzhou, Guangdong, China; Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Guangzhou, Guangdong, China; Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Guangzhou, Guangdong, China; Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Guangzhou, Guangdong, China; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China; Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Guangzhou, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","26 Jul 2022",2022,33.0,12.0,3901,3914,"Loosely-coupled and light-weight microservices running in containers are replacing monolithic applications gradually. Understanding the characteristics of microservices is critical to make good use of microservice architectures. However, there is no comprehensive study about microservice and its related systems in production environments so far. In this paper, we present a solid analysis of large-scale deployments of microservices at Alibaba clusters. Our study focuses on the characterization of microservice dependency as well as its runtime performance. We conduct an in-depth anatomy of microservice call graphs to quantify the difference between them and traditional DAGs of data-parallel jobs. In particular, we observe that microservice call graphs are heavy-tail distributed and their topology is similar to a tree and moreover, many microservices are hot-spots. We also discover that the structure of call graphs for long-term developed applications is much simpler so as to provide better performance. Our investigation on microservice runtime performance indicates most microservices are much more sensitive to CPU interference than memory interference. Moreover, we design resource management policies to efficiently tune memory resources.","1558-2183","","10.1109/TPDS.2022.3174631","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164003); National Natural Science Foundation of China(grant numbers:62072451); Start-up Research Grant of University of Macau(grant numbers:SRG2021-00004-FST); Alibaba Innovative Research Program; Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2019349); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774016","Trace analysis;microservice;performance characterization","Microservice architectures;Runtime;Containers;Interference;Topology;Production;Memory management","mobile computing;query processing;resource allocation;software architecture;software performance evaluation","memory interference;CPU interference;data-parallel jobs;DAG;Alibaba clusters;production environments;microservice call graph;microservice runtime performance;microservice dependency;microservice architectures;light-weight microservices","",4.0,"",53.0,"IEEE","12 May 2022","","","IEEE","IEEE Journals"
"SMURF: Efficient and Scalable Metadata Access for Distributed Applications","B. Zhang; T. Kosar","National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL, USA; Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY, USA","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,3915,3928,"In parallel with big data processing and analysis dominating the usage of distributed and Cloud infrastructures, the demand for distributed metadata access and transfer has increased. The volume of data generated by many application domains exceeds petabytes, while the corresponding metadata amounts to terabytes or even more. This article proposes a novel solution for efficient and scalable metadata access for distributed applications across wide-area networks, dubbed SMURF. Our solution combines novel pipelining and concurrent transfer mechanisms with reliability, provides distributed continuum caching and semantic locality-aware prefetching strategies to sidestep fetching latency, and achieves scalable and high-performance metadata fetch/prefetch services in the Cloud. We incorporate the phenomenon of semantic locality awareness for increased prefetch prediction rate using real-life application I/O traces from Yahoo! Hadoop audit logs and propose a novel prefetch predictor. By effectively caching and prefetching metadata based on the access patterns, our continuum caching and prefetching mechanism significantly improves the local cache hit rate and reduces the average fetching latency. We replay approximately 20 Million metadata access operations from real audit traces, where SMURF achieves 90% accuracy during prefetch prediction and reduced the average fetch latency by 50% compared to the state-of-the-art mechanisms.","1558-2183","","10.1109/TPDS.2022.3175596","National Science Foundation(grant numbers:OAC-1724898,CCF-2007829); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779105","Heterogeneity;scalability;metadata access;prefetch prediction;continuum caching;semantic locality","Metadata;Prefetching;Servers;Internet of Things;Protocols;Wide area networks;Semantics","Big Data;cache storage;cloud computing;data handling;meta data;storage management","scalable metadata access;distributed applications;big data processing;distributed metadata access;application domains;corresponding metadata;wide-area networks;dubbed SMURF;concurrent transfer mechanisms;continuum caching;semantic locality-aware prefetching strategies;semantic locality awareness;increased prefetch prediction rate;novel prefetch predictor;effectively caching;prefetching metadata;access patterns;prefetching mechanism;local cache hit rate;approximately 20 Million metadata access operations","","","",52.0,"IEEE","19 May 2022","","","IEEE","IEEE Journals"
"PhaST: Hierarchical Concurrent Log-Free Skip List for Persistent Memory","Z. Li; B. Jiao; S. He; W. Yu","Zhejiang Laboratory, Hangzhou, China; Department of Computer Science, Florida State University, Tallahassee, FL, USA; Zhejiang Laboratory, Hangzhou, China; Department of Computer Science, Florida State University, Tallahassee, FL, USA","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,3929,3941,"Skip list (skiplist) is a competitive index structure that offers superior concurrency and excellent performance but with high memory overhead and low access locality. Emerging persistent memory (PM) technologies present an opportunity to mitigate the capacity constraint of DRAM. However, data consistency on PM typically results in excessive write overhead. In addition, fast concurrent access to an index is critical to the throughput on high-end contemporary computer systems. In this article, we propose a Partitioned HierArchical SkiplisT called PhaST, which can simultaneously reduce the skiplist height and improve its access locality, through its hierarchy of component structures, while enabling fast parallel recovery in case of failure. To ensure high concurrency and fast data consistency, we also have developed writelock-free concurrent insert and log-free atomic split. Furthermore, we have developed a durable lock-free concurrent search that can discern transient structural inconsistencies and deliver highly concurrent read operations. We have conducted an extensive evaluation of PhaST compared to state-of-the-art studies such as NV-Skiplist, wB+-Tree, FPTree, and FAST-FAIR. Our evaluation results show PhaST outperforms other indexing structures by up to 4.05× and 2.87× in single-threaded inserts and searches, and 1.56× and 2.62× in concurrent inserts and searches.","1558-2183","","10.1109/TPDS.2022.3173707","National Key Research and Development Program of China(grant numbers:2021ZD0110700); National Natural Science Foundation of China(grant numbers:62172361); Zhejiang Lab Research Project(grant numbers:2020KC0AC01); Alibaba Innovative Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9772399","Skiplist;persistent memory;concurrency","Indexing;Random access memory;Concurrent computing;Vegetation;Performance evaluation;Concurrency control;Bandwidth","concurrency control;data handling;data mining;data structures;parallel processing;storage management;tree data structures","FAST-FAIR;PhaST;indexing structures;concurrent inserts;hierarchical concurrent log-free skip list;competitive index structure;superior concurrency;high memory;low access locality;persistent memory technologies;fast concurrent access;high-end contemporary computer systems;Partitioned HierArchical SkiplisT;skiplist height;component structures;fast parallel recovery;high concurrency;fast data consistency;writelock-free concurrent insert;log-free atomic split;durable lock-free;transient structural inconsistencies;highly concurrent read operations;NV-Skiplist","",1.0,"",53.0,"IEEE","10 May 2022","","","IEEE","IEEE Journals"
"Busy-Time Scheduling on Heterogeneous Machines: Algorithms and Analysis","M. Liu; X. Tang","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,3942,3958,"We study a generalized busy-time scheduling model on heterogeneous machines. The input to the model includes a set of jobs and a set of machine types. Each job has a size and a time interval during which it should be processed. Each job is to be placed on a machine for execution. Different types of machines have distinct capacities and cost rates. The total size of the jobs running on a machine must always be kept within the machine's capacity, giving rise to placement restrictions for jobs of various sizes among the machine types. Each machine used is charged according to the time duration in which it is busy, i.e., it is processing jobs. The objective is to schedule the jobs into machines to minimize the total cost of all the machines used. We develop an $O(1)$O(1)-approximation algorithm in the offline setting and an $O(\mu)$O(μ)-competitive algorithm in the online setting (where $\mu$μ is the max/min job length ratio), both of which are asymptotically optimal. This article significantly improves the analysis of the algorithms over our preliminary work.","1558-2183","","10.1109/TPDS.2022.3176665","Ministry of Education - Singapore(grant numbers:MOE-T2EP20121-0005); Academic Research Fund(grant numbers:RG112/19 (S)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779914","Busy time;scheduling;analysis of algorithms;approximation ratio;competitive ratio","Costs;Approximation algorithms;Scheduling;Cloud computing;Servers;Schedules;Optimal scheduling","approximation theory;computational complexity;scheduling","heterogeneous machines;busy-time scheduling model;machine types;time interval;distinct capacities;cost rates;generalized busy-time scheduling model;approximation algorithm;competitive algorithm","",1.0,"",25.0,"IEEE","23 May 2022","","","IEEE","IEEE Journals"
"Run-Time Remapping Algorithm of Dataflow Actors on NoC-Based Heterogeneous MPSoCs","M. Rizk; K. J. M. Martin; J. -P. Diguet","Physics and Electronics Department, Lebanese University, Hadath, Lebanon; Lab-STICC UMR CNRS 6285, Université de Bretagne-Sud (UBS), Lorient, France; CROSSING, IRL CNRS 2010, CNRS, Adelaide, Australia","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,3959,3976,"Multiprocessor system-on-chip (MPSoC) platforms have been emerging as the main solution to cope with processor frequency ceiling and power density issues while still improving performances. Then, network-on-chip (NoC) has been adopted to provide the increasing number of processors with the required communication bandwidth as well as with the necessary flexibility. Video processing and streaming applications are adopting dynamic dataflow model of computation as the need for high performance parallel computing is growing. Dataflow applications executed on modern MPSoC-based architectures are becoming increasingly dynamic and more data-dependent. Different tasks execute concurrently with significant modifications in their workloads and resource demanding over time depending on the input data. Hence, adopting any static or offline dynamic scheduling for mapping tasks will not cope with the computation variations. This article introduces an original run-time mapping algorithm based on the Move Based (MB) method targeting a dedicated heterogeneous NoC-based MPSoC architecture to achieve workload balancing and optimized communication traffic. The performance of the proposed algorithm is verified by conducting cycle-accurate SystemC simulations of the adopted NoC implementing a real MPEG4-SP decoder. The obtained results reveal the effectiveness of our proposed algorithm. For various real-life videos, the proposed algorithm systematically succeeded to enhance significantly the performance.","1558-2183","","10.1109/TPDS.2022.3177957","Région Bretagne; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782520","NoC;heterogeneous MPSoC;run-time remapping;dataflow actor;move-based algorithm","Computer architecture;Task analysis;Program processors;Computational modeling;Behavioral sciences;Streaming media;Monitoring","microprocessor chips;multiprocessing systems;network-on-chip;system-on-chip","run-time remapping algorithm;dataflow actors;system-on-chip platforms;processor frequency ceiling;power density issues;network-on-chip;video processing;streaming applications;high performance parallel computing;dataflow applications;static scheduling;offline dynamic scheduling;mapping tasks;computation variations;run-time mapping algorithm;workload balancing;optimized communication traffic;heterogeneous NoC-based MPSoC architecture;MPSoC-based architectures;NoC-based heterogeneous MPSoC","","","",36.0,"IEEE","26 May 2022","","","IEEE","IEEE Journals"
"VCSR: An Efficient GPU Memory-Aware Sparse Format","E. Karimi; N. B. Agostini; S. Dong; D. Kaeli","ECE Department, Northeastern University, Boston, MA, USA; ECE Department, Northeastern University, Boston, MA, USA; ECE Department, Northeastern University, Boston, MA, USA; ECE Department, Northeastern University, Boston, MA, USA","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,3977,3989,"The Sparse Matrix-Vector Multiplication (SpMV) kernel is used in a broad class of linear algebra computations. SpMV computations result in a performance bottleneck in many high performance applications, so optimizing SpMV performance is paramount. While implementing this kernel on a GPU can potentially boost performance significantly, current GPU libraries either provide modest performance gains or are burdened with high sparse format conversion overhead. In this paper we introduce the Vertical Compressed Sparse Row (VCSR) format, a novel memory-aware format that out-performs previous proposed formats on a GPU. We first motivate the design of our baseline VCSR format and then step through a series of enhancements that further improve VCSR's memory efficiency (VCSR-MEM) and performance (VCSR-INTRLV), while also considering conversion overhead. VCSR attempts to produce a high degree of thread-level parallelism and memory utilization by exploiting knowledge of GPU memory microarchitecture. VCSR can reduce the number of global memory transactions significantly, an issue not addressed by most other sparse formats. In addition, VCSR provides a novel reordering mechanism. It minimizes the size of the compressed matrix, handles both regular/irregular sparse matrices, and can be customized based on matrix size. VCSR also minimizes conversion overhead, as compared to full or partial row reordering. Our methodology is highly configurable and can be optimized for any sparse matrix. We have evaluated the VCSR format for the SpMV kernel when run on two different NVIDIA GPUs, the Kepler K40 and the Volta V100. We compare VCSR with NVIDIA's cuSPARSE library (the HYB format), a state-of-the-art sparse library. We also compare against other state-of-the-art CSR-based formats, including CSR5, merge-base SpMV and HOLA. We evaluate the benefits of VCSR over the entire University of Florida's SuiteSparse dataset collection. The VCSR-baseline format achieves an average speedup ranging from $1.10\times$1.10× to $1.39\times$1.39× when compared to the performance of the four state-of-the-art formats on an NVIDIA V100. While the VCSR-MEM format can save a significant amount of memory space, it is a bit slower than our VCSR-baseline. VCSR-INTRLV performs much better than the VCSR-baseline, and even when including the conversion overhead, achieves an average speedup of $1.08\times$1.08× as compared to HOLA (the best performing format among the prior schemes).","1558-2183","","10.1109/TPDS.2022.3177291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787804","GPU;memory patterns;SpMV;sparse matrices","Graphics processing units;Sparse matrices;Kernel;Instruction sets;Libraries;Parallel processing;Indexes","graphics processing units;matrix multiplication;optimisation;parallel architectures;sparse matrices;vectors","sparse matrix-vector multiplication kernel;high sparse format conversion overhead;thread-level parallelism;GPU memory microarchitecture;HYB format;VCSR-MEM format;VCSR-INTRLV;GPU memory-aware sparse format;memory-aware format;NVIDIAs cuSPARSE library;vertical compressed sparse row format;optimization SpMV;linear algebra computations","","","",40.0,"IEEE","3 Jun 2022","","","IEEE","IEEE Journals"
"Optimized Page Fault Handling During RDMA","A. Psistakis; N. Chrysos; F. Chaix; M. Asiminakis; M. Gianioudis; P. Xirouchakis; V. Papaefstathiou; M. Katevenis","Department of Computer Science, University of Illinois at Urbana-Champaign (UIUC), Urbana, IL, USA; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,3990,4005,"Remote Direct Memory Access (RDMA) is widely used in High-Performance Computing (HPC) while making inroads in datacenters and accelerators. State-of-the-art RDMA engines typically do not endure page faults, therefore users are forced to pin their buffers, which complicates the programming model, limits the memory utilization, and moves the pressure to the Network Interface Cards (NICs). In this article we introduce a mechanism for handling dynamic page faults during RDMA, named PART, suitable for emerging processors that also integrate the Network Interface. PART leverages the IOMMU already present in modern processors for translations. PART avoids the pinning overheads, allows any buffer to be used for communication, and enables overlapping page fault handling with serving subsequent RDMA transfers. We implement and optimize PART for a cluster of ARMv8 cores with tightly-coupled network interfaces. Handling a minor page-fault of a small transfer at the destination takes approximately 38 $\mu$μsecs, while there is no performance degradation when running three full MPI applications in 16 nodes and 64 cores. Detailed breakdown uncovers the hardware and system software components of this overhead and was used to further optimize the system. A 4MB RDMA transfer performs 1.46x better over pinning.","1558-2183","","10.1109/TPDS.2022.3175666","European Commission(grant numbers:671553,754337); European High-Performance Computing Joint Undertaking(grant numbers:955776); European Union's Horizon 2020 Research and Innovation Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779430","Page fault;RDMA;IOMMU;MPI;low-power ARM processors;pinning avoidance","Program processors;Costs;Network interfaces;Memory management;Pins;Programming;Optimization","application program interfaces;cache storage;message passing;microprocessor chips;multiprocessing systems;network interfaces;parallel processing","optimized page fault handling;remote direct memory access;high-performance computing;memory utilization;network interface cards;dynamic page faults;PART;tightly-coupled network interfaces;RDMA engines;NICs;ARMv8 cores;IOMMU;MPI","","","",45.0,"CCBY","20 May 2022","","","IEEE","IEEE Journals"
"Adaptive Vertical Federated Learning on Unbalanced Features","J. Zhang; S. Guo; Z. Qu; D. Zeng; H. Wang; Q. Liu; A. Y. Zomaya","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Key Laboratory of Water Resources Big Data Technology of Ministry of Water Resources, School of Computer and Information, Hohai University, Nanjing, China; School of Computer Science and Technology, China University of Geosciences, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, Hong Kong Baptist University, Hong Kong; High Performance Computing & Networking, School of Information Technologies, Sydney University, Camperdown, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4006,4018,"Most of the existing FL systems focus on a data-parallel architecture where training data are partitioned by samples among several parties. In some real-life applications, however, partitioning by features is also of practical relevance and the number of features is usually unbalanced among parties. The corresponding learning framework is referred to as Vertical Federated Learning (VFL). Though some pioneering work focused on VFL, the convergence properties of VFL on unbalanced features, especially when parties conduct different numbers of local updates concerning heterogeneous computational capabilities are still unknown. In this article, we propose a new learning framework to improve the training efficiency of VFL on unbalanced features. Given the number of features and the computational capability owned by each party, our thorough theoretical analysis exhibits that the number of local updates conducted by each party has a great effect on the convergence rate and the computational complexity, both of which jointly determine the overall training efficiency in an interrelated and sophisticated way. Based on our theoretical findings, we formulate an optimization problem and derive the optimal solution by selecting an adaptive number of local training rounds for each party. Extensive experiments on various datasets and models demonstrate that our approach significantly improves the training efficiency of VFL.","1558-2183","","10.1109/TPDS.2022.3178443","Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101400003); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,152203/20E,152244/21E); National Natural Science Foundation of China(grant numbers:61872310,62102131); Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ20200109142008673); Natural Science Foundation of Jiangsu Province(grant numbers:BK20210361); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9783035","Vertical federated learning;unbalanced feature distribution;convergence analysis","Training;Convergence;Collaborative work;Computational modeling;Data models;Training data;Servers","computational complexity;convergence;feature extraction;learning (artificial intelligence);optimisation","optimization problem;computational complexity;convergence properties;adaptive vertical federated learning;training data;data-parallel architecture;heterogeneous computational capabilities;unbalanced features;VFL","",1.0,"",52.0,"IEEE","27 May 2022","","","IEEE","IEEE Journals"
"DAG Scheduling and Analysis on Multi-Core Systems by Modelling Parallelism and Dependency","S. Zhao; X. Dai; I. Bate","Department of Computer Science, University of York, York, U.K.; Department of Computer Science, University of York, York, U.K.; Department of Computer Science, University of York, York, U.K.","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4019,4038,"With ever more complex functionalities being implemented in emerging real-time applications, multi-core systems are demanded for high performance, with directed acyclic graphs (DAG) being used to model functional dependencies. For a single DAG task, our previous work presented a concurrent provider and consumer (CPC) model that captures the node-level dependency and parallelism, which are the two key factors of a DAG. Based on the CPC, scheduling and analysis methods were constructed to reduce makespan and tighten the analytical bound of the task. However, the CPC-based methods cannot support multi-DAGs as the interference between DAGs (i.e., inter-task interference) is not taken into account. To address this limitation, this article proposes a novel multi-DAG scheduling approach which specifies the number of cores a DAG can utilise so that it does not incur the inter-task interference. This is achieved by modelling and understanding the workload distribution of the DAG and the system. By avoiding the inter-task interference, the constructed schedule provides full compatibility for the CPC-based methods to be applied on each DAG and reduces the pessimism of the existing analysis. Experimental results show that the proposed multi-DAG method achieves an improvement up to 80% in schedulability against the original work that it extends, and outperforms the existing multi-DAG methods by up to 60% for tightening the interference.","1558-2183","","10.1109/TPDS.2022.3177046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779935","Multi-core systems;directed acyclic graphs;schedulability tests","Task analysis;Interference;Schedules;Parallel processing;Time factors;Computational modeling;Analytical models","directed graphs;multiprocessing systems;processor scheduling","multicore systems;parallelism;functional dependencies;concurrent provider and consumer model;node-level dependency;CPC;inter-task interference;multiDAG scheduling;directed acyclic graphs","",2.0,"",39.0,"IEEE","23 May 2022","","","IEEE","IEEE Journals"
"CNNPC: End-Edge-Cloud Collaborative CNN Inference With Joint Model Partition and Compression","S. Yang; Z. Zhang; C. Zhao; X. Song; S. Guo; H. Li","Pazhou Laboratory, Guangzhou, China; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China; Department of Computing, Imperial College London, London, U.K.; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4039,4056,"Edge Intelligence (EI) aims at addressing concerns like response latency risen by the conflict between predominating Cloud-based deployments of computationally intensive AI applications and the expensive uploading of explosive end data. Convolutional Neural Networks (CNNs) leading the latest flourish of AI inevitably suffer from the aforementioned conflict. There emerge increasing EI-driven attempts on fast CNN inference with high accuracy in the End-Edge-Cloud (EEC) collaborative computing paradigm, where, however, neither model compression approaches for on-device inference nor collaborative inference methods across devices can effectively achieve the trade-off between latency and accuracy of End-to-End (E2E) inference. In this article, we present CNNPC that jointly partitions and compresses CNNs for fast inference with high accuracy in collaborative EEC systems. We implemented CNNPC (source code available at https://github.com/IoTDATALab/CNNPC) and evaluated its performance within extensive real-world EEC scenarios. Experimental results demonstrate that, compared with state-of-the-art single-end and collaborative approaches, without obvious accuracy loss, collaborative inference based on CNNPC is up to $1.6\times$1.6× and $5.6\times$5.6× faster, and requires as low as $4.30\%$4.30% and $6.48\%$6.48% communications, respectively. Besides, when determines the optimal strategy, CNNPC requires as low as $0.1\%$0.1% actual compression operations that the traversal method (the only viable method providing the theoretically optimal strategy) requires.","1558-2183","","10.1109/TPDS.2022.3177782","National Key Research and Development Program of China(grant numbers:2020YFA0713900); National Natural Science Foundation of China(grant numbers:61772410,61802298,62172329,U1811461,U21A6005,11690011); China Postdoctoral Science Foundation(grant numbers:2020T130513,2019M663726); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782528","Edge computing;edge intelligence;collaborative CNN inference;CNN partition and compression","Collaboration;Convolutional neural networks;Computational modeling;Solid modeling;Data models;Artificial intelligence;Cloud computing","cloud computing;convolutional neural nets;groupware;inference mechanisms","model compression approaches;on-device inference;fast inference;collaborative EEC systems;joint model partition;edge intelligence;cloud-based deployments;computationally intensive AI applications;convolutional neural networks;CNNPC;end-edge-cloud collaborative CNN inference;end-edge-cloud collaborative computing paradigm;actual compression operations","",1.0,"",60.0,"IEEE","26 May 2022","","","IEEE","IEEE Journals"
"Exploring Query Processing on CPU-GPU Integrated Edge Device","J. Liu; F. Zhang; H. Li; D. Wang; W. Wan; X. Fang; J. Zhai; X. Du","Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4057,4070,"Huge amounts of data have been generated on edge devices every day, which requires efficient data analytics and management. However, due to the limited computing capacity of these edge devices, query processing at the edge faces tremendous pressure. Fortunately, in recent years, hardware vendors have integrated heterogeneous coprocessors, such as GPUs, into the edge device, which can provide much more computing power. Furthermore, the CPU-GPU integrated edge device has shown significant benefits in a variety of situations. Therefore, the exploration of query processing on such CPU-GPU integrated edge devices becomes an urgent need. In this article, we develop a fine-grained query processing engine, called FineQuery, which can perform efficient query processing on CPU-GPU integrated edge devices. Particularly, FineQuery can take advantage of both architectural features of edge devices and query characteristics by performing fine-grained workload scheduling between the CPU and the GPU. Experiments show that on TPC-H workloads, FineQuery reduces 42.81% latency and improves 2.39× bandwidth utilization on average compared to the implementation of using only GPU or CPU. Furthermore, query processing at the edge can bring significant performance-per-cost benefits and energy efficiency. On average, FineQuery at the edge brings 21× performance-per-cost ratio and 4× energy efficiency compared with processing the data on a discrete GPU platform.","1558-2183","","10.1109/TPDS.2022.3177811","National Natural Science Foundation of China(grant numbers:61732014,62172419,U20A20226,62072459); Beijing Natural Science Foundation(grant numbers:4202031); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program(grant numbers:20191080594); CCF-Tencent Open Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782546","CPU;GPU;integrated architecture;edge device;query processing","Graphics processing units;Query processing;Performance evaluation;Computer architecture;Databases;Structured Query Language;Engines","graphics processing units;microprocessor chips;query processing","CPU-GPU integrated edge device;fine-grained query processing engine;integrated heterogeneous coprocessors;FineQuery;fine-grained workload scheduling;TPC-H workloads","","","",62.0,"IEEE","26 May 2022","","","IEEE","IEEE Journals"
"Highly Accurate Clock Synchronization With Drift Correction for the Controller Area Network","M. Akpınar; E. G. Schmidt; K. W. Schmidt","Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey; Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey; Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4071,4082,"Modern vehicles, that have to be considered as safety-critical cyber-physical systems, require highly accurate clock synchronization (CS) among their distributed computing devices. Since Controller Area Network (CAN) is the predominant in-vehicle communication bus, it is highly relevant to support CS for CAN. This article proposes an original CS method for distributed in-vehicle networks based on CAN with both offset and drift correction. While offset correction is performed based on timestamps in periodic reference messages (RMs), our new method benefits from the re-synchronization mechanism of the CAN bit timing to apply highly accurate drift correction. Our algorithm does not make any modifications to the CAN protocol but requires the measurement of the phase error from the CAN controller. We derive analytical bounds for the expected clock differences and further validate the practicability of the proposed method by comprehensive experiments. As the main result, our method achieves a clock accuracy below $2\;\mu$2μs independent of important parameters such as the bit rate, RM period, bus utilization and time-varying clock drifts.","1558-2183","","10.1109/TPDS.2022.3179316","Türkiye Bilimsel ve Teknolojik Araştırma Kurumu(grant numbers:119E277); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785903","Controller area network;clock synchronization;offset correction;drift correction;phase error","Silicon;Clocks;Synchronization;Protocols;Real-time systems;Time-frequency analysis;Hardware","controller area networks;protocols;synchronisation;vehicular ad hoc networks","clock synchronization;controller area network;safety-critical cyber-physical systems;distributed computing devices;predominant in-vehicle communication bus;original CS method;distributed in-vehicle networks;offset correction;resynchronization mechanism;CAN controller;bus utilization;time-varying clock drifts;RM","","","",45.0,"IEEE","31 May 2022","","","IEEE","IEEE Journals"
"Energy-Aware Non-Preemptive Task Scheduling With Deadline Constraint in DVFS-Enabled Heterogeneous Clusters","Q. Wang; X. Mei; H. Liu; Y. -W. Leung; Z. Li; X. Chu","Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Computing, Hang Seng University of Hong Kong, Siu Lek Yuen, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4083,4099,"Energy conservation of large data centers for high performance computing workloads, such as deep learning with Big Data, is of critical significance, where cutting down a few percent of electricity translates into million-dollar savings. This work studies energy conservation on emerging CPU-GPU hybrid clusters through dynamic voltage and frequency scaling (DVFS). We aim at minimizing the total energy consumption of processing a batch of offline tasks or a sequence of real-time tasks under deadline constraints. We derive a fast and accurate analytical model to compute the appropriate voltage/frequency setting for each task, and assign multiple tasks to the cluster with heuristic scheduling algorithms. In particular, our model stresses the nonlinear relationship between task execution time and processor speed for GPU-accelerated applications, for more accurately capturing real-world GPU energy consumption. In performance evaluation driven by real-world power measurement traces, our scheduling algorithm shows comparable energy savings to the theoretical upper bound. With a GPU scaling interval where analytically at most 36% of energy can be saved, we record 33-35% of energy savings. Our results are applicable to energy management on modern heterogeneous clusters.","1558-2183","","10.1109/TPDS.2022.3181096","Hong Kong RGC GRF(grant numbers:HKBU 12200418); Hong Kong Research Matching Scheme(grant numbers:RMGS2019_1_23); Hang Seng University of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9790352","Graphics processing units;dynamic voltage and frequency scaling;task scheduling","Graphics processing units;Task analysis;Energy consumption;Voltage;Scheduling algorithms;Voltage control;Kernel","energy conservation;energy consumption;graphics processing units;multiprocessing systems;parallel processing;performance evaluation;power aware computing;processor scheduling;real-time systems;scheduling","heuristic scheduling algorithms;task execution time;GPU-accelerated applications;real-world GPU energy consumption;performance evaluation;scheduling algorithm;energy savings;GPU scaling interval;energy management;heterogeneous clusters;energy-aware nonpreemptive task scheduling;deadline constraint;DVFS-enabled heterogeneous clusters;data centers;high performance computing workloads;deep learning;Big Data;CPU-GPU hybrid clusters;frequency scaling;total energy consumption","",1.0,"",57.0,"IEEE","8 Jun 2022","","","IEEE","IEEE Journals"
"SmartVM: A Smart Contract Virtual Machine for Fast On-Chain DNN Computations","T. Li; Y. Fang; Y. Lu; J. Yang; Z. Jian; Z. Wan; Y. Li","Tianjin Key Laboratory of Network and Data Science Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Science Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Science Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Science Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Science Technology, Tianjin, China; Zhejiang Lab, Hangzhou, Zhejiang, China; College of Computer Science, Nankai University, Tianjin, China","IEEE Transactions on Parallel and Distributed Systems","22 Sep 2022",2022,33.0,12.0,4100,4116,"Blockchain-based artificial intelligence (BC-AI) has been applied for protecting deep neural network (DNN) data from being tampered with, which is expected to further boost trusted distributed AI applications in many fields. However, due to smart contract execution environment architectural defects, it is challenging for previous BC-AI systems to support computing-intensive tasks on-chain performing such as DNN convolution operations. They have to offload computations and a large amount of data from blockchain to off-chain platforms to execute smart contracts as native code. This failure to take advantage of data locality has become one of the major critical performance bottlenecks in BC-AI system. To this end, in this article, we propose SmartVM with optimization methods to support on-chain DNN inference for BC-AI system. The key idea is to design and optimize the computing mechanism and storage structure of smart contract execution environment according to the characteristics of DNN such as high computational parallelism and large data volume. We decompose SmartVM into three components: 1) a compact DNN-oriented instruction set to describe computations in a short number of instructions to reduce interpretation time. 2) a memory management mechanism to make SmartVM memory dynamic free/allocated according to the size of DNN feature maps. 3) a block-based weight prefetching and parallel computing method to organize each layer's computing and weights prefetching in a pipelined manner. We perform the typical image classification in a private Ethereum blockchain testbed to evaluate SmartVM performance. Experimental results highlight that SmartVM can support DNN inference on-chain with roughly the same efficiency against the native code execution. Compared with the traditional off-chain computing, SmartVM can speed up the overall execution by 70×, 16×, 11×, and 12× over LeNet5, AlexNet, ResNet18, and MobileNet, respectively. The memory footprint can be reduced by 84%, 90.8%, 94.3%, and 93.7% over the above four models, while offering the same level model accuracy. This article sheds light on the design space of the smart contract virtual machine for DNN computation and is promising to further boost BC-AI applications.","1558-2183","","10.1109/TPDS.2022.3177405","CCF-AFSG Research Fund(grant numbers:CCF-AFSG RF20210031); Special Funding for Excellent Enterprise Technology Correspondent of Tianjin(grant numbers:21YDTPJC00380); National Natural Science Foundation of China(grant numbers:62002175); Open Project Fund of State Key Laboratory of Computer Architecture; Institute of Computing Technology, Chinese Academy of Sciences(grant numbers:CARCHB202016); Key Research Project of Zhejiang Lab(grant numbers:2021KF0AB04); Natural Science Foundation of Tianjin City(grant numbers:20JCZDJC00610); Foundation of Information Security Evaluation Center of Civil Aviation; Civil Aviation University of China(grant numbers:ISECCA-202102); People's Republic of China ministry of education science and technology development center(grant numbers:2019J02019); Tianjin Graduate Scientific Research Innovation Project(grant numbers:2021YJSB014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780577","Deep neural network;smart contract;virtual machine;architectural support technology","Smart contracts;Blockchains;Virtual machining;Convolutional neural networks;Convolution;Task analysis;Random access memory","deep learning (artificial intelligence);image classification;instruction sets;multiprocessing systems;power aware computing;storage management;virtual machines","memory management mechanism;SmartVM memory dynamic free;DNN feature maps;block-based weight prefetching;private Ethereum blockchain;SmartVM performance;DNN inference on-chain;off-chain computing;smart contract virtual machine;boost BC-AI applications;blockchain-based artificial intelligence;deep neural network data;smart contract execution environment architectural defects;computing-intensive tasks on-chain performing;DNN convolution operations;off-chain platforms;data locality;BC-AI system;optimization methods;DNN such as high computational parallelism;fast on-chain DNN computations;parallel computing method;image classification;DNN-oriented instruction;instruction set","","","",52.0,"IEEE","24 May 2022","","","IEEE","IEEE Journals"
"Real-Time Scheduling of Parallel Task Graphs With Critical Sections Across Different Vertices","X. Jiang; N. Guan; M. Yang; Y. Wang; Y. Tang; W. Yi","Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, China; City University of Hong Kong, Hong Kong; University of Electronic Science and Technology of China, Chengdu, China; Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, China; Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, China; Uppsala University, Uppsala, Sweden","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4117,4133,"All existing work on real-time scheduling of parallel task graph models with shared resources assumes that a critical section must be contained inside a single vertex. However, this assumption does not hold in many realistic parallel real-time software. In this work, we conduct the first study on real-time scheduling and analysis of parallel task graphs where critical sections are allowed to cross different vertices. We show that allowing this may potentially lead to deadlocks and the so-called resource unrelated blocking time problem. We formalize the conditions for the deadlocks and resource unrelated blocking time to happen, and propose two different solutions to address them and develop corresponding schedulability analysis techniques. We conduct comprehensive experiments to evaluate our method. The results indicate that there is a significant impact to the system schedulability when tasks incur deadlock and resource unrelated blocking. Moreover, the schedulability can benefit from the execution of workload in parallel with critical sections if tasks can be carefully designed so that all deadlocks and resource unrelated blocking time can be avoided, and our methods are efficient to determine the schedulability of systems where critical sections across different vertices exist.","1558-2183","","10.1109/TPDS.2022.3179328","National Natural Science Foundation of China(grant numbers:NSFC 62102072); Research Grants Council of Hong Kong(grant numbers:GRF 15206221); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785844","Multi-core;parallel tasks;critical section;real-time scheduling","Task analysis;Real-time systems;Protocols;System recovery;Analytical models;Heuristic algorithms;Computational modeling","concurrency control;graph theory;real-time systems;scheduling;software engineering","real-time scheduling;parallel task graph models;shared resources;real-time software;parallel task graphs;deadlocks;resource unrelated blocking time problem;schedulability analysis techniques;system schedulability","","","",41.0,"IEEE","31 May 2022","","","IEEE","IEEE Journals"
"Understanding the Impact of Data Staging for Coupled Scientific Workflows","A. Gainaru; L. Wan; R. Wang; E. Suchyta; J. Chen; N. Podhorszki; J. Kress; D. Pugmire; S. Klasky","Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4134,4147,"The rate of data generated by cutting-edge experimental science facilities and large-scale simulations enabled by current high-performance computing (HPC) systems has continued to grow at a far greater pace than the development of the network and storage capabilities on which these systems rely. To cope with this challenge, scientist are moving toward the creation of autonomous experiments and HPC simulations using machine learning. However, efficiently moving, storing, and processing large amounts of data away from the point of origin presents an incredible challenge. In-memory computing, in situ analysis, data staging, and data streaming are recognized viable alternatives to traditional file-based methods for transferring data between coupled workflows. However, the performance trade-offs and limitations for these methods are not fully understood when used in HPC applications. This article presents a comprehensive performance assessment of the current solutions for data staging when applied to applications that are not necessary I/O intensive which makes them not ideal candidates for these methods. Our study is based on experiments running at scale on Oak Ridge National Laboratory's Summit supercomputer using applications and simulations that cover typical computational motifs and patterns. We investigated the usability and cost/benefit trade-offs of staging algorithms for HPC applications under different scenarios and highlight opportunities for optimizing the dataflow between coupled simulation workflows.","1558-2183","","10.1109/TPDS.2022.3179989","Exascale Computing Project(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787790","Data staging;high-performance computing;data management;workflow;coupled simulations;in situ analytics;data streaming","Computational modeling;Data models;Codes;Data visualization;Libraries;Distributed databases;Analytical models","data analysis;learning (artificial intelligence);mainframes;natural sciences computing;parallel machines;parallel processing;performance evaluation;scientific information systems;storage management;workflow management software","large-scale simulations;current high-performance computing systems;storage capabilities;HPC simulations;incredible challenge;In-memory computing;data staging;traditional file-based methods;coupled workflows;performance trade-offs;HPC applications;comprehensive performance assessment;cover typical computational motifs;coupled simulation workflows;coupled scientific workflows;cutting-edge experimental science facilities","","","",46.0,"IEEE","3 Jun 2022","","","IEEE","IEEE Journals"
"Content Collaborative Caching Strategy in the Edge Maintenance of Communication Network: A Joint Download Delay and Energy Consumption Method","L. Rui; D. Song; S. Chen; Y. Yang; Y. Yang; Z. Gao","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4148,4163,"With the development of Big Data technology and Internet, the surge of data in the network will cause network congestion and untimely task processing. Additionally, caching content in the core network may cause redundant access of content and backhaul bottlenecks. Due to the increasing requirements of users for task processing efficiency, the centralized maintenance system based on traditional cloud computing cannot meet the current computing requirements. In view of these problems, we propose a content collaborative caching mechanism based on joint decision of download delay and energy consumption. By integrating network coding and content caching technology, the work content maintained in the communication network is deployed near the edge of the network in the form of coding to reduce the redundant transmission of content and acquisition time of content. This article establishes a user QoE satisfaction model, which consists of two indexes that measure time delay and energy consumption. This article proposes a $\varepsilon$ɛ-hybrid Q-learning algorithm to optimize the placement of cache files, and made the cache action selection based on the combination of improved heuristic greedy algorithm and simulated annealing algorithm. The experimental results show that the proposed cache strategy can reduce the delay of users downloading content and the energy consumption of content cache, so as to improve the quality of field maintenance work in communication network.","1558-2183","","10.1109/TPDS.2022.3179271","National Key R&D Program of China(grant numbers:2020YFB1807802,2020YFB1807800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785848","Multi-access edge computing;communication network;content caching;quality of experience","Delays;Maintenance engineering;Energy consumption;Servers;Device-to-device communication;Communication networks;Heuristic algorithms","cache storage;cloud computing;greedy algorithms;Internet;learning (artificial intelligence);simulated annealing","content collaborative caching strategy;edge maintenance;communication network;joint download delay;energy consumption method;Big Data technology;network congestion;untimely task processing;caching content;core network;redundant access;backhaul bottlenecks;centralized maintenance system;traditional cloud computing;current computing requirements;content collaborative caching mechanism;joint decision;network coding;content caching technology;work content;user QoE satisfaction model;indexes that measure time delay;cache files;cache action selection;cache strategy;users downloading content;content cache;field maintenance work","","","",32.0,"IEEE","31 May 2022","","","IEEE","IEEE Journals"
"SconeKV: A Scalable, Strongly Consistent Key-Value Store","J. Gonçalves; M. Matos; R. Rodrigues","INESC-ID, Lisboa, Portugal; INESC-ID, Lisboa, Portugal; INESC-ID, Lisboa, Portugal","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4164,4175,"For decades, relational databases provided a strong foundation for constructing applications due to their ACID properties. However, distributed applications reached a scale, both in terms of data volume and number of concurrent clients, that traditional databases cannot accommodate. NoSQL databases addressed this problem by trading consistency for scalability, namely through horizontal scalability schemes supported by optimistic replication protocols, which only guarantee eventual consistency. In this paper, we explore a novel design between the two extremes, which is able to scale to large deployments while still offering strong consistency guarantees in the form of serializable transactions. Our key insight is to leverage recent advances in membership services that provide strongly consistent views at scale. Those assurances from the membership layer simplify building efficient and consistent storage protocols. Our evaluation of the resulting system, SconeKV, in a realistic scenario shows that it scales and performs better than CockroachDB while being competitive with Cassandra.","1558-2183","","10.1109/TPDS.2022.3179903","FCT; Fundação para a Ciência e a Tecnologia(grant numbers:UIDB/50021/2020,PTDC/CCI-INF/6762/2020,Lisboa-01-0145-FEDER-031456); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9786754","Consistency;distributed systems;scalability;storage","Protocols;Scalability;Semantics;Distributed databases;Relational databases;Peer-to-peer computing;Synchronization","cloud computing;concurrency control;database management systems;protocols;relational databases;replicated databases;SQL;storage management;transaction processing","strong consistency guarantees;serializable transactions;leverage recent advances;membership services;strongly consistent views;membership layer;efficient storage protocols;consistent storage protocols;SconeKV;scalable;key-value store;relational databases;strong foundation;ACID properties;distributed applications;data volume;concurrent clients;traditional databases;NoSQL databases;trading consistency;horizontal scalability schemes;optimistic replication protocols;eventual consistency","","","",40.0,"IEEE","2 Jun 2022","","","IEEE","IEEE Journals"
"Privacy-Preserving Deduplication of Sensor Compressed Data in Distributed Fog Computing","C. Zhang; Y. Miao; Q. Xie; Y. Guo; H. Du; X. Jia","Department of Computer Science, City University of Hong Kong, Hong Kong, China; Department of Computer Science, City University of Hong Kong, Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; Department of Computer Science and Technology, Harbin Institute of Technology Shenzhen, Shenzhen, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4176,4191,"Distributed fog computing has received wide attention recently. It enables distributed computing and data management on the network nodes within the close vicinity of IoT devices. An important service of fog-cloud based systems is data deduplication. With the increasing concern of privacy, some privacy-preserving data deduplication schemes have been proposed. However, they cannot support lossless deduplication of encrypted similar data in the fog-cloud network. Meanwhile, no existing design can protect message equality information while resisting brute-force and frequency analysis attacks. In this paper, we propose a privacy-preserving and compression-based data deduplication system under the fog-cloud network, which supports lossless deduplication of similar data in the encrypted domain. Specifically, we first use the generalized deduplication technique and cryptographic primitives to implement secure deduplication over similar data. Then, we devise a two-level deduplication protocol that can perform secure and efficient deduplication at distributed fog nodes and the cloud. The proposed system can not only resist brute-force and frequency analysis attacks but also ensure that only the data operator can capture the message equality information. We formally analyze the security of our design. Performance evaluations demonstrate that our proposed design is efficient in computing, storage, and communication.","1558-2183","","10.1109/TPDS.2022.3179992","Research Grants Council of Hong Kong(grant numbers:CityU 11213920); National Natural Science Foundation of China(grant numbers:62102035,62072361,62172124); Fundamental Research Funds for the Central Universities(grant numbers:2020NTST32); Shenzhen Basic Research Program(grant numbers:JCYJ20190806143011274); Shenzhen Colleges and Universities Stable Support Program(grant numbers:GXWD20201230155427003-20200822080602001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787776","Brute-force attacks;fog-cloud network;frequency analysis;message equality information;similar data deduplication","Cloud computing;Cryptography;Sensors;Internet of Things;Data models;Costs;Computer science","cloud computing;cryptography;data compression;data privacy;Internet of Things;security of data;storage management","privacy-preserving deduplication;sensor compressed data;distributed fog computing;data management;network nodes;fog-cloud based systems;privacy-preserving data deduplication schemes;lossless deduplication;encrypted similar data;fog-cloud network;message equality information;brute-force;frequency analysis attacks;data deduplication system;generalized deduplication technique;secure deduplication;two-level deduplication protocol;efficient deduplication;distributed fog nodes;data operator","",1.0,"",55.0,"IEEE","3 Jun 2022","","","IEEE","IEEE Journals"
"Hydra: A Decentralized File System for Persistent Memory and RDMA Networks","S. Zheng; J. Wang; D. Xue; J. Shu; L. Huang","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4192,4206,"Emerging byte-addressable persistent memory (PM) has the potential to disrupt the boundary between memory and storage. Combined with high-speed RDMA networks, distributed PM-based storage systems offer the opportunity to provide huge increases in storage performance by closely coupling PM and RDMA features. However, existing distributed file systems adopt the conventional centralized client-server architecture designed for traditional disks, leading to excessive access latency, limited scalability, and high recovery overhead. In this paper, we propose a fully decentralized PM-based file system, Hydra. By exploiting the performance advantages of local PM, Hydra leverages data access locality to achieve high performance. To accelerate file transmission among Hydra nodes, file metadata and data are decoupled and updated differentially through one-sided RDMA reads. Hydra also batches RDMA requests and classifies RPCs into synchronous and asynchronous types to minimize network overhead. Decentralization enables Hydra to tolerate node failures and achieve load balancing. Experimental results show that Hydra outperforms existing distributed file systems by a large margin, and shows good scalability on multi-threaded and parallel workloads.","1558-2183","","10.1109/TPDS.2022.3180369","Natural Science Foundation of Shanghai(grant numbers:22ZR1435400); Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792291","Persistent memory;file system;RDMA;distributed system;decentralization","Servers;Metadata;Scalability;Distributed databases;Random access memory;Tail;File systems","client-server systems;distributed databases;file organisation;input-output programs;meta data;multi-threading;power aware computing;random-access storage;resource allocation;storage management","fully decentralized PM-based file system;data access locality;file transmission;Hydra nodes;file metadata;one-sided RDMA;network overhead;existing distributed file systems;decentralized file system;persistent memory;emerging byte-addressable;high-speed RDMA networks;distributed PM-based storage systems;storage performance;RDMA features;conventional centralized client-server architecture;high recovery","","","",61.0,"IEEE","9 Jun 2022","","","IEEE","IEEE Journals"
"Online Orchestration of Collaborative Caching for Multi-Bitrate Videos in Edge Computing","S. Yang; L. Jiao; R. Yahyapour; J. Cao","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG), and Institute of Computer Science, University of Göttingen, Göttingen, Germany; Department of Computing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4207,4220,"In the traditional video streaming service provisioning paradigm, users typically request video contents through nearby Content Delivery Network (CDN) server(s). However, because of the uncertain wide area networks delays, the (remote) users usually suffer from long video streaming delay, which affects the quality of experience. Multi-Access Edge Computing (MEC) offers caching infrastructures in closer proximity to end users than conventional Content Delivery Networks (CDNs). Yet, for video caching, MEC's potential has not been fully unleashed as it overlooks the opportunities of collaborative caching and multi-bitrate video transcoding. In this paper, we model and formulate an Integer Linear Program (ILP) to capture the long-term cost minimization problem for caching videos at MEC, allowing joint exploitation of MEC with CDN and real-time video transcoding to satisfy arbitrary user demands. While this problem is intractable and couples the caching decisions for adjacent time slots, we design a polynomial-time online orchestration framework which first relaxes and carefully decomposes the problem into a series of subproblems solvable in each individual time slot and then converts the fractional solutions into integers without violating constraints. We have formally proved a parameterized-constant competitive ratio as the performance guarantee for our approach, and also conducted extensive evaluations to confirm its superior practical performance. Simulation results demonstrate that our proposed algorithm outperforms the state-of-the-art algorithms, with 13.6% improvement on average in terms of total cost.","1558-2183","","10.1109/TPDS.2022.3182022","National Natural Science Foundation of China(grant numbers:62172038,61802018); Beijing Institute of Technology Research Fund Program for Young Scholars; National Science Foundation(grant numbers:CNS-2047719); Hong Kong RGC TRS(grant numbers:T-41-603/20R); Hong Kong GRC GRF PolyU(grant numbers:15217919); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795154","Multi-Access edge computing;multi-bitrate video;online caching;quality of experience","Videos;Costs;Quality of experience;Delays;Cloud computing;Bit rate;Servers","cache storage;computational complexity;integer programming;linear programming;minimisation;transcoding;video coding;video streaming","collaborative caching;multibitrate videos;traditional video streaming service provisioning paradigm;video contents;nearby Content Delivery Network server;CDN;uncertain wide area networks delays;long video streaming delay;quality of experience;MultiAccess Edge Computing;closer proximity;conventional Content Delivery Networks;video caching;MEC's potential;multibitrate video transcoding;Integer Linear Program;long-term cost minimization problem;caching videos;real-time video transcoding;arbitrary user demands;caching decisions;adjacent time slots;polynomial-time online orchestration;individual time slot","","","",39.0,"IEEE","13 Jun 2022","","","IEEE","IEEE Journals"
"STRETCH: Virtual Shared-Nothing Parallelism for Scalable and Elastic Stream Processing","V. Gulisano; H. Najdataei; Y. Nikolakopoulos; A. V. Papadopoulos; M. Papatriantafilou; P. Tsigas","Chalmers University of Technology, Göteborg, Sweden; Chalmers University of Technology, Göteborg, Sweden; ZeroPoint Technologies, Gothenburg, Sweden; Mälardalen University, Västerås, Sweden; Chalmers University of Technology, Göteborg, Sweden; Chalmers University of Technology, Göteborg, Sweden","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4221,4238,"Stream processing applications extract value from raw data through Directed Acyclic Graphs of data analysis tasks. Shared-nothing (SN) parallelism is the de-facto standard to scale stream processing applications. Given an application, SN parallelism ins9tantiates several copies of each analysis task, making each instance responsible for a dedicated portion of the overall analysis, and relies on dedicated queues to exchange data among connected instances. On the one hand, SN parallelism can scale the execution of applications both up and out since threads can run task instances within and across processes/nodes. On the other hand, its lack of sharing can cause unnecessary overheads and hinder the scaling up when threads operate on data that could be jointly accessed in shared memory. This trade-off motivated us in studying a way for stream processing applications to leverage shared memory and boost the scale up (before the scale out) while adhering to the widely-adopted and SN-based APIs for stream processing applications. We introduce STRETCH, a framework that maximizes the scale up and offers instantaneous elastic reconfigurations (without state transfer) for stream processing applications. We propose the concept of Virtual Shared-Nothing (VSN) parallelism and elasticity and provide formal definitions and correctness proofs for the semantics of the analysis tasks supported by STRETCH, showing they extend the ones found in common Stream Processing Engines. We also provide a fully implemented prototype and show that STRETCH's performance exceeds that of state-of-the-art frameworks such as Apache Flink and offers, to the best of our knowledge, unprecedented ultra-fast reconfigurations, taking less than 40 ms even when provisioning tens of new task instances.","1558-2183","","10.1109/TPDS.2022.3181979","Swedish Foundation for Strategic Research(grant numbers:GMT14-0032); Vetenskapsrådet(grant numbers:2016-03800,2020-05094,2021-05424,2021-05443); Chalmers AoA frameworks Energy and Production; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795090","Stream processing;shared-nothing parallelism;shared-memory;elasticity;scalability","Parallel processing;Watermarking;Task analysis;Semantics;Metadata;Standards;Prototypes","application program interfaces;data analysis;directed graphs;distributed shared memory systems;parallel processing","elastic stream processing;data analysis;stream processing engines;virtual shared-nothing parallelism;directed acyclic graphs;SN parallelism;de-facto standard;data exchange;shared memory;SN-based APIs;VSN parallelism;semantic analysis","",2.0,"",48.0,"IEEE","13 Jun 2022","","","IEEE","IEEE Journals"
"Detecting Performance Variance for Parallel Applications Without Source Code","J. Zhai; L. Zheng; F. Zhang; X. Tang; H. Wang; T. Yu; Y. Jin; S. L. Song; W. Chen","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering, School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science, University of Sydney, Sydney, NSW, Australia; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4239,4255,"For parallel applications, performance variance is a critical issue that can degrade performance and make applications’ behavior difficult to explain. Therefore, users and application developers should be able to detect and diagnose performance variance. Previous detection methods either introduce too much overhead and slow down applications, or rely on nontrivial source code analysis, which is impractical for production-run parallel systems. In this article, we propose Vapro, a framework for detecting and diagnosing performance variance in production-run parallel systems. Our method is based on an observation that most parallel programs contain code snippets that are executed repeatedly with a fixed workload and can be utilized to detect performance variance. We present State Transition Graph (STG) to track program execution and then do light-weight workload analysis on STG to locate performance variance. Vapro is able to successfully identify these snippets at runtime even without program source code. To diagnose the discovered variation, Vapro uses a progressive diagnosis method based on a hybrid model combining variance breakdown and statistical analysis. According to evaluating results, Vapro's performance overhead is only 1.38% on average. Vapro can identify performance variance in real applications caused by hardware issues, such as memory and IO. The standard deviation of the execution time is decreased by up to 73.5% when the identified variance is fixed. Vapro achieves 30.0% larger detection coverage than the state-of-the-art variance detection approach based on source code analysis.","1558-2183","","10.1109/TPDS.2022.3181799","National Key R&D Program of China(grant numbers:2021YFB0300300); National Natural Science Foundation of China(grant numbers:U20A20226,62072459,62172419); Beijing Natural Science Foundation(grant numbers:4202031,L192027,L192027); SOAR fellowship; University of Sydney faculty startup funding; Australian Research Council(grant numbers:DP210101984); China Postdoctoral Science Foundation(grant numbers:2020TQ0169); ShuiMu Tsinghua Scholar Fellowship(grant numbers:2019SM131); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program(grant numbers:20191080594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9794597","Performance variance;anomaly detection;system noise;parallel computing","Codes;Runtime;Hardware;Performance evaluation;Libraries;Benchmark testing;Electric breakdown","fault diagnosis;graph theory;parallel programming;program diagnostics;software performance evaluation;source code (software);statistical analysis","production-run parallel systems;parallel applications;nontrivial source code analysis;performance variance detection;Vapro performance overhead;performance variance diagnosis;state transition graph;STG;program execution tracking;lightweight workload analysis;statistical analysis;parallel programs","","","",71.0,"IEEE","13 Jun 2022","","","IEEE","IEEE Journals"
"PushBox: Making Use of Every Bit of Time to Accelerate Completion of Data-Parallel Jobs","C. Tian; Y. Wang; B. Tian; Y. Zhao; Y. Zhou; C. Wang; H. Guan; W. Dou; G. Chen","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Modern Posts, Nanjing University of Posts and Telecommunications, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Computer Science, University of Sydeny, Sydney, NSW, Australia; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4256,4269,"To minimize a job's completion time, we need to minimize the completion time of its final stage's last task. Scheduling of machine slots and networks largely dominates the variable part of each task's duration. Finding an optimal schedule is NP-hard even for offline and simplified scenarios. Previous work does lead to improved performance with various strategies. State-of-the-art task placement and network scheduling efforts are largely disjunctive. Without joint optimization, they are sub-optimal and myopic in many scenarios. Task placement usually treats the network as a black box. Thus, we use prioritized bandwidth allocation among tasks making the network both predictable and efficient to achieve joint scheduling. With this feature, joint scheduling can be transformed into a special bin-packing problem. Over this minimal yet power-enough abstraction, we propose PushBox to schedule data-parallel jobs in multi-tenant clusters. When designing the joint scheduling algorithm, we not only embrace the wisdom of prior art but also respect administrators’ fairness intent, which is so far largely ignored. We implement PushBox on Hadoop 3. PushBox performs persistently well on both a small testbed and a trace-driven simulator.","1558-2183","","10.1109/TPDS.2022.3182037","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101390001); National Natural Science Foundation of China(grant numbers:92067206,62072228,61972222); Fundamental Research Funds for the Central Universities; Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9798860","Task scheduling;distributed system;datacenter","Task analysis;Processor scheduling;Optimal scheduling;Semantics;Computational modeling;Schedules;Resource management","bandwidth allocation;bin packing;computational complexity;data handling;optimisation;parallel processing;processor scheduling;resource allocation;tree data structures","NP-hard;task placement;network scheduling;joint optimization;prioritized bandwidth allocation;special bin-packing problem;joint scheduling algorithm;PushBox;data-parallel job completion;machine slot scheduling;multitenant clusters;Hadoop 3","","","",50.0,"IEEE","17 Jun 2022","","","IEEE","IEEE Journals"
"Formulating Cost-Effective Data Distribution Strategies Online for Edge Cache Systems","X. Xia; F. Chen; Q. He; J. Grundy; M. Abdelrazek; J. Shen; A. Bouguettaya; H. Jin","School of Information Tecnology, Deakin University, Burwood, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computer Science, University of Sydney, Camperdown, NSW, Australia; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4270,4281,"Edge Computing (EC) enables a new kind of caching system in close geographic proximity to end-users by allowing app vendors to cache popular data on edge servers deployed at base stations. This edge cache system can better support latency-sensitive applications. However, transmitting data from the centralized cloud to the edge servers without proper transmission strategies may cost app vendors dearly. Cost-effective data distribution strategies are of particular importance for applications, whose data to be cached at the edge often changes dynamically. In this paper, we study this Online Edge Data Distribution (OEDD) problem, aiming to minimize app vendors’ total transmission cost, while ensuring low transmission latency in the long term. We first model this problem and prove its $\mathcal {NP}$NP-hardness. We then combine Lyapunov optimization and game theory to propose a novel Latency-Aware Online (LAO) approach for solving this OEDD problem over time in a distributed manner with provable performance guarantees. The evaluation of LAO based on a real-world dataset demonstrates that it can help app vendors formulate cost-effective edge data distribution strategies in an online manner.","1558-2183","","10.1109/TPDS.2022.3185250","Australian Research Council Discovery(grant numbers:DP200102491); Laureate Fellowship(grant numbers:FL190100035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9804351","Data distribution;edge cache system;online algorithm;optimization","Servers;Costs;Videos;Optimization;Data communication;Australia;Cloud computing","cache storage;cloud computing;computational complexity;game theory;mobile computing;optimisation","edge servers;edge cache system;latency-sensitive applications;transmission strategies;app vendors;cost-effective edge data distribution strategies;caching system;latency-aware online approach;online edge data distribution problem;edge computing;geographic proximity;NP-hardness;Lyapunov optimization;game theory;OEDD problem;centralized cloud","","","",35.0,"IEEE","22 Jun 2022","","","IEEE","IEEE Journals"
"Eiffel: Efficient and Fair Scheduling in Adaptive Federated Learning","A. Sultana; M. M. Haque; L. Chen; F. Xu; X. Yuan","School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; Department of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4282,4294,"Emerging machine learning (ML) technologies, in combination with the increasing computational power of mobile devices, lead to the extensive adoption of ML-based applications. Different from conventional model training that needs to collect all the user data in centralized cloud servers, federated learning (FL) has recently drawn increasing research attention as it enables privacy-preserving model training. With FL, decentralized edge devices in participation, train their model copies locally over their siloed datasets, and periodically synchronize the model parameters. However, model training is computationally extensive which easily drains the battery of mobile devices. In addition, due to the uneven distribution of siloed datasets, the shared model may become biased. To address the efficiency and fairness concerns in a resource-constrained federated learning setting, in this paper, we propose Eiffel to judiciously select mobile devices to participate in the global model aggregation, and adaptively adjust the frequency of local and global model updates. Eiffel aims to make scheduling and coordination for the federated learning towards both resource efficiency and model fairness. We have conducted theoretical analysis of Eiffel from the perspectives of fairness and convergence. Extensive experiments with a wide variety of real-world datasets and models, both on a networked prototype system and in a larger-scale simulated environment, have demonstrated that while maintaining similar accuracy performance, Eiffel outperforms existing baselines with respect to reducing communication overhead by up to 6× for higher efficiency and improving the fairness metric by up to 57% compared to the state-of-the-art algorithms.","1558-2183","","10.1109/TPDS.2022.3187365","National Science Foundation(grant numbers:2019511,1763620,1948374); Louisiana Board of Regents(grant numbers:LEQSF(2019-22)-RD-A-21); National Natural Science Foundation of China(grant numbers:61972158); Science and Technology Commission of Shanghai Municipality(grant numbers:20511102802,18DZ2270800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810502","Efficiency;fairness;federated learning;resource constraints;scheduling","Adaptation models;Collaborative work;Data models;Computational modeling;Performance evaluation;Training;Convergence","cloud computing;data privacy;learning (artificial intelligence);mobile computing;scheduling","Eiffel;fair scheduling;adaptive federated learning;machine learning technologies;increasing computational power;mobile devices;extensive adoption;ML-based applications;conventional model training;user data;centralized cloud servers;privacy-preserving model training;decentralized edge devices;model copies;siloed datasets;model parameters;shared model;fairness concerns;resource-constrained federated learning;global model aggregation;local model updates;global model updates;resource efficiency;model fairness;convergence;fairness metric","","","",37.0,"IEEE","29 Jun 2022","","","IEEE","IEEE Journals"
"Cloud Object Storage Synchronization: Design, Analysis, and Implementation","F. Chen; Z. Li; C. Jiang; T. Xiang; Y. Yang","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science, Chongqing University, Chongqing, China; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4295,4310,"Cloud storage synchronization among different computing terminals has attracted large-scale uses among enterprise and individual users. It enables users to maintain the same copy of data in real time, which eases users the tedious yet error-prone data management burden. However, existing cloud storage synchronization systems are in a closed form. Users are fixed to a certain cloud service provider, which makes it hard to transfer from one provider to another when balancing factors such as performance, cost, security, etc. To bridge this gap, this article proposes a new synchronization system based on standard cloud object storage. Specifically, we first formulate the cloud object storage synchronization problem by defining some useful concepts. We then use the idea of state encoding and a push-pull paradigm to propose a cloud object storage synchronization system. The proposed system supports real-time, multiple-terminal, and cloud-independent storage synchronization. We also prototyped the proposed system. The experimental results show that the proposed system is promising for practical usages.","1558-2183","","10.1109/TPDS.2022.3185067","National Natural Science Foundation of China(grant numbers:61872243,62072062,U20A20176,61902255); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515011489); Science and Technology Plan Projects of Shenzhen(grant numbers:JCYJ20180305124126741,JCYJ20190808163417094); Natural Science Foundation of Chongqing(grant numbers:cstc2022ycjh-bgzxm0031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9802905","Cloud object storage;synchronization;state encoding;push-pull paradigm;system prototype","Cloud computing;Synchronization;Real-time systems;History;Computers;Browsers;Costs","cloud computing;storage management;synchronisation","cloud service provider;standard cloud object storage;cloud object storage synchronization system;cloud-independent storage synchronization;computing terminals;error-prone data management burden;push-pull paradigm","","","",37.0,"IEEE","21 Jun 2022","","","IEEE","IEEE Journals"
"Efficient and Secure Deep Learning Inference in Trusted Processor Enabled Edge Clouds","Y. Li; D. Zeng; L. Gu; Q. Chen; S. Guo; A. Zomaya; M. Guo","School of Computer Science, Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science, Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Hong Kong Polytechnic University Shenzhen Research Institute, Shenzhen, Guangdong, China; School of Computer Science, The University of Sydney, Camperdown, NSW, Australia; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4311,4325,"Edge intelligence has emerged as a prevalent enabling technology to support various intelligent applications. Along with the prosperity, it also raises great concern on the security and privacy since the edge servers are usually shared and untrusted. The security-sensitive code (i.e., the pre-trained model) and data may be easily stolen by malicious tenants, and even untrusted infrastructure providers. To this end, Software Guard Extensions (SGX) is proposed to provide an isolated Trust Execution Environment (TEE) for security and privacy guarantee. However, we find that running tasks in SGX suffer certain performance degradation due to the limited Enclave Page Cache (EPC) size. This further leads to frequent page swapping operations and the high enclave call overhead, which are also influenced by the task (i.e., DNN layer) dispatching and scheduling. To this end, in this paper, we design Lasagna, as an SGX based secure DNN inference acceleration framework, which explores the layered-structure of DNN models to well balance the usage of the scarce EPC resources and the computation resources. Lasagna mainly consists of a global task balancer and a local task scheduler, responding for task dispatching across distributed edge servers and task scheduling in local server, respectively. We evaluate Lasagna over different well-known DNN models, and the results show that Lasagna effectively speeds up the inference performance by $1.11\times -1.51\times$1.11×-1.51×.","1558-2183","","10.1109/TPDS.2022.3187772","National Natural Science Foundation of China(grant numbers:62172375,61972171,61872240,61872310); Open Research Projects of Zhejiang Lab(grant numbers:2021KE0AB02); Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101400003); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,152203/20E,152244/21E); Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ20200109142008673); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817054","DNN Inference;edge intelligence;SGX;task scheduling","Task analysis;Security;Servers;Memory management;Dispatching;Deep learning;Computational modeling","cache storage;cloud computing;data privacy;deep learning (artificial intelligence);neural nets;scheduling;trusted computing","DNN layer;SGX;secure DNN inference acceleration framework;layered-structure;DNN models;global task balancer;local task scheduler;task dispatching;distributed edge servers;task scheduling;inference performance;trusted processor enabled edge clouds;edge intelligence;prevalent enabling technology;security-sensitive code;malicious tenants;software guard extensions;trust execution environment;privacy guarantee;frequent page swapping operations;Enclave page cache size;EPC resources;enclave call overhead","",1.0,"",45.0,"IEEE","6 Jul 2022","","","IEEE","IEEE Journals"
"AccTFM: An Effective Intra-Layer Model Parallelization Strategy for Training Large-Scale Transformer-Based Models","Z. Zeng; C. Liu; Z. Tang; K. Li; K. Li","College of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Hunan, China; College of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Hunan, China; College of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Hunan, China; College of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Hunan, China; Department of Computer Science, State University of New York, New Paltz, NY, USA","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4326,4338,"Transformer-based deep neural networks have recently swept the field of natural language processing due to their outstanding performance, and are gradually spreading to more applications such as image/video processing. However, compared with general DNNs, training a sizeable transformer-based model is further time-consuming and memory-hungry. The existing distributed training strategies for general DNNs are not appropriate or can not efficiently handle transformer-based networks. In view of this, we propose an intra-layer model parallelization optimization strategy, AccTFM, which introduces a novel fine-grained pipeline execution and hybrid communication compression strategy to overcome the synchronization bottleneck. Specifically, on one hand, it first decouples the inter-layer computation and communication dependencies, and then searches for the optimal partitioning strategy to maximize the overlap of computation and communication. On the other hand, the hybrid communication compression module consists of token-level top-$k$k sparsification and piecewise quantization methods aiming at minimizing communication traffic. Experimental results show that AccTFM accelerates transformer-based DNNs training by up to 2.08x compared to state-of-the-art distributed training techniques.","1558-2183","","10.1109/TPDS.2022.3187815","National Key Research and Development Program of China(grant numbers:2021YFB0300300); National Natural Science Foundation of China(grant numbers:62072165); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815837","Communication hiding;deep learning;intra-layer model parallelization;quantization;Top- k  sparsification","Training;Transformers;Pipelines;Computational modeling;Tensors;Load modeling;Synchronization","data compression;deep learning (artificial intelligence);natural language processing;optimisation;parallel processing;pipeline processing;telecommunication traffic;video signal processing","communication traffic;AccTFM;DNNs training;distributed training techniques;large-scale transformer-based models;transformer-based deep neural networks;natural language processing;intra-layer model parallelization optimization strategy;fine-grained pipeline execution;inter-layer computation;communication dependencies;optimal partitioning strategy;hybrid communication compression module;intra-layer model parallelization strategy;transformer-based networks;token-level top-k sparsification;piecewise quantization methods","","","",37.0,"IEEE","5 Jul 2022","","","IEEE","IEEE Journals"
"TriangleKV: Reducing Write Stalls and Write Amplification in LSM-Tree Based KV Stores With Triangle Container in NVM","C. Ding; T. Yao; H. Jiang; Q. Cui; L. Tang; Y. Zhang; J. Wan; Z. Tan","Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, USA; PingCAP, Beijing, China; PingCAP, Beijing, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Shenzhen Huazhong University of Science and Technology Research Institute, Shenzhen, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4339,4352,"Popular LSM-tree based key-value stores suffer from suboptimal and unpredictable performance due to write amplification and write stalls that cause application performance to periodically drop to nearly zero. Our preliminary experimental studies reveal that (1) write stalls mainly stem from the significantly large amount of data involved in each compaction between $L_{0}$L0-$L_{1}$L1 (i.e., the first two levels of LSM-tree), and (2) write amplification increases with the depth of LSM-trees. Existing work mainly focus on reducing write amplification, while only a couple of them target mitigating write stalls. In this paper, we exploit unique features of non-volatile memory (NVM) to address these two limitations and propose TriangleKV, a new LSM-tree based persistent KV store with multi-tier DRAM-NVM-SSD storage. TriangleKV's design principles include performing smaller and cheaper $L_{0}$L0-$L_{1}$L1 compaction to reduce write stalls while reducing the depth of LSM-trees to mitigate write amplification. To this end, four novel techniques are proposed. First, we relocate and manage the $L_{0}$L0 level in NVM with our proposed triangle container. Second, the new right-angle side compaction is devised to compact $L_{0}$L0 to $L_{1}$L1 at fine-grained key ranges, thus substantially reducing the amount of compaction data. Third, TriangleKV increases the width of each level to decrease the depth of LSM-trees thus mitigating write amplification. Finally, the cross-row hint search is introduced for the triangle container to keep adequate read performance. We implement TriangleKV based on MatrixKV and evaluate it on a hybrid DRAM/NVM/SSD system using Intel's latest 3D Xpoint NVM device Optane DC PMM. Evaluation results show that, with the same amount of NVM, TriangleKV outperforms RocksDB, NoveLSM and MatrixKV in 99th-percentile latencies by $5.5\times$5.5×, $2.1\times$2.1× and $1.1\times$1.1×, and random write throughput by $4.9\times$4.9×, $3.5\times$3.5× and $1.4\times$1.4× respectively.","1558-2183","","10.1109/TPDS.2022.3188268","National Natural Science Foundation of China(grant numbers:62072196); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20190809095001781); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2021B0101400003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815150","Key-value stores;LSM-tree;non-volatile memory","Nonvolatile memory;Compaction;Random access memory;Throughput;Containers;Tail;System performance","cache storage;DRAM chips;flash memories;random-access storage;tree data structures","write stalls;write amplification;triangle container;TriangleKV;LSM-tree based persistent KV store;multitier DRAM-NVM-SSD storage;Optane DC PMM;RocksDB;NoveLSM;MatrixKV","","","",54.0,"IEEE","4 Jul 2022","","","IEEE","IEEE Journals"
"Context-Aware Online Client Selection for Hierarchical Federated Learning","Z. Qu; R. Duan; L. Chen; J. Xu; Z. Lu; Y. Liu","Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; Shanghai Key Laboratory of Integrated Administration Technologies for Information Security, Shanghai, China; Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA","IEEE Transactions on Parallel and Distributed Systems","23 Aug 2022",2022,33.0,12.0,4353,4367,"Federated Learning (FL) has been considered as an appealing framework to tackle data privacy issues of mobile devices compared to conventional Machine Learning (ML). Using Edge Servers (ESs) as intermediaries to perform model aggregation in proximity can reduce the transmission overhead, and it enables great potential in low-latency FL, where the hierarchical architecture of FL (HFL) has been attracted more attention. Designing a proper client selection policy can significantly improve training performance, and it has been widely investigated in conventional FL studies. However, to the best of our knowledge, systematic client selection policies have not yet been fully studied for HFL. In addition, client selection for HFL faces more challenges than conventional FL (e.g., the time-varying connection of client-ES pairs and the limited budget of the Network Operator (NO)). In this article, we investigate a client selection problem for HFL, where the NO learns the number of successful participating clients to improve training performance (i.e., select as many clients in each round) as well as under the limited budget on each ES. An online policy, called Context-aware Online Client Selection (COCS), is developed based on Contextual Combinatorial Multi-Armed Bandit (CC-MAB). COCS observes the side-information (context) of local computing and transmission of client-ES pairs and makes client selection decisions to maximize NO's utility given a limited budget. Theoretically, COCS achieves a sublinear regret compared to an Oracle policy on both strongly convex and non-convex HFL. Simulation results also support the efficiency of the proposed COCS policy on real-world datasets.","1558-2183","","10.1109/TPDS.2022.3186960","National Science Foundation(grant numbers:CNS-2044516,ECCS-2033681,ECCS-2029858,CNS-2044991); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9809926","Hierarchical federated learning;client selection;contextual combinatorial multi-armed bandit","Training;Servers;Computational modeling;Data models;Computer architecture;Convergence;Faces","combinatorial mathematics;data privacy;decision making;learning (artificial intelligence);mobile computing","model aggregation;systematic client selection policies;time-varying connection;client-ES pairs;online policy;side-information;local computing;client selection decisions;Oracle policy;nonconvex HFL;COCS policy;hierarchical federated learning;data privacy issues;mobile devices;conventional machine learning;edge server;context-aware online client selection;FL hierarchical architecture;contextual combinatorial multiarmed bandit;CC-MAB;sublinear regret","","","",48.0,"IEEE","28 Jun 2022","","","IEEE","IEEE Journals"
"HEROv2: Full-Stack Open-Source Research Platform for Heterogeneous Computing","A. Kurth; B. Forsberg; L. Benini","Integrated Systems Laboratory, Zurich, Switzerland; D-ITET, Eidgenossische Technische Hochschule Zurich Departement Informationstechnologie und Elektrotechnik, Zurich, Switzerland; DEIS, University of Bologna, Bologna, BO, Italy","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4368,4382,"Heterogeneous computers integrate general-purpose host processors with domain-specific accelerators to combine versatility with efficiency and high performance. To realize the full potential of heterogeneous computers, however, many hardware and software design challenges have to be overcome. While architectural and system simulators can be used to analyze heterogeneous computers, they are faced with unavoidable compromises between simulation speed and performance modeling accuracy. In this work we present HEROv2, an FPGA-based research platform that enables accurate and fast exploration of heterogeneous computers consisting of accelerators based on clusters of 32-bit RISC-V cores and an application-class 64-bit ARMv8 or RV64 host processor. HEROv2 allows to seamlessly share data between 64-bit host s and 32-bit accelerators and comes with a fully open-source on-chip network, a unified heterogeneous programming interface, and a mixed-data-model, mixed-ISA heterogeneous compiler based on LLVM. We evaluate HEROv2 in four case studies from the application level over toolchain and system architecture down to accelerator microarchitecture. We demonstrate how HEROv2 enables effective research and development on the full stack of heterogeneous computing. For instance, the compiler can tile loops and infer data transfers to and from the accelerators, which leads to a speedup of up to 4.4 × compared to the original program and in most cases is only 15% slower than a handwritten implementation, which requires 2.6 × more code.","1558-2183","","10.1109/TPDS.2022.3189390","FRACTAL(grant numbers:877056); Croatian-Swiss Research Programme; Heterogeneous Computing Systems with Customized Accelerators(grant numbers:180625); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9822980","Heterogeneous (Hybrid) systems;parallel processors;shared memory;hardware/software interfaces;system architectures","Hardware;Program processors;Codes;Computers;System-on-chip;Programming;Engines","CMOS digital integrated circuits;microprocessor chips;multiprocessing systems;performance evaluation;power aware computing;program compilers;reduced instruction set computing","heterogeneous computers;HEROv2;FPGA-based research platform;unified heterogeneous programming interface;mixed-ISA heterogeneous compiler;heterogeneous computing;full-stack open-source research platform;LLVM","",1.0,"",72.0,"IEEE","8 Jul 2022","","","IEEE","IEEE Journals"
"Automated Scheduling Algorithm Selection and Chunk Parameter Calculation in OpenMP","A. Mohammed; J. H. M. Korndörfer; A. Eleliemy; F. M. Ciorba","HPE's HPC/AI EMEA Research Lab (ERL), Basel, Switzerland; Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland; Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland; Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4383,4394,"Increasing node and cores-per-node counts in supercomputers render scheduling and load balancing critical for exploiting parallelism. OpenMP applications can achieve high performance via careful selection of scheduling kind and chunk parameters on a per-loop, per-application, and per-system basis from a portfolio of advanced scheduling algorithms (Korndörfer et al., 2022). This selection approach is time-consuming, challenging, and may need to change during execution. We propose Auto4OMP, a novel approach for automated load balancing of OpenMP applications. With Auto4OMP, we introduce three scheduling algorithm selection methods and an expert-defined chunk parameter for OpenMP's schedule clause's kind and chunk, respectively. Auto4OMP extends the OpenMP schedule(auto) and chunk parameter implementation in LLVM's OpenMP runtime library to automatically select a scheduling algorithm and calculate a chunk parameter during execution. Loop characteristics are inferred in Auto4OMP from the loop execution over the application's time-steps. The experiments performed in this work show that Auto4OMP improves applications performance by up to $11\%$11% compared to LLVM's schedule(auto) implementation and outperforms manual selection. Auto4OMP improves MPI+OpenMP applications performance by explicitly minimizing thread- and implicitly reducing process-load imbalance.","1558-2183","","10.1109/TPDS.2022.3189270","Swiss National Science Foundation(grant numbers:169123); Swiss Platform for Advanced Scientific Computing; DAPHNE; European Union's Horizon 2020 research and innovation programme(grant numbers:957407); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825675","Automatic selection;algorithm selection problem;dynamic load balancing;self-scheduling;runtime library;OpenMP;multithreaded programming;shared-memory systems","Scheduling algorithms;Heuristic algorithms;Runtime library;Dynamic scheduling;Load management;Standards;Parallel processing","application program interfaces;message passing;parallel programming;processor scheduling;program compilers;program control structures;resource allocation;shared memory systems;software libraries","chunk parameter calculation;Auto4OMP;automated load balancing;expert-defined chunk parameter;LLVM schedule;LLVM OpenMP runtime library;automated scheduling algorithm selection methods;MPI;MPI+OpenMP application performance;process-load imbalance reduction","","","",45.0,"CCBY","11 Jul 2022","","","IEEE","IEEE Journals"
"A Survey of Storage Systems in the RDMA Era","S. Ma; T. Ma; K. Chen; Y. Wu","Beijing HaiZhi XingTu Technology Co., Ltd, Beijing, China; Alibaba Group, Hangzhou, China; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4395,4409,"Remote Direct Memory Access (RDMA) based network devices are increasingly being deployed in modern data centers. RDMA brings significant performance improvements over traditional network devices such as Ethernet due to its unique features: protocol offloading and memory semantics. In particular, it can achieve microsecond level latency, which is about 2$\sim$∼3 orders of magnitude improvement. With such improvement in hardware, the software stack, including device drivers and programming libraries, is becoming a new performance bottleneck. Developers need to use new programming libraries to take full advantage of the performance of the underlying hardware. Storage systems are very important in modern data centers. This article surveys the current efforts to use RDMA for optimizing storage systems. We first present five classes of RDMA-based storage systems, including key-value stores, file systems, distributed memory systems, database systems, and systems using smart NICs, to demonstrate different design choices. Then, we examine the core modules of storage systems from different perspectives: communication mode, concurrency control, fault tolerance, caching, and resource management. Finally, we provide some design guidelines for new RDMA-based storage systems, as well as a discussion of opportunities and challenges.","1558-2183","","10.1109/TPDS.2022.3188656","National Key Research & Development Program of China(grant numbers:2020YFC1522702); National Natural Science Foundation of China(grant numbers:62141216,61877035); Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9827961","Network;storage;RDMA;RPC;key-value store;file system;distributed memory;smart NIC","Semantics;Random access memory;Protocols;Hardware;Programming;Performance evaluation;File systems","computer centres;concurrency control;device drivers;distributed memory systems;file organisation;local area networks;network interfaces;protocols;storage management","programming libraries;modern data centers;RDMA-based storage systems;file systems;distributed memory systems;database systems;RDMA era;Remote Direct Memory Access based network devices;traditional network devices;protocol offloading;memory semantics;magnitude improvement;device drivers","","","",122.0,"IEEE","12 Jul 2022","","","IEEE","IEEE Journals"
"Power-Aware Checkpointing for Multicore Embedded Systems","M. Ansari; S. Safari; H. Khdr; P. Gohari-Nazari; J. Henkel; A. Ejlali; S. Hessabi","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran; Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4410,4424,"Increasing the number of cores integrated on a single chip offers a great potential for the implementation of fault-tolerant techniques to achieve high reliability in real-time embedded systems. Checkpointing with rollback-recovery is a well-established technique to tolerate transient faults in multicore platforms. To consider the worst-case fault occurrence scenario, checkpointing technique requires to re-execute some parts of the tasks, and that might lead to simultaneous execution of task parts with high power consumptions, which eventually might result in a peak power increase beyond the thermal design power (TDP). Exceeding TDP can elevate on-chip temperatures beyond safe limits, and thereby triggering countermeasures that throttle down the voltage and frequency levels or power gate the cores. Such countermeasures might lead to violating task deadlines and degrading the system's reliability. To avoid such severe scenarios, it is inevitable to consider the impact of applying fault-tolerant techniques on the power consumption and prevent violating the power constraint of the chip, i.e., TDP. This article presents for the first time, a peak-power-aware checkpointing (PPAC) technique that tolerates a given number of faults, k, while at the same time meets the power constraint in hard real-time embedded systems. To do this, our proposed technique (PPAC) adjusts the timing of the checkpoints, which have lower power consumption than the tasks to the execution time points that have power spikes beyond TDP. Moreover, PPAC exploits the available slack times on the cores to delay the execution of some tasks to avoid the remaining power spikes beyond TDP, which could not be mitigated by solely adjusting checkpoints. To evaluate our technique, we extend the state-of-the-art system-level simulator, gem5, with the state-of-the-art checkpointing module in Linux. Our experimental results show that our proposed technique is able to tolerate a given number of faults without exceeding the timing and power constraints in hard real-time embedded systems. The resulting peak power reduction achieved by our technique compared to state-of-the-art techniques is an average of 23%. Moreover, our technique employs the Dynamic Power Management (DPM) during the slack times resulting at runtime in the case of fault-free scenarios, which provides energy savings with an average of 17.28% and up to 61.1%.","1558-2183","","10.1109/TPDS.2022.3188568","Deutsche Forschungsgemeinschaft; German Research Foundation, Invasive Computing Project(grant numbers:146371743); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815557","Peak power consumption;checkpointing;multicore platforms;embedded systems","Task analysis;Checkpointing;Timing;Power demand;Real-time systems;Embedded systems;Reliability","checkpointing;embedded systems;energy conservation;fault tolerance;fault tolerant computing;Linux;microprocessor chips;multiprocessing systems;power aware computing;power consumption;real-time systems","power constraint;hard real-time embedded systems;resulting peak power reduction;dynamic power management;fault-free scenarios;multicore embedded systems;single chip;fault-tolerant techniques;high reliability;transient faults;multicore platforms;worst-case fault occurrence scenario;simultaneous execution;task parts;high power consumptions;peak power increase;thermal design power;TDP;on-chip temperatures;power gate;task deadlines;peak-power-aware checkpointing technique;PPAC;checkpoints;execution time points;available slack times;remaining power spikes;state-of-the-art system-level simulator;state-of-the-art checkpointing module","","","",48.0,"IEEE","5 Jul 2022","","","IEEE","IEEE Journals"
"Enabling Large Scale Simulations for Particle Accelerators","K. Iliakis; H. Timko; S. Xydis; P. Tsapatsaris; D. Soudris","Accelerator Systems Department, European Organization for Nuclear Research (CERN), Geneva, Switzerland; Accelerator Systems Department, European Organization for Nuclear Research (CERN), Geneva, Switzerland; Digital Technology Department, Harokopio University of Athens, Athens, Greece; Microprocessors and Digital Systems Laboratory, Electrical and Computer Engineering Department, National Technical University of Athens, Athens, Greece; Microprocessors and Digital Systems Laboratory, Electrical and Computer Engineering Department, National Technical University of Athens, Athens, Greece","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4425,4439,"International high-energy particle physics research centers, like CERN and Fermilab, require excessive studies and simulations to plan for the upcoming upgrades of the world's largest particle accelerators, and the design of future machines given the technological challenges and tight budgetary constraints. The Beam Longitudinal Dynamics (BLonD) simulator suite incorporates the most detailed and complex physics phenomena in the field of longitudinal beam dynamics, required for providing extremely accurate predictions. Modern challenges in beam dynamics dictate for longer, larger and numerous simulation studies to draw meaningful conclusions that will drive the baseline choices for the daily operation of current machines and the design choices of future projects. These studies are extremely time consuming, and would be impractical to perform without a High-Performance Computing oriented simulator framework. In this article, at first, we design and evaluate a highly-optimized distributed version of BLonD. We combine approximate computing techniques, and leverage a dynamic load-balancing scheme to relax synchronization and improve scalability. In addition, we employ GPUs to accelerate the distributed implementation. We evaluate the highly optimized distributed beam longitudinal dynamics simulator in a supercomputing system and demonstrate speedups of more than two orders of magnitude when run on 32 GPU platforms, w.r.t. the previous state-of-art. By driving a wide range of new studies, the proposed high performance beam longitudinal dynamics simulator forms an invaluable tool for accelerator physicists.","1558-2183","","10.1109/TPDS.2022.3192707","European Organization for Nuclear Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9834128","Distributed systems;GPU acceleration;approximate computing;network traffic optimisation;accelerator physics","Radio frequency;Particle beams;Synchrotrons;Physics;Computational modeling;Voltage;Large Hadron Collider","graphics processing units;multiprocessing systems;optimisation;parallel architectures;parallel machines;particle accelerators;particle beam dynamics;resource allocation","dynamic load-balancing scheme;highly optimized distributed beam longitudinal dynamics;high performance beam longitudinal dynamics;accelerator physicists;scale simulations;particle accelerators;international high-energy particle physics research centers;excessive studies;upcoming upgrades;world;future machines;tight budgetary constraints;Beam Longitudinal Dynamics simulator suite;detailed physics phenomena;complex physics phenomena;longitudinal beam dynamics;extremely accurate predictions;modern challenges;larger simulation studies;numerous simulation studies;baseline choices;current machines;future projects;extremely time consuming;High-Performance Computing;approximate computing techniques","","","",41.0,"CCBY","20 Jul 2022","","","IEEE","IEEE Journals"
"Optimizing Error-Bounded Lossy Compression for Scientific Data With Diverse Constraints","Y. Liu; S. Di; K. Zhao; S. Jin; C. Wang; K. Chard; D. Tao; I. Foster; F. Cappello","Department of Computer Science, University of Chicago, Chicago, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Department of Computer Science and Engineering, University of California Riverside, Riverside, CA, USA; School of EECS, Washington State University, Pullman, WA, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; School of EECS, Washington State University, Pullman, WA, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4440,4457,"Vast volumes of data are produced by today's scientific simulations and advanced instruments. These data cannot be stored and transferred efficiently because of limited I/O bandwidth, network speed, and storage capacity. Error-bounded lossy compression can be an effective method for addressing these issues: not only can it significantly reduce data size, but it can also control the data distortion based on user-defined error bounds. In practice, many scientific applications have specific requirements or constraints for lossy compression, in order to guarantee that the reconstructed data are valid for post hoc analysis. For example, some datasets contain irrelevant data that should be isolated in particular and users often have intuition regarding value ranges, geospatial regions, and other data subsets that are crucial for subsequent analysis. Existing state-of-the-art error-bounded lossy compressors, however, do not consider these constraints during compression, resulting in inferior compression ratios with respect to user's post hoc analysis, due to the fact that the data itself provides little or no value for post hoc analysis. In this work we address this issue by proposing an optimized framework that can preserve diverse constraints during the error-bounded lossy compression, e.g., cleaning the irrelevant data, efficiently preserving different precision for multiple value intervals, and allowing users to set diverse precision over both regular and irregular regions. We perform our evaluation on a supercomputer with up to 2,100 cores. Experiments with six real-world applications show that our proposed diverse constraints based error-bounded lossy compressor can obtain a higher visual quality or data fidelity on reconstructed data with the same or even higher compression ratios compared with the traditional state-of-the-art compressor SZ. Our experiments also demonstrate very good scalability in compression performance compared with the I/O throughput of the parallel file system.","1558-2183","","10.1109/TPDS.2022.3194695","Exascale Computing Project(grant numbers:17-SC-20-SC); U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:OAC-2003709,OAC-2003624/2042084,CSSI-2104023/2104024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844293","Big data;error-bounded lossy compression;data reduction;large-scale scientific simulation","Data models;Compressors;Quantization (signal);Predictive models;Analytical models;Encoding;Dark matter","Big Data;computer simulation;data compression;data visualisation;input-output programs;natural sciences computing;parallel processing","error-bounded lossy compression;scientific data;diverse constraints;scientific simulations;data size;data distortion;data reconstruction;post hoc analysis;error-bounded lossy compressor;I/O bandwidth;network speed;storage capacity;supercomputer;visual quality;data fidelity;parallel file system;Big Data","",2.0,"",40.0,"USGov","28 Jul 2022","","","IEEE","IEEE Journals"
"Redesigning and Optimizing UCSF DOCK3.7 on Sunway TaihuLight","K. Xu; J. Zhang; X. Duan; X. Wan; N. Huang; B. Schmidt; W. Liu; G. Yang","National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; Computational Medicine (Beijing) Co. Ltd, Beijing, China; Tsinghua Institute of Multidisciplinary Biomedical Research, Tsinghua University, Beijing, China; Institute for Computer Science, Johannes Gutenberg University, Mainz, Germany; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4458,4471,"Molecular docking is the process of posing, scoring, and ranking small molecules at the binding sites of proteins to prioritize compounds for experimental testing. It is a widely-used computational method in the drug discovery process. However, it is a highly time-consuming procedure since a receptor may need to find favorable ligand orientations in billions of ligands. UCSF DOCK3.7 is one of the most widely used molecular docking applications. In this paper, we port and optimize UCSF DOCK3.7 on the Sunway TaihuLight supercomputer. To avoid the impact of load imbalance, we employ a producer-consumer strategy that can overlap I/O and computation in order to achieve high performance. Furthermore, we present a new binary file format to replace the mol2db2 file format for ligand storage and adopt xzip rather than gzip to compress ligand files. We show that our file format can reduce I/O time significantly while xzip saves significant storage. For the routines which determine the orientation of a ligand relative to the receptor, we present an improved algorithm to discard geometrically similar orientations. Furthermore, we fuse loops and compress memory usage to store data in fast Local Device Memory (LDM) in order to score ligand orientations with high efficiency. In addition, we propose a number of architecture-specific optimizations. Asynchronous data transfer and vectorization of computation are implemented to take full advantage of the SW26010 processor. Our experiments show that a speedup of 167 can be achieved by using the proposed strategies. Compared to a core of an Intel(R) Core(TM) i9-10900K CPU, our approach achieves speedups of 15 on a SW26010 core group. Furthermore, our implementation achieves strong scalability to hundreds of thousands of heterogeneous cores on the next-generation Sunway supercomputer.","1558-2183","","10.1109/TPDS.2022.3194916","National Natural Science Foundation of China(grant numbers:61972231,U1806205); Key Project of Joint Fund of Shandong Province(grant numbers:ZR2019LZH00); Ministry of Education, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845705","Molecular docking;drug discovery;heterogeneous architecture;high-performance computing","Memory management;Optimization;Supercomputers;Mathematical models;Graphics processing units;Field programmable gate arrays;Drugs","bioinformatics;biology computing;data compression;drugs;molecular biophysics;parallel architectures;program control structures;proteins","UCSF DOCK3.7;SW26010 processor;asynchronous data transfer;LDM;local device memory;I/O time reduction;data storage;memory usage compression;next-generation Sunway supercomputer;architecture-specific optimizations;score ligand orientations;geometrically similar orientations;ligand files;xzip;ligand storage;mol2db2 file format;binary file;producer-consumer strategy;Sunway TaihuLight supercomputer;UCSF DOCK3;favorable ligand orientations;drug discovery process;molecular docking","","","",42.0,"IEEE","1 Aug 2022","","","IEEE","IEEE Journals"
"Gaviss : Boosting the Performance of GPU-Accelerated NFV Systems via Data Sharing","L. Guo; K. Zhang; X. S. Wang","School of Computer Science and Technology, Fudan University, Shanghai, China; School of Computer Science and Technology, Fudan University, Shanghai, China; School of Computer Science and Technology, Fudan University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4472,4483,"GPUs have demonstrated the capability of significantly improving the performance of network functions (NF). In an Network Function Virtualization (NFV) system, multiple NFs form a service chain to provide services. However, NFs in state-of-the-art GPU-accelerated NFV systems still utilize a GPU independently where each NF needs to transfer data to the GPU memory for acceleration. As a result, a packet might be transferred into the GPU memory by each NF when it passes through the service chain. We find these expensive and repetitive transfers are the main factor that limits the overall performance of an NFV system. We propose Gaviss, a GPU-accelerated NFV system with effective data sharing. By sharing packets in the GPU memory among network functions, a packet needs to be transferred to the GPU only once, eliminating the performance overhead caused by repetitive transfers. Extensive experimental results show that Gaviss can improve the overall throughput by 2.6-13.2× and reduce the latency by up to 37.9%, when compared with state-of-the-art approaches. Moreover, Gaviss also demonstrates up to 2.5× higher price-performance ratio than CPU-based implementations, making GPUs competitive for building NFV systems.","1558-2183","","10.1109/TPDS.2022.3193368","VMware(grant numbers:10.13039/100016682); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9839394","Communications technology;computer networks;network function virtualization","Graphics processing units;Data transfer;Noise measurement;Synchronization;Kernel;Throughput;Logic gates","computer network reliability;data handling;graphics processing units;hardware accelerators;virtualisation","service chain;Gaviss;effective data sharing;GPU memory;GPU-accelerated NFV systems;network functions performance;network function virtualization","","","",37.0,"IEEE","25 Jul 2022","","","IEEE","IEEE Journals"
"Accelerating Tensor Swapping in GPUs With Self-Tuning Compression","P. Chen; S. He; X. Zhang; S. Chen; P. Hong; Y. Yin; X. -H. Sun","Zhejiang Laboratory, Hangzhou, China; Zhejiang Laboratory, Hangzhou, China; School of Engineering and Computer Science, Washington State University Vancouver, Vancouver, WA, USA; Zhejiang Laboratory, Hangzhou, China; Zhejiang Laboratory, Hangzhou, China; Institute of Open Source Chip, Beijing, China; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4484,4498,"Data swapping between CPUs and GPUs is widely used to address the GPU memory shortage issue when training deep neural networks (DNNs) requiring a larger amount of memory than that a GPU may have. Data swapping may become a bottleneck when its latency is longer than the latency of DNN computations. Tensor compression in GPUs can reduce the data swapping time. However, existing works on compressing tensors in the virtual memory of GPUs have three major issues: lack of portability because its implementation requires additional (de)compression units in memory controllers, sub-optimal compression performance for varying tensor compression ratios and sizes, and poor adaptation to dense tensors because they only focus on sparse tensors. We propose a self-tuning tensor compression framework, named CSwap+, for improving the virtual memory management of GPUs. It uses GPUs for (de)compression directly and thus has high portability and is minimally dependent on GPU architecture features. Furthermore, it only applies compression on tensors that are deemed to be cost-effective considering their compression ratio, size, and the characteristics of compression algorithms at runtime. Finally, to adapt to DNN models with dense tensors, it also supports cost-effective lossy compression for dense tensors with nearly no model training accuracy degradation. We conduct the experiments through six representative memory-intensive DNN models. Compared to vDNN, CSwap+ reduces tensor swapping latency by up to 50.9% and 46.1% with NVIDIA V100 GPU, for DNN models with sparse and dense tensors, respectively.","1558-2183","","10.1109/TPDS.2022.3193867","National Key Research and Development Program of China(grant numbers:2021ZD0110700); National Natural Science Foundation of China(grant numbers:62172361); Zhejiang Lab Research Project(grant numbers:2020KC0AC01); National Science Foundation(grant numbers:CNS 1906541); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9841008","DNN;GPU;tensor;swapping;compression","Tensors;Training;Graphics processing units;Memory management;Adaptation models;Prefetching;Computational modeling","computer graphics;data compression;deep learning (artificial intelligence);graphics processing units;storage management;tensors","tensor swapping;self-tuning compression;GPU memory shortage issue;deep neural networks;data swapping time;compression units;memory controllers;tensor compression ratios;dense tensors;sparse tensors;self-tuning tensor compression framework;virtual memory management;GPU architecture features;compression ratio;compression algorithms;cost-effective lossy compression;representative memory-intensive DNN models;suboptimal compression performance","",1.0,"",59.0,"IEEE","26 Jul 2022","","","IEEE","IEEE Journals"
"HiTDL: High-Throughput Deep Learning Inference at the Hybrid Mobile Edge","J. Wu; L. Wang; Q. Pei; X. Cui; F. Liu; T. Yang","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; TU Darmstadt, Darmstadt, Germany; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Peng Cheng Laboratory, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4499,4514,"Deep neural networks (DNNs) have become a critical component for inference in modern mobile applications, but the efficient provisioning of DNNs is non-trivial. Existing mobile- and server-based approaches compromise either the inference accuracy or latency. Instead, a hybrid approach can reap the benefits of the two by splitting the DNN at an appropriate layer and running the two parts separately on the mobile and the server respectively. Nevertheless, the DNN throughput in the hybrid approach has not been carefully examined, which is particularly important for edge servers where limited compute resources are shared among multiple DNNs. This article presents HiTDL, a runtime framework for managing multiple DNNs provisioned following the hybrid approach at the edge. HiTDL's mission is to improve edge resource efficiency by optimizing the combined throughput of all co-located DNNs, while still guaranteeing their SLAs. To this end, HiTDL first builds comprehensive performance models for DNN inference latency and throughout with respect to multiple factors including resource availability, DNN partition plan, and cross-DNN interference. HiTDL then uses these models to generate a set of candidate partition plans with SLA guarantees for each DNN. Finally, HiTDL makes global throughput-optimal resource allocation decisions by selecting partition plans from the candidate set for each DNN via solving a fairness-aware multiple-choice knapsack problem. Experimental results based on a prototype implementation show that HiTDL improves the overall throughput of the edge by $4.3\times$4.3× compared with the state-of-the-art.","1558-2183","","10.1109/TPDS.2022.3195664","National Natural Science Foundation of China(grant numbers:61761136014,61520106005); National Key Research & Development (R&D) Plan(grant numbers:2017YFB1001703); DFG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847073","Deep learning inference;edge computing;resource allocation;systems for machine learning","Throughput;Graphics processing units;Servers;Resource management;Task analysis;Mobile handsets;Mobile applications","deep learning (artificial intelligence);inference mechanisms;knapsack problems;mobile computing;resource allocation","hybrid mobile edge applications;HiTDL mission;fairness-aware multiple-choice knapsack problem;throughput-optimal resource allocation decisions;cross-DNN interference;DNN partition plan;resource availability;DNN inference accuracy;edge resource efficiency;edge servers;server-based approaches compromise;deep neural networks;high-throughput deep learning inference","",2.0,"",65.0,"IEEE","1 Aug 2022","","","IEEE","IEEE Journals"
"Improving Federated Learning With Quality-Aware User Incentive and Auto-Weighted Model Aggregation","Y. Deng; F. Lyu; J. Ren; Y. -C. Chen; P. Yang; Y. Zhou; Y. Zhang","Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China; School of Computer Science and Engineering, Central South University, Changsha, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4515,4529,"Federated learning enables distributed model training over various computing nodes, e.g., mobile devices, where instead of sharing raw user data, computing nodes can solely commit model updates without compromising data privacy. The quality of federated learning relies on the model updates contributed by computing nodes training with their local data. However, with various factors (e.g., training data size, mislabeled data samples, skewed data distributions), the model update qualities of computing nodes can vary dramatically, while inclusively aggregating low-quality model updates can deteriorate the global model quality. To achieve efficient federated learning, in this paper, we propose a novel framework named FAIR, i.e., Federated leArning with qualIty awaReness. Particularly, FAIR integrates three major components: 1) learning quality estimation: we adopt the model aggregation weight (learned in the third component) to reversely quantify the individual learning quality of nodes in a privacy-preserving manner, and leverage the historical learning records to infer the next-round learning quality; 2) quality-aware incentive mechanism: within the recruiting budget, we model a reverse auction problem to stimulate the participation of high-quality and low-cost computing nodes, and the method is proved to be truthful, individually rational, and computationally efficient; and 3) auto-weighted model aggregation: based on the gradient descent method, we devise an auto-weighted model aggregation algorithm to automatically learn the optimal aggregation weights to further enhance the global model quality. Based on real-world datasets and learning tasks, extensive experiments are conducted to demonstrate the efficacy of FAIR.","1558-2183","","10.1109/TPDS.2022.3195207","National Key R&D Program of China(grant numbers:2022YFC2009800); National Natural Science Foundation of China(grant numbers:62002389,62001180,62122095,62072472,U19A2067); Guoqiang Institute, Tsinghua University; Young Elite Scientists Sponsorship Program(grant numbers:YESS20200238); Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010137005); Natural Science Foundation of Hunan Province(grant numbers:2020JJ2050,2021JJ20079); Key R&D Program of Hunan Province of China(grant numbers:2022GK2013); Young Talents Plan of Hunan Province of China(grant numbers:2021RC3004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847055","Edge computing;incentive mechanism;learning quality;mobile computing;model aggregation;federated learning","Task analysis;Computational modeling;Collaborative work;Data models;Training;Resource management;Training data","data privacy;gradient methods;learning (artificial intelligence);mobile computing;query processing;ubiquitous computing","quality-aware user incentive;auto-weighted model aggregation;model training;raw user data;compromising data privacy;data size;mislabeled data samples;skewed data distributions;model update qualities;low-quality model updates;global model quality;efficient federated learning;qualIty awaReness;model aggregation weight;historical learning records;next-round learning quality;low-cost computing nodes;optimal aggregation weights;learning tasks","","","",48.0,"IEEE","1 Aug 2022","","","IEEE","IEEE Journals"
"Improving Cache Utilization of Nested Parallel Programs by Almost Deterministic Work Stealing","S. Shiina; K. Taura","Department of Information and Communication Engineering, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, Japan; Department of Information and Communication Engineering, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, Japan","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4530,4546,"Nested (fork-join) parallelism eases parallel programming by enabling high-level expression of parallelism and leaving the mapping between parallel tasks and hardware to the runtime scheduler. A challenge in dynamic scheduling of nested parallelism is how to exploit data locality, which has become more demanding in the deep cache hierarchies of modern processors with a large number of cores. This paper introduces almost deterministic work stealing (ADWS), which efficiently exploits data locality by deterministically planning a cache-hierarchy-aware schedule, while allowing a little scheduling variety to facilitate dynamic load balancing. Furthermore, we propose an extension of our prior work on ADWS to achieve better shared cache utilization. The improved version of the scheduler is called multi-level ADWS. The idea is that only part of a computation whose working set size is small enough to fit into a shared cache is scheduled by ADWS within the cache recursively, thus avoiding excessive capacity misses. Our evaluation on a benchmark of parallel decision tree construction demonstrated that multi-level ADWS outperformed the conventional random work stealing of Cilk Plus by 61% and it showed a 40% performance improvement over the previous ADWS design.","1558-2183","","10.1109/TPDS.2022.3196192","JSPS KAKENHI(grant numbers:21J22305); New Energy and Industrial Technology Development Organization(grant numbers:JPNP16007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9848990","Dynamic load balancing;locality;nested parallelism;task parallelism;task scheduling;work stealing","Task analysis;Parallel processing;Dynamic scheduling;Decision trees;Program processors;Processor scheduling;Runtime","cache storage;decision trees;multiprocessing systems;parallel programming;resource allocation;scheduling;shared memory systems","shared cache utilization;multilevel ADWS;parallel decision tree construction;nested parallel programs;deterministic work stealing;nested parallelism;fork-join;parallel programming;parallel tasks;runtime scheduler;dynamic scheduling;data locality;deep cache hierarchies;cache-hierarchy-aware schedule;scheduling variety;dynamic load balancing;ADWS design","","","",57.0,"CCBY","3 Aug 2022","","","IEEE","IEEE Journals"
"A Bi-Objective Learn-and-Deploy Scheduling Method for Bursty and Stochastic Requests on Heterogeneous Cloud Servers","X. Cai; H. Xu; X. Li; K. Wang; L. Chen; R. R. García; Q. Zhang","Key Laboratory of Intelligent Control and Optimization for Industrial Equipment of Ministry of Education (MOE), School of Control Science and Engineering, Dalian University of Technology, Dalian, Liaoning, China; Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing, Jiangsu, China; Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, Jiangsu, China; Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing, Jiangsu, China; Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, Jiangsu, China; Grupo de Sistemas de Optimización Aplicada, Universitat Politècnica de València, València, Spain; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4547,4562,"In this article, we consider the dynamic allocation of bursty requests stochastically arriving at heterogeneous servers with uncertain setup times. Lower expected response time and less power consumption are desirable objectives of users and service providers respectively. However, sudden increase and decrease of cloud servers caused by bursty requests are rather challenging to get an appropriate trade-off between the two conflicting objectives which are closely related to the launched servers. The heterogeneity of the cloud servers further makes it more difficult to decide how to switch on and off servers and effectively and efficiently allocate bursty requests with balanced objectives. Based on a Markov decision process, a real-time bilevel decision-making model is constructed for unallocated requests which includes: whether to launch a server and which type of server to launch. A learn-and-deploy algorithm framework is proposed which contains two complementary stages. In the first stage, an effective offline bi-objective optimization algorithm is proposed to learn a set of policies, which provides helpful trade-off information for a decision-maker to choose a preferred policy a posteriori. In terms of the system status, a policy decides whether to launch a server according to a state-action table and which server to launch using a server priority sequence. In the second stage, a computationally efficient policy deployment method is proposed to search the corresponding action in the selected policy based on the current system status and apply it to the real-time system. Experimental studies over a large number of random and real instances have been conducted to validate the effectiveness of the proposed bilevel model and algorithm. Compared to the most recent existing method, the performance of the proposed approach can at most achieve an 80% improvement on power consumption and 20% improvement on response time.","1558-2183","","10.1109/TPDS.2022.3196475","Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101200003); National Natural Science Foundation of China(grant numbers:62072234,61732006,61832004); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20181288); China Postdoctoral Science Foundation(grant numbers:2015M571751); Collaborative Innovation Center of Wireless Communications Technology; Spanish Ministry of Science, Innovation, and Universities(grant numbers:RTI2018-094940-B-I00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857657","Cloud computing;bursty and stochastic requests;heterogeneous servers;multiobjective optimization;learn-and-deploy;Markov decision process;dynamic programming","Servers;Cloud computing;Power demand;Optimization;Time factors;Analytical models;Real-time systems","cloud computing;decision making;learning (artificial intelligence);Markov processes;optimisation;scheduling;stochastic processes","bilevel model;power consumption;heterogeneous cloud servers;dynamic allocation;bursty requests;heterogeneous servers;uncertain setup times;expected response time;service providers;launched servers;balanced objectives;Markov decision process;real-time bilevel decision-making model;unallocated requests;effective offline bi-objective optimization algorithm;server priority sequence;computationally efficient policy deployment method;real-time system;learn-and-deploy algorithm framework","","","",38.0,"IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Elastic and Reliable Bandwidth Reservation Based on Distributed Traffic Monitoring and Control","X. Zhang; T. Wang","Shandong Provincial Key Laboratory of Computer Networks, Qilu University of Technology (Shandong Academy of Sciences), Jinan, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","22 Aug 2022",2022,33.0,12.0,4563,4580,"Bandwidth reservation can effectively improve the service quality for data transfers because of dedicated network resources. However, it is difficult to achieve a desired tradeoff between resource utilization and reliable bandwidth guarantees for data transfers with time-varying traffic. In this article, we study a novel bandwidth reservation solution based on distributed traffic monitoring and control for applications that require reliable bandwidth guarantees. In the proposed solution, designated bandwidth is allocated for an application in advance according to its maximum traffic peak, and idle reserved bandwidth resources are dynamically shared according to regular traffic. Dynamic resource sharing evidently improves resource utilization and effectively eliminates the potential congestion caused by sudden traffic bursts. To ensure that the congestion that occurs occasionally can dissipate rapidly, our solution monitors and manages traffic by a distributed monitoring and control strategy. Hence, we study a delay-constrained and proxy-assisted traffic monitoring structure construction problem and propose an algorithm to solve it. The proposed algorithm can also be used to build a delay-constrained traffic control structure. In addition to the above algorithm, we propose a dynamic traffic control algorithm that can achieve a desirable tradeoff between resource utilization and congestion avoidance capability.","1558-2183","","10.1109/TPDS.2022.3196840","National Natural Science Foundation of China(grant numbers:92067108); Shandong Provincial Natural Science Foundation of China(grant numbers:ZR2020MF057); Basic Research Enhancement Plan of Qilu University of Technology(grant numbers:2021JC03001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851561","Bandwidth reservation;resource utilization;traffic monitoring;traffic control;industrial internet","Bandwidth;Monitoring;Reliability;Resource management;Delays;Heuristic algorithms;Dynamic scheduling","bandwidth allocation;Internet;quality of service;resource allocation;telecommunication congestion control;telecommunication traffic","delay-constrained traffic control structure;dynamic traffic control algorithm;resource utilization;distributed traffic monitoring;data transfers;dedicated network resources;desired tradeoff;reliable bandwidth guarantees;time-varying traffic;novel bandwidth reservation solution;designated bandwidth;maximum traffic peak;idle reserved bandwidth resources;regular traffic;dynamic resource sharing;sudden traffic bursts;distributed monitoring;control strategy;proxy-assisted traffic monitoring structure construction problem","",14.0,"",46.0,"IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"Joint Application Placement and Request Routing Optimization for Dynamic Edge Computing Service Management","R. Li; Z. Zhou; X. Zhang; X. Chen","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","2 Sep 2022",2022,33.0,12.0,4581,4596,"As mobile edge computing (MEC) hosting applications at the network edge with limited capacities, service providers are facing the new challenge of how to make full use of the scarce edge resources to maximize the system performance. Accommodating this challenge requires careful application placement and request routing to coordinate diverse MEC nodes. However, frequent application re-placement would greatly increase the system reconfiguration cost, indicating a performance-cost trade-off. In response, in this paper, we study the problem of joint optimization on application placement and request routing to maximize the system performance, under a long-term budget of the application reconfiguration cost. Solving this problem is non-trivial since the long-term budget is coupled with the future system states (e.g., user request arrivals) that are typically unpredictable. To address this challenge, we first advocate an approximated dynamic optimization framework to decompose the long-term optimization problem into a series of one-shot problems which do not require the future system states. Moreover, since the decomposed problem is a mixed integer linear program (MILP) which is proven to be NP-hard, we then devise an efficient dependent rounding based approximation algorithm, which can achieve the near-optimal performance in a fast manner. Both rigorous theoretical analysis and extensive trace-driven evaluations demonstrate the proposed framework can achieve superior performance gain over existing schemes.","1558-2183","","10.1109/TPDS.2022.3195205","National Natural Science Foundation of China(grant numbers:61972432,U20A20159,62172455); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021B151520008); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Pearl River Talent Program(grant numbers:2017GC010465); National Natural Science Foundation of China(grant numbers:62172454); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515011912); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847031","Application placement;approximated dynamic optimization;request routing;edge computing","Servers;Cloud computing;Optimization;Routing;Costs;Heuristic algorithms;Delays","approximation theory;cloud computing;computational complexity;integer programming;linear programming;mobile computing;resource allocation","joint application placement;request routing optimization;dynamic edge computing service management;network edge;service providers;edge resources;system performance;MEC nodes;system reconfiguration cost;performance-cost trade-off;joint optimization;long-term budget;application reconfiguration cost;long-term optimization problem;one-shot problems;near-optimal performance;dependent rounding based approximation algorithm;approximated dynamic optimization;mobile edge computing;mixed integer linear program;MILP;NP-hard","","","",51.0,"IEEE","1 Aug 2022","","","IEEE","IEEE Journals"
"Fairness-Aware VNF Sharing and Rate Coordination for High Efficient Service Scheduling","B. Yi; X. Wang; M. Huang; S. K. Das; K. Li","College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; Department of Computer Science, Missouri University of Science and Technology, Rolla, MO, USA; Department of Computer Science, State University of New York, New York, USA","IEEE Transactions on Parallel and Distributed Systems","1 Sep 2022",2022,33.0,12.0,4597,4611,"Network service provisioning becomes flexible and programmable with the help of Network Function Virfitualization (NFV), since NFV abstracts various service functions into software components called Virtual Network Function (VNF) and VNFs can be flexibly and quickly composed to form new services. It is commonly known that sharing the same VNF among different services can improve the resource utilization. However, we should be aware that such sharing also leads to serious resource preemption. In addition, VNF sharing aggravates the generation of the performance bottleneck, which then causes the rate mismatch problem between the upstream and downstream VNFs belonging to the same service chain. In this article, we propose a dynamic and flexible algorithm to jointly address the VNF sharing resource allocation and the rate coordination between the upstream and downstream VNFs. Specifically, 1) the VNFs are shared among different service chains with a fairness factor considered for the purpose of reducing the resource preemption probability and improving the resource utilization; 2) the backpressure indicator of each VNF is defined to judge its pressure condition, based on which we can dynamically adjust the processing rates between it and its downstream or upstream VNFs by maximizing the idle resource utilization. The experimental results indicate that the proposed algorithm outperforms the other methods in terms of the average delay, the flow completion time, the throughput and the backlog, etc. Meanwhile, the proposed algorithm achieves more stable performance than the other methods.","1558-2183","","10.1109/TPDS.2022.3199392","National Key R&D Program of China(grant numbers:2019YFB1802800); National Natural Science Foundation of China(grant numbers:62002055,62032013,61872073); China Postdoctoral Science Foundation(grant numbers:2020M680972); Postdoctoral Research Fund of Northeastern University of China(grant numbers:20200103); Fundamental Research Funds for the Central Universities(grant numbers:N2016012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858330","Network function virtualization;performance bottleneck;rate coordination;service chain;traffic scheduling","Resource management;Noise measurement;Heuristic algorithms;Network function virtualization;Software;Computer science;Mathematical models","computer networks;probability;resource allocation;scheduling;virtualisation","software components;Virtual Network Function;rate mismatch problem;upstream VNFs;downstream VNFs;dynamic algorithm;flexible algorithm;resource allocation;rate coordination;service chains;fairness factor;idle resource utilization;fairness-aware VNF sharing;high efficient service scheduling;network service provisioning;Network Function Virfitualization;NFV;resource utilization improvement;resource preemption probability reduction;backpressure indicator","",2.0,"",42.0,"IEEE","17 Aug 2022","","","IEEE","IEEE Journals"
"BARM: A Batch-Aware Resource Manager for Boosting Multiple Neural Networks Inference on GPUs With Memory Oversubscription","Z. -W. Qiu; K. -S. Liu; Y. -S. Chen","Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","IEEE Transactions on Parallel and Distributed Systems","30 Aug 2022",2022,33.0,12.0,4612,4624,"Modern intelligent devices usually execute multiple neural networks to improve service quality. However, system performance degrades significantly when the working set exceeds the physical memory capability, a phenomenon called memory oversubscription. To support the execution of multiple independent neural networks with limited physical memory, this article explores resource management in GPUs with unified virtual memory and demand paging. We first analyze the relationship between the simultaneous execution of multiple neural networks from streaming multiprocessors (SM) assignment and page fault overhead from memory thrashing. To boost performance by reducing the page fault penalty, we propose a batch-aware resource management approach, BARM, including (1) batch-aware SM resource allocation to increase the batch size and (2) thrashing-preventing memory allocation to eliminate run-time thrashing. The performance of the proposed method was evaluated using a series of workloads, and response latency is reduced significantly over the state-of-the-art page fault prefetcher and batch-aware TLP management. The proposed framework was also implemented on the real platform and evaluated by a case study, and impressive results were obtained.","1558-2183","","10.1109/TPDS.2022.3199806","Ministry of Science and Technology, Taiwan(grant numbers:MOST 107-2221-E-011-028-MY3,MOST 110-2628-E-011-002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861695","Memory oversubscription;memory management;memory thrashing","Graphics processing units;Neural networks;Memory management;Resource management;Kernel;Registers;Random access memory","cache storage;neural nets;paged storage;resource allocation;storage management","BARM;batch-aware resource manager;boosting multiple neural networks inference;GPUs;modern intelligent devices;physical memory capability;memory oversubscription;neural networks;unified virtual memory;demand paging;simultaneous execution;memory thrashing;page fault penalty;batch-aware resource management approach;batch-aware TLP management;page fault prefetcher;run-time thrashing elimination;streaming multiprocessors assignment;SM;page fault overhead;response latency reduction","","","",51.0,"IEEE","18 Aug 2022","","","IEEE","IEEE Journals"
"PS+: A Simple yet Effective Framework for Fast Training on Parameter Server","A. -L. Jin; W. Xu; S. Guo; B. Hu; K. Yeung","University of Hong Kong, Hong Kong, China; Hong Kong Polytechnic University, Hong Kong, China; Hong Kong Polytechnic University, Hong Kong, China; Zhejiang University, Hangzhou, China; University of Hong Kong, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","1 Sep 2022",2022,33.0,12.0,4625,4637,"In distributed training, workers collaboratively refine the global model parameters by pushing their updates to the Parameter Server and pulling fresher parameters for the next iteration. This introduces high communication costs for training at scale, and incurs unproductive waiting time for workers. To minimize the waiting time, existing approaches overlap communication and computation for deep neural networks. Yet, these techniques not only require the layer-by-layer model structures, but also need significant efforts in runtime profiling and hyperparameter tuning. To make the overlapping optimization simple and generic, in this article, we propose a new Parameter Server framework. Our solution decouples the dependency between push and pull operations, and allows workers to eagerly pull the global parameters. This way, both push and pull operations can be easily overlapped with computations. Besides, the overlapping manner offers a different way to address the straggler problem, where the stale updates greatly retard the training process. In the new framework, with adequate information available to workers, they can explicitly modulate the learning rates for their updates. Thus, the global parameters can be less compromised by stale updates. We implement a prototype system in PyTorch and demonstrate its effectiveness on both CPU/GPU clusters. Experimental results show that our prototype saves up to 54% less time for each iteration and up to 37% fewer iterations for model convergence, achieving up to 2.86× speedup over widely-used synchronization schemes.","1558-2183","","10.1109/TPDS.2022.3200518","Hong Kong RGC(grant numbers:PolyU 15222621,RIF R5060-19,GRF 152221/19E,GRF 152203/20E,GRF 152244/21E); National Natural Science Foundation of China(grant numbers:61872310); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2021B0101400003); Shenzhen Science and Technology Innovation Commission(grant numbers:R2020A045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864063","Machine learning;distributed training;parameter server","Training;Servers;Synchronization;Computational modeling;Hardware;Data models;Computational efficiency","deep learning (artificial intelligence);iterative methods;optimisation;scheduling;synchronisation","synchronization schemes;CPU-GPU clusters;PyTorch system;learning rates;straggler problem;collaborative workers;distributed training;iteration;parameter server framework;overlapping optimization;runtime profiling;layer-by-layer model structures;deep neural networks","","","",46.0,"IEEE","22 Aug 2022","","","IEEE","IEEE Journals"
"A Parallel Secure Flow Control Framework for Private Data Sharing in Mobile Edge Cloud","Q. Huang; L. Chen; C. Wang","School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","5 Sep 2022",2022,33.0,12.0,4638,4653,"Nowadays, the rapid development of edge computing is accelerating the data sharing between cloud computing platforms and mobile users. These data often contain sensitive information, which faces severe leakage risks not only from the semi-trusted cloud servers but also from the malicious senders in the organizations. Fortunately, access control encryption (ACE) has been utilized to secure the data with access control policies, in which a sanitizer (e.g., the edge node) is employed to check all the communications between the sender and receiver, and drop illegal ciphertexts according to the access control policy. However, previous schemes have some limitations in mobile edge cloud, e.g., the sender's attributes are not strictly authenticated in the attribute-based access control policy, or the sanitization time is the bottleneck of fast data sharing. To this end, we introduce PSFlow, a parallel secure flow control framework for private data sharing in mobile edge cloud. First, we propose an attribute-based outsourced ACE (AOACE) scheme, which achieves secure fine-grained data read and write control, and reduces the computational cost of the sender and receiver with outsourced computations in edge nodes. Then, we propose a concrete construction of PSFlow from AOACE, and accelerate the sanitization process with parallel computing. Specifically, PSFlow parallelizes the sanitization operations with a multi-server model in each edge node, and optimizes the sanitization efficiency in each edge server by constructing a shared pool from the attribute universe. The experimental results show that PSFlow is more efficient and practical than previous schemes in mobile edge cloud.","1558-2183","","10.1109/TPDS.2022.3200959","National Natural Science Foundation of China(grant numbers:U1736212,61572080); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9865141","Access control encryption;data sharing;sanitization;mobile edge cloud;parallel computing","Receivers;Encryption;Access control;Cloud computing;Servers;Mobile handsets;Costs","authorisation;cloud computing;cryptography;data privacy;outsourcing","parallel secure flow control framework;private data sharing;mobile edge cloud;edge computing;cloud computing platforms;mobile users;cloud servers;access control encryption;edge node;receiver;sender;attribute-based access control policy;fast data sharing;PSFlow;attribute-based outsourced ACE scheme;fine-grained data;outsourced computations;parallel computing;edge server","",2.0,"",37.0,"IEEE","23 Aug 2022","","","IEEE","IEEE Journals"
"Tenant-Grained Request Scheduling in Software-Defined Cloud Computing","H. Tu; G. Zhao; H. Xu; X. Fang","Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu, China; Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu, China; Suzhou Institute for Advanced Research, University of Science and Technology of China, Suzhou, Jiangsu, China; College of Computer Science and Engineering, Anhui University of Science and Technology, Huainan, Anhui, China","IEEE Transactions on Parallel and Distributed Systems","5 Sep 2022",2022,33.0,12.0,4654,4671,"Cloud providers host various services for tenants’ requests (e.g., software-as-a-service) and seek to serve as many requests as possible for revenue maximization. Considering a large number of requests, the previous works on fine-grained request scheduling may lead to poor system scalability (or high schedule overhead) and break tenant isolation. In this article, we design a tenant-grained request scheduling framework to conquer the above two disadvantages. We formulate the tenant-grained request scheduling problem as an integer linear programming and prove its NP-hardness. We consider two complementary cases: the offline case (where we know all request demands in advance), and the online case (where we have to make immediate scheduling decisions for requests arriving online). A normalization-based algorithm with an approximation factor of $ {O}(1)$O(1) is proposed to solve the offline problem and a primal-dual-based algorithm with a competitive ratio of $[(1-\epsilon), {O}(\log 3\cdot n+\log (1/\epsilon))]$[(1-ε),O(log3·n+log(1/ε))] is designed for the online scenario, where $\epsilon \in (0,1)$ε∈(0,1) and $n$n is the number of racks in the cloud. We also discuss how to integrate our proposed algorithms with the previous (fine-grained) request scheduling mechanism. Extensive simulation and experiment results show that our algorithms can obtain significant performance gains, e.g., the online algorithm reduces the scheduler's overhead more than $90\%$90% and achieves tenant isolation, while obtaining similar network performance (e.g., throughput) compared with the fine-grained request scheduling methods.","1558-2183","","10.1109/TPDS.2022.3199031","National Natural Science Foundation of China(grant numbers:62132019,62102392); Open Research of Projects of Zhejiang Lab(grant numbers:2022QA0AB04); Natural Science Foundation of Jiangsu Province(grant numbers:BK20210121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857998","Software-defined cloud;cloud computing;request scheduling;scalability;approximation","Servers;Job shop scheduling;Cloud computing;Scalability;Approximation algorithms;Scheduling algorithms;Schedules","approximation theory;cloud computing;computational complexity;integer programming;linear programming;scheduling","software-defined cloud computing;software-as-a-service;system scalability;request demands;normalization-based algorithm;primal-dual-based algorithm;tenant isolation;schedule overhead;cloud providers;tenant-grained request scheduling;revenue maximization;integer linear programming;NP-hardness;approximation factor;network performance","","","",54.0,"IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Theoretical Analysis of an Adaptive Periodic Multi Installment Scheduling With Result Retrieval for SAR Image Processing","G. M. Chinnappan; B. Veeravalli","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Parallel and Distributed Systems","5 Sep 2022",2022,33.0,12.0,4672,4683,"Processing a large-scale Synthetic Aperture Radar (SAR) image dataset on a distributed computing infrastructure poses a challenging problem. Large-scale load distribution strategies like multi-installment scheduling (MIS) assume that the size of the result is negligible compared to the input workloads and hence ignore it in their design. Similarly, numerical methods like particle swarm optimization and their variants are not practical for real-time applications, given their run-time complexities. As both the results retrieval and completion time are crucial for SAR image data processing, in this article, we attempt to provide a thorough theoretical analysis of an adaptive MIS that includes the result retrieval phase. We use the periodic nature of the internal installments to keep the strategy simple and fine-tune the last installment to avoid any idle times in the processors. We derive a closed-form solution for the load fractions and hence, the overall processing time, schedule feasibility criteria, and certain other properties that lead to adaptive scheduling. Finally, we validate our theoretical findings through rigorous simulation studies using a loosely connected virtual machines (VMs) topology for the SAR dataset.","1558-2183","","10.1109/TPDS.2022.3194542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844291","SAR image;multi-installment scheduling;load distribution;front-end processors;heterogeneous cluster","Program processors;Radar polarimetry;Processor scheduling;Load modeling;Distributed databases;Computational modeling;Schedules","computational complexity;particle swarm optimisation;radar imaging;synthetic aperture radar;virtual machines","theoretical analysis;adaptive periodic multiinstallment scheduling;SAR image processing;distributed computing infrastructure;large-scale load distribution strategies;input workloads;numerical methods;particle swarm optimization;real-time applications;run-time complexities;results retrieval;completion time;SAR image data processing;adaptive MIS;result retrieval phase;periodic nature;internal installments;load fractions;processing time;adaptive scheduling;SAR dataset;large-scale synthetic aperture radar image dataset;result retrieval;multiinstallment scheduling;overall processing time;schedule feasibility criteria;loosely connected virtual machines topology","","","",18.0,"IEEE","28 Jul 2022","","","IEEE","IEEE Journals"
"Robustness of Subsystem Reliability of $k$k-Ary $n$n-Cube Networks Under Probabilistic Fault Model","X. Liu; S. Zhou; S. -Y. Hsieh; H. Zhang","Center for Applied Mathematics of Fujian Province and School of Mathematics and Statistics, Fujian Normal University, Fuzhou, China; Center for Applied Mathematics of Fujian Province and School of Mathematics and Statistics, Fujian Normal University, Fuzhou, China; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Center for Applied Mathematics of Fujian Province and School of Mathematics and Statistics, Fujian Normal University, Fuzhou, China","IEEE Transactions on Parallel and Distributed Systems","16 Sep 2022",2022,33.0,12.0,4684,4693,"With the emergence of the Big Data era, as multiprocessor systems consisting of multiple processors play a vital role in big data analytics, we are prompted to explore the qualitative and quantitative metric to characterize the reliability of the systems. As the size of the multiprocessor systems grows, the probability of the occurrence of failing processors increases. One metric of the macroscopic reliability of a system is the measure of the collective effect when its subsystems are out of function. The subsystem reliability of a system is the quantitative metric that a fault-free subsystem of specific size is operational as before with the occurrence of individual faults. Although some networks have the same order and similar topologies, there are differences in their subsystem reliabilities. In this work, we focus on the comparison of two distinct topologies of $k$k-ary $n$n-cube networks with the same order and calculate the robustness of reliability bounds of $k$k-ary $n$n-cube networks. We analytically show that the subsystem reliability is negatively correlated with the dimension $n$n, even if two subsystems of $Q_{n}^{k}$Qnk are of the same order. That is, the smaller $n$n is, the larger subsystem reliability of $Q_{n}^{k}$Qnk will be. This work provides a theoretical methodology to choose the more dependable topology of $k$k-ary $n$n-cube networks with the same order. Finally, we apply some numerical simulations to validate the results we established.","1558-2183","","10.1109/TPDS.2022.3199251","National Natural Science Foundation of China(grant numbers:61977016,61572010); Natural Science Foundation of Fujian Province(grant numbers:2020J01164,2017J01738); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858633","  $k$   k     -ary   $n$   n     -cube networks;subsystem reliability;robustness;principle of inclusion-exclusion (PIE)","Reliability;Topology;Probabilistic logic;Computer network reliability;Robustness;Program processors;Multiprocessing systems","Big Data;data analysis;fault diagnosis;multiprocessing systems;parallel processing;probability;reliability","cube networks;probabilistic fault model;Big Data era;multiprocessor systems;big data analytics;macroscopic reliability;fault-free subsystem;ary;reliability bounds;larger subsystem reliability","","","",32.0,"IEEE","17 Aug 2022","","","IEEE","IEEE Journals"
"Optimizing DNN Compilation for Distributed Training With Joint OP and Tensor Fusion","X. Yi; S. Zhang; L. Diao; C. Wu; Z. Zheng; S. Fan; S. Wang; J. Yang; W. Lin","Computer Science, University of Hong Kong, Hong Kong, Hong Kong; Computer Science, University of Hong Kong, Hong Kong, Hong Kong; Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China; Computer Science, University of Hong Kong, Hong Kong, Hong Kong; Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China; Compute Arch, NVIDIA Corp, Beijing, China; Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China","IEEE Transactions on Parallel and Distributed Systems","12 Sep 2022",2022,33.0,12.0,4694,4706,"This article proposes DisCo, an automatic deep learning compilation module for data-parallel distributed training. Unlike most deep learning compilers that focus on training or inference on a single device, DisCo optimizes a DNN model for distributed training over multiple GPU machines. Existing single-device compilation strategies do not work well in distributed training, due mainly to communication inefficiency that they incur. DisCo generates optimized, joint computation operator and communication tensor fusion strategies to enable highly efficient distributed training. A GNN-based simulator is built to effectively estimate per-iteration training time achieved by operator/tensor fusion candidates. A backtracking search algorithm is driven by the simulator, navigating efficiently in the large strategy space to identify good operator/tensor fusion strategies that minimize distributed training time. We compare DisCo with existing DL fusion schemes and show that it achieves good training speed-up close to the ideal, full computation-communication overlap case.","1558-2183","","10.1109/TPDS.2022.3201531","Alibaba Group; Hong Kong RGC contracts HKU(grant numbers:17204619,17208920); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9866830","Distributed systems;machine learning","Training;Optimization;Tensors;Computational modeling;Codes;Performance evaluation;Hardware","backtracking;deep learning (artificial intelligence);gradient methods;graphics processing units;parallel processing;program compilers;tensors","DNN compilation;DisCo;automatic deep learning compilation module;single-device compilation strategies;joint computation operator;communication tensor fusion;highly efficient distributed training;per-iteration training time;DL fusion schemes;multiple GPU machines;data-parallel distributed training;backtracking search algorithm;joint OP","","","",59.0,"IEEE","25 Aug 2022","","","IEEE","IEEE Journals"
"PayDebt: Reduce Buffer Occupancy Under Bursty Traffic on Large Clusters","K. Liu; C. Tian; Q. Wang; Y. Chen; B. Tian; W. Sun; K. Meng; L. Yan; L. Han; J. Fu; W. Dou; G. Chen","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Huawei Technologies Co., Ltd, Shenzhen, China; Huawei Technologies Co., Ltd, Shenzhen, China; Huawei Technologies Co., Ltd, Shenzhen, China; Huawei Technologies Co., Ltd, Shenzhen, China; Huawei Technologies Co., Ltd, Shenzhen, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","13 Sep 2022",2022,33.0,12.0,4707,4722,"The average/tail Flow Completion Times (FCTs) are critical to many datacenter applications. Congestion control plays a central role in optimizing FCT. Inappropriate congestion control can exacerbate buffer occupancy, thus hurting the flow performance. Our observations are that current approaches are too aggressive in injecting packets into underlying networks. Instead of handling buffer explosion afterward, we reduce buffer occupancy in the first place. We propose PayDebt, a novel and readily-deployable proactive congestion control protocol. At its heart, a debt mechanism provides bandwidth coordination between the already-buffered and the forthcoming packets. We evaluate PayDebt both in a testbed and large-scale simulations. The buffer occupancy can be decreased by up to 8.0×-35.9× compared to DCQCN and Homa.","1558-2183","","10.1109/TPDS.2022.3202504","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101390001); National Natural Science Foundation of China(grant numbers:92067206,62072228,61972222); Fundamental Research Funds for the Central Universities; Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869721","Token;proactive congestion control;datacenter networks","Protocols;Receivers;Delays;Bandwidth;Topology;Network topology;Channel allocation","computer centres;computer networks;telecommunication congestion control;telecommunication traffic;transport protocols","datacenter applications;inappropriate congestion control;flow performance;buffer explosion;reduce buffer occupancy;PayDebt;readily-deployable proactive congestion control protocol;bursty traffic","","","",70.0,"IEEE","29 Aug 2022","","","IEEE","IEEE Journals"
"Parallel Dynamic Sparse Approximate Inverse Preconditioning Algorithm on GPU","J. Gao; X. Chu; X. Wu; J. Wang; G. He","School of Computer and Electronic Information, Nanjing Normal University, Nanjing, China; School of Computer and Electronic Information, Nanjing Normal University, Nanjing, China; School of Computer and Electronic Information, Nanjing Normal University, Nanjing, China; Futurewei Technologies, Santa Clara, CA, USA; Zhijiang College, Zhejiang University of Technology, Hangzhou, China","IEEE Transactions on Parallel and Distributed Systems","13 Sep 2022",2022,33.0,12.0,4723,4737,"The dynamic sparse approximate inverse (SPAI) preconditioner has proven to be effective in accelerating the convergence of iterative methods for large linear systems. Recently, accelerating it on graphics processing unit (GPU) has attracted considerable attention due to the fact that the cost of constructing the preconditioner is high. However, the existing parallel dynamic SPAI preconditioning algorithms on GPU are usually ineffective because of the out-of-memory error for large matrices. This motivates us to investigate how to accelerate the construction of dynamic SPAI preconditioners on GPU. In this article, we propose an efficient dynamic SPAI preconditioning algorithm on GPU, called GDSPAI. For our proposed GDSPAI, there are the following novelties: (1) a well-known dynamic SPAI preconditioning algorithm is substantially modified to address the main challenges of parallelization on GPU, (2) a parallel framework of constructing the dynamic SPAI preconditioner on GPU is presented on the basis of the modified dynamic SPAI preconditioning algorithm; and (3) each component of the preconditioner is computed in parallel inside a group of threads. Experimental results show that the proposed GDSPAI is effective for large matrices, and outperforms the popular preconditioning algorithms in three public libraries, as well as a recent parallel static SPAI preconditioning algorithm.","1558-2183","","10.1109/TPDS.2022.3202214","National Natural Science Foundation of China(grant numbers:61872422); Natural Science Foundation of Zhejiang Province(grant numbers:LY19F020028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869300","Sparse approximate inverse;preconditioning;dynamic;CUDA;GPU","Heuristic algorithms;Graphics processing units;Sparse matrices;Approximation algorithms;Message systems;Symmetric matrices;Libraries","approximation theory;conjugate gradient methods;convergence of numerical methods;graphics processing units;iterative methods;linear systems;matrix algebra;parallel algorithms;sparse matrices","recent parallel static SPAI preconditioning algorithm;parallel dynamic sparse approximate inverse preconditioning algorithm;GPU;dynamic sparse approximate inverse preconditioner;existing parallel dynamic SPAI preconditioning algorithms;dynamic SPAI preconditioner;efficient dynamic SPAI preconditioning algorithm;parallel framework;modified dynamic SPAI preconditioning algorithm;popular preconditioning algorithms","","","",45.0,"IEEE","29 Aug 2022","","","IEEE","IEEE Journals"
"COIN: A Container Workload Prediction Model Focusing on Common and Individual Changes in Workloads","Z. Ding; B. Feng; C. Jiang","Department of Computer Science and Technology, Tongji University, Shanghai, China; Department of Computer Science and Technology, Tongji University, Shanghai, China; Department of Computer Science and Technology, Tongji University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","13 Sep 2022",2022,33.0,12.0,4738,4751,"Recently, containers have become the primary deployment form for cloud applications. Predicting container workload accurately is critical to ensure the quality of service (QoS) and cost-efficiency of the applications and meet service level agreements (SLAs) with users. However, facing multiple challenges, including model unavailability due to insufficient data, model maladaptation due to dynamic workload changes, and model non-generalization due to changeable workload patterns in container workload prediction, existing methods have not yet provided a united and effective solution. To this end, we propose a novel integrated forecasting model named COIN that combines COmmon and INdividual changes in container workloads to ensure the availability, adaptivity, and generality of the prediction model based on transfer learning and online learning. Besides, we present a container similarity calculation algorithm for real cloud scenarios, which combines the static and dynamic information of containers and comprehensively depicts the similarity between containers. Through experiments based on two public datasets, the COIN model achieves a higher accuracy than existing state-of-the-art solutions, demonstrating the effectiveness and robustness of our proposed model, which provides a new solution to container workload prediction.","1558-2183","","10.1109/TPDS.2022.3202833","National Key Research and Development Program of China(grant numbers:2019YFB1704102); China National Scientific Seafloor Observatory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870561","Cloud Computing;container;workload prediction;container similarity;online learning;transfer learning;integrated model","Containers;Predictive models;Data models;Adaptation models;Forecasting;Cloud computing;Load modeling","cloud computing;learning (artificial intelligence);quality of service","integrated forecasting model;container similarity calculation algorithm;COIN model;container workload prediction model focusing;cloud applications;service level agreements;model unavailability;model maladaptation;dynamic workload changes;model nongeneralization;changeable workload patterns;COmmon and INdividual changes;quality of service;QoS;SLA","",1.0,"",50.0,"IEEE","30 Aug 2022","","","IEEE","IEEE Journals"
"Increasing the Efficiency of Massively Parallel Sparse Matrix-Matrix Multiplication in First-Principles Calculation on the New-Generation Sunway Supercomputer","X. Chen; Y. Gao; H. Shang; F. Li; Z. Xu; X. Liu; D. Chen","National Research Center of Parallel Computer Engineering and Technology, Beijing, China; National Supercomputer Center in Tianjin, Tianjin, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Research Center of Parallel Computer Engineering and Technology, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Research Center of Parallel Computer Engineering and Technology, Beijing, China; National Research Center of Parallel Computer Engineering and Technology, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","13 Sep 2022",2022,33.0,12.0,4752,4766,"The first-principles approach based on density-functional theory (DFT)/density-functional perturbation theory (DFPT) is widely used in calculations of the systems’ ground state energy, response properties (e.g., polarizability, phonon dispersions) and is playing an increasingly important role in chemistry, physics and materials science. For the large-scale calculations, the computation of the density matrix/response density matrix in DFT/DFPT has become the main performance bottleneck. One of the solutions is using the linear scaling method to get the density matrix and response density matrix. Here a massively parallel medium sparse matrix-matrix multiplication algorithm is designed for first-principle calculations and implemented on the new-generation Sunway supercomputer. Experiments show that the proposed method has obvious performance advantages compared to the original parallel version under moderate sparsity. The computing cores scale to 3,900,000 with strong scalability of 77.3$\%$%.","1558-2183","","10.1109/TPDS.2022.3202518","National Natural Science Foundation of China(grant numbers:22003073); National Science and Technology Major(grant numbers:2017-I-0004-0004); National Key R&D Program of China(grant numbers:2019YFA0709402); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869723","Density matrix;density-functional theory;new-generation sunway supercomputer;parallel sparse matrix-matrix multiplication","Sparse matrices;Purification;Supercomputers;Discrete Fourier transforms;Tensors;Table lookup;Perturbation methods","ab initio calculations;density functional theory;ground states;matrix multiplication;perturbation theory;phonon dispersion relations;polarisability;sparse matrices","new-generation Sunway supercomputer;systems;response properties;large-scale calculations;linear scaling method;massively parallel medium sparse matrix-matrix multiplication algorithm;first-principle calculations;massively parallel sparse matrix-matrix multiplication;first-principles calculation","",1.0,"",58.0,"IEEE","29 Aug 2022","","","IEEE","IEEE Journals"
"Adaptive Federated Deep Reinforcement Learning for Proactive Content Caching in Edge Computing","D. Qiao; S. Guo; D. Liu; S. Long; P. Zhou; Z. Li","Key Laboratory of Dependable Service Computing in Cyber-Physical-Society (Ministry of Education), College of computer science, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber-Physical-Society (Ministry of Education), College of computer science, Chongqing University, Chongqing, China; Key Laboratory of Biorheological Science and Technology, Ministry of Education, College of Bioengineering, Chongqing University, Chongqing, China; National & Local Joint Engineering Research Center of Network Security Detection and Protection Technology, Guangdong Provincial Key Laboratory of Data Security and Privacy Protection College of Information Science and Technology, Jinan University, Guangzhou, China; Key Laboratory of Dependable Service Computing in Cyber-Physical-Society (Ministry of Education), College of computer science, Chongqing University, Chongqing, China; National & Local Joint Engineering Research Center of Network Security Detection and Protection Technology, Guangdong Provincial Key Laboratory of Data Security and Privacy Protection College of Information Science and Technology, Jinan University, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","13 Sep 2022",2022,33.0,12.0,4767,4782,"With the aggravation of data explosion and backhaul loads on 5 G edge network, it is difficult for traditional centralized cloud to meet the low latency requirements for content access. The federated learning (FL)-based proactive content caching (FPC) can alleviate the matter by placing content in local cache to achieve fast and repetitive data access while protecting the users’ privacy. However, due to the non-independent and identically distributed (Non-IID) data across the clients and limited edge resources, it is unrealistic for FL to aggregate all participated devices in parallel for model update and adopt the fixed iteration frequency in local training process. To address this issue, we propose a distributed resources-efficient FPC policy to improve the content caching efficiency and reduce the resources consumption. Through theoretical analysis, we first formulate the FPC problem into a stacked autoencoders (SAE) model loss minimization problem while satisfying resources constraint. We then propose an adaptive FPC (AFPC) algorithm combined deep reinforcement learning (DRL) consisting of two mechanisms of client selection and local iterations number decision. Next, we show that when training data are Non-IID, aggregating the model parameters of all participated devices may be not an optimal strategy to improve the FL-based content caching efficiency, and it is more meaningful to adopt adaptive local iteration frequency when resources are limited. Finally, experimental results in three real datasets demonstrate that AFPC can effectively improve cache efficiency up to 38.4$\%$% and 6.84$\%$%, and save resources up to 47.4$\%$% and 35.6$\%$%, respectively, compared with traditional multi-armed bandit (MAB)-based and FL-based algorithms.","1558-2183","","10.1109/TPDS.2022.3201983","Natural Science Key Foundation of Chongqing(grant numbers:cstc2020jcyj-zdxmX0026); National Key Research and Development Program of China(grant numbers:2021YFB3101200); National Natural Science Foundation of China(grant numbers:62032020,62172350); Hunan Science and Technology Planning Project(grant numbers:2019RS3019); Hunan Province Department of Education(grant numbers:21B0120); Natural Science Foundation of Hunan(grant numbers:2021JJ40544); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9868114","Content caching;deep reinforcement learning;edge computing;federated learning;resource constraint","Servers;Data models;Adaptation models;Internet of Things;Reinforcement learning;Feature extraction;Delays","cache storage;cloud computing;iterative methods;learning (artificial intelligence);minimisation;optimisation","local iterations number decision;training data;NonIID;participated devices;FL-based content caching efficiency;adaptive local iteration frequency;cache efficiency;FL-based algorithms;adaptive federated deep reinforcement learning;edge computing;data explosion;backhaul loads;5 G edge network;traditional centralized cloud;low latency requirements;content access;federated learning-based proactive content caching;local cache;data access;limited edge resources;model update;fixed iteration frequency;local training process;distributed resources-efficient FPC policy;resources consumption;FPC problem;stacked autoencoders model loss minimization problem;satisfying resources constraint;adaptive FPC;client selection","",1.0,"",50.0,"IEEE","26 Aug 2022","","","IEEE","IEEE Journals"
"A Decentralized Federated Learning Framework via Committee Mechanism With Convergence Guarantee","C. Che; X. Li; C. Chen; X. He; Z. Zheng","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Software Engineering, Sun Yat-sen University, Zhuhai, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","29 Sep 2022",2022,33.0,12.0,4783,4800,"Federated learning allows multiple participants to collaboratively train an efficient model without exposing data privacy. However, this distributed machine learning training method is prone to attacks from Byzantine clients, which interfere with the training of the global model by modifying the model or uploading the false gradient. In this article, we propose a novel serverless federated learning framework Committee Mechanism based Federated Learning (CMFL), which can ensure the robustness of the algorithm with convergence guarantee. In CMFL, a committee system is set up to screen the uploaded local gradients. The committee system selects the local gradients rated by the elected members for the aggregation procedure through the selection strategy, and replaces the committee member through the election strategy. Based on the different considerations of model performance and defense, two opposite selection strategies are designed for the sake of both accuracy and robustness. Extensive experiments illustrate that CMFL achieves faster convergence and better accuracy than the typical Federated Learning, in the meanwhile obtaining better robustness than the traditional Byzantine-tolerant algorithms, in the manner of a decentralized approach. In addition, we theoretically analyze and prove the convergence of CMFL under different election and selection strategies, which coincides with the experimental results.","1558-2183","","10.1109/TPDS.2022.3202887","National Key R&D Program of China(grant numbers:2020YFB1006001); National Natural Science Foundation of China(grant numbers:62176269,62006252); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010165003); Innovative Research Foundation of Ship General Performance(grant numbers:25622112); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870745","Decentralized federated learning;committee mechanism;byzantine robustness;theoretical convergence analysis","Servers;Convergence;Training;Hidden Markov models;Robustness;Collaborative work;Voting","data privacy;fault tolerant computing;learning (artificial intelligence)","decentralized Federated Learning framework;convergence guarantee;multiple participants;data privacy;distributed machine;training method;Byzantine clients;global model;false gradient;federated learning framework Committee Mechanism;CMFL;committee system;uploaded local gradients;elected members;selection strategy;committee member;opposite selection strategies;typical Federated Learning;Byzantine-tolerant algorithms;different election","",3.0,"",58.0,"IEEE","31 Aug 2022","","","IEEE","IEEE Journals"
"ScaleFlux: Efficient Stateful Scaling in NFV","L. Liu; H. Xu; Z. Niu; J. Li; W. Zhang; P. Wang; J. Li; J. C. Xue; C. Wang","Zhongguancun Laboratory, Beijing, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China; Microsoft Research Asia, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China; Shandong Provincial Key Laboratory of Computer Networks, Shandong Computer Science Center (NationalSupercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, Shandong, China; Theory Lab, Huawei Hong Kong Research Center, Hong Kong SAR, China; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China","IEEE Transactions on Parallel and Distributed Systems","20 Sep 2022",2022,33.0,12.0,4801,4817,"Network function virtualization (NFV) enables elastic scaling to middlebox deployment and management. Therefore, efficient stateful scaling is an important task because operators often need to shift traffic and the associated flow states across VNF instances to deal with time-varying loads. Existing NFV scaling methods, however, typically focus on one aspect of the scaling pipeline and does not offer an end-to-end scaling framework. This article presents ScaleFlux, a complete stateful scaling system that efficiently reduces flow-level latency and achieves near-optimal resource usage. ScaleFlux (1) monitors traffic load for each VNF instance and adopts a queue-based mechanism to detect load burstiness timely, (2) deploys a flow bandwidth predictor to predict flow bandwidth time-series with the ABCNN-LSTM model, and (3) schedules the necessary flow and state migration using the simulated annealing algorithm to achieve both flow-level latency guarantee and resource usage minimization. Testbed evaluation with a five-machine cluster shows that ScaleFlux reduces flow completion time by at least 8.7× for all the workloads and achieves near-optimal CPU usage during scaling.","1558-2183","","10.1109/TPDS.2022.3204209","National Natural Science Foundation of China(grant numbers:61802233); Qilu University of Technology(grant numbers:2020KJC-ZD02); General Research Fund from Hong Kong Research(grant numbers:11209520); Chinese University of Hong Kong(grant numbers:5501329,5501517,4937007,4937008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9875954","Network function virtualization;network load detection;flow bandwidth prediction;stateful scaling;service level agreements","Logic gates;Bandwidth;Service level agreements;Packet loss;Computer science;Cloud computing;Wide area networks","cloud computing;computer centres;computer networks;resource allocation;simulated annealing;telecommunication scheduling;telecommunication traffic;virtualisation","efficient stateful scaling;network function virtualization;elastic scaling;middlebox deployment;associated flow states;VNF instance;time-varying loads;NFV scaling methods;scaling pipeline;end-to-end scaling framework;complete stateful scaling system;flow-level;ScaleFlux monitors traffic load;flow bandwidth predictor;flow bandwidth time-series;necessary flow;state migration;flow completion time","","","",74.0,"IEEE","5 Sep 2022","","","IEEE","IEEE Journals"
"QoS-Aware Co-Scheduling for Distributed Long-Running Applications on Shared Clusters","J. Zhu; R. Yang; X. Sun; T. Wo; C. Hu; H. Peng; J. Xiao; A. Y. Zomaya; J. Xu","Department of Computing, North China Electric Power University, Baoding, China; School of Computing, University of Leeds, Leeds, U.K.; School of Computing, University of Leeds, Leeds, U.K.; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Alibaba Group, Hangzhou, Zhejiang, China; University of Sydney, Camperdown, NSW, Australia; School of Computing, University of Leeds, Leeds, U.K.","IEEE Transactions on Parallel and Distributed Systems","29 Sep 2022",2022,33.0,12.0,4818,4834,"To achieve a high degree of resource utilization, production clusters need to co-schedule diverse workloads – including both batch analytic jobs with short-lived tasks and long-running applications (LRAs) that execute for a long time frame from hours to months – onto the shared resources. Microservice architecture advances the manifestation of distributed LRAs (DLRAs), comprising multiple interconnected microservices that are executed in long-lived distributed containers and serve massive user requests. Detecting and mitigating QoS violation become even more intractable due to the network uncertainties and latency propagation across dependent microservices. However, current resource managers are only responsible for resource allocation among applications/jobs but agnostic to runtime QoS such as latency at application level. The state-of-the-art QoS-aware scheduling approaches are dedicated for monolithic applications, without considering the temporal-spatio performance variability across distributed microservices. In this paper, we present Toposch, a new scheduling and execution framework to prioritize the QoS of DLRAs whilst balancing the performance of batch jobs and maintaining high cluster utilization through harvesting idle resources. Toposch tracks footprints of every single request across microservices and uses critical path analysis, based on the end-to-end latency graph, to identify microservices that have high risk of QoS violation. Based on microservice and node level risk assessment, we intervene the batch scheduling by adaptively reducing the visible resources to batch tasks and thus delaying their execution to give way to DLRAs. We propose a prediction-based vertical resource auto-scaling mechanism, with the aid of resource-performance modeling and fine-grained resource inference and access control, for prompt recovery of QoS violation. A cost-effective task preemption is leveraged to ensure a low-cost task preemption and resource reclamation during the auto-scaling. Toposch is integrated with Apache YARN and experiments show that Toposch outperforms other baselines in terms of performance guarantee of DLRAs, at an acceptable cost of batch job slowdown. The tail latency of DLRAs is merely 1.12x of the case of executing alone on average in Toposch with a 26% JCT increase of Spark analytic jobs.","1558-2183","","10.1109/TPDS.2022.3202493","MIIT of China(grant numbers:2105-370171-07-02-860873); S&T Program of Hebei(grant numbers:20310101D); Fundamental Research Funds for the Central Universities(grant numbers:20226941); UK EPSRC(grant numbers:EP/T01461X/1); Alan Turing Pilot Project; Alan Turing PDEA Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869329","Resource scheduling;cluster management;QoS;tail latency;datacenters","Quality of service;Microservice architectures;Task analysis;Runtime;Containers;Resource management;Databases","cloud computing;parallel processing;quality of service;resource allocation;scheduling","QoS-aware co-scheduling;distributed long-running applications;shared clusters;resource utilization;production clusters;batch analytic jobs;microservice architecture advances;distributed LRAs;DLRAs;multiple interconnected microservices;long-lived distributed containers;QoS violation;latency propagation;dependent microservices;current resource managers;resource allocation;runtime QoS;monolithic applications;temporal-spatio performance variability;distributed microservices;Toposch;batch jobs;end-to-end latency graph;node level risk assessment;batch scheduling;prediction-based vertical resource auto-scaling mechanism;resource-performance modeling;fine-grained resource inference;cost-effective task preemption;low-cost task preemption;resource reclamation;batch job slowdown;Spark analytic jobs;QoS-aware scheduling approaches","",1.0,"",65.0,"CCBY","29 Aug 2022","","","IEEE","IEEE Journals"
"TDFL: Truth Discovery Based Byzantine Robust Federated Learning","C. Xu; Y. Jia; L. Zhu; C. Zhang; G. Jin; K. Sharif","School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","29 Sep 2022",2022,33.0,12.0,4835,4848,"Federated learning (FL) enables data owners to train a joint global model without sharing private data. However, it is vulnerable to Byzantine attackers that can launch poisoning attacks to destroy model training. Existing defense strategies rely on the additional datasets to train trustable server models or trusted execution environments to mitigate attacks. Besides, these strategies can only tolerate a small number of malicious users or resist a few types of poisoning attacks. To address these challenges, we design a novel federated learning method TDFL, Truth Discovery based Federated Learning, which can defend against multiple poisoning attacks without additional datasets even when the Byzantine users are $\geq 50\%$≥50%. Specifically, the TDFL considers different scenarios with different malicious proportions. For Honest-majority setting (Byzantine $< 50\%$<50%), we design a special robust truth discovery aggregation scheme to remove malicious model updates, which can assign weights according to users’ contribution; for Byzantine-majority setting (Byzantine $\geq 50\%$≥50%), we use maximum clique-based filter to guarantee global model quality. To the best of our knowledge, this is the first study that uses truth discovery to defend against poisoning attacks. It is also the first scheme which can achieve strong robustness under multiple kinds of attacks launched by high proportion attackers without root datasets. Extensive comparative experiments are designed with five state-of-the-art aggregation rules under five types of classical poisoning attacks on different datasets. The experimental results demonstrate that TDFL is practical and achieves reasonable Byzantine-robustness.","1558-2183","","10.1109/TPDS.2022.3205714","National Natural Science Foundation of China(grant numbers:61972037,61872041,U1804263); China Postdoctoral Science Foundation(grant numbers:2021M700435,2021TQ0042); National Cryptography Development Fund(grant numbers:MMJJ20180412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887909","Federated learning;truth discovery;poisoning attack","Data models;Collaborative work;Servers;Training;Robustness;Data privacy;Soft sensors","data privacy;learning (artificial intelligence)","private data;Byzantine attackers;defense strategies;trustable server models;trusted execution environments;malicious users;multiple poisoning attacks;Byzantine users;honest-majority setting;special robust truth discovery;malicious model updates;Byzantine-majority setting;global model quality;high proportion attackers;classical poisoning attacks;reasonable Byzantine-robustness;Byzantine robust federated learning;malicious proportions;maximum clique-based filter","","","",35.0,"IEEE","12 Sep 2022","","","IEEE","IEEE Journals"
"LosaTM: A Hardware Transactional Memory Integrated With a Low-Overhead Scenario-Awareness Conflict Manager","C. Fu; L. Wan; J. Han","State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","28 Sep 2022",2022,33.0,12.0,4849,4862,"The vigorous development of high compute-intensive applications has led to the demand for maximizing the concurrency of multicore processors. The best-effort hardware transactional memory(HTM) is an important technology adopted by vendors to improve the potential concurrency of multicore processors, but the HTM implementations on commercial products have some drawbacks for its simplicity and need some further optimizations to enable more exploitation of concurrency. In this article, we propose and evaluate a novel design of HTM, called LosaTM, which can provide a scenario-awareness conflict management strategy. By leveraging the proposed feature of multiple-grained coherency maintenance in the coherence protocol, LosaTM resolves most false conflicts at a half-cache-line granularity. Furthermore, we design a winner/aborter vector conflict management algorithm to improve the efficiency of LosaTM in handling friendly-fire and unfairness competition that we have newly defined. In order to coordinate these integrated conflict management strategies, a scheduling strategy is also proposed to adaptively select the appropriate management according to the specific conflict scenario. We use gem5 to simulate LosaTM in detail on an 8-core tiled CMP system, and the simulation result shows that it only causes 0.7% of the L1 cache size hardware overhead while achieving a 38% average execution time reduction on the native STAMP. The speedup also demonstrates that LosaTM outperforms the state-of-the-art designs in previous works.","1558-2183","","10.1109/TPDS.2022.3206777","National Natural Science Foundation of China(grant numbers:61934002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893383","Scenario-specific conflict management strategies;hardware transactional memory;cache coherence protocols;false sharing","Hardware;Coherence;Program processors;Optimization;Memory management;Concurrent computing;Protocols","cache storage;memory architecture;microprocessor chips;multiprocessing systems;power aware computing","hardware transactional memory integrated;low-overhead scenario-awareness conflict manager;vigorous development;high compute-intensive applications;multicore processors;potential concurrency;HTM implementations;commercial products;called LosaTM;scenario-awareness conflict management strategy;multiple-grained coherency maintenance;coherence protocol;false conflicts;half-cache-line granularity;vector conflict management algorithm;friendly-fire;unfairness competition;integrated conflict management strategies;scheduling strategy;appropriate management;specific conflict scenario;L1 cache size hardware overhead","","","",33.0,"IEEE","15 Sep 2022","","","IEEE","IEEE Journals"
"Federated Learning With Nesterov Accelerated Gradient","Z. Yang; W. Bao; D. Yuan; N. H. Tran; A. Y. Zomaya","School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","10 Oct 2022",2022,33.0,12.0,4863,4873,"Federated learning (FL) is a fast-developing technique that allows multiple workers to train a global model based on a distributed dataset. Conventional FL (FedAvg) employs gradient descent algorithm, which may not be efficient enough. Momentum is able to improve the situation by adding an additional momentum step to accelerate the convergence and has demonstrated its benefits in both centralized and FL environments. It is well-known that Nesterov Accelerated Gradient (NAG) is a more advantageous form of momentum, but it is not clear how to quantify the benefits of NAG in FL so far. This motives us to propose FedNAG, which employs NAG in each worker as well as NAG momentum and model aggregation in the aggregator. We provide a detailed convergence analysis of FedNAG and compare it with FedAvg. Extensive experiments based on real-world datasets and trace-driven simulation are conducted, demonstrating that FedNAG increases the learning accuracy by 3–24% and decreases the total training time by 11–70% compared with the benchmarks under a wide range of settings.","1558-2183","","10.1109/TPDS.2022.3206480","Australian Research Council(grant numbers:DP200103718,DP200103494); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9891808","Edge computing;federated learning;nesterov accelerated gradient","Convergence;Training;Computational modeling;Collaborative work;Quantization (signal);Servers;Internet of Things","gradient methods;learning (artificial intelligence)","federated learning;Nesterov accelerated gradient;multiple workers;global model;distributed dataset;FL;FedAvg;gradient descent algorithm;FedNAG;NAG momentum;model aggregation;convergence analysis;trace-driven simulation","","","",43.0,"IEEE","14 Sep 2022","","","IEEE","IEEE Journals"
"Artemis: A Latency-Oriented Naming and Routing System","X. Li; Y. Chen; M. Zhou; T. Guo; C. Wang; Y. Xiao; J. Wan; X. Wang","Department of Communications and Networking, Aalto University, Espoo, Finland; Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai, China; Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai, China; Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai, China; Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai, China; Department of Communications and Networking, Aalto University, Espoo, Finland; Huawei Technologies Co. Ltd., Shenzhen, China; Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","29 Sep 2022",2022,33.0,12.0,4874,4890,"Today, Internet service deployment is typically implemented with server replication at multiple locations. Domain name system (DNS), which translates human-readable domain names into network-routable IP addresses, is typically used for distributing users to different server replicas. However, DNS relies on several network-based queries and the queries delay the connection setup process between the client and the server replica. In this article, we propose Artemis, a practical low-latency naming and routing system that supports optimal server (replica) selection based on user-defined policies and provides lower query latencies than DNS. Artemis uses a DNS-like domain name-IP mapping for replica selection and achieves low query latency by combining the name resolution process with the transport layer handshake process. In Artemis, all server replicas at different locations share the same anycast IP address, called Service Address. Clients use the Service Address to establish a transport layer connection with the server. The client's initial handshake packet is routed over an overlay network to reach the optimal server. Then the server migrates the transport layer connection to its original unicast IP address after finishing the handshake process. After that, service discovery is completed, and the client communicates with the server directly via IP addresses. To validate the effectiveness of Artemis, we evaluate its performance via both real trace-driven simulation and real-world deployment. The result shows that Artemis can handle a large number of connections and reduce the connection setup latency compared with state-of-the-art solutions. More specifically, our deployment across 11 Google data centers shows that Artemis reduces the connection setup latency by 39.4% compared with DNS.","1558-2183","","10.1109/TPDS.2022.3207189","National Natural Science Foundation of China(grant numbers:61971145); HUAWEI(grant numbers:YBN2019125184); Academy of Finland(grant numbers:317432); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894091","Service discovery;name resolution;overlay routing;anycast","Servers;Routing;IP networks;Internet;Delays;Data centers;Overlay networks","Internet;IP networks;overlay networks;telecommunication network routing","overlay network;transport layer connection;original unicast IP address;service discovery;Artemis;Internet service deployment;server replication;domain name system;human-readable domain names;network-routable IP addresses;server replicas;network-based queries;connection setup process;user-defined policies;DNS-like domain name-IP mapping;replica selection;name resolution process;transport layer handshake process;anycast IP address;service address;low-latency naming system;low-latency routing system","",1.0,"",56.0,"IEEE","16 Sep 2022","","","IEEE","IEEE Journals"
"Accelerating Backward Aggregation in GCN Training With Execution Path Preparing on GPUs","S. Xu; Z. Shao; C. Yang; X. Liao; H. Jin","Zhejiang Lab, Hangzhou, China; Zhejiang Lab, Hangzhou, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","4 Oct 2022",2022,33.0,12.0,4891,4902,"The emerging Graph Convolutional Network (GCN) has been widely used in many domains, where it is important to improve the efficiencies of applications by accelerating GCN trainings. Due to the sparsity nature and exploding scales of input real-world graphs, state-of-the-art GCN training systems (e.g., GNNAdvisor) employ graph processing techniques to accelerate the message exchanging (i.e., aggregations) among the graph vertices. Nevertheless, these systems treat both the aggregation stages of forward and backward propagation phases as all-active graph processing procedures that indiscriminately conduct computations on all vertices of an input graph. In this article, we first point out that in a GCN training problem with a given training set on an input graph, its aggregation stages of backward propagation phases (called as backward aggregations in this article) can be equivalently converted to partially-active graph processing procedures, which conduct computations on only partial vertices of the input graph. By leveraging such a finding, we propose an execution path preparing method that collects and coalesces the graph data used during different training layers of backward aggregations, and constructs their corresponding sub-graphs (called as execution paths in this article) as inputs to conduct the backward training on GPUs. Further, we propose a structural-aware strategy for the execution paths to compute their optimal group sizes, so as to gain as high as possible performances on GPUs during the backward aggregations. The experiment results by conducting GCN training in typical real-world graphs show that compared with GNNAdvisor, our approach improves the performance of backward aggregations by up to 5.68x on NVIDIA P100 GPU, and up to 6.57x on NVIDIA V100S GPU","1558-2183","","10.1109/TPDS.2022.3205642","National Natural Science Foundation of China(grant numbers:61972444,61825202,62072195,61832006); Zhejiang Lab(grant numbers:2022P10AC02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887890","Backward aggregation;graph convolutional network;graph processing;graphics processing unit","Training;Backpropagation;Graphics processing units;Blogs;Electric breakdown;Computational modeling;Social networking (online)","convolutional neural nets;graph theory;learning (artificial intelligence);optimisation","graph vertices;aggregation stages;backward propagation phases;GCN training problem;backward aggregation;partially-active graph processing procedures;graph data;sub-graphs;backward training;GPUs;execution path preparing;graph convolutional network;NVIDIA P100 GPU;GNNAdvisor","","","",46.0,"IEEE","12 Sep 2022","","","IEEE","IEEE Journals"
"DRAS: Deep Reinforcement Learning for Cluster Scheduling in High Performance Computing","Y. Fan; B. Li; D. Favorite; N. Singh; T. Childers; P. Rich; W. Allcock; M. E. Papka; Z. Lan","Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; University of Illinois, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA","IEEE Transactions on Parallel and Distributed Systems","4 Oct 2022",2022,33.0,12.0,4903,4917,"Cluster schedulers are crucial in high-performance computing (HPC). They determine when and which user jobs should be allocated to available system resources. Existing cluster scheduling heuristics are developed by human experts based on their experience with specific HPC systems and workloads. However, the increasing complexity of computing systems and the highly dynamic nature of application workloads have placed tremendous burden on manually designed and tuned scheduling heuristics. More aggressive optimization and automation are needed for cluster scheduling in HPC. In this work, we present an automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for Scheduling) by leveraging deep reinforcement learning. DRAS is built on a hierarchical neural network incorporating special HPC scheduling features such as resource reservation and backfilling. An efficient training strategy is presented to enable DRAS to rapidly learn the target environment. Once being provided a specific scheduling objective given by the system manager, DRAS automatically learns to improve its policy through interaction with the scheduling environment and dynamically adjusts its policy as workload changes. We implement DRAS into a HPC scheduling platform called CQGym. CQGym provides a common platform allowing users to flexibly evaluate DRAS and other scheduling methods such as heuristic and optimization methods. The experiments using CQGym with different production workloads demonstrate that DRAS outperforms the existing heuristic and optimization approaches by up to 50%.","1558-2183","","10.1109/TPDS.2022.3205325","National Science Foundation(grant numbers:CNS-1717763,CCF-2109316,CCF-2119294); U.S. Department of Energy(grant numbers:DE-AC02-06CH11357,DE-AC02-05CH11231); National Energy Research Scientific Computing Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894371","High-performance computing;cluster scheduling;deep reinforcement learning;job starvation;backfilling;resource reservation;OpenAI Gym","Processor scheduling;Dynamic scheduling;Runtime;Neural networks;Training;Q-learning;Production","deep learning (artificial intelligence);optimisation;parallel processing;processor scheduling;reinforcement learning","DRAS;deep reinforcement learning;high performance computing;cluster scheduling heuristics;highly dynamic nature;automated HPC scheduling agent;system manager;heuristic optimization approaches;deep reinforcement agent for scheduling;CQGym","","","",51.0,"IEEE","16 Sep 2022","","","IEEE","IEEE Journals"
"Enabling In-Network Floating-Point Arithmetic for Efficient Computation Offloading","P. Cui; H. Pan; Z. Li; P. Zhang; T. Miao; J. Zhou; H. Guan; G. Xie","University of Chinese Academy of Sciences, Beijing, China; Purple Mountain Laboratories, Nanjing, China; Purple Mountain Laboratories, Nanjing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Southern University of Science and Technology, Shenzhen, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","4 Oct 2022",2022,33.0,12.0,4918,4934,"Programmable switches are recently used for accelerating data-intensive distributed applications. Some computational tasks, traditionally performed on servers in data centers, are offloaded into the network on programmable switches. These tasks may require the support of on-the-fly floating-point operations. Unfortunately, programmable switches are restricted to simple integer arithmetic operations. Existing systems circumvent this restriction by converting floats to integers or relying on local CPUs of switches, incurring extra processing delayed and accuracy loss. To address this gap, we propose NetFC, a table-lookup method to achieve on-the-fly in-network floating-point arithmetic operations nearly without accuracy loss. Specifically, NetFC utilizes logarithm projection and transformation to convert the original huge table enumerating all operands and results into several much smaller tables that can fit into the data plane of programmable switches. To cope with the table inflation problem on 32-bit floats, we also propose an approximation method that further breaks the large tables into smaller ones. In addition, NetFC leverages two optimizations to improve accuracy and reduce on-chip memory consumption. We use both synthetic and real-life datasets to evaluate NetFC. The experimental results show that the average accuracy of NetFC is above 99.9% with only 448KB memory consumption for 16-bit floats and 99.1% with 496KB memory consumption for 32-bit floats. Furthermore, we integrate NetFC into two distributed applications and two in-network telemetry systems to show its effectiveness in further improving the performance.","1558-2183","","10.1109/TPDS.2022.3208425","National Key R&D Program of China(grant numbers:2020YFB1805600); National Natural Science Foundation of China(grant numbers:U20A20180,62002344); Beijing Natural Science Foundation(grant numbers:JQ20024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9896997","In-network computation;computation offloading;floating-point operation","Open area test sites;Arithmetic;Memory management;Task analysis;Training;Standards;Servers","computer centres;distributed programming;floating point arithmetic;table lookup;telemetry","programmable switches;table inflation problem;NetFC;16-bit floats;32-bit floats;in-network telemetry systems;data-intensive distributed applications;data centers;table lookup;in-network floating-point arithmetic operations;computation offloading;on-chip memory consumption reduction","","","",54.0,"IEEE","21 Sep 2022","","","IEEE","IEEE Journals"
"Co-Concurrency Mechanism for Multi-GPUs in Distributed Heterogeneous Environments","X. Zhang; Z. Tang; X. Zhang; K. Li","National Supercomputing Center in Changsha, Hunan, Changsha, China; National Supercomputing Center in Changsha, Hunan, Changsha, China; Alibaba Group, Hangzhou, Zhejiang, China; National Supercomputing Center in Changsha, Hunan, Changsha, China","IEEE Transactions on Parallel and Distributed Systems","13 Oct 2022",2022,33.0,12.0,4935,4947,"The high concurrency and high throughput characteristics of graphics processing units (GPUs) have made researchers continue to use it to optimize distributed parallel computing architectures. With the upgrading of processor architecture, GPUs allow multiple kernels to execute concurrently through stream queues. However, due to the different hardware characteristics and kernel properties in distributed architectures, existing research lacks careful consideration of optimization schemes for concurrent streams and kernel block sizes. Unreasonable stream concurrency and kernel block size configuration will lead to prolonged execution time and waste of computing resources during application execution. Therefore, we propose a multi-GPU multi-stream co-concurrency mechanism (MGSC) in a distributed heterogeneous environment, dynamically adjusting the number of concurrent streams and exploring the optimal block size in task scheduling. According to the memory resources and startup overhead occupied in concurrent stream scheduling, a resource-aware concurrent stream adaptive adjustment mechanism is proposed, which can dynamically adjust the number of streams. To explore the optimal block size, we abstract it as a multi-armed bandit problem (MAB) and propose a block size adjustment algorithm based on the upper confidence bound (UCB). We implement MGSC in Spark 3.1.1 and NVIDIA CUDA 11.2. We conduct comparative experiments with multiple typical benchmarks to evaluate the performance of MGSC. The experimental results show that the algorithm can make full use of the computing power of the GPU and significantly reduce the execution time of tasks.","1558-2183","","10.1109/TPDS.2022.3208082","National Key Research and Development Program of China(grant numbers:2018YFB1701400); National Natural Science Foundation of China(grant numbers:62225205,92055213,61873090); Guangdong Province research and development plan(grant numbers:2020B0101100001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9896169","Concurrent kernel execution;heterogeneous computing;kernel;stream scheduling","Kernel;Graphics processing units;Task analysis;Processor scheduling;Concurrent computing;Computer architecture;Hardware","graphics processing units;optimisation;parallel architectures;parallel processing;processor scheduling","distributed heterogeneous environment;concurrent streams;optimal block size;memory resources;concurrent stream scheduling;resource-aware concurrent stream adaptive adjustment mechanism;block size adjustment algorithm;MGSC;multiGPUs;high concurrency;processor architecture;multiple kernels;stream queues;hardware characteristics;kernel properties;distributed architectures;optimization schemes;kernel block;unreasonable stream concurrency;prolonged execution time;computing resources;application execution;multiGPU multistream;co-concurrency mechanism","","","",36.0,"IEEE","20 Sep 2022","","","IEEE","IEEE Journals"
"𝑓uncX: Federated Function as a Service for Science","Z. Li; R. Chard; Y. Babuji; B. Galewsky; T. J. Skluzacek; K. Nagaitsev; A. Woodard; B. Blaiszik; J. Bryan; D. S. Katz; I. Foster; K. Chard","Department of Computer Science and Engineering, Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, Shenzhen, China; Argonne National Laboratory, Lemont, IL, USA; University of Chicago, Chicago, IL, USA; NCSA, University of Illinois, Urbana, IL, USA; University of Chicago, Chicago, IL, USA; Northwestern University, Evanston, IL, USA; University of Chicago, Chicago, IL, USA; University of Chicago, Chicago, IL, USA; University of Chicago, Chicago, IL, USA; NCSA, CS, ECE, and the iSchool, University of Illinois, Urbana, IL, USA; Argonne National Laboratory, Lemont, IL, USA; University of Chicago, Chicago, IL, USA","IEEE Transactions on Parallel and Distributed Systems","12 Oct 2022",2022,33.0,12.0,4948,4963,"ƒuncX is a distributed function as a service (FaaS) platform that enables flexible, scalable, and high performance remote function execution. Unlike centralized FaaS systems, ƒuncX decouples the cloud-hosted management functionality from the edge-hosted execution functionality. ƒuncX's endpoint software can be deployed, by users or administrators, on arbitrary laptops, clouds, clusters, and supercomputers, in effect turning them into function serving systems. ƒuncX's cloud-hosted service provides a single location for registering, sharing, and managing both functions and endpoints. It allows for transparent, secure, and reliable function execution across the federated ecosystem of endpoints—enabling users to route functions to endpoints based on specific needs. ƒuncX uses containers (e.g., Docker, Singularity, and Shifter) to provide common execution environments across endpoints. ƒuncX implements various container management strategies to execute functions with high performance and efficiency on diverse ƒuncX endpoints. ƒuncX also integrates with an in-memory data store and Globus for managing data that may span endpoints. We motivate the need for ƒuncX, present our prototype design and implementation, and demonstrate, via experiments on two supercomputers, that ƒuncX can scale to more than 130000 concurrent workers. We show that ƒuncX's container warming-aware routing algorithm can reduce the completion time for 3,000 functions by up to 61% compared to a randomized algorithm and the in-memory data store can speed up data transfers by up to 3x compared to a shared file system.","1558-2183","","10.1109/TPDS.2022.3208767","National Science Foundation(grant numbers:2004894,2004932); U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9899739","Function-as-a-service;cyberinfrastructure;distributed computing","Containers;Computational modeling;Task analysis;Registers;Python;Cloud computing;Supercomputers","cloud computing;data handling;storage management","container management;high performance remote function execution;cloud-hosted management functionality;edge-hosted execution functionality;function serving systems;cloud-hosted service;funcX;endpoint software;container warming-aware routing algorithm;federated function as a service;distributed function as a service;FaaS;supercomputers;in-memory data store;Globus;data management;data transfers","",1.0,"",82.0,"IEEE","22 Sep 2022","","","IEEE","IEEE Journals"
"Votes-as-a-Proof (VaaP): Permissioned Blockchain Consensus Protocol Made Simple","X. Fu; H. Wang; P. Shi","National Key Laboratory of Parallel and Distributed Processing and Key Laboratory of Software Engineering for Complex Systems, College of Computer Science, National University of Defense Technology, Changsha, Hunan, China; National Key Laboratory of Parallel and Distributed Processing and Key Laboratory of Software Engineering for Complex Systems, College of Computer Science, National University of Defense Technology, Changsha, Hunan, China; National Key Laboratory of Parallel and Distributed Processing and Key Laboratory of Software Engineering for Complex Systems, College of Computer Science, National University of Defense Technology, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","18 Oct 2022",2022,33.0,12.0,4964,4973,"With the development of Blockchain technology, permissioned Blockchains are getting more and more attention from researchers because applications based on permissioned Blockchains are more practical and easier to be carried out. This paper aims to design a dedicated consensus protocol for permissioned Blockchains. The existing consensus protocols applied to permissioned Blockchains are either derived from public Blockchains such as Proof of Work (PoW) or Proof of Stake (PoS), with full decentralization, resulting in low transaction processing efficiency; or derived from traditional Byzantine fault-tolerant (BFT) consensus protocols such as Practical BFT (PBFT) or HoneyBadgerBFT, with high communication complexity of the consensus process, resulting in low scalability. Therefore, we propose a dedicated consensus protocol for permissioned Blockchains called Votes-as-a-Proof (VaaP) with high transaction processing efficiency while ensuring high scalability. Every node in VaaP runs a simple consensus process based on voting in parallel. Faulty nodes will only deprive themselves of using consensus service. We present the comparison of VaaP and Sphinx, one of the state-of-the-art consensus protocols, analytically and experimentally (up to 500 nodes). The results indicate that VaaP outperforms Sphinx in throughput, latency and scalability.","1558-2183","","10.1109/TPDS.2022.3211829","National Natural Science Foundation of China(grant numbers:61772030); Zhejiang Lab(grant numbers:2021PE0AC01); GF Innovative Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910415","BFT;blockchain;consensus protocol","Consensus protocol;Philosophical considerations;Throughput;Complexity theory;Scalability;Safety;Prototypes","blockchains;communication complexity;fault tolerance;protocols;transaction processing","communication complexity;proof of stake;PoS;proof of work;PoW;HoneyBadgerBFT;practical BFT;votes-as-a-proof;permissioned Blockchain consensus protocol;VaaP;Byzantine fault-tolerant consensus protocols;Blockchain technology","","","",28.0,"IEEE","4 Oct 2022","","","IEEE","IEEE Journals"
"Outperforming Sequential Full-Word Long Addition With Parallelization and Vectorization","A. Chusov","Far-Eastern Federal University, Vladivostok, Russia","IEEE Transactions on Parallel and Distributed Systems","18 Oct 2022",2022,33.0,12.0,4974,4985,"The article presents algorithms for parallel and vectorized full-word addition of big unsigned integers with carry propagation. Because of the propagation, software parallelization and vectorization of non-polynomial addition of big integers have long been considered impractical due to data dependencies between digits of the operands. The presented algorithms are based upon parallel and vectorized detection of carry origins within elements of vector operands, masking bits which correspond to those elements and subsequent scalar addition of the resulting integers. The acquired bits can consequently be taken into account to adjust the sum using the proposed generalization of the Kogge-Stone method. Essentially, the article formalizes and experimentally verifies parallel and vectorized implementation of carry-lookahead adders applied at arbitrary granularity of data. This approach is noticeably beneficial for manycore, CUDA and vectorized implementation using AVX-512 with masked instructions. Experiments show that the parallel and vectorized implementations of the proposed algorithms can be multiple times faster compared to a sequential ripple-carry adder or adders based on redundant number systems such as one used in the GNU Multiple Precision library.","1558-2183","","10.1109/TPDS.2022.3211937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910379","AVX-512;CUDA;full-word addition;Long arithmetic;parallel arithmetic;SIMD;SMP;vectorization","Adders;Arithmetic;Signal processing algorithms;Additives;Complexity theory;Energy efficiency;Vector processors","adders;parallel architectures;redundant number systems","carry propagation;software parallelization;vectorization;nonpolynomial addition;big integers;data dependencies;parallel detection;vectorized detection;carry origins;vector operands;subsequent scalar addition;resulting integers;acquired bits;article formalizes;vectorized implementation;carry-lookahead adders;parallel implementations;vectorized implementations;sequential ripple-carry adder;sequential full-word long addition;parallel word addition;vectorized full-word addition;big unsigned integers","",1.0,"",30.0,"IEEE","4 Oct 2022","","","IEEE","IEEE Journals"
"Extreme-Scale Many-against-Many Protein Similarity Search","O. Selvitopi; S. Ekanayake; G. Guidi; M. G. Awan; G. A. Pavlopoulos; A. Azad; N. Kyrpides; L. Oliker; K. Yelick; A. Buluç","Applied Mathematics & Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, USA; Microsoft Corporation, Redmond, USA; University of California, Berkeley, USA; NERSC, Lawrence Berkeley National Laboratory, Berkeley, USA; BSRC “Alexander Fleming”, Institute for Fundamental Biomedical Research, Vari, Greece; Indiana University, Bloomington, USA; Lawrence Berkeley National Laboratory, Joint Genome Institute, Berkeley, USA; Applied Mathematics & Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,12,"Similarity search is one of the most fundamental computations that are regularly performed on ever-increasing protein datasets. Scalability is of paramount importance for uncovering novel phenomena that occur at very large scales. We unleash the power of over 20,000 GPUs on the Summit system to perform all-vs-all protein similarity search on one of the largest publicly available datasets with 405 million proteins, in less than 3.5 hours, cutting the time-to-solution for many use cases from weeks. The variability of protein sequence lengths, as well as the sparsity of the space of pairwise comparisons, make this a challenging problem in distributed memory. Due to the need to construct and maintain a data structure holding indices to all other sequences, this application has a huge memory footprint that makes it hard to scale the problem sizes. We overcome this memory limitation by innovative matrix-based blocking techniques, without introducing additional load imbalance.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046115","High performance computing;Computational biology;Proteins;Protein similarity search;Parallel algorithms;Sparse matrices","Scalability;High performance computing;Data structures;Protein sequence;Biology","","","","","",24.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Reshaping Geostatistical Modeling and Prediction for Extreme-Scale Environmental Applications","Q. Cao; S. Abdulah; R. Alomairy; Y. Pei; P. Nag; G. Bosilca; J. Dongarra; M. G. Genton; D. E. Keyes; H. Ltaief; Y. Sun","Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Extreme Computing Research Center, Division of Computer, Electrical and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Thuwal, KSA; Extreme Computing Research Center, Division of Computer, Electrical and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Thuwal, KSA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Extreme Computing Research Center, Division of Computer, Electrical and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Thuwal, KSA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; University of Manchester, Manchester, UK; Extreme Computing Research Center, Division of Computer, Electrical and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Thuwal, KSA; Extreme Computing Research Center, Division of Computer, Electrical and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Thuwal, KSA; Extreme Computing Research Center, Division of Computer, Electrical and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Thuwal, KSA; Extreme Computing Research Center, Division of Computer, Electrical and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Thuwal, KSA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,12,"We extend the capability of space-time geostatistical modeling using algebraic approximations, illustrating application-expected accuracy worthy of double precision from majority low-precision computations and low-rank matrix approximations. We exploit the mathematical structure of the dense covariance matrix whose inverse action and determinant are repeatedly required in Gaussian log-likelihood optimization. Geostatistics augments first-principles modeling approaches for the prediction of environmental phenomena given the availability of measurements at a large number of locations; however, traditional Cholesky-based approaches grow cubically in complexity, gating practical extension to continental and global datasets now available. We combine the linear algebraic contributions of mixed-precision and low-rank computations within a tile based Cholesky solver with on-demand casting of precisions and dynamic runtime support from PaRSEC to orchestrate tasks and data movement. Our adaptive approach scales on various systems and leverages the Fujitsu A64FX nodes of Fugaku to achieve up to 12X performance speedup against the highly optimized dense Cholesky implementation.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046053","Space-Time Geospatial Statistics;Climate/Weather Prediction;Task-Based Programming Models;Dynamic Runtime Systems;Mixed-Precision Computations;Low-Rank Matrix Approximations;High Performance Computing","Analytical models;Runtime;Computational modeling;High performance computing;Predictive models;Programming;Mathematical models","","","","","",41.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Pushing the Frontier in the Design of Laser-Based Electron Accelerators with Groundbreaking Mesh-Refined Particle-In-Cell Simulations on Exascale-Class Supercomputers","L. Fedeli; A. Huebl; F. Boillod-Cerneux; T. Clark; K. Gott; C. Hillairet; S. Jaure; A. Leblanc; R. Lehe; A. Myers; C. Piechurski; M. Sato; N. Zaim; W. Zhang; J. -L. Vay; H. Vincenti","LIDYL, CEA-Universite Paris-Saclay, CEA Saclay, Gif-sur-Yvette, France; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; LIDYL, CEA-Universite Paris-Saclay, CEA Saclay, Gif-sur-Yvette, France; LIDYL, CEA-Universite Paris-Saclay, CEA Saclay, Gif-sur-Yvette, France; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Arm, Biot-Sophia Antipolis, France; ATOS, Echirolles, France; Laboratoire d'Optique Appliquée, ENSTA Paris, CNRS, Ecole poly technique, Palaiseau, France; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; GENCI, 6 bis rue Auguste Vitu, Paris, France; RIKEN, Kobe, Hyogo, Japan; LIDYL, CEA-Universite Paris-Saclay, CEA Saclay, Gif-sur-Yvette, France; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; LIDYL, CEA-Universite Paris-Saclay, CEA Saclay, Gif-sur-Yvette, France","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,12,"(150 word max) We present a first-of-kind mesh-refined (MR) massively parallel Particle-In-Cell (PIC) code for kinetic plasma simulations optimized on the Frontier, Fugaku, Summit, and Perlmutter supercomputers. Major innovations, implemented in the WarpX PIC code, include: (i) a three level parallelization strategy that demonstrated performance portability and scaling on millions of A64FX cores and tens of thousands of AMD and Nvidia GPUs (ii) a groundbreaking mesh refinement capability that provides between 1.5 x to 4 x savings in computing requirements on the science case reported in this paper, (iii) an efficient load balancing strategy between multiple MR levels. The MR PIC code enabled 3D simulations of laser-matter interactions on Frontier, Fugaku, and Summit, which have so far been out of the reach of standard codes. These simulations helped remove a major limitation of compact laser-based electron accelerators, which are promising candidates for next generation high-energy physics experiments and ultra-high dose rate FLASH radiotherapy.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046112","high-field science;laser-matter interaction;Plasma accelerators;Particle-In-Cell method;Adaptive mesh refinement;High performance computing;Exascale computing","Technological innovation;Solid modeling;Codes;Three-dimensional displays;Computational modeling;Electron accelerators;Supercomputers","","","","","",52.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Extreme Scale Earthquake Simulation with Uncertainty Quantification","T. Ichimura; K. Fujita; R. Kusakabe; K. Koyama; S. Murakami; Y. Kikuchi; T. Hori; M. Hori; H. Inoue; T. Nose; T. Kawashima; M. Lalith","Center for Computational Science, RIKEN, Kobe, Japan; Center for Computational Science, RIKEN, Kobe, Japan; Earthquake Research Institute and Department of Civil Engineering, The University of Tokyo, Tokyo, Japan; Fujitsu Ltd., Tokyo, Japan; Earthquake Research Institute and Department of Civil Engineering, The University of Tokyo, Tokyo, Japan; Earthquake Research Institute and Department of Civil Engineering, The University of Tokyo, Tokyo, Japan; Japan Agency for Marine-Earth Science and Technology, Yokohama, Japan; Japan Agency for Marine-Earth Science and Technology, Yokohama, Japan; Fujitsu Ltd., Tokyo, Japan; Fujitsu Ltd., Tokyo, Japan; Fujitsu Ltd., Tokyo, Japan; Earthquake Research Institute and Department of Civil Engineering, The University of Tokyo, Tokyo, Japan","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,11,"We develop a stochastic finite element method with ultra-large degrees of freedom that discretize probabilistic and physical spaces using unstructured second-order tetrahedral elements with double precision using a mixed-precision implicit iterative solver that scales to the full Fugaku system and enables fast Uncertainty Quantification (UQ). The developed solver designed to attain high performance on a variety of CPU/GPU-based supercomputers enabled solving 37 trillion degrees-of-freedom problem with 19.8% peak FP64 performance on full Fugaku (89.8 PFLOPS) with 87.7% weak scaling efficiency, corresponding to 224-fold speedup over the state of the art solver running on full Summit. This method, which has shown its effectiveness via solving huge (32-trillion degrees-of-freedom) practical problems, is expected to be a breakthrough in damage mitigation, and is expected to facilitate the scientific understanding of earthquake phenomena and have a ripple effect on other fields that similarly require UQ.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046058","iterative solver;uncertainty quantification;finite-element method;low-order unstructured element","Analytical models;Uncertainty;High performance computing;Computational modeling;Earthquakes;Probabilistic logic;Supercomputers","","","","","",33.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"2.5 Million-Atom Ab Initio Electronic-Structure Simulation of Complex Metallic Heterostructures with DGDFT","W. Hu; H. An; Z. Guo; Q. Jiang; X. Qin; J. Chen; W. Jia; C. Yang; Z. Luo; J. Li; W. Wu; G. Tan; D. Jia; Q. Lu; F. Liu; M. Tian; F. Li; Y. Huang; L. Wang; S. Liu; J. Yang","University of Science and Technology of China, Hefei, Anhui, China; Pilot National Laboratory for Marine Science and Technology (Qingdao), China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; Pilot National Laboratory for Marine Science and Technology (Qingdao), China; University of Science and Technology of China, Hefei, Anhui, China; School of Mathematical Sciences, Peking University, Beijing, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Pilot National Laboratory for Marine Science and Technology (Qingdao), China; Institute of Software, Chinese Academy of Sciences, Beijing, China; Institute of Software, Chinese Academy of Sciences, Beijing, China; Qilu University of Technology, Shandong Computer Science Center, Jinan, Shangdong, China; National Research Center of Parallel Computer Engineering and Technology, Beijing, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,13,"Over the past three decades, ab initio electronic structure calculations of large, complex and metallic systems are limited to tens of thousands of atoms in computational accuracy and efficiency on leadership supercomputers. We present a massively parallel discontinuous Galerkin density functional theory (DGDFT) implementation, which adopts adaptive local basis functions to discretize the Kohn-Sham equation, resulting in a block-sparse Hamiltonian matrix. A highly efficient pole expansion and selected inversion (PEXSI) sparse direct solver is implemented in DGDFT to achieve O(N1.5) scaling for quasi two-dimensional systems. DGDFT allows us to compute the electronic structures of complex metallic heterostructures with 2.5 million atoms (17.2 million electrons) using 35.9 million cores on the new Sunway supercomputer. The peak performance of PEXSI can achieve 64 PFLOPS (~5% of theoretical peak), which is un-precedented for sparse direct solvers. This accomplishment paves the way for quantum mechanical simulations into mesoscopic scale for designing next-generation electronic devices.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046132","First-principles density functional theory;ab initio electronic structures;discontinuous Galerkin method;pole expansion and selected inversion algorithm;new Sunway supercomputer;complex metallic mesoscale heterostructures;next-generation devices","Performance evaluation;Analytical models;Leadership;Quantum computing;Computational modeling;Supercomputers;Mathematical models","","","","","",54.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Exaflops Biomedical Knowledge Graph Analytics","R. Kannan; P. Sao; H. Lu; J. Kurzak; G. Schenk; Y. Shi; S. Lim; S. Israni; V. Thakkar; G. Cong; R. Patton; S. E. Baranzini; R. Vuduc; T. Potok","Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Advanced Micro Devices, Inc., USA; University of California, San Francisco, CA, USA; University of California, San Francisco, CA, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; University of California, San Francisco, CA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; University of California, San Francisco, CA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,11,"We are motivated by newly proposed methods for mining large-scale corpora of scholarly publications (e.g., full biomedical literature), which consists of tens of millions of papers spanning decades of research. In this setting, analysts seek to discover relationships among concepts. They construct graph representations from annotated text databases and then formulate the relationship-mining problem as an all-pairs shortest paths (APSP) and validate connective paths against curated biomedical knowledge graphs (e.g., Spoke). In this context, we present Coast (Exascale Communication-Optimized All-Pairs Shortest Path) and demonstrate 1.004 EF/s on 9,200 Frontier nodes (73,600 GCDs). We develop hyperbolic performance models (HYPERMOD), which guide optimizations and parametric tuning. The proposed Coast algorithm achieved the memory constant parallel efficiency of 99% in the single-precision tropical semiring. Looking forward, Coast will enable the integration of scholarly corpora like PubMed into the Spoke biomedical knowledge graph.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046083","Shortest Path Problem;High-Performance Computing;Parallel Algorithms","Databases;High performance computing;Biological system modeling;Memory management;Knowledge graphs;Tuning;Optimization","","","","","",19.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Scaling Correlated Fragment Molecular Orbital Calculations on Summit","G. M. J. Barca; C. Snowdon; J. L. G. Vallejo; F. Kazemian; A. P. Rendell; M. S. Gordon","School of Computing, Australian National University, Canberra, Australia; School of Computing, Australian National University, Canberra, Australia; Department of Chemistry, Iowa State University, Ames, IA, United States; School of Computing, Australian National University, Canberra, Australia; College of Science and Engineering, Flinders University, Adelaide, Australia; Department of Chemistry, Iowa State University, Ames, IA, United States","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Correlated electronic structure calculations enable an accurate prediction of the physicochemical properties of complex molecular systems; however, the scale of these calculations is limited by their extremely high computational cost. The Fragment Molecular Orbital (FMO) method is arguably one of the most effective ways to lower this computational cost while retaining predictive accuracy. In this paper, a novel distributed many-GPU algorithm and implementation of the FMO method are presented. When applied in tandem with the Hartree-Fock and RI-MP2 methods, the new implementation enables correlated calculations on 623,016 electrons and 146,592 atoms in less than 45 minutes using 99.8% of the Summit supercomputer (27,600 GPUs). The implementation demonstrates remarkable speedups with respect to other current GPU and CPU codes, and excellent strong scalability on Summit achieving 94.6 % parallel efficiency on 4600 nodes. This work makes feasible correlated quantum chemistry calculations on significantly larger molecular systems than before and with higher accuracy.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00012","Air Force Office of Scientific Research(grant numbers:FA9550-18-1-0321); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046114","electronic structure;SCF;MP2;GPU;FMO;Summit","Technological innovation;Orbital calculations;Codes;Scalability;Quantum chemistry;Graphics processing units;Prediction algorithms","","","","","",24.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Image Gradient Decomposition for Parallel and Memory-Efficient Ptychographic Reconstruction","X. Wang; A. Tsaris; D. Mukherjee; M. Wahib; P. Chen; M. Oxley; O. Ovchinnikova; J. Hinkle","Oak Ridge National Laboratory, Oak Ridge, United States; Oak Ridge National Laboratory, Oak Ridge, United States; Oak Ridge National Laboratory, Oak Ridge, United States; RIKEN Center for Computational Science, Tokyo, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; Oak Ridge National Laboratory, Oak Ridge, United States; Oak Ridge National Laboratory, Oak Ridge, United States; Oak Ridge National Laboratory, Oak Ridge, United States","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,13,"Ptychography is a popular microscopic imaging modality for many scientific discoveries and sets the record for highest image resolution. Unfortunately, the high image resolution for ptychographic reconstruction requires significant amount of memory and computations, forcing many applications to compromise their image resolution in exchange for a smaller memory footprint and a shorter reconstruction time. In this paper, we propose a novel image gradient decomposition method that significantly reduces the memory footprint for ptychographic reconstruction by tessellating image gradients and diffraction measurements into tiles. In addition, we propose a parallel image gradient decomposition method that enables asynchronous point-to-point communications and parallel pipelining with minimal overhead on a large number of GPUs. Our experiments on a Titanate material dataset (PbTiO3) with 16632 probe locations show that our Gradient Decomposition algorithm reduces memory footprint by 51 times. In addition, it achieves time-to-solution within 2.2 minutes by scaling to 4158 GPUs with a super-linear strong scaling efficiency at 364% compared to runtimes at 6 GPUs. This performance is 2.7 times more memory efficient, 9 times more scalable and 86 times faster than the state-of-the-art algorithm.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045785","image reconstruction;electron microscopy;parallel partitioning;high performance computing","Technological innovation;Image resolution;Three-dimensional displays;Microscopy;Memory management;Imaging;Titanium compounds","","","","","",14.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"P-Massive: A Real-Time Search Engine for a Multi-Terabyte Mass Spectrometry Database","N. Batsoyol; B. Pullman; M. Wang; N. Bandeira; S. Swanson","Computer Science and Engineering, University of California, San Diego, La Jolla, CA, USA; Computer Science and Engineering, Center for Computational Mass Spectrometry University of California, San Diego, La Jolla, CA, USA; Skaggs School of Pharmacy, University of California, San Diego, La Jolla, CA, USA; Computer Science and Engineering, Center for Computational Mass Spectrometry, Skaggs School of Pharmacy, University of California, San Diego, La Jolla, CA, USA; Computer Science and Engineering, University of California, San Diego, La Jolla, CA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Queries of multi-TB Mass Spectrometry (MS) repositories provide deep insights into biological processes and pose challenging data processing problems. The key bottleneck for running these queries is the number of small random reads. Byte-addressable persistent main memory (PMEM) technologies enable real-time MS search systems by delivering low-latency, high-bandwidth storage. This work presents P-Massive, real-time multi-terabyte scale MS search system. P-Massive takes advantage of PMEM and the underlying nature of its data access patterns to maximize performance. We evaluate P-Massive across various storage hierarchies and project forward over the next decade to understand how MS query systems might evolve. Our evaluation shows that P-Massive offers a cost-effective solution that achieves near-DRAM performance. A single query takes 1.7 seconds in P-Massive, 69× faster than state-of-the-art implementation. In an end-to-end, user-facing application, P-Massive delivers a 90% shorter wait time than the latest MS search tool, returning results within seconds rather than minutes.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00014","Semiconductor Research Corporation (SRC); DARPA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046108","Nonvolatile memory;Indexing;Bioinformatics;Mass Spectrometry;Search engines","Proteins;Databases;Nonvolatile memory;Scalability;Instruction sets;Search engines;Mass spectroscopy","","","","","",41.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Building Blocks for Network-Accelerated Distributed File Systems","S. Di Girolamo; D. De Sensi; K. Taranov; M. Malesevic; M. Besta; T. Schneider; S. Kistler; T. Hoefler","Dept. of Computer Science, ETH Zürich, Zürich, Switzerland; Dept. of Computer Science, ETH Zürich, Zürich, Switzerland; Dept. of Computer Science, ETH Zürich, Zürich, Switzerland; Dept. of Computer Science, ETH Zürich, Zürich, Switzerland; Dept. of Computer Science, ETH Zürich, Zürich, Switzerland; Dept. of Computer Science, ETH Zürich, Zürich, Switzerland; Dept. of Computer Science, ETH Zürich, Zürich, Switzerland; Dept. of Computer Science, ETH Zürich, Zürich, Switzerland","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"High-performance clusters and datacenters pose increasingly demanding requirements on storage systems. If these systems do not operate at scale, applications are doomed to become I/O bound and waste compute cycles. To accelerate the data path to remote storage nodes, remote direct memory access (RDMA) has been embraced by storage systems to let data flow from the network to storage targets, reducing overall latency and CPU utilization. Yet, this approach still involves CPUs on the data path to enforce storage policies such as authentication, replication, and erasure coding. We show how storage policies can be offloaded to fully programmable SmartNICs, without involving host CPUs. By using PsPIN, an open-hardware SmartNIC, we show latency improvements for writes (up to 2x), data replication (up to 2x), and erasure coding (up to 2x), when compared to respective CPU- and RDMA-based alternatives.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046100","File systems;next generation networking","Costs;File systems;High performance computing;Authentication;Media;Encoding;Next generation networking","","","","","",60.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"HammingMesh: A Network Topology for Large-Scale Deep Learning","T. Hoefler; T. Bonato; D. De Sensi; S. Di Girolamo; S. Li; M. Heddes; J. Belk; D. Goel; M. Castro; S. Scott","Microsoft Corporation, One Microsoft Way, Redmond, Washington, United States of America; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Microsoft Corporation, One Microsoft Way, Redmond, Washington, United States of America; Microsoft Corporation, One Microsoft Way, Redmond, Washington, United States of America; Microsoft Corporation, One Microsoft Way, Redmond, Washington, United States of America; Microsoft Corporation, One Microsoft Way, Redmond, Washington, United States of America; Microsoft Corporation, One Microsoft Way, Redmond, Washington, United States of America","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,18,"Numerous microarchitectural optimizations unlocked tremendous processing power for deep neural networks that in turn fueled the AI revolution. With the exhaustion of such optimizations, the growth of modern AI is now gated by the performance of training systems, especially their data movement. Instead of focusing on single accelerators, we investigate data-movement characteristics of large-scale training at full system scale. Based on our workload analysis, we design HammingMesh, a novel network topology that provides high bandwidth at low cost with high job scheduling flexibility. Specifically, HammingMesh can support full bandwidth and isolation to deep learning training jobs with two dimensions of parallelism. Furthermore, it also supports high global bandwidth for generic traffic. Thus, HammingMesh will power future large-scale deep learning systems with extreme bandwidth requirements.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046066","Network architecture;Deep Learning;Clusters;Software defined networking","Deep learning;Training;Costs;Network topology;Neural networks;Bandwidth;Parallel processing","","","","","",83.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"PolarFly: A Cost-Effective and Flexible Low-Diameter Topology","K. Lakhotia; M. Besta; L. Monroe; K. Isham; P. Iff; T. Hoefler; F. Petrini","Intel Labs, Santa Clara, CA, USA; Scalable Parallel Computing Laboratory, ETH Zürich, Zürich, Switzerland; High Performance Computing Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Colgate University, Hamilton, NY, USA; Scalable Parallel Computing Laboratory, ETH Zürich, Zürich, Switzerland; Scalable Parallel Computing Laboratory, ETH Zürich, Zürich, Switzerland; Intel Labs, Santa Clara, CA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"In this paper we present PolarFly, a diameter-2 network topology based on the Erdos-Renyi family of polarity graphs from finite geometry. This is the first known diameter-2 topology that asymptotically reaches the Moore bound on the number of nodes for a given network degree and diameter. PolarFly achieves high Moore bound efficiency even for the moderate radixes commonly seen in current and near-future routers, reaching more than 96% of the theoretical peak. It also offers more feasible router degrees than the state-of-the-art solutions, greatly adding to the selection of scalable diameter-2 networks. PolarFly enjoys many other topological properties highly relevant in practice, such as a modular design and expandability that allow incremental growth in network size without rewiring the whole network. Our evaluation shows that PolarFly outperforms competitive networks in terms of scalability, cost and performance for various traffic patterns.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00017","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046084","Network architecture;Network Topology;Co-packaged Photonics;Polarity Graphs;Projective Planes","Performance evaluation;Optical losses;Upper bound;Network topology;Scalability;High performance computing;Traffic control","","","",1.0,"",67.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Combining Hard and Soft Constraints in Quantum Constraint-Satisfaction Systems","E. Wilson; F. Mueller; S. Pakin","North Carolina State University Raleigh, North Carolina; North Carolina State University Raleigh, North Carolina; Los Alamos National Laboratory, Los Alamos, New Mexico","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"This work presents a generalization of NchooseK, a constraint satisfaction system designed to target both quantum circuit devices and quantum annealing devices. Previously, NchooseK supported only hard constraints, which made it suitable for expressing problems in NP (e.g., 3-SAT) but not NP-hard problems (e.g., minimum vertex cover). In this paper we show how support for soft constraints can be added to the model and implementation, broadening the classes of problems that can be expressed elegantly in NchooseK without sacrificing portability across different quantum devices. Through a set of examples, we argue that this enhanced version of NchooseK enables problems to be expressed in a more concise, less error-prone manner than if these problems were encoded manually for quantum execution. We include an empirical evaluation of performance, scalability, and fidelity on both a large IBM Q system and a large D- Wave system.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046072","circuit-model quantum computing;quantum annealing;programming models","Performance evaluation;Annealing;NP-hard problem;Computational modeling;Scalability;Qubit;Quantum annealing","","","","","",42.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Large-Scale Simulation of Quantum Computational Chemistry on a New Sunway Supercomputer","H. Shang; L. Shen; Y. Fan; Z. Xu; C. Guo; J. Liu; W. Zhou; H. Ma; R. Lin; Y. Yang; F. Li; Z. Wang; Y. Zhang; Z. Li","Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Hefei National Research Center for Physical Sciences at the Microscale, University of Science and Technology of China, Hefei, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Shanghai Research Center for Quantum Sciences, Shanghai, China; Hefei National Laboratory, University of Science and Technology of China, Hefei, China; National Supercomputing Center in Wuxi, Wuxi, China; Hefei National Research Center for Physical Sciences at the Microscale, University of Science and Technology of China, Hefei, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; Pilot National Laboratory for Marine Science and Technology (Qingdao), Qingdao, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Hefei National Research Center for Physical Sciences at the Microscale, University of Science and Technology of China, Hefei, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Quantum computational chemistry (QCC) is the use of quantum computers to solve problems in computational quantum chemistry. We develop a high performance variational quantum eigensolver (VQE) simulator for simulating quantum computational chemistry problems on a new Sunway supercomputer. The major innovations include: (1) a Matrix Product State (MPS) based VQE simulator to reduce the amount of memory needed and increase the simulation efficiency; (2) a combination of the Density Matrix Embedding Theory with the MPS-based VQE simulator to further extend the simulation range; (3) A three-level parallelization scheme to scale up to 20 million cores; (4) Usage of the Julia script language as the main programming language, which both makes the programming easier and enables cutting edge performance as native C or Fortran; (5) Study of real chemistry systems based on the VQE simulator, achieving nearly linearly strong and weak scaling. Our simulation demonstrates the power of VQE for large quantum chemistry systems, thus paves the way for large-scale VQE experiments on near-term quantum computers.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046049","quantum computing;quantum computational chemistry;matrix product state;variational quantum eigensolver;density matrix embedding theory;Julia","Technological innovation;Quantum computing;Computational modeling;Quantum chemistry;Qubit;Quantum mechanics;Programming","","","","","",57.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Charter: Identifying the Most-Critical Gate Operations in Quantum Circuits via Amplified Gate Reversibility","T. Patel; D. Silver; D. Tiwari","Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,16,"When quantum programs are executed on noisy intermediate-scale quantum (NISQ) computers, they experience hardware noise; consequently, the program outputs are often erroneous. To mitigate the adverse effects of hardware noise, it is necessary to understand the effect of hardware noise on the program output and more fundamentally, understand the impact of hardware noise on specific regions within a quantum program. Identifying and optimizing regions that are more noise-sensitive is the key to expanding the capabilities of NISQ computers. Toward achieving that goal, we propose Charter, a novel technique to pinpoint specific gates and regions within a quantum program that are the most affected by the hardware noise and that have the highest impact on the program output. Using Charter's methodology, programmers can obtain a precise understanding of how different components of their code affect the output and optimize those components without the need for non-scalable quantum simulation on classical computers.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00020","Northeastern University(grant numbers:1910601,2144540); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046102","Quantum Computing;NISQ Computing;Quantum Error Detection;Quantum Error Mitigation","Computers;Codes;High performance computing;Logic gates;Hardware;Noise measurement;Quantum circuit","","","","","",64.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"A Taxonomy of Error Sources in HPC I/O Machine Learning Models","M. Isakov; M. Currier; E. del Rosario; S. Madireddy; P. Balaprakash; P. Carns; R. B. Ross; G. K. Lockwood; M. A. Kinsy","Secure, Trusted and Assured Microelectronics (STAM) Center, Ira A. Fulton Schools of Engineering, Arizona State University, Tempe, AZ; Secure, Trusted and Assured Microelectronics (STAM) Center, Ira A. Fulton Schools of Engineering, Arizona State University, Tempe, AZ; Secure, Trusted and Assured Microelectronics (STAM) Center, Ira A. Fulton Schools of Engineering, Arizona State University, Tempe, AZ; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Lawrence Berkeley National Laboratory, Berkeley, CA; Secure, Trusted and Assured Microelectronics (STAM) Center, Ira A. Fulton Schools of Engineering, Arizona State University, Tempe, AZ","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"I/O efficiency is crucial to productivity in scientific computing, but the growing complexity of HPC systems and applications complicates efforts to understand and optimize I/O behavior at scale. Data-driven machine learning-based I/O throughput models offer a solution: they can be used to identify bottlenecks, automate I/O tuning, or optimize job scheduling with minimal human intervention. Unfortunately, current state-of-the-art I/O models are not robust enough for production use and underperform after being deployed. We analyze four years of application, scheduler, and storage system logs on two leadership-class HPC platforms to understand why I/O models underperform in practice. We propose a taxonomy consisting of five categories of I/O modeling errors: poor application and system modeling, inadequate dataset coverage, I/O contention, and I/O noise. We develop litmus tests to quantify each category, allowing researchers to narrow down failure modes, enhance I/O throughput models, and improve future generations of HPC logging and analysis tools.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046097","High performance computing;I/O;storage;machine learning","Productivity;Analytical models;Scientific computing;Processor scheduling;Computational modeling;Taxonomy;Machine learning","","","","","",33.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Mitigating Silent Data Corruptions in HPC Applications across Multiple Program Inputs","Y. Huang; S. Guo; S. Di; G. Li; F. Cappello","Computer Science Department, University of Iowa, Iowa City, IA, USA; Baidu Security, Sunnyvale, CA, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Computer Science Department, University of Iowa, Iowa City, IA, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"With the ever-shrinking size of transistors, silent data corruptions (SDCs) are becoming a common yet serious issue in HPC. Selective instruction duplication (SID) is a widely used fault-tolerance technique that can obtain high SDC coverage with low performance overhead. However, existing SID methods are confined to single program input in its assessment, assuming that error resilience of a program remains similar across inputs. Nevertheless, we observe that the assumption cannot always hold, leading to a drastic loss in SDC coverage across different inputs, compromising HPC reliability. We notice that the SDC coverage loss correlates with a small set of instructions - we call them incubative instructions, which reveal elusive error propagation characteristics across multiple inputs. We propose Minpsid, an automated SID framework that automatically identifies and re-prioritizes incubative instructions in a given program to enhance SDC coverage. Evaluation shows Minpsid can effectively mitigate the loss of SDC coverage across multiple inputs.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046091","Silent Data Corruption;Error Resilience;Fault Injection;Instruction Duplication;Program Analysis;Software Testing;High Performance Computing","Fault tolerance;High performance computing;Fault tolerant systems;Propagation losses;Transistors;Resilience","","","","","",55.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Optimizing Random Access to Hierarchically-Compressed Data on GPU","F. Zhang; Y. Hu; H. Ding; Z. Yao; Z. Wei; X. Zhang; X. Du","Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"GPU's powerful computational capacity holds great potentials for processing hierarchically-compressed data without decompression in data science domain. Unfortunately, existing GPU approaches offer only traversal-based data analytics; random access is extremely inefficient, substantially limiting their utility. To solve this problem, we develop a novel and broadly applicable optimization that enables efficient random access to hierarchically-compressed data without decompression in GPU memory. We address three major challenges for enabling efficient random access to compressed data on GPUs. The first challenge is designing GPU data structures that support random access. The second challenge is efficiently generating data structures on GPU. Generating data structures for random access is costly on the CPU, and the inefficiency increases dramatically when PCIe data transmission is incorporated. The third challenge is query processing on compressed data in GPU memory. Random accesses, including data updates, result in significant conflicts between massive threads. To solve the first challenge, we propose and modify a number of compressed data structures, including indexing within the complicated GPU memory hierarchy. To address the second challenge, we develop a two-phase process for generating these data structures on the GPU. To handle the third challenge, we propose a double-parsing design to avoid data conflicts. We evaluate our solution on two GPU platforms using five real-world datasets. Experiments show that the random access operations on GPU can achieve 65.04x average speedup compared to the state-of-the-art method.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00023","National Key Research and Development Program of China(grant numbers:2018YFB1004401); National Natural Science Foundation of China(grant numbers:62172419,61732014,61972401,61932001,61832017,62072458); Beijing Natural Science Foundation(grant numbers:L192027,4222028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046120","GPU optimization;random access;hierarchically-compression;compressed data direct computing","Limiting;Query processing;Instruction sets;High performance computing;Memory management;Graphics processing units;Data science","","","","","",78.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Scaling Graph 500 SSSP to 140 Trillion Edges with over 40 Million Cores","Y. Wang; H. Cao; Z. Ma; W. Yin; W. Chen","Department of Computer Science and Technology & BRNist, Tsinghua University, Beijing, China; Department of Computer Science and Technology & BRNist, Tsinghua University, Beijing, China; Department of Computer Science and Technology & BRNist, Tsinghua University, Beijing, China; National Supercomputing Center in Wuxi, Wuxi, China; Department of Computer Science and Technology & BRNist, Tsinghua University, Beijing, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"The SSSP kernel was first introduced into the Graph 500 benchmark in 2017. However, there has been no result from a full-scale world-top supercomputer. The primary reason is the poor work-inefficiency of existing algorithms at large scales. In this paper, we propose an SSSP implementation for The Newest Generation Sunway Supercomputer,including an SSSP algorithm to achieve work-efficiency, along with an adaptive dense/sparse-mode selection approach to achieve communication-efficiency. Our implementation reaches 7638 GTEPS, with 103158 processors (over 40 million cores), and achieves 3.7× in performance and 512× in graph size compared with the current top one on the Graph 500 SSSP list. Based on our experience of running extreme-scale SSSP, we uncover the root cause of its poor scalability: the weight distribution allows edges with weights close to zero, making the SSSP tree deeper on larger graphs. We further explore a scalability-friendly weight distribution by setting a non-zero lower bound to the edge weights.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00024","NSFC(grant numbers:U20B2044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046090","Graphs;Benchmark testing;Scalability;Super-computers;Shortest path problem","Program processors;Scalability;High performance computing;Benchmark testing;Parallel processing;Supercomputers;Kernel","","","","","",27.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Study of Workload Interference with Intelligent Routing on Dragonfly","Y. Kang; X. Wang; Z. Lan","Department of Computer Science, Illinois Institute of Technology, Chicago, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Dragonfly interconnect is a crucial network technol-ogy for supercomputers. To support exascale systems, network resources are shared such that links and routers are not dedicated to any node pair. While link utilization is increased, workload performance is often offset by network contention. Recently, intelligent routing built on reinforcement learning demonstrates higher network throughput with lower packet latency. However, its effectiveness in reducing workload interference is unknown. In this work, we present extensive network simulations to study multi-workload contention under different routing mechanisms, intelligent routing and adaptive routing, on a large-scale Dragon-fly system. We develop an enhanced network simulation toolkit, along with a suite of workloads with distinctive communication patterns. We also present two metrics to characterize application communication intensity. Our analysis focuses on examining how different workloads interfere with each other under different routing mechanisms by inspecting both application-level and network-level metrics. Several key insights are made from the analysis.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046126","HPC;interconnect network;Dragonfly;network interference","Measurement;Adaptation models;Network topology;Exascale computing;Interference;Reinforcement learning;Routing","","","",1.0,"",42.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"SERVIZ: A Shared In Situ Visualization Service","S. Ramesh; H. Childs; A. Malony","University of Oregon, Eugene, OR, USA; University of Oregon, Eugene, OR, USA; University of Oregon, Eugene, OR, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Inline and in transit visualization are popular in situ visualization models for high performance computing (HPC) applications. Inline visualization is invoked through a library call on the HPC application (simulation), while in transit methods invoke a visualization module running on in transit resources. In transit methods can offer better efficiency than inline by running the visualization at a lower concurrency level than the simulation. State-of-the-art in transit schemes are limited to employing a dedicated in transit resource for every simulation. The resulting idle time on the in transit resource can severely limit the cost savings over inline methods. This research proposes SERVIZ, an in transit visualization service that can be shared amongst multiple simulations to reduce idle time, thereby efficiently using in transit resources. SERVIZ achieves cost savings of up to 26% over inline and up to $4\mathbf{x}$ reduction in idle time compared to a dedicated in transit implementation.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046135","visualization;services;in-situ;performance;cost","Concurrent computing;Visualization;Costs;Computational modeling;High performance computing;Libraries","","","","","",27.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"DayDream: Executing Dynamic Scientific Workflows on Serverless Platforms with Hot Starts","R. B. Roy; T. Patel; D. Tiwari","Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,18,"HPC applications are increasingly being designed as dynamic workflows for the ease of development and scaling. This work demonstrates how the serverless computing model can be leveraged for efficient execution of complex, real-world scientific workflows, although serverless computing was not originally designed for executing scientific workflows. This work characterizes, quantifies, and improves the execution of three real-world, complex, dynamic scientific workflows: ExaFEL (workflow for investigating the molecular structures via X-Ray diffraction), Cosmoscout-Vr(workflow for large scale virtual reality simulation), and Core Cosmology Library (a cosmology workflow for investigating dark matter). The proposed technique, DayDream, employs the hot start mechanism for warming up the components of the workflows by decoupling the runtime environment from the component function code to mitigate cold start overhead. DayDream optimizes the service time and service cost jointly to reduce the service time by 45% and service cost by 23% over the state-of-the-art HPC workload manager.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046081","Serverless Computing;HPC Workflows;Cloud Computing","Solid modeling;Runtime environment;Costs;Codes;Computational modeling;High performance computing;Serverless computing","","","","","",91.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"LabStor: A Modular and Extensible Platform for Developing High-Performance, Customized I/O Stacks in Userspace","L. Logan; J. C. Garcia; J. Lofstead; X. Sun; A. Kougkas","Illinois Institute of Technology, Chicago, USA; Illinois Institute of Technology, Chicago, USA; Sandia National Laboratories, Albequerque, USA; Illinois Institute of Technology, Chicago, USA; Illinois Institute of Technology, Chicago, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Traditionally, I/O systems have been developed within the confines of a centralized OS kernel. This led to monolithic and rigid storage systems that are limited by low development speed, expressiveness, and performance. Various assumptions are imposed including reliance on the UNIX-file abstraction, the POSIX standard, and a narrow set of I/O policies. However, this monolithic design philosophy makes it difficult to develop and deploy new I/O approaches to satisfy the rapidly-evolving I/O requirements of modern scientific applications. To this end, we propose LabStor: a modular and extensible platform for developing high-performance, customized I/O stacks. Single-purpose I/O modules (e.g, I/O schedulers) can be developed in the comfort of userspace and released as plug-ins, while end-users can compose these modules to form workload- and hardware-specific I/O stacks. Evaluations show that by switching to a fully modular design, tailored I/O stacks can yield performance improvements of up to 60% in various applications.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046077","Clouds and Distributed Computing;Programming Frameworks and System Software","Runtime environment;Philosophical considerations;Linux;High performance computing;Switches;Performance gain;System software","","","","","",77.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Towards Scalable Resource Management for Supercomputers","Y. Dai; Y. Dong; K. Lu; R. Wang; W. Zhang; J. Chen; M. Shao; Z. Wang","National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; University of Leeds, Leeds, United Kingdom","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Today's supercomputers offer massive computation resources to execute a large number of user jobs. Effectively managing such large-scale hardware parallelism and workloads is essential for supercomputers. However, existing HPC resource management (RM) systems fail to capitalize on the hardware parallelism by following a centralized design used decades ago. They give poor scalability and inefficient performance on today's supercomputers, which will worsen in exascale computing. We present ESlurm, a better RM for supercomputers. As a departure from existing HPC RMs, ESlurm implements a distributed communication structure. It employs a new communication tree strategy and uses job runtime estimation to improve communications and job scheduling efficiency. ESlurm is deployed into production in a real supercomputer. We evaluate ESlurm on up to 20K nodes. Compared to state-of-the-art RM solutions, ESlurm exhibits better scalability, significantly reducing the resource usage of master nodes and improving data transfer and job scheduling efficiency by a large margin.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00029","National Natural Science Foundation of China (NSFC)(grant numbers:61902405,61872294); National University of Defense Technology Foundation(grant numbers:ZK20-09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046103","Resource management;Exascale computing;Scheduling","Runtime;Processor scheduling;Scalability;Estimation;Production;Parallel processing;Supercomputers","","","","","",66.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Deinsum: Practically I/O Optimal Multi-Linear Algebra","A. N. Ziogas; G. Kwasniewski; T. Ben-Nun; T. Schneider; T. Hoefler","Department of Computer Science, ETH Zurich, Zurich, Switzerland; NextSilicon, Tel Aviv, Israel; Department of Computer Science, ETH Zurich, Zurich, Switzerland; Department of Computer Science, ETH Zurich, Zurich, Switzerland; Department of Computer Science, ETH Zurich, Zurich, Switzerland","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Multilinear algebra kernel performance on modern massively-parallel systems is determined mainly by data movement. However, deriving data movement-optimal distributed schedules for programs with many high-dimensional inputs is a notoriously hard problem. State-of-the-art libraries rely on heuristics and often fall back to suboptimal tensor folding and BLAS calls. We present Deinsum, an automated framework for distributed multilinear algebra computations expressed in Einstein notation, based on rigorous mathematical tools to address this problem. Our framework automatically derives data movement-optimal tiling and generates corresponding distributed schedules, further optimizing the performance of local computations by increasing their arithmetic intensity. To show the benefits of our approach, we test it on two important tensor kernel classes: Matricized Tensor Times Khatri-Rao Products and Tensor Times Matrix chains. We show performance results and scaling on the Piz Daint supercomputer, with up to 19x speedup over state-of-the-art solutions on 512 nodes.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00030","Swiss National Science Foundation(grant numbers:185778); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046073","automatic programming;distributed computing;performance analysis;linear algebra;tensors;hardware acceleration","Schedules;Tensors;Algebra;High performance computing;Distributed databases;Supercomputers;Libraries","","","","","",61.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Addressing Irregular Patterns of Matrix Computations on GPUs and Their Impact on Applications Powered by Sparse Direct Solvers","A. Abdelfattah; P. Ghysels; W. Boukaram; S. Tomov; X. S. Li; J. Dongarra","Innovative Computing Laboratory, University of Tennessee, Knoxville, USA; Lawrence Berkeley National Laboratory, Scalable Solvers Group, Berkeley, USA; Lawrence Berkeley National Laboratory, Scalable Solvers Group, Berkeley, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, USA; Lawrence Berkeley National Laboratory, Scalable Solvers Group, Berkeley, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Many scientific applications rely on sparse direct solvers for their numerical robustness. However, performance optimization for these solvers remains a challenging task, especially on GPUs. This is due to workloads of small dense matrices that are different in size. Matrix decompositions on such irregular workloads are rarely addressed on GPUs. This paper addresses irregular workloads of matrix computations on GPUs, and their application to accelerate sparse direct solvers. We design an interface for the basic matrix operations supporting problems of different sizes. The interface enables us to develop irrLU-GPU, an LU decomposition on matrices of different sizes. We demonstrate the impact of irrLU-GPU on sparse direct LU solvers using NVIDIA and AMD GPUs. Experimental results are shown for a sparse direct solver based on a multifrontal sparse LU decomposition applied to linear systems arising from the simulation, using finite element discretization on unstructured meshes, of a high-frequency indefinite Maxwell problem.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046092","Irregular computational workloads;GPU Computing;LU factorization;multifrontal solvers;sparse direct solvers","Linear systems;High performance computing;Computational modeling;Robustness;Finite element analysis;Matrix decomposition;Sparse matrices","","","","","",32.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Accelerating Elliptic Curve Digital Signature Algorithms on GPUs","Z. Feng; Q. Xie; Q. Luo; Y. Chen; H. Li; H. Li; Q. Yan","Hong Kong University of Science and Technology, Hong Kong SAR, China; Hong Kong University of Science and Technology, Hong Kong SAR, China; Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China; WeBank, Shenzhen, China; WeBank, Shenzhen, China; WeBank, Shenzhen, China; WeBank, Shenzhen, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,13,"The Elliptic Curve Digital Signature Algorithm (ECDSA) is an essential building block of various cryptographic protocols. In particular, most blockchain systems adopt it to ensure transaction integrity. However, due to its high computational intensity, ECDSA is often the performance bottleneck in blockchain transaction processing. Recent work has accelerated ECDSA algorithms on the CPU; in contrast, success has been limited on the GPU, which has great potential for parallelization but is challenging for implementing elliptic curve functions. In this paper, we propose RapidEC, a GPU-based ECDSA implementation for SM2, a popular elliptic curve. Specifically, we design architecture-aware parallel primitives for elliptic curve point operations, and parallelize the processing of a single SM2 request as well as batches of requests. Consequently, our GPU-based RapidEC outperformed the state-of-the-art CPU-based algorithm by orders of magnitude. Additionally, our GPU-based modular arithmetic functions as well as point operation primitives can be applied to other computation tasks.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00032","WeBank(grant numbers:WEB19EG01-H); Hong Kong Research Grants Council(grant numbers:16209821); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046064","ECDSA;Parallel Processing;GPU","Elliptic curves;Source coding;Memory management;Graphics processing units;Throughput;Libraries;Blockchains","","","","","",30.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"CA3DMM: A New Algorithm Based on a Unified View of Parallel Matrix Multiplication","H. Huang; E. Chow","School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, Georgia, U.S.A.; School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, Georgia, U.S.A.","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"This paper presents the Communication-Avoiding 3D Matrix Multiplication (CA3DMM) algorithm, a simple and novel algorithm that has optimal or near-optimal communication cost. CA3DMM is based on a unified view of parallel matrix multiplication. Such a view generalizes 1D, 2D, and 3D matrix multiplication algorithms to reduce the data exchange volume for different shapes of input matrices. CA3DMM further minimizes the actual communication costs by carefully organizing its communication patterns. CA3DMM is much simpler than some other generalized 3D algorithms, and CA3DMM does not require low-level optimization. Numerical experiments show that CA3DMM has good parallel scalability and has similar or better performance when compared to state-of-the-art PGEMM implementations for a wide range of matrix dimensions and number of processes.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00033","U.S. Department of Energy(grant numbers:DE-SC0019410); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046111","matrix multiplication;communication optimization;parallel algorithm;high-performance computing","Three-dimensional displays;Costs;Shape;Scalability;High performance computing;Optimization","","","","","",32.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Symmetric Block-Cyclic Distribution: Fewer Communications Leads to Faster Dense Cholesky Factorization","O. Beaumont; P. Duchon; L. Eyraud-Dubois; J. Langou; M. Vérité","French Institute for Research in Computer Science and Automation (INRIA), Bordeaux, France; Laboratoire Bordelais de Recherche en Informatique (LaBRI), Bordeaux, France; French Institute for Research in Computer Science and Automation (INRIA), Bordeaux, France; University of Colorado Denver, Denver, Colorado, US; French Institute for Research in Computer Science and Automation (INRIA), Bordeaux, France","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"We consider the distributed Cholesky factorization on homogeneous nodes. Inspired by recent progress on asymptotic lower bounds on the total communication volume required to perform Cholesky factorization, we present an original data distribution, Symmetric Block Cyclic (SBC), designed to take advantage of the symmetry of the matrix. We prove that SBC reduces the overall communication volume between nodes by a factor of square root of 2 compared to the standard 2D block-cyclic distribution. SBC can easily be implemented within the paradigm of task-based runtime systems. Experiments using the Chameleon library over the StarPU runtime system demonstrate that the SBC distribution reduces the communication volume as expected, and also achieves better performance and scalability than the classical 2D block-cyclic allocation scheme in all configurations. We also propose a 2.5D variant of SBC and prove that it further improves the communication and performance benefits.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046122","Algorithms for numerical methods and algebraic systems;Load balancing and scheduling algorithms","Runtime;Upper bound;Symmetric matrices;Scheduling algorithms;Scalability;Libraries;Resource management","","","","","",18.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Scalable Distributed High-Order Stencil Computations","M. Jacquelin; M. Araya–Polo; J. Meng","Cerebras Systems, Sunnyvale, California, USA; TotalEnergies EP Research & Technology US, LLC, Houston, Texas, USA; TotalEnergies EP Research & Technology US, LLC, Houston, Texas, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,13,"Stencil computations lie at the heart of many scientific and industrial applications. Stencil algorithms pose several challenges on machines with cache based memory hierarchy, due to low re-use of memory accesses if special care is not taken to optimize them. This work shows that for stencil computation a novel algorithm that leverages a localized communication strategy effectively exploits the second generation Cerebras Wafer-Scale Engine (WSE-2), which has no cache hierarchy. This study focuses on a 25-point stencil finite-difference method for the 3D wave equation, a kernel frequently used in earth modeling as numerical simulation. In essence, the algorithm trades memory accesses for data communication and takes advantage of the fast communication fabric provided by the architecture. The algorithm -historically memory-bound- becomes compute-bound. This allows the implementation to achieve near perfect weak scaling, reaching up to 503 TFLOPs on WSE-2, a figure that only full clusters can eventually yield.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046080","Stencil computation;high performance computing;energy;wafer-scale;distributed memory;multi-processor architecture and micro-architecture","Semiconductor device modeling;Solid modeling;Three-dimensional displays;Computational modeling;Memory architecture;Clustering algorithms;Throughput","","","","","",41.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Memory Optimizations in an Array Language","P. Munksgaard; T. Henriksen; P. Sadayappan; C. Oancea","University of Copenhagen, Copenhagen, Denmark; University of Copenhagen, Copenhagen, Denmark; University of Utah, Salt Lake City, USA; University of Copenhagen, Copenhagen, Denmark","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"We present a technique for introducing and op-timizing the use of memory in a functional array language, aimed at GPU execution, that supports correct-by-construction parallelism. Using linear memory access descriptors as building blocks, we define a notion of memory in the compiler IR that enables cost-free change-of-layout transformations (e.g., slicing, transposition), whose results can even be carried across control flow such as ifs/loops without manifestation in memory. The memory notion allows a graceful transition to an unsafe IR that is automatically optimized (1) to mix reads and writes to the same array inside a parallel construct, and (2) to map semantically different arrays to the same memory block. The result is code similar to what imperative users would write. Our evaluation shows that our optimizations have significant impact (1.1 x -2 x) and result in performance competitive to hand-written code from challenging benchmarks, such as Rodinia's NW, LUD, Hotspot.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046117","GPU;parallelism;functional programming;op-timizing compiler","Codes;High performance computing;Graphics processing units;Benchmark testing;Programming;Parallel processing;Optimization","","","","","",36.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Vectorizing Sparse Matrix Computations with Partially-Strided Codelets","K. Cheshmi; Z. Cetinic; M. M. Dehnavi","McMaster University, Canada; University of Toronto, Toronto, Canada; University of Toronto, Toronto, Canada","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"The compact data structures and irregular computation patterns in sparse matrix computations introduce challenges to vectorizing these codes. Available approaches primarily vec-torize strided computation regions of a sparse code. In this work, we propose a locality-based codelet mining (LCM) algorithm that efficiently searches for strided and partially strided regions in sparse matrix computations for vectorization. We also present a classification of partially strided codelets and a differentiation-based approach to generate codelets from memory accesses in the sparse computation. LCM is implemented as an inspector-executor framework called LCM I/E that generates vectorized code for the sparse matrix-vector multiplication (SpMV), sparse matrix times dense matrix (SpMM), and sparse triangular solver (SpTRSV). LCM I/E outperforms the MKL library with an average speedup of 1.67x, 4.1x, and 1.75x for SpMV, SpTRSV, and SpMM, respectively. It is also faster than the state-of-the-art inspector-executor framework Sympiler [1] for the SpTRSV kernel with an average speedup of 1.9 x.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00037","NSF(grant numbers:ACI-1548562); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046127","Vectorization;Parallel programming;Polyhedral analysis;Sparse matrix computation","Codes;High performance computing;Memory management;Data structures;Libraries;Computational efficiency;Classification algorithms","","","","","",61.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Finding Inputs that Trigger Floating-Point Exceptions in GPUs via Bayesian Optimization","I. Laguna; G. Gopalakrishnan","Lawrence Livermore National Laboratory Livermore, Center for Applied Scientific Computing, USA; School Of Computing, University of Utah, Salt Lake City, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Testing code for floating-point exceptions is crucial as exceptions can quickly propagate and produce unreliable numerical answers. The state-of-the-art to test for floating-point exceptions in GPUs is quite limited and solutions require the ap-plication's source code, which precludes their use in accelerated libraries where the source is not publicly available. We present an approach to find inputs that trigger floating-point exceptions in black-box GPU functions, i.e., functions where the source code and information about input bounds are unavailable. Our approach is the first to use Bayesian optimization (BO) to identify such inputs and uses novel strategies to overcome the challenges that arise in applying BO to this problem. We implement our approach in the XSCOPE framework and demonstrate it on 58 functions from the CUDA Math Library and functions from ten HPC programs. XSCOPE is able to identify inputs that trigger exceptions in about 72% of the tested functions.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00038","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046079","Floating-point exceptions;GPU computing;Bayesian optimization","Codes;Source coding;Graphics processing units;Closed box;Life estimation;Libraries;Bayes methods","","","","","",22.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Mapping Out the HPC Dependency Chaos","F. Zakaria; T. R. W. Scogland; T. Gamblin; C. Maltzahn","University of California Santa Cruz, Santa Cruz, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; University of California Santa Cruz, Santa Cruz, CA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,12,"High Performance Computing (HPC) software stacks have become complex, with the dependencies of some applications numbering in the hundreds. Packaging, distributing, and administering software stacks of that scale is a complex undertaking anywhere. HPC systems deal with esoteric compilers, hardware, and a panoply of uncommon combinations. In this paper, we explore the mechanisms available for packaging software to find its own dependencies in the context of a taxonomy of software distribution, and discuss their benefits and pitfalls. We discuss workarounds for some common problems caused by using these composed stacks and introduce Shrinkwrap: A solution to producing binaries that directly load their dependencies from precise locations and in a precise order. Beyond simplifying the use of the binaries, this approach also speeds up loading as much as 7× for a large dynamically-linked MPI application in our evaluation.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00039","National Science Foundation(grant numbers:NA0003525); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046051","toolchains;package management;operating systems;filesystem hierarchy","Chaos;Biological system modeling;High performance computing;Taxonomy;Semantics;Full stack;Packaging","","","","","",32.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Using Answer Set Programming for HPC Dependency Solving","T. Gamblin; M. Culpo; G. Becker; S. Shudler","Lawrence Livermore National Laboratory, Livermore, CA, USA; np-complete, S.r.l., Italy; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Modern scientific software stacks have become extremely complex, using many programming models and libraries to exploit a growing variety of GPUs and accelerators. Package managers can mitigate this complexity using dependency solvers, but they are reaching their limits. Finding compatible dependency versions is NP-complete, and modeling the semantics of package compatibility modulo build-time options, GPU runtimes, flags, and other parameters is extremely difficult. Within this enormous configuration space, defining a “good” configuration is daunting. We tackle this problem using Answer Set Programming (ASP), a declarative model for combinatorial search problems. We show, using the Spack package manager, that ASP programs can concisely express the compatibility rules of HPC software stacks and provide strong quality-of-solution guarantees. Using ASP, we can mix new builds with preinstalled binaries, and solver performance is acceptable even when considering tens of thousands of packages.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00040","U.S. Department of Energy; Lawrence Livermore National Laboratory(grant numbers:DE-AC52-07NA27344); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046107","High performance computing;Software packages;Package management;Logic programming;Answer set programming;Software reusability;Dependency management","Runtime;Semantics;Full stack;Programming;Syntactics;Search problems;Software","","","","","",43.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"SPATL: Salient Parameter Aggregation and Transfer Learning for Heterogeneous Federated Learning","S. Yu; P. Nguyen; W. Abebe; W. Qian; A. Anwar; A. Jannesari","Department of Computer Science, Iowa State University, Ames, IA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA; IBM Almaden Research Center, San Jose, CA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Federated learning (FL) facilitates the training and deploying AI models on edge devices. Preserving user data privacy in FL introduces several challenges, including expensive communication costs, limited resources, and data heterogeneity. In this paper, we propose SPATL, an FL method that addresses these issues by: (a) introducing a salient parameter selection agent and communicating selected parameters only; (b) splitting a model into a shared encoder and a local predictor, and transferring its knowledge to heterogeneous clients via the locally customized predictor. Additionally, we leverage a gradient control mechanism to further speed up model convergence and increase robustness of training processes. Experiments demonstrate that SPATL reduces communication overhead, accelerates model inference, and enables stable training processes with better results compared to state-of-the-art methods. Our approach reduces communication cost by up to 86.45%, accelerates local inference by reducing up to 39.7% FLOPs on VGG-11, and requires 7.4× less communication overhead when training ResNet-20.11Code is available at: https://github.com/yusx-swapp/SPATL","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046104","Federated Learning;Heterogeneous System;Machine Learning;ML;FL","Training;Data privacy;Costs;Federated learning;High performance computing;Transfer learning;Process control","","","","","",73.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Efficient Quantized Sparse Matrix Operations on Tensor Cores","S. Li; K. Osawa; T. Hoefler","Department of Computer Science, ETH Zurich, Zurich, Switzerland; Department of Computer Science, ETH Zurich, Zurich, Switzerland; Department of Computer Science, ETH Zurich, Zurich, Switzerland","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"The exponentially growing model size drives the continued success of deep learning, but it brings prohibitive computation and memory cost. From the algorithm perspective, model sparsification and quantization have been studied to alleviate the problem. From the architecture perspective, hardware vendors provide Tensor cores for acceleration. However, it is very challenging to gain practical speedups from sparse, low-precision matrix operations on Tensor cores, because of the strict requirements for data layout and lack of support for efficiently manipulating the low-precision integers. We propose Magicube, a high-performance sparse-matrix library for low-precision integers on Tensor cores. Magicube supports SpMM and SDDMM, two major sparse operations in deep learning with mixed precision. Experimental results on an NVIDIA A100show that Magicube achieves on average 1.44x (up to 2.37x) speedup over the vendor-optimized library for sparse kernels, and 1.43x speedup over the state-of-the-art with a comparable accuracy for end-to-end sparse Transformer inference.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046057","Sparse Matrix;Tensor Cores;GPU;Low-Precision Integers;Quantization;Sparse Transformer","Deep learning;Tensors;Quantization (signal);Computational modeling;Layout;Transformer cores;Transformers","","","","","",77.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"LightSeq2: Accelerated Training for Transformer-Based Models on GPUs","X. Wang; Y. Wei; Y. Xiong; G. Huang; X. Qian; Y. Ding; M. Wang; L. Li","ByteDance AI Lab, Shanghai, China; ByteDance AI Lab, Shanghai, China; ByteDance AI Lab, Shanghai, China; University of California, Santa Barbara, California, USA; ByteDance AI Lab, Mountain View, USA; University of California, Santa Barbara, California, USA; ByteDance AI Lab, Shanghai, China; University of California, Santa Barbara, California, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Transformer-based neural models are used in many AI applications. Training these models is expensive, as it takes huge GPU resources and long duration. It is challenging because typical data like sentences have variable lengths, and Transformer's computation patterns are more complex than convolutional neural networks. Existing systems either only focus on model inference or optimization for only BERT-like encoder models. In this paper, we present LightSeq2, a system to accelerate training for a general family of Transformer models on GPUs. We propose a series of GPU optimization techniques tailored to the specific computation flow and memory access patterns of Transformer models. LightSeq2 supports many model architectures, including BERT (encoder-only), GPT (decoder-only), Transformer (encoder-decoder), and vision Transformer. Our experiments for a variety of models and benchmarks show that LightSeq2 is consistently faster (1.4-3.5 x) than previous systems on different GPUs. In particular, it gains 308 % training speedup compared with existing systems on a large public machine translation benchmark (WMTI4 English-German).","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046070","Transformer;GPU Acceleration;Training;Natural Language Processing;Computer Vision","Training;Computational modeling;Scalability;Memory management;Graphics processing units;Benchmark testing;Transformers","","","","","",42.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"CoGNN: Efficient Scheduling for Concurrent GNN Training on GPUs","Q. Sun; Y. Liu; H. Yang; R. Zhang; M. Dun; M. Li; X. Liu; W. Xiao; Y. Li; Z. Luan; D. Qian","Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Unaffiliated, Beijing, China; Unaffiliated, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Graph neural networks (GNNs) suffer from low GPU utilization due to frequent memory accesses. Existing concurrent training mechanisms cannot be directly adapted to GNNs because they fail to consider the impact of input irregularity. This requires pre-profiling the memory footprint of concurrent tasks based on input dimensions to ensure successful co-location on GPU. Moreover, massive training tasks generated from scenarios such as hyper-parameter tuning require flexible scheduling strategies. To address these problems, we propose CoGNN that enables efficient management of GNN training tasks on GPUs. Specifically, the CoGNN organizes the tasks in a queue and estimates the memory consumption of each task based on cost functions at operator basis. In addition, the CoGNN implements scheduling policies to generate task groups, which are iteratively submitted for execution. The experiment results show that the CoGNN can achieve shorter completion and queuing time for training tasks from diverse GNN models.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046055","Graph Neural Networks;GPU;Concurrent Training;Task Scheduling;Estimation Model","Training;Processor scheduling;Computational modeling;High performance computing;Memory management;Graphics processing units;Graph neural networks","","","","","",65.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Using Unused: Non-Invasive Dynamic FaaS Infrastructure with HPC-Whisk","B. Przybylski; M. Pawlik; P. Żuk; B. Lagosz; M. Malawski; K. Rzadca","University of Warsaw, Institute of Informatics, Warsaw, Poland; Academic Computer Centre Cyfronet AGH, Krakow, Poland; University of Warsaw, Institute of Informatics, Warsaw, Poland; AGH University of Science and Technology, Institute of Computer Science, Krakow, Poland; AGH University of Science and Technology, Institute of Computer Science, Krakow, Poland; University of Warsaw, Institute of Informatics, Warsaw, Poland","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Modern HPC workload managers and their careful tuning contribute to the high utilization of HPC clusters. However, due to inevitable uncertainty it is impossible to completely avoid node idleness. Although such idle slots are usually too short for any HPC job, they are too long to ignore them. Function-as-a-Service (FaaS) paradigm promisingly fills this gap, and can be a good match, as typical FaaS functions last seconds, not hours. Here we show how to build a FaaS infrastructure on idle nodes in an HPC cluster in such a way that it does not affect the performance of the HPC jobs significantly. We dynamically adapt to a changing set of idle physical machines, by integrating open-source software Slurm and OpenWhisk. We designed and implemented a prototype solution that allowed us to cover up to 90% of the idle time slots on a 50k-core cluster that runs production workloads.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046086","supercomputer;function as a service;FaaS;serverless;high-performance computing;HPC","Reactive power;Uncertainty;Computational modeling;Benchmark testing;User experience;System software;Transient analysis","","","","","",34.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Canary: Fault-Tolerant FaaS for Stateful Time-Sensitive Applications","M. Arif; K. Assogba; M. M. Rafique","Department of Computer Science, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Science, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Science, Rochester Institute of Technology, Rochester, NY, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,16,"Function-as-a-Service (FaaS) platforms have recently gained rapid popularity. Many stateful applications have been migrated to FaaS platforms due to their ease of deployment, scalability, and minimal management overhead. However, failures in FaaS have not been thoroughly investigated, thus making these desirable platforms unreliable for guaranteeing function execution and ensuring performance requirements. In this paper, we propose Canary, a highly resilient and fault-tolerant framework for FaaS that mitigates the impact of failures and reduces the overhead of function restart. Canary utilizes replicated container runtimes and application-level checkpoints to reduce application recovery time over FaaS platforms. Our evaluations using representative stateful FaaS applications show that Canary reduces the application recovery time and dollar cost by up to 83% and 12%, respectively over the default retry-based strategy. Moreover, it improves application availability with an additional average execution time and cost overhead of 14% and 8%, respectively, as compared to the ideal failure-free execution.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046074","Data-intensive Computing;Serverless Computing;Deep Learning;Data Parallelism;OpenWhisk;TensorFlow","Fault tolerance;Costs;Runtime;Scalability;High performance computing;Fault tolerant systems;Containers","","","","","",97.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"SFS: Smart OS Scheduling for Serverless Functions","Y. Fu; L. Liu; H. Wang; Y. Cheng; S. Chen","University of Virginia, Charlottesville, VA, USA; George Mason University, Fairfax, VA, USA; Adobe Research, San Jose, CA, USA; University of Virginia, Charlottesville, VA, USA; George Mason University, Fairfax, VA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,16,"Serverless computing enables a new way of building and scaling cloud applications by allowing developers to write fine-grained serverless or cloud functions. The execution duration of a cloud function is typically short-ranging from a few milliseconds to hundreds of seconds. However, due to resource contentions caused by public clouds' deep consolidation, the function execution duration may get significantly prolonged and fail to accurately account for the function's true resource usage. We observe that the function duration can be highly unpredictable with huge amplification of more than 50× for an open-source FaaS platform (OpenLambda). Our experiments show that the OS scheduling policy of cloud functions' host server can have a crucial impact on performance. The default Linux scheduler, CFS (Completely Fair Scheduler), being oblivious to workloads, frequently context-switches short functions, causing a turnaround time that is much longer than their service time. We propose SFS (Smart Function Scheduler), which works entirely in the user space and carefully orchestrates existing Linux FIFO and CFS schedulers to approximate Shortest Remaining Time First (SRTF). SFS uses two-level scheduling that seamlessly combines a new FILTER policy with Linux CFS, to trade off increased duration of long functions for significant performance improvement for short functions. We implement SFS in the Linux user space and port it to OpenLambda. Evaluation results show that SFS significantly improves short functions' duration with a small impact on relatively longer functions, compared to CFS.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00047","NSF CAREER Award(grant numbers:CNS-2045680,CCF-1919075,CCF-1919113,OAC-2106446,CMMI-2134689,CNS-2007153); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046062","Cloud computing;Operating systems;Performance evaluation","Processor scheduling;Linux;High performance computing;Buildings;Serverless computing;Dynamic scheduling","","","","","",65.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"ProbGraph: High-Performance and High-Accuracy Graph Mining with Probabilistic Set Representations","M. Besta; C. Miglioli; P. S. Labini; J. Tětek; P. Iff; R. Kanakagiri; S. Ashkboos; K. Janda; M. Podstawski; G. Kwaśniewski; N. Gleinig; F. Vella; O. Mutlu; T. Hoefler","ETH Zurich, Zürich, Switzerland; Research Center for Statistics, University of Geneva, Geneva, Switzerland; Free University of Bozen-Bolzano, Bolzano, Italy; BARC, University of Copenhagen, Copenhagen, Denmark; ETH Zurich, Zürich, Switzerland; UIUC, Champaign, USA; ETH Zurich, Zürich, Switzerland; AGH-UST, Kraków, Poland; TCL Research Europe, Warsaw, Poland; ETH Zurich, Zürich, Switzerland; ETH Zurich, Zürich, Switzerland; University of Trento, Trento, Italy; ETH Zurich, Zürich, Switzerland; ETH Zurich, Zürich, Switzerland","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,17,"Important graph mining problems such as Clustering are computationally demanding. To significantly accelerate these problems, we propose ProbGraph: a graph representation that enables simple and fast approximate parallel graph mining with strong theoretical guarantees on work, depth, and result accuracy. The key idea is to represent sets of vertices using probabilistic set representations such as Bloom filters. These representations are much faster to process than the original vertex sets thanks to vectorizability and small size. We use these representations as building blocks in important parallel graph mining algorithms such as Clique Counting or Clustering. When enhanced with ProbGraph, these algorithms significantly outperform tuned parallel exact baselines (up to nearly 50 x on 32 cores) while ensuring accuracy of more than 90% for many input graph datasets. Our novel bounds and algorithms based on probabilistic set representations with desirable statistical properties are of separate interest for the data analytics community. Proofs of theorems & more results: http://arxiv.org/abs/2208.11469","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00048","European Research Council (Project DAPP)(grant numbers:678880,101002047); VILLUM Foundation(grant numbers:16582); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046121","Approximate Graph Mining;Approximate Graph Pattern Matching;Approximate Triangle Counting;Approximate Community Detection;Approximate Graph Clustering;Bloom Filters;MinHash;K Minimum Values;High-Performance Graph Computations;Graph Sketching","Matched filters;Runtime;Data analysis;Instruction sets;High performance computing;Clustering algorithms;Filtering algorithms","","","","","",116.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Blaze: Fast Graph Processing on Fast SSDs","J. Kim; S. Swanson","Computer Science and Engineering University of California San Diego, La Jolla, U.S.; Computer Science and Engineering University of California San Diego, La Jolla, U.S.","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Out-of-core graph processing is an attractive solution for processing very large graphs that do not fit in the memory of a single machine. The new class of ultra-low-latency SSDs should expand the impact and utility of out-of-core graph processing systems. However, current out-of-core systems cannot fully leverage the high IOPS these devices can deliver. We introduce Blaze, a new out-of-core graph processing system optimized for ultra-low-latency SSDs. Blaze offers high-performance out-of-core graph analytics by constantly saturating these fast SSDs with a new scatter-gather technique called online binning that allows value propagation among graph vertices without atomic synchronization. Blaze offers succinct APIs to allow programmers to write efficient out-of-core graph algorithms without the burden to manage complex IO executions. Our evaluation shows that Blaze outperforms current out-of-core systems by a wide margin on seven datasets and a set of representative graph queries on Intel Optane SSD.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046050","Network theory (graphs);Data analysis;High performance computing;Parallel processing","Message passing;High performance computing;Synchronization;Network theory (graphs)","","","","","",29.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"GraphFly: Efficient Asynchronous Streaming Graphs Processing via Dependency-Flow","D. Chen; C. Gui; Y. Zhang; H. Jin; L. Zheng; Y. Huang; X. Liao","National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System, Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System, Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System, Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System, Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System, Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System, Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System, Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Existing streaming graph processing systems typically adopt two phases of refinement and recomputation to ensure the correctness of the incremental computation. However, severe redundant memory accesses exist due to the unnecessary synchronization among independent edge updates. In this paper, we present GraphFly, a high-performance asynchronous streaming graph processing system based on dependency-flows. GraphFly features three key designs: 1) Dependency trees (D-trees), which helps quickly identify independent graph updates with low cost; 2) Dependency-flow based processing model, which exploits the space-time dependent co-scheduling for cache efficiency; 3) Specialized graph data layout, which further reduces memory accesses. We evaluate GraphFly, and the results show that GraphFly significantly outperforms state-of-the-art systems KickStarter and GraphBolt by 5.81× and 1.78× on average, respectively. Also, GraphFly scales well with different sizes of update batch and compute resources.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046059","streaming graphs;incremental processing;redundant memory accesses","Costs;High performance computing;Memory management;Layout;Data models;Synchronization","","","","","",44.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale","R. Y. Aminabadi; S. Rajbhandari; A. A. Awan; C. Li; D. Li; E. Zheng; O. Ruwase; S. Smith; M. Zhang; J. Rasley; Y. He","Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"The landscape of transformer model inference is increasingly diverse in model size, model characteristics, latency and throughput requirements, hardware requirements, etc. With such diversity, designing a versatile inference system is challenging. DeepSpeed-Inference addresses these challenges by (1) a multi-GPU inference solution to minimize latency while maximizing throughput for both dense and sparse transformers when the model fits in aggregate GPU memory, and (2) a heterogeneous inference solution that leverages CPU/NVMe/GPU memory to enable high-throughput inference for models larger than aggregate GPU memory. DeepSpeed-Inference reduces latency by 6.4× and increases throughput by 1.5 ×over the state-of-the-art. It enables trillion parameter scale inference under real-time latency constraints by leveraging hundreds of GPUs, an unprecedented scale for inference. It can inference 25 ×larger models than with GPU-only solutions, while delivering a high throughput of 84 TFLOPS (over 50% of A6000 peak).","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046087","Deep Learning;Distributed Inference;Mixture of Experts;PyTorch;DeepSpeed;Transformer models","Technological innovation;Computational modeling;Aggregates;High performance computing;Graphics processing units;Production;Transformers","","","","","",39.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"HyLo: A Hybrid Low-Rank Natural Gradient Descent Method","B. Mu; S. Soori; B. Can; M. Gürbüzbalaban; M. M. Dehnavi","University of Toronto, Toronto, Canada; University of Toronto, Toronto, Canada; Rutgers University, Piscataway, NJ, US; Rutgers University, Piscataway, NJ, US; University of Toronto, Toronto, CA, UK","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,16,"This work presents a Hybrid Low-Rank Natural Gradient Descent method, called HyLo, that accelerates the training time of deep neural networks. Natural gradient descent (NGD) requires computing the inverse of the Fisher information matrix (FIM), which is typically expensive at largescale. Kronecker factorization methods such as KFAC attempt to improve NGD's running time by approximating the FIM with Kronecker factors. However, the size of Kronecker factors increases quadratically as the model size grows. Instead, in HyLo, we use the Sherman-Morrison-Woodbury variant of NGD (SNGD) and propose a reformulation of SNGD to resolve its scalability issues. HyLo uses a computationally-efficient low-rank factorization to achieve superior timing for Fisher inverses. We evaluate HyLo on large models including ResNet-50, U-Net, and ResNet-32 on up to 64 GPUs. HyLo converges 1.4×-2.1× faster than the state-of-the-art distributed implementation of KFAC and reduces the computation and communication time up to 350 × and 10.7× on ResNet-50.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046123","Natural Gradient Descent;Deep Neural Networks;Optimization","Training;Monte Carlo methods;Computational modeling;Scalability;High performance computing;Neural networks;Switches","","","","","",48.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"AI for Quantum Mechanics: High Performance Quantum Many-Body Simulations via Deep Learning","X. Zhao; M. Li; Q. Xiao; J. Chen; F. Wang; L. Shen; M. Zhao; W. Wu; H. An; L. He; X. Liang","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; Pilot National Laboratory for Marine Science and Technology, Qingdao, China; Tsinghua University, Beijing, China; University of Science and Technology of China, Hefei, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; Pilot National Laboratory for Marine Science and Technology, Qingdao, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Solving quantum many-body problems is one of the most fascinating research fields in condensed matter physics. An efficient numerical method is crucial to understand the mechanism of novel physics, such as the high Tc superconductivity, as one has to find the optimal solution in the exponentially large Hilbert space. The development of Artificial Intelligence (AI) provides a unique opportunity to solve the quantum many-body problems, but there is still a large gap from the goal. In this work, we present a novel computational framework, and adapt it to the Sunway supercomputer. With highly efficient scalability up to 40 million heterogeneous cores, we can drastically increase the number of variational parameters, which greatly improves the accuracy of the solutions. The investigations of the spin-1/2 J1-J2 model and the t-J model achieve unprecedented accuracy and time-to-solution far beyond the previous state of the art.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00053","National Key Research and Development Program of China(grant numbers:2016YFB1000403,2017YFB0202002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046063","Convolutional neural networks;Unsupervised learning;Scientific computing;Multicore processing","Adaptation models;High-temperature superconductors;Quantum computing;Computational modeling;Scalability;High performance computing;Quantum mechanics","","","","","",36.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"UniQ: A Unified Programming Model for Efficient Quantum Circuit Simulation","C. Zhang; H. Wang; Z. Ma; L. Xie; Z. Song; J. Zhai","Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,16,"Quantum circuit simulation is critical for verifying quantum computers. Given exponential complexity in the simulation, existing simulators use different architectures to accelerate the simulation. However, due to the variety of both simulation methods and modern architectures, it is challenging to design a high-performance yet portable simulator. In this work, we propose UniQ, a unified programming model for multiple simulation methods on various hardware architectures. We provide a unified application abstraction to describe different applications, and a unified hierarchical hardware abstraction upon different hardware. Based on these abstractions, UniQ can perform various circuit transformations without being aware of either concrete application or architecture detail, and generate high-performance execution schedules on different platforms without much human effort. Evaluations on CPU, GPU, and Sunway platforms show that UniQ can accelerate quantum circuit simulation by up to 28.59× (4.47× on average) over state-of-the-art frameworks, and successfully scale to 399,360 cores on 1,024 nodes.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00054","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045784","Quantum Simulation;Parallel Programming","Schedules;Computational modeling;High performance computing;Graphics processing units;Computer architecture;Programming;Hardware","","","","","",37.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Scalable Irregular Parallelism with GPUs: Getting CPUs Out of the Way","Y. Chen; B. Brock; S. Porumbescu; A. Buluç; K. Yelick; J. D. Owens","University of California, Davis, Davis, USA; University of California, Berkeley, Berkeley, USA; University of California, Davis, Davis, USA; Lawrence Berkeley National Laboratory, Berkeley, USA; University of California, Berkeley, Berkeley, USA; University of California, Davis, Davis, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,16,"We present Atos, a dynamic scheduling framework for multi-node-GPU systems that supports PGAS-style lightweight one-sided memory operations within and between nodes. Atos's lightweight GPU-to-GPU communication enables latency hiding and can smooth the interconnection usage for bisection-limited problems. These benefits are significant for dynamic, irregular applications that often involve fine-grained communication at unpredictable times and without predetermined patterns. Some principles for high performance: (1) do not involve the CPU in the communication control path; (2) allow GPU communication within kernels, addressing memory consistency directly rather than relying on synchronization with the CPU; (3) perform dynamic communication aggregation when interconnections have limited bandwidth. By lowering the overhead of communication and allowing it within GPU kernels, we support large, high-utilization GPU kernels but with more frequent communication. We evaluate Atos on two irregular problems: Breadth-First-Search and PageRank. Atos outperforms the state-of-the-art graph libraries Gunrock, Groute and Galois on both single-node-multi-GPU and multi-node-GPU settings.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046082","PGAS;distributed GPUs;asynchronous;irregular application","Computational modeling;Graphics processing units;Bandwidth;Parallel processing;Data structures;Turning;Data models","","","","","",26.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Parla: A Python Orchestration System for Heterogeneous Architectures","H. Lee; W. Ruys; I. Henriksen; A. Peters; Y. Yan; S. Stephens; B. You; H. Fingler; M. Burtscher; M. Gligoric; K. Schulz; K. Pingali; C. J. Rossbach; M. Erez; G. Biros","The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; Texas State University, San Marcos, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Python's ease of use and rich collection of numeric libraries make it an excellent choice for rapidly developing scientific applications. However, composing these libraries to take advantage of complex heterogeneous nodes is still difficult. To simplify writing multi-device code, we created Parla, a heterogeneous task-based programming framework that fully supports Python's scientific programming stack. Parla's API is based on Python decorators and allows users to wrap code in Parla tasks for parallel execution. Parla arrays enable automatic movement of data between devices. The Parla runtime handles resource-aware mapping, scheduling, and execution of tasks. Compared to other Python tasking systems, Parla is unique in its parallelization of tasks within a single process, its GPU context and resource-aware runtime, and its design around gradual adoption to provide easy migration of and integration into existing Python applications. We show that Parla can achieve performance competitive with hand-optimized code while improving ease of development.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00056","NSF(grant numbers:CNN-2006943,CNS-1846169); CCF(grant numbers:1922862); U.S. Department of Energy(grant numbers:DE-SC0019393); National Nuclear Security Administration(grant numbers:DE-NA0003969); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046101","Parallel application frameworks;task based parallelism;heterogeneous computing;load balancing and scheduling algorithms","Runtime;Codes;Scheduling algorithms;Parallel programming;Prefetching;High performance computing;Writing","","","","","",34.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"VSGM: View-Based GPU-Accelerated Subgraph Matching on Large Graphs","G. Jiang; Q. Zhou; T. Jin; B. Li; Y. Zhao; Y. Li; J. Cheng","The Chinese University of Hong Kong, Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Subgraph matching is a fundamental building block in graph analytics. Due to its high time complexity, GPU-based solutions have been proposed for sub graph matching. Most existing GPU-based works can only cope with relatively small graphs that fit in GPU memory. To support efficient subgraph matching on large graphs, we propose a view-based method to hide communication overhead and improve GPU utilization. We develop VSGM, a sub graph matching framework that supports efficient pipelined execution and multi-GPU architecture. Ex-tensive experimental evaluation shows that VSGM significantly outperforms the state-of-the-art solutions.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046118","subgraph matching;GPU acceleration","Performance evaluation;High performance computing;Loading;Graphics processing units;Computer architecture;Time complexity","","","","","",47.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"STMatch: Accelerating Graph Pattern Matching on GPU with Stack-Based Loop Optimizations","Y. Wei; P. Jiang","University of Iowa, Iowa City, USA; University of Iowa, Iowa City, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,13,"Graph pattern matching is a fundamental task in many graph analytics and graph mining applications. As an NP-hard problem, it is often a performance bottleneck in these applications. Previous work has proposed to use GPU to accelerate the computation. However, we find that the existing GPU solutions fail to show a performance advantage over the state-of-the-art CPU implementation due to their subgraph-centric design. This work proposes a novel stack-based graph pattern matching system on GPU that avoids the synchronization and memory consumption issues of the previous subgraph-centric systems. We also propose a two-level work-stealing and a loop-unrolling technique to improve the inter-warp and intra-warp GPU resource utilization of our system. The experiments show that our system significantly advances the state-of-the-art for graph pattern matching on GPU.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046078","Parallel programming;Backtracking","NP-hard problem;High performance computing;Memory management;Graphics processing units;Synchronization;Resource management;Task analysis","","","","","",33.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"WholeGraph: A Fast Graph Neural Network Training Framework with Multi-GPU Distributed Shared Memory Architecture","D. Yang; J. Liu; J. Qi; J. Lai","NVIDIA, Beijing, China; NVIDIA, Beijing, China; NVIDIA, Beijing, China; NVIDIA, Beijing, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Graph neural networks (GNNs) are prevalent to deal with graph-structured datasets, encoding graph data into low dimensional vectors. In this paper, we present a fast training graph neural network framework, i.e., WholeGraph, based on a multi-GPU distributed shared memory architecture. Whole-Graph consists of partitioning the graph and corresponding node or edge features to multi-GPUs, eliminating the bottleneck of communication between CPU and GPUs during the training process. And the communication between different GPUs is implemented by GPUDirect Peer-to-Peer (P2P) memory access technology. Furthermore, WholeGraph provides several optimized computing operators. Our evaluations show that on large-scale graphs WholeGraph outperforms state-of-the-art GNN frameworks, such as Deep Graph Library (DGL) and Pytorch Geometric (PyG). The speedups of WholeGraph are up to $57.32\mathrm{x}$ and $242.98\mathrm{x}$ compared with DGL and PyG on a single machine multi-GPU node, respectively. The ratio of GPU utilization can sustain above 95% during GNN training process.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046129","graph neural network;GPU;shared memory architecture","Training;Computational modeling;High performance computing;Memory architecture;Graphics processing units;Distributed databases;Graph neural networks","","","","","",46.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"SeqDLM: A Sequencer-Based Distributed Lock Manager for Efficient Shared File Access in a Parallel File System","Q. Chen; S. Ma; K. Chen; T. Ma; X. Liu; D. Chen; Y. Wu; Z. Chen","Tsinghua University, China; Tsinghua University, China; Tsinghua University, China; Alibaba Inc, China; National Supercomputing Center in Wuxi, China; National Supercomputing Center in Wuxi, China; Tsinghua University, China; Chinese Academy of Engineering, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Distributed locks are used to guarantee the distributed client-cache coherence in parallel file systems. However, they lead to poor performance in the case of parallel writes under high-contention workloads. We analyze the distributed lock manager and find out that lock conflict resolution is the root cause of the poor performance, which involves frequent lock revocations and slow data flushing from client caches to data servers. We design a distributed lock manager named SeqDLM by exploiting the sequencer mechanism. SeqDLM mitigates the lock conflict resolution overhead using early grant and early revocation while keeping the same semantics as traditional distributed locks. To evaluate SeqDLM, we have implemented a parallel file system called ccPFS using both SeqDLM and traditional distributed locks. Evaluations on 96 nodes show SeqDLM outperforms the traditional distributed locks by up to $\boldsymbol{10.3}\times$ for high-contention parallel writes on a shared file with multiple stripes.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046124","high performance computing;file systems;cache coherence;distributed lock;sequencer","File systems;High performance computing;Semantics;Distributed databases;Coherence;Servers","","","","","",41.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"MetaWBC: POSIX-Compliant Metadata Write-Back Caching for Distributed File Systems","Y. Qian; W. Cheng; L. Zeng; M. -A. Vef; O. Drokin; A. Dilger; S. Ihara; W. Zhang; Y. Wang; A. Brinkmann","Data Direct Networks (DDN), China; Huazhong University of Science and Technology (HUST), China; Zhejiang Lab, China; Johannes Gutenberg University Mainz, Germany; Whamcloud Inc, USA; Whamcloud Inc, USA; Data Direct Networks (DDN), Japan; Tsinghua University, China; Shenzhen Institute of Advanced Technology Chinese Academy of Sciences, China; Johannes Gutenberg University Mainz, Germany","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,20,"In parallel and distributed file systems, caching can improve data performance and metadata operations. Currently, most distributed file systems adopt a write-back data cache for performance and a write-through metadata cache for simplifying consistency. However, with modern file systems scales and workloads, write-through metadata caching can impact overall file system performance, e.g., through lock contention and heavy RPC loads required for namespace synchronization and transaction serialization. This paper proposes a novel metadata write-back caching (MetaWBC) mechanism to improve the performance of metadata operations in distributed environments. To achieve extreme metadata performance, we developed a fast, lightweight, and POSIXcompatible memory file system as a metadata cache. Further, we designed a file caching state machine and included other performance optimizations. We coupled MetaWbc with Lustre and evaluated that MetaWbc can outperform the native parallel file system by up to 8x for metadata-intensive benchmarks, and up to 7x for realistic workloads in throughput.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046054","File system;metadata;write-back caching;dataintensive application;Lustre","File systems;High performance computing;Distributed databases;Metadata;Benchmark testing;Throughput;Synchronization","","","","","",56.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"GUFI: Fast, Secure File System Metadata Search for Both Privileged and Unprivileged Users","D. Manno; J. Lee; P. Challa; Q. Zheng; D. Bonnie; G. Grider; B. Settlemyer","Los Alamos National Laboratory, Los Alamos, NM, USA; Los Alamos National Laboratory, Los Alamos, NM, USA; University of Texas at Arlington, Arlington, TX, USA; Los Alamos National Laboratory, Los Alamos, NM, USA; Los Alamos National Laboratory, Los Alamos, NM, USA; Los Alamos National Laboratory, Los Alamos, NM, USA; NVIDIA, Austin, TX, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Modern High-Performance Computing (HPC) data centers routinely store massive data sets resulting in millions of directories and billions of files. To efficiently search and sift through these files and directories we present the Grand Unified File Index (GUFI), a novel file system metadata index that enables both privileged and regular users to rapidly locate and characterize data sets of interest. GUFI uses a hierarchical index that preserves file access permissions such that the index can be securely accessed by users while still enabling efficient, advanced analysis of storage system usage by cluster administrators. Compared with the current state-of-the-art indexing for file system metadata, GUFI is able to provide speedups of 1.5× to 230× for queries executed by administrators on a real production file system namespace. Queries executed by users, which typically cannot rely on cluster-wide indexing, see even greater speedups using GUFI.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046106","Parallel query processing;file system metadata management;file system access control;index sharding","Data centers;Sharding;File systems;Databases;High performance computing;Process control;Production","","","","","",48.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"AD for an Array Language with Nested Parallelism","R. Schenck; O. Rϕnning; T. Henriksen; C. E. Oancea","Dept. of Computer Science, University of Copenhagen, Copenhagen, Denmark; Dept. of Computer Science, University of Copenhagen, Copenhagen, Denmark; Dept. of Computer Science, University of Copenhagen, Copenhagen, Denmark; Dept. of Computer Science, University of Copenhagen, Copenhagen, Denmark","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"We present a technique for applying reverse mode automatic differentiation (AD) on a non-recursive second-order functional array language that supports nested parallelism and is primarily aimed at efficient GPU execution. The key idea is to eliminate the need for a tape by relying on redundant execution to bring into each new scope all program variables that may be needed by the differentiated code. Efficient execution is enabled by the observation that perfectly nested scopes do not introduce re-execution and that such perfect nests can be readily produced by application of known compiler transformations. Our technique differentiates loops and bulk-parallel operators-e.g., map, reduce(-by-index), scan, and scatter-by specific rewrite rules and aggressively optimizes the resulting nested-parallel code. We report an evaluation that compares with established AD solutions and demonstrates competitive performance on ten common benchmarks from recent applied AD literature.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046130","automatic differentiation;functional data parallel language;compilers;GPGPU","Enzymes;Codes;High performance computing;Graphics processing units;Parallel processing;Benchmark testing;Hardware","","","",1.0,"",50.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"SpDISTAL: Compiling Distributed Sparse Tensor Computations","R. Yadav; A. Aiken; F. Kjolstad","Stanford University, Stanford, CA, United States; Stanford University, Stanford, CA, United States; Stanford University, Stanford, CA, United States","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"We introduce SpDISTAL, a compiler for sparse tensor algebra that targets distributed systems. SpDISTAL combines separate descriptions of tensor algebra expressions, sparse data structures, data distribution, and computation distribution. Thus, it enables distributed execution of sparse tensor algebra expressions with a wide variety of sparse data structures and data distributions. SpDISTAL is implemented as a C++ library that targets a distributed task-based runtime system and can generate code for nodes with both multi-core CPUs and multiple GPUs. SpDISTAL generates distributed code that achieves performance competitive with hand-written distributed functions for specific sparse tensor algebra expressions and that outperforms general interpretation-based systems by one to two orders of magnitude.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046085","Computer Science;Programming;Parallel Programming","Tensors;Codes;Runtime;Program processors;Algebra;Distributed databases;Programming","","","","","",34.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Scalable Automatic Differentiation of Multiple Parallel Paradigms through Compiler Augmentation","W. S. Moses; S. H. K. Narayanan; L. Paehler; V. Churavy; M. Schanen; J. Hückelheim; J. Doerfert; P. Hovland","MIT CSAIL, Cambridge, MA; Argonne National Laboratory, Lemont, IL; Technical University of Munich, Munich, Germany; MIT CSAIL, Cambridge, MA; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,18,"Derivatives are key to numerous science, engineering, and machine learning applications. While existing tools generate derivatives of programs in a single language, modern parallel applications combine a set of frameworks and languages to leverage available performance and function in an evolving hardware landscape. We propose a scheme for differentiating arbitrary DAG-based parallelism that preserves scalability and efficiency, implemented into the LLVM-based Enzyme automatic differentiation framework. By integrating with a full-fledged compiler backend, Enzyme can differentiate numerous parallel frameworks and directly control code generation. Combined with its ability to differentiate any LLVM-based language, this flexibility permits Enzyme to leverage the compiler tool chain for parallel and differentiation-specitic optimizations. We differentiate nine distinct versions of the LULESH and miniBUDE applications, written in different programming languages (C++, Julia) and parallel frameworks (OpenMP, MPI, RAJA, Julia tasks, MPI.jl), demonstrating similar scalability to the original program. On benchmarks with 64 threads or nodes, we find a differentiation overhead of 3.4–6.8× on C++ and 5.4–12.5× on Julia.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00065","NSF(grant numbers:OAC-1835443,AGS-1835860,AGS-1835881); DARPA(grant numbers:HR0011-20-9-0016 (PaPPa)); Department of Energy; National Nuclear Security Administration(grant numbers:DE-NA0003965); NSF Cyberinfrastructure for Sustained Scientific Innovation (CSSI)(grant numbers:2104068,2103804); German Research Council (DFG)(grant numbers:326472365); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046093","automatic differentiation;MPI;OpenMP;Tasks;compiler;LLVM;hybrid parallelization;parallel;distributed;C++;Raja;Julia;Enzyme","Enzymes;Codes;Program processors;Runtime;Parallel programming;Scalability;C++ languages","","","","","",59.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Accelerating Parallel Write via Deeply Integrating Predictive Lossy Compression with HDF5","S. Jin; D. Tao; H. Tang; S. Di; S. Byna; Z. Lukic; F. Cappello","Indiana University, Bloomington, IN, USA; Indiana University, Bloomington, IN, USA; Lawrence Berkeley National Lab, Berkeley, CA, USA; Argonne National Laboratory, Lemont, IL, USA; Lawrence Berkeley National Lab, Berkeley, CA, USA; Lawrence Berkeley National Lab, Berkeley, CA, USA; Argonne National Laboratory, Lemont, IL, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Lossy compression is one of the most efficient solutions to reduce storage overhead and improve I/O performance for HPC applications. However, existing parallel I/O libraries cannot fully utilize lossy compression to accelerate parallel write due to the lack of deep understanding on compression-write performance. To this end, we propose to deeply integrate predictive lossy compression with HDF5 to significantly improve the parallel-write performance. Specifically, we propose analytical models to predict the time of compression and parallel write before the actual compression to enable compression-write overlapping. We also introduce an extra space in the process to handle possible data overflows resulting from prediction uncertainty in compression ratios. Moreover, we propose an optimization to reorder the compression tasks to increase the overlapping efficiency. Experiments with up to 4,096 cores from Summit show that our solution improves the write performance by up to $4.5\times$ and $2.9\times$ over the non-compression and lossy compression solutions, respectively, with only 1.5% storage overhead (compared to original data) on two real-world HPC applications.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046061","parallel I/O;lossy compresion;HDF5","Analytical models;Uncertainty;High performance computing;Libraries;Task analysis;Optimization","","","","","",57.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Dynamic Quality Metric Oriented Error Bounded Lossy Compression for Scientific Datasets","J. Liu; S. Di; K. Zhao; X. Liang; Z. Chen; F. Cappello","University of California, Riverside, CA, USA; Argonne National Laboratory, Lemont, IL, USA; University of Alabama at Birmingham, Biringham, AL, USA; University of Kentucky, Lexington, KY, USA; University of California, Riverside, CA, USA; University IIIinois at Urbana-Champaign, Urbana, IL, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"With ever-increasing execution scale of the high performance computing (HPC) applications, vast amount of data are being produced by scientific research every day. Error-bounded lossy compression has been considered a very promising solution to address the big-data issue for scientific applications, because it can significantly reduce the data volume with low time cost meanwhile allowing users to control the compression errors with a specified error bound. The existing error-bounded lossy compressors, however, are all developed based on inflexible designs or compression pipelines, which cannot adapt to diverse compression quality requirements/metrics favored by different application users. In this paper, we propose a novel dynamic quality metric oriented error-bounded lossy compression frame-work, namely QoZ. The detailed contribution is three fold. (1) We design a novel highly-parameterized multi-level interpolation-based data predictor, which can significantly improve the overall compression quality with the same compressed size. (2) We design the error bounded lossy compression framework QoZ based on the adaptive predictor, which can auto-tune the critical parameters and optimize the compression result according to user-specified quality metrics during online compression. (3) We evaluate QoZ carefully by comparing its compression quality with multiple state-of-the-arts on various real-world scientific application datasets. Experiments show that, compared with the second best lossy compressor, QoZ can achieve up to 70% compression ratio improvement under the same error bound, up to 150% compression ratio improvement under the same PSNR, or up to 270% compression ratio improvement under the same SSIM.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046076","error-bounded lossy compression;interpolation;quality metrics;scientific datasets","Measurement;Costs;High performance computing;Pipelines;Compressors","","","","","",49.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"vGraph: Memory-Efficient Multicore Graph Processing for Traversal-Centric Algorithms","M. Jia; Y. Zhang; X. Gan; D. Li; E. Xu; R. Wang; K. Lu","National University of Defense Technology, Changsha, China; Xiamen University, Xiamen, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"To lower the monetary/energy cost, single-machine multicore graph processing is gaining increasing attention for a wide range of traversal-centric graph algorithms such as BFS, SSSP, CC, and PageRank, of which the processing is relatively simple and the topology data (vertices and edges) dominates the memory footprint. This paper presents $v$ Graph, a NUMA-aware, memory-efficient multicore graph processing system for traversal-centric algorithms. $v$ Graph proposes an ultralight NUMA-aware graph preprocessing scheme which eliminates almost all complex preprocessing steps and pipelines per-NUMA graph loading and compressing, to effectively reduce inter-NUMA memory accesses while keeping both preprocessing cost and peak memory footprint low. We further optimize $v$ Graph with effective HPC techniques including prefetching and work-stealing. Evaluation on a 384GB-memory, four-NUMA machine shows that compared to the state-of-the-art NUMA-aware/-unaware systems, $v$ Graph can process much larger real-world and synthetic graphs with various traversal-centric algorithms, achieving significantly higher memory efficiency and lower processing time.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00068","National Natural Science Foundation of China(grant numbers:62025208,61872376,61932001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046098","graph processing;traversal-centric algorithms;memory-efficient;multicore;NUMA","Costs;Multicore processing;Source coding;Prefetching;Memory management;Loading;Pipelines","","","","","",56.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Boosting Performance Optimization with Interactive Data Movement Visualization","P. Schaad; T. Ben-Nun; T. Hoefler","Department of Computer Science, ETH Zürich, ZüRich, Switzerland; Department of Computer Science, ETH Zürich, ZüRich, Switzerland; Department of Computer Science, ETH Zürich, ZüRich, Switzerland","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,16,"Optimizing application performance in today's hardware architecture landscape is an important, but increasingly complex task, often requiring detailed performance analyses. In particular, data movement and reuse play a crucial role in optimization and are often hard to improve without detailed program inspection. Performance visualizations can assist in the diagnosis of performance problems, but generally rely on data gathered through lengthy program executions. In this paper, we present a performance visualization geared towards analyzing data movement and reuse to inform impactful optimization decisions, without requiring program execution. We propose an approach that combines static dataflow analysis with parameterized program simulations to analyze both global data movement and fine-grained data access and reuse behavior, and visualize insights in-situ on the program representation. Case studies analyzing and optimizing real-world applications demonstrate our tool's effectiveness in guiding optimization decisions and making the performance tuning process more interactive.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046131","performance analysis;software performance","Analytical models;Visualization;Source coding;Instruments;Layout;Data visualization;Performance analysis","","","","","",62.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Not All GPUs Are Created Equal: Characterizing Variability in Large-Scale, Accelerator-Rich Systems","P. Sinha; A. Guliani; R. Jain; B. Tran; M. D. Sinclair; S. Venkataraman","Computer Sciences Department, University of Wisconsin-Madison, Madison, United States of America; Computer Sciences Department, University of Wisconsin-Madison, Madison, United States of America; Computer Sciences Department, University of Wisconsin-Madison, Madison, United States of America; Computer Sciences Department, University of Wisconsin-Madison, Madison, United States of America; Computer Sciences Department, University of Wisconsin-Madison, Madison, United States of America; Computer Sciences Department, University of Wisconsin-Madison, Madison, United States of America","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Scientists are increasingly exploring and utilizing the massive parallelism of general-purpose accelerators such as GPUs for scientific breakthroughs. As a result, datacenters, hyperscalers, national computing centers, and supercomputers have procured hardware to support this evolving application paradigm. These systems contain hundreds to tens of thousands of accelerators, enabling peta- and exa-scale levels of compute for scientific workloads. Recent work demonstrated that power management (PM) can impact application performance in CPU-based HPC systems, even when machines have the same architecture and SKU (stock keeping unit). This variation occurs due to manufacturing variability and the chip's PM. However, while modern HPC systems widely employ accelerators such as GPUs, it is unclear how much this variability affects applications. Accordingly, we seek to characterize the extent of variation due to GPU PM in modern HPC and supercomputing systems. We study a variety of applications that stress different GPU components on five large-scale computing centers with modern GPUs: Oak Ridge's Summit, Sandia's Vortex, TACC's Frontera and Longhorn, and Livermore's Corona. These clusters use a variety of cooling methods and GPU vendors. In total, we collect over 18,800 hours of data across more than 90% of the GPUs in these clusters. Regardless of the application, cluster, GPU vendor, and cooling method, our results show significant variation: 8% (max 22%) average performance variation even though the GPU architecture and vendor SKU are identical within each cluster, with outliers up to 1.5× slower than the median GPU. These results highlight the difficulty in efficiently using existing GPU clusters for modern HPC and scientific workloads, and the need to embrace variability in future accelerator-based systems.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046096","Accelerator architectures;Dynamic voltage scaling;Power measurement;Temperature measurement;Time measurement","Voltage measurement;Cooling;Power system management;High performance computing;Graphics processing units;Computer architecture;Parallel processing","","","",1.0,"",93.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"AlphaSparse: Generating High Performance SpMV Codes Directly from Sparse Matrices","Z. Du; J. Li; Y. Wang; X. Li; G. Tan; N. Sun","University of Chinese Academy of Sciences, Beijing, China; North Carolina State University, Raleigh, NC, USA; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Sparse Matrix-Vector multiplication (SpMV) is an essential computational kernel in many application scenarios. Tens of sparse matrix formats and implementations have been proposed to compress the memory storage and speed up SpMV performance. We develop AlphaSparse, a superset of all existing works that goes beyond the scope of human-designed format(s) and implementation(s). AlphaSparse automatically creates novel machine-designed formats and SpMV kernel implementations en-tirely from the knowledge of input sparsity patterns and hard-ware architectures. Based on our proposed Operator Graph that expresses the path of SpMV format and kernel design, AlphaS-parse consists of three main components: Designer, Format & Kernel Generator, and Search Engine. It takes an arbitrary sparse matrix as input while outputs the performance machine-designed format and SpMV implementation. By extensively evaluating 843 matrices from SuiteSparse Matrix Collection, AlphaSparse achieves significant performance improvement by 3.2 × on average compared to five state-of-the-art artificial formats and 1.5 × on average (up to 2.7×) over the up-to-date implementation of traditional auto-tuning philosophy.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00071","National Natural Science Foundation of China(grant numbers:T2125013,62032023,61972377,61702483); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046119","auto-tuner;sparse matrix-vector multiplication;SpMV;GPU;code generator;sparse data structures","Philosophical considerations;Codes;High performance computing;Search engines;Data structures;Generators;Sparse matrices","","","","","",97.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Approximate Computing Through the Lens of Uncertainty Quantification","K. Parasyris; J. Diffenderfer; H. Menon; I. Laguna; J. Vanover; R. Vogt; D. Osei-Kuffuor","Lawrence Livermore National Lab, Livermore, USA; Lawrence Livermore National Lab, Livermore, USA; Lawrence Livermore National Lab, Livermore, USA; Lawrence Livermore National Lab, Livermore, USA; University of California, Davis, Davis, USA; Lawrence Livermore National Lab, Livermore, USA; Lawrence Livermore National Lab, Livermore, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"As computer system technology approaches the end of Moore's law, new computing paradigms that improve performance become a necessity. One such paradigm is approximate computing (AC). AC can present significant performance improvements, but a challenge lies in providing confidence that approximations will not overly degrade the application output quality. In AC, application domain experts manually identify code regions amenable to approximation. However, automatically guiding a developer where to apply AC is still a challenge. We propose Puppeteer, a novel method to rank code regions based on amenability to approximation. Puppeteer uses uncertainty quantification methods to measure the sensitivity of application outputs to approximation errors. A developer annotates possible application code regions and Puppeteer estimates the sensitivity of each region. Puppeteer successfully identifies insensitive regions on different benchmarks. We utilize AC on these regions and we obtain speedups of $1.18\times, 1.8\times$, and $1.3\times$ for HPCCG. DCT, and BlackScholes, respectively.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045786","Approximate Computing;Post-Moore Computing;Uncertainty Quantification;Global Sensitivity Analysis","Semiconductor device modeling;Codes;Uncertainty;Runtime;Sensitivity analysis;Measurement uncertainty;Approximate computing","","","","","",52.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Positive-Phase Temperature Scaling for Quantum-Assisted Boltzmann Machine Training","J. P. Pinilla; S. J. E. Wilton","University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,12,"Quantum-assisted sampling is a promising technique to enable training probabilistic ML models, which otherwise depend on slow-mixing classical sampling methods; such as, the use of Quantum Annealing Processors (QAP) to train Boltzmann Machines (BMs). Previous work has shown that QAPs can sample from a Boltzmann distribution, although, at an unknown instance-dependent temperature. Due to this distribution divergence, existing training algorithms have resorted to negative-phase temperature scaling. This method, although effective under arduous tuning, introduces unwanted noise to the sampleset due to the quantization errors caused by the underutilization of the QAP bias ranges; and is prone to bias overflow. We introduce a change in the training algorithm to allow positive-phase temperature scaling; an approach that reduces the impact of quantization noise, while still incorporating temperature scaling. As a result, we see an overall improvement in the convergence rate and testing accuracy, when compared to the state-of-the-art approach.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046133","Quantum annealing;Boltzmann machines","Training;Temperature distribution;Quantization (signal);Annealing;Program processors;Quantum annealing;Sampling methods","","","","","",33.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"QoS-Aware Irregular Collaborative Inference for Improving Throughput of DNN Services","K. Fu; J. Shi; Q. Chen; N. Zheng; W. Zhang; D. Zeng; M. Guo","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Microsoft Research Asia, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; China University of Geosciences, Wuhan, China; Shanghai Jiao Tong University, Shanghai, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"With collaborative DNN inference, part of queries run on their source edge device to reduce latencies. Because edges show diverse performance and network conditions, different layers should run on different devices, and queries on the datacenter show irregular structures. However, emerging schemes are not able to process such irregular queries. We propose ICE, a collaborative inference service scheme that effectively supports irregular queries. ICE comprises a query slicer, a query manager, and a lag enhancer. The query slicer maps the execution of queries based on the edges' performance and network conditions. The query manager batches irregular queries adaptively and schedules the irregular queries based on their progress. The lag enhancer reduces the QoS violation when queries run slower due to interference on the edge. Experiments show that ICE improves the supported peak load of the datacenter by 43.2% on average while guaranteeing the required 99%-ile latencies compared with state-of-the-art techniques.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00074","National Natural Science Foundation of China(grant numbers:61832006,62022057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046047","Cloud Computing;Deep Neural Networks;Quality of Service;Irregular Services","Performance evaluation;Schedules;High performance computing;Collaboration;Quality of service;Interference;Throughput","","","","","",50.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"EL-Rec: Efficient Large-Scale Recommendation Model Training via Tensor-Train Embedding Table","Z. Wang; Y. Wang; B. Feng; D. Mudigere; B. Muthiah; Y. Ding","University of California, Santa Barbara, Santa Barbara, CA, USA; University of California, Santa Barbara, Santa Barbara, CA, USA; University of California, Santa Barbara, Santa Barbara, CA, USA; Meta, San Francisco, CA, USA; Meta, San Francisco, CA, USA; University of California, Santa Barbara, Santa Barbara, CA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Deep learning Recommendation Models (DLRMs) plays an important role in various application domains. However, existing DLRM training systems require a large number of GPUs due to the memory-intensive embedding tables. To this end, we propose EL-Rec, an efficient computing framework harnessing the Tensor-train (TT) technique to democratize the training of large-scale DLRMs with limited GPU resources. Specifically, EL-Rec optimizes TT decomposition based on key computation primitives of embedding tables and implements a high-performance compressed embedding table which is a drop-in replacement of Pytorch API. EL-Rec introduces an index reordering technique to harvest the performance gains from both local and global information of training inputs. EL-Rec also highlights a pipeline training paradigm to eliminate the communication overhead between the host memory and the training worker. Comprehensive experiments demonstrate that EL-Rec can handle the largest publicly available DLRM dataset with a single GPU and achieves 3× speedup over the state-of-the-art DLRM frameworks.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00075","NSF(grant numbers:2124039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046052","Recommender systems;High performance computing;Deep learning","Training;Deep learning;Costs;Limiting;High performance computing;Pipelines;Graphics processing units","","","","","",54.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training","X. Sun; W. Wang; S. Qiu; R. Yang; S. Huang; J. Xu; Z. Wang","Alibaba Group, China; Alibaba Group, China; University of Leeds, UK; University of Leeds, UK; Alibaba Group, China; University of Leeds, UK; University of Leeds, UK","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,17,"Deep neural networks (DNNs) with billion-scale parameters have demonstrated impressive performance in solving many tasks. Unfortunately, training a billion-scale DNN is out of the reach of many data scientists because it requires high-performance GPU servers that are too expensive to purchase and maintain. We present STRONGHOLD, a novel approach for enabling large DNN model training with no change to the user code. STRONGHOLD scales up the largest trainable model size by dynamically offloading data to the CPU RAM and enabling the use of secondary storage. It automatically determines the minimum amount of data to be kept in the GPU memory to minimize GPU memory usage. Compared to state-of-the-art offloading-based solutions, STRONGHOLD improves the trainable model size by 1.9x~6. Sx on a 32GB V100 GPU, with 1.2x~3.7x improvement on the training throughput. It has been deployed into production to successfully support large-scale DNN training.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046110","Deep learning;Distributed training;DNNs training acceleration","Training;Deep learning;Codes;Computational modeling;Memory management;Graphics processing units;Random access memory","","","","","",61.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"HGL: Accelerating Heterogeneous GNN Training with Holistic Representation and Optimization","Y. Gui; Y. Wu; H. Yang; T. Jin; B. Li; Q. Zhou; J. Cheng; F. Yu","The Chinese-University of Hong Kong, Hong Kong SAR, China; The Chinese-University of Hong Kong, Hong Kong SAR, China; The Chinese-University of Hong Kong, Hong Kong SAR, China; The Chinese-University of Hong Kong, Hong Kong SAR, China; The Chinese-University of Hong Kong, Hong Kong SAR, China; The Chinese-University of Hong Kong, Hong Kong SAR, China; The Chinese-University of Hong Kong, Hong Kong SAR, China; Huawei Technologies Co. Ltd, Shenzhen, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Graph neural networks (GNNs) have shown to significantly improve graph analytics. Existing systems for GNN training are primarily designed for homogeneous graphs. In industry, however, most graphs are actually heterogeneous in nature (i.e., having multiple types of nodes and edges). Existing systems train a heterogeneous GNN (HetGNN) as a composition of homogeneous GNN (HomoGNN) and thus suffer from critical limitations such as lack of memory optimization and limited operator parallelism. To address these limitations, we propose HGL - a heterogeneity-aware system for GNN training. At the core of HGL is an intermediate representation, called HIR, which provides a holistic representation for GNNs and enables cross-relation optimization in HetGNN training. We devise tailored optimizations on HIR, including graph stitching, operator fusion and operator bundling. Compared with DGL and PyG, HGL achieves a speedup from 7 to 22 times for training HetGNNs.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046125","Graph Neural Networks;Heterogeneous Graphs;Deep Learning Systems","Training;Learning systems;Industries;High performance computing;Graphics processing units;Parallel processing;Graph neural networks","","","","","",51.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Productive Performance Engineering for Weather and Climate Modeling with Python","T. Ben-Nun; L. Groner; F. Deconinck; T. Wicky; E. Davis; J. Dahm; O. D. Elbert; R. George; J. McGibbon; L. Trümper; E. Wu; O. Fuhrer; T. Schulthess; T. Hoefler","Department of Computer Science, ETH Zürich, Zürich, Switzerland; Swiss National Supercomputing Centre (CSCS), Lugano, Switzerland; Allen Institute for Artificial Intelligence, Seattle, Washington, USA; Allen Institute for Artificial Intelligence, Seattle, Washington, USA; Allen Institute for Artificial Intelligence, Seattle, Washington, USA; Allen Institute for Artificial Intelligence, Seattle, Washington, USA; Allen Institute for Artificial Intelligence, Seattle, Washington, USA; Allen Institute for Artificial Intelligence, Seattle, Washington, USA; Allen Institute for Artificial Intelligence, Seattle, Washington, USA; Department of Computer Science, ETH Zürich, Zürich, Switzerland; Allen Institute for Artificial Intelligence, Seattle, Washington, USA; Allen Institute for Artificial Intelligence, Seattle, Washington, USA; Swiss National Supercomputing Centre (CSCS), Lugano, Switzerland; Department of Computer Science, ETH Zürich, Zürich, Switzerland","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Earth system models are developed with a tight coupling to target hardware, often containing specialized code predicated on processor characteristics. This coupling stems from using imperative languages that hard-code computation schedules and layout. We present a detailed account of optimizing the Finite Volume Cubed-Sphere Dynamical Core (FV3), improving productivity and performance. By using a declarative Python-embedded stencil domain-specific language and data-centric optimization, we abstract hardware-specific details and define a semi-automated workflow for analyzing and optimizing weather and climate applications. The workflow utilizes both local and full-program optimization, as well as user-guided fine-tuning. To prune the infeasible global optimization space, we automatically utilize repeating code motifs via a novel transfer tuning approach. On the Piz Daint supercomputer, we scale to 2,400 GPUs, achieving speedups of up to 3.92× over the tuned production implementation at a fraction of the original code.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00078","Swiss National Science Foundation(grant numbers:185778); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046105","Numerical Weather Prediction;Python;Data-Centric Programming","Couplings;Codes;Computational modeling;Layout;Mathematical models;Hardware;Supercomputers","","","","","",48.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Optimization of Full-Core Reactor Simulations on Summit","M. Min; Y. -H. Lan; P. Fischer; E. Merzari; S. Kerkemeier; M. Phillips; T. Rathnayake; A. Novak; D. Gaston; N. Chalmers; T. Warburton","Mathematics and Computer Science, Argonne National Laboratory, Lemont, USA; Computer Science, University of Illinois, Urbana, USA; Computer Science and Mechanical Science & Engineering, University of Illinois, Urbana, USA; Nuclear Engineering, Penn State University Park, USA; Mathematics and Computer Science, Argonne National Laboratory, Lemont, USA; Computer Science, University of Illinois, Urbana, USA; Computer Science, University of Illinois, Urbana, USA; Computational Science, Argonne National Laboratory, Lemont, USA; Modeling and Simulation, Idaho National Laboratory, Idaho Falls, USA; AMD Research, Advanced Micro Devices Inc., Austin, USA; Mathematics, Virginia Tech, Blacksburg, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,11,"Nek5000/RS, a highly-performant open-source spectral element code, has recently achieved an unprecedented milestone in the simulation of nuclear reactors: the first full core computational fluid dynamics simulations of reactor cores, including pebble beds with 352,625 pebbles and 98M spectral elements (51 billion gridpoints), advanced in less than 0.25 seconds per Navier-Stokes timestep. The authors present performance and optimization considerations necessary to achieve this milestone when running on all of Summit. These optimizations led to a fourfold reduction in time-to-solution, making it possible to perform high-fidelity simulations of a single flow-through time in less than six hours for a full reactor core under prototypical conditions.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00079","U.S. Department of Energy(grant numbers:DE-AC02-06CHl1357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046048","Nek5000;NekRS;Scalability;Spectral Element;Incompressible Navier-Stokes;Exascale","Analytical models;Uncertainty;Computational modeling;Biological system modeling;Predictive models;Topology;Thermal analysis","","","","","",44.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"A GPU-Accelerated AMR Solver for Gravitational Wave Propagation","M. Fernando; D. Neilsen; E. Hirschmann; Y. Zlochower; H. Sundar; O. Ghattas; G. Biros","The University of Texas at Austin, Austin, TX, USA; Brigham Young University, Provo, UT, USA; Brigham Young University, Provo, UT, USA; Rochester Institute of Technology, Rochester, NY, USA; The University of Utah, Salt Lake City, UT, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Simulations to calculate a single gravitational waveform (GW) can take several weeks. Yet, thousands of such simulations are needed for the detection and interpretation of gravitational waves. Future detectors will require even more accurate waveforms than those currently used. We present here the first large scale, adaptive mesh, multi-GPU numerical relativity (NR) code together with performance analysis and benchmarking. While comparisons are difficult to make, our GPU extension of the Dendro-grNr code achieves a 6x speedup over existing state-of-the-art codes. We achieve 800 GFlops/s on a single NVIDIA A100 GPU with an overall 2.5x speedup over a two-socket, 128-core AMD EPYC 7763 CPU node with an equivalent CPU implementation. We present detailed performance analyses, parallel scalability results, and accuracy assessments for GWs computed for mass ratios $\mathrm{q}=1,2,4$., We also present strong scalability up to 8 A100s and weak scaling up to 229,376 x86 cores on the Texas Advanced Computing Center's Frontera system.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046060","High performance computing;Gravitational waves;Astrophysics;Numerical simulation;Scientific computing","Adaptation models;Codes;Scalability;High performance computing;Gravitational waves;Graphics processing units;Mathematical models","","","","","",56.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"From Correctable Memory Errors to Uncorrectable Memory Errors: What Error Bits Tell","C. Li; Y. Zhang; J. Wang; H. Chen; X. Liu; T. Huang; L. Peng; S. Zhou; L. Wang; S. Ge","Intel Corporation, Shanghai, China; ByteDance, Beijing, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; ByteDance, Beijing, China; Intel Corporation, Shanghai, China; ByteDance, Beijing, China; Intel Corporation, Shanghai, China; ByteDance, Beijing, China; ByteDance, Shanghai, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Uncorrectable memory errors are one of the major failure causes in datacenters. In this paper, we present an empirical study correlating correctable errors (CEs) and uncorrectable errors (UEs) using the large-scale field data across 3 major dual in-line memory module (DIMM) manufacturers from a contemporary server farm of ByteDance. Different from the previous studies, our study is the first to comprehend the error-bit information of CEs and the DIMM part numbers. Unlike the traditional chipkill error correction code (ECC), in contemporary Intel server platforms the ECC gets weakened, not able to tolerate some error-bit patterns from a single chip. Using obtainable coarse-grained ECC knowledge, we derive a new indicator from the error-bit information: risky CE occurrence in terms of ECC guaranteed coverage. From the data, we show that the new indicator has a consistently high sensitivity and specificity in the test of future UE occurrences across DIMMs from different manufacturers. This leads us to conjecture that the weakened ECC substantially contributes to many UEs today. The new risky CE indicator is then applied in predicting the future UE occurrence based on the CE history. We empirically demonstrate how practically useful predictors are constructed in conjunction with other useful attributes such as certain micro-level fault indicators and DIMM part numbers, achieving the state-of-the-art performance.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046095","Memory reliability;error correction code;risky errors;uncorrectable error prediction","Fault diagnosis;High performance computing;Random access memory;Memory modules;Sensitivity and specificity;Error correction codes;Servers","","","","","",37.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Lessons Learned on MPI+Threads Communication","R. Zambre; A. Chandramowlishwaran","Electrical Engineering and Computer Science, University of California, Irvine, Irvine, CA, USA; Electrical Engineering and Computer Science, University of California, Irvine, Irvine, CA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,16,"Hybrid MPI+threads programming is gaining prominence, but, in practice, applications perform slower with it compared to the MPI everywhere model. The most critical challenge to the parallel efficiency of MPI+threads applications is slow MPI_THREAD_MULTIPLE performance. MPI libraries have recently made significant strides on this front, but to exploit their capabilities, users must expose the communication parallelism in their MPI+threads applications. Recent studies show that MPI 4.0 provides users with new performance-oriented options to do so, but our evaluation of these new mechanisms shows that they pose several challenges. An alternative design is MPI Endpoints. In this paper, we present a comparison of the different designs from the perspective of MPI's end-users: domain scientists and application developers. We evaluate the mechanisms on metrics beyond performance such as usability, scope, and portability. Based on the lessons learned, we make a case for a future direction.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00082","National Science Foundation(grant numbers:1750549); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046065","Exascale computing;Message-oriented middleware;MPI Endpoints;MPI+OpenMP;MPI+threads;MPI_THREAD_MULTIPLE;Partitioned communication","Measurement;High performance computing;Programming;Parallel processing;Libraries;Usability","","","","","",69.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Climbing the Summit and Pushing the Frontier of Mixed Precision Benchmarks at Extreme Scale","H. Lu; M. Matheson; V. Oles; A. Ellis; W. Joubert; F. Wang","Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"The rise of machine learning (ML) applications and their use of mixed precision to perform interesting science are driving forces behind AI for science on HPC. The convergence of ML and HPC with mixed precision offers the possibility of transformational changes in computational science. The HPL-AI benchmark is designed to measure the performance of mixed precision arithmetic as opposed to the HPL benchmark which measures double precision performance. Pushing the limits of systems at extreme scale is nontrivial -little public literature explores optimization of mixed precision computations at this scale. In this work, we demonstrate how to scale up the HPL-AI benchmark on the pre-exascale Summit and exascale Frontier systems at the Oak Ridge Leadership Computing Facility (OLCF) with a cross-platform design. We present the implementation, performance results, and a guideline of optimization strategies employed for delivering portable performance on both AMD and NVIDIA GPUs at extreme scale.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00083","U.S. Department of Energy(grant numbers:DE-AC05-000R22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046068","Parallel programming;High performance computing;Exascale computing;Linear algebra","Leadership;Data centers;Scientific computing;High performance computing;Machine learning;Benchmark testing;Supercomputers","","","","","",33.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Scalable Deep Learning-Based Microarchitecture Simulation on GPUs","S. Pandey; L. Li; T. Flynn; A. Hoisie; H. Liu","Stevens Institute of Technology, Hoboken, USA; Brookhaven National Laboratory, Upton, USA; Brookhaven National Laboratory, Upton, USA; Brookhaven National Laboratory, Upton, USA; Stevens Institute of Technology, Hoboken, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Cycle-accurate microarchitecture simulators are es-sential tools for designers to architect, estimate, optimize, and manufacture new processors that meet specific design expectations. However, conventional simulators based on discrete-event methods often require an exceedingly long time-to-solution for the simulation of applications and architectures at full complexity and scale. Given the excitement around wielding the machine learning (ML) hammer to tackle various architecture problems, there have been attempts to employ ML to perform architecture simulations, such as Ithemal and SimNet. However, the direct application of existing ML approaches to architecture simulation may be even slower due to overwhelming memory traffic and stringent sequential computation logic. This work proposes the first graphics processing unit (GPU)-based microarchitecture simulator that fully unleashes the poten-tial of GPUs to accelerate state-of-the-art ML-based simulators. First, considering the application traces are loaded from central processing unit (CPU) to GPU for simulation, we introduce various designs to reduce the data movement cost between CPUs and GPUs. Second, we propose a parallel simulation paradigm that partitions the application trace into sub-traces to simulate them in parallel with rigorous error analysis and effective error correction mechanisms. Combined, this scalable GPU-based simulator outperforms by orders of magnitude the traditional CPU-based simulators and the state-of-the-art ML-based simulators, i.e., SimNet and Ithemal.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00084","Brookhaven National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046116","Computer microarchitecture simulation;Machine learning;High performance computing;GPU acceleration","Performance evaluation;Microarchitecture;Computational modeling;Scalability;Graphics processing units;Computer architecture;Machine learning","","","","","",48.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"TD-NUCA: Runtime Driven Management of NUCA Caches in Task Dataflow Programming Models","P. Caheny; L. Alvarez; M. Casas; M. Moreto","Intel Corporation, Leixlip, Ireland; Polytechnic University of Catalonia, Barcelona, Spain; Polytechnic University of Catalonia, Barcelona, Spain; Polytechnic University of Catalonia, Barcelona, Spain","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"In high performance processors, the design of on-chip memory hierarchies is crucial for performance and energy efficiency. Current processors rely on large shared Non-Uniform Cache Architectures (NUCA) to improve performance and reduce data movement. Multiple solutions exploit information available at the microarchitecture level or in the operating system to optimize NUCA performance. However, existing methods have not taken advantage of the information captured by task dataflow programming models to guide the management of NUCA caches. In this paper we propose TD-NUCA, a hardware/software co-designed approach that leverages information present in the run-time system of task dataflow programming models to efficiently manage NUCA caches. TD-NUCA identifies the data access and reuse patterns of parallel applications in the runtime system and guides the operation of the NUCA caches in the hardware. As a result, TD-NUCA achieves a 1.18x average speedup over the baseline S-NUCA while requiring only 0.62x the data movement.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046056","cache memory;data flow computing;parallel architectures","Runtime;Program processors;Microarchitecture;Operating systems;Memory management;Programming;Hardware","","","","","",101.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"ReSemble: Reinforced Ensemble Framework for Data Prefetching","P. Zhang; R. Kannan; A. Srivastava; A. V. Nori; V. K. Prasanna","University of Southern California, Los Angeles, USA; United States Army Research Laboratory, Los Angeles, USA; University of Southern California, Los Angeles, USA; Intel Corporation, Bangalore, India; University of Southern California, Los Angeles, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Data prefetching hides memory latency by predicting and loading necessary data into cache beforehand. Most prefetchers in the literature are efficient for specific memory address patterns thereby restricting their utility to specialized applications-they do not perform well on hybrid applications with multifarious access patterns. Therefore we propose ReSem-ble: a Reinforcement Learning (RL) based adaptive enSemble framework that enables multiple prefetchers to complement each other on hybrid applications. Our RL trained ensemble controller takes prefetch suggestions from all prefetchers as input, selects the best suggestion dynamically, and learns online toward getting higher cumulative rewards, which are collected from prefetch hits/misses. Our ensemble framework using a simple multilayer perceptron as the controller achieves on the average 85.27 % (accuracy) and 44.22 % (coverage), leading to 31.02 % IPC improvement, which outperforms state-of-the-art individual prefetchers by 8.35%-26.11 %, while also outperforming SBP, a state-of-the-art (non-RL) ensemble prefetcher by 5.69%.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046113","reinforcement learning;ensemble;prefetching","Sensitivity;Prefetching;High performance computing;Memory management;Loading;Reinforcement learning;Multilayer perceptrons","","","","","",59.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"W-Cycle SVD: A Multilevel Algorithm for Batched SVD on GPUs","J. Xiao; Y. Pang; Q. Xue; C. Shui; K. Meng; H. Ma; M. Li; X. Zhang; G. Tan","Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Alibaba Group, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,16,"As a basic matrix factorization operation, Singular Value Decomposition (SVD) is widely used in diverse domains. In real-world applications, the computational bottleneck of matrix factorization is on small matrices, and many GPU-accelerated batched SVD algorithms have been developed recently for higher performance. However, these algorithms failed to achieve both high data locality and convergence speed, because they are size-sensitive. In this work, we propose a novel W-cycle SVD to accelerate the batched one-sided Jacobi SVD on GPUs. The W-cycle SVD, which is size-oblivious, successfully exploits the data reuse and ensures the optimal convergence speed for batched SVD. Further, we present the efficient batched kernel design, and propose a tailoring strategy based on auto-tuning to improve the batched matrix multiplication in SVDs. The evaluation demonstrates that the proposed algorithm achieves 2.6∼10.2× speedup over the state-of-the-art cuSOLVER. In a real-world data assimilation application, our algorithm achieves 2.73∼3.09× speedup compared with MAGMA.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00087","National Natural Science Foundation of China(grant numbers:62172391,61972377,62032023,T2125013); Chinese Academy of Sciences(grant numbers:171111KYSB20180011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046109","Singular Value Decomposition;GPU","Jacobian matrices;High performance computing;Matrix decomposition;Kernel;Convergence;Singular value decomposition;Data assimilation","","","","","",46.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Scalable Linear Time Dense Direct Solver for 3-D Problems without Trailing Sub-Matrix Dependencies","Q. Ma; S. Deshmukh; R. Yokota","School of Computing, Tokyo Institute of Technology, Tokyo, Japan; School of Computing, Tokyo Institute of Technology, Tokyo, Japan; Global Scientific Information and Computing Center, Tokyo Institute of Technology, Tokyo, Japan","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,12,"Factorization of large dense matrices are ubiquitous in engineering and data science applications, e.g. preconditioners for iterative boundary integral solvers, frontal matrices in sparse multifrontal solvers, and computing the determinant of covariance matrices. HSS and $\mathcal{H}^{2}$ -matrices are hierarchical low-rank matrix formats that can reduce the complexity of factorizing such dense matrices from $\mathrm{O}(N^{3})$ to $\mathrm{O}(N)$. For HSS matrices, it is possible to remove the dependency on the trailing matrices during Cholesky/LU factorization, which results in a highly parallel algorithm. However, the weak admissibility of HSS causes the rank of off-diagonal blocks to grow for 3-D problems, and the method is no longer $\mathrm{O}(N)$. On the other hand, the strong admissibility of $\mathcal{H}^{2}$ -matrices allows it to handle 3-D problems in $\mathcal{O}(N)$, but introduces a dependency on the trailing matrices. In the present work, we pre-compute the fill-ins and integrate them into the shared basis, which allows us to remove the dependency on trailing-matrices even for $\mathcal{H}^{2}$ -matrices. Comparisons with a block low-rank factorization code LORAPO showed a maximum speed up of 4,700x for a 3-D problem with complex geometry.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046069","Dense Direct Solver;H2-Matrix;LU;ULV","Geometry;Codes;High performance computing;Integral equations;Data science;Complexity theory;Sparse matrices","","","","","",25.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Solving Linear Systems on a GPU with Hierarchically Off-Diagonal Low-Rank Approximations","C. Chen; P. -G. Martinsson","Oden Institute for Computational Engineering and Sciences, University of Texas at Austin, Austin, United States; Department of Mathematics, Oden Institute for Computational Engineering and Sciences, University of Texas at Austin, Austin, United States","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"We are interested in solving linear systems arising from three applications: (1) kernel methods in machine learning, (2) discretization of boundary integral equations from mathematical physics, and (3) Schur complements formed in the factorization of many large sparse matrices. The coefficient matrices are often data-sparse in the sense that their off-diagonal blocks have low numerical ranks; specifically, we focus on “hierarchically off-diagonal low-rank (HODLR)” matrices. We introduce algorithms for factorizing HODLR matrices and for applying the factorizations on a GPU. The algorithms leverage the efficiency of batched dense linear algebra, and they scale nearly linearly with the matrix size when the numerical ranks are fixed. The accuracy of the HODLR-matrix approximation is a tunable parameter, so we can construct high-accuracy fast direct solvers or low-accuracy robust preconditioners. Numerical results show that we can solve problems with several millions of unknowns in a couple of seconds on a single GPU.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00089","Office of Naval Research(grant numbers:N00014-18-1-2354); National Science Foundation(grant numbers:DMS-1952735,DMS-2012606); Department of Energy(grant numbers:DE-SC0022251); ASCR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046094","Linear solver on GPU;boundary integral equation;kernel matrix;elliptic partial differential equations;hierarchical low-rank approximation;batched dense linear algebra;rank structured matrix;hierarchical matrix;LU factorization","Linear systems;High performance computing;Integral equations;Graphics processing units;Machine learning;Approximation algorithms;Matrices","","","","","",63.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Graph Neural Networks Based Memory Inefficiency Detection Using Selective Sampling","P. Li; Y. Guo; Y. Luo; X. Wang; Z. Wang; X. Liu","TikTok Inc., CA, USA; Peking University, Beijing, China; Peng Cheng Lab, Shenzhen, China; Peng Cheng Lab, Shenzhen, China; Michigan Technological University, Houghton, MI, USA; North Carolina State University, Raleigh, NC, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"Production software of data centers oftentimes suffers from unnecessary memory inefficiencies caused by inappropriate use of data structures, conservative compiler optimizations, and so forth. Nevertheless, whole-program monitoring tools often incur incredibly high overhead due to fine-grained memory access instrumentation. Consequently, the fine-grained monitoring tools are not viable for long-running, large-scale data center applications due to strict latency criteria (e.g., service-level agreement or SLA). To this end, this work presents a novel learning-aided system, namely Puffin, to identify three kinds of unnecessary memory operations including dead stores, silent loads and silent stores, by applying gated graph neural networks onto fused static and dynamic program semantics with respect to relative positional embedding. To deploy the system in large-scale data centers, this work explores a sampling-based detection infrastructure with high efficacy and negligible overhead. We evaluate Puffin upon the well-known SPEC CPU 2017 benchmark suite for four compilation options. Experimental results show that the proposed method is able to capture the three kinds of memory inefficiencies with as high accuracy as 96% and a reduced checking overhead by $5.66\times$ over the state-of-the-art tool.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046134","graph neural network;program embedding;memory inefficiency detection;sampling","Data centers;Semantics;Production;Benchmark testing;Logic gates;Graph neural networks;Software","","","","","",63.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Predicting Reuse Interval for Optimized Web Caching: An LSTM-Based Machine Learning Approach","P. Li; Y. Guo; Y. Gu","TikTok Inc., Mountain View, CA, USA; Peking University, Beijing, China; Meta Platforms Inc., Menlo Park, CA, USA","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,15,"Caching techniques are widely used in the era of cloud computing from applications, such as Web caches to infrastructures, Memcached and memory caches in computer architectures. Prediction of cached data can greatly help improve cache management and hit rate. The recent advancement of deep learning techniques enables the design of novel intelligent cache replacement policies. In this work, we propose a learning-aided approach to predict future data accesses. We find that a powerful LSTM-based recurrent neural network can provide high prediction accuracy based on only a cache trace as input. The high accuracy results from a carefully crafted locality-driven feature design. Inspired by the high prediction accuracy, we propose a pseudo OPT policy and evaluate it upon 13 real-world storage workloads from Microsoft Cloud. Results demonstrate that our new policy improves the state-of-art by up to 19.2% and incurs only 2.3% higher miss ratio than OPT on average.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046089","reuse interval;cache;LSTM;machine learning","Deep learning;Adaptation models;Cloud computing;Recurrent neural networks;Computational modeling;High performance computing;Optimized production technology","","","","","",77.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Out of Hypervisor (OoH): Efficient Dirty Page Tracking in Userspace Using Hardware Virtualization Features","S. Bitchebe; A. Tchana","Univ. Côte d'Azur & ENS Lyon, Nice & Lyon, France; ENS Lyon, Lyon, France","SC22: International Conference for High Performance Computing, Networking, Storage and Analysis","23 Feb 2023",2022,"","",1,14,"This paper introduces Out of Hypervisor (OoH), a new virtualization research axis. Instead of emulating full virtual hardware inside a VM to support a hypervisor, the OoH principle is to individually expose current hypervisor-oriented hardware virtualization features to the guest OS. This way, guest's processes could also take benefit from those features. We illustrate OoH with Intel PML (Page Modification Logging), a feature that allows efficient dirty page tracking to improve VM live migration. Because dirty page tracking is at the heart of many essential tasks including process checkpointing (e.g., CRIU) and concurrent garbage collection (e.g, Boehm GC), OoH exposes PML to accelerate these tasks in the guest. We present two OoH solutions namely Shadow PML (SPML) and Extended PML (EPML) that we integrated into CRIU and Boehm GC. Evaluation results showed that EPML speeds up CRIU checkpointing by about 13 x and Boehm garbage collection by up to 6x compared to SPML, /proc, and userfaultfd while reducing their overhead on monitored applications by about 16 x.","2167-4337","978-1-6654-5444-5","10.1109/SC41404.2022.00092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046088","virtualization;page modification logging;check-pointing;garbage collection","Checkpointing;Virtual machine monitors;Target tracking;Linux;Data processing;Hardware;Data models","","","","","",32.0,"IEEE","23 Feb 2023","","","IEEE","IEEE Conferences"
"Anton 3: Twenty Microseconds of Molecular Dynamics Simulation Before Lunch","D. E. Shaw; P. J. Adams; A. Azaria; J. A. Bank; B. Batson; A. Bell; M. Bergdorf; J. Bhatt; J. A. Butts; T. Correia; R. M. Dirks; R. O. Dror; M. P. Eastwood; B. Edwards; A. Even; P. Feldmann; M. Fenn; C. H. Fenton; A. Forte; J. Gagliardo; G. Gill; M. Gorlatova; B. Greskamp; J. P. Grossman; J. Gullingsrud; A. Harper; W. Hasenplaugh; M. Heily; B. C. Heshmat; J. Hunt; D. J. Ierardi; L. Iserovich; B. L. Jackson; N. P. Johnson; M. M. Kirk; J. L. Klepeis; J. S. Kuskin; K. M. Mackenzie; R. J. Mader; R. McGowen; A. McLaughlin; M. A. Moraes; M. H. Nasr; L. J. Nociolo; L. O'Donnell; A. Parker; J. L. Peticolas; G. Pocina; C. Predescu; T. Quan; J. K. Salmon; C. Schwink; K. S. Shim; N. Siddique; J. Spengler; T. Szalay; R. Tabladillo; R. Tartler; A. G. Taube; M. Theobald; B. Towles; W. Vick; S. C. Wang; M. Wazlowski; M. J. Weingarten; J. M. Williams; K. A. Yuh","Department of Biochemistry and Molecular Biophysics, Columbia University, New York, NY; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA; D. E. Shaw Research, New York, NY, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,11,"Anton 3 is the newest member in a family of supercomputers specially designed for atomic-level simulation of molecules relevant to biology (e.g., DNA, proteins, and drug molecules). Anton 3 achieves order-of-magnitude improvements in time-to-solution over its predecessor, Anton 2 (the current state of the art), and is over 100-fold faster than any other currently available supercomputer, thereby enabling broad new avenues of research on critical questions in biology and drug discovery. This speedup means that a 512-node Anton 3 simulates a million atoms at over 100 microseconds per day. Furthermore, Anton 3 attains this performance while consuming an order of magnitude less energy per simulated microsecond than any other machine. Like its predecessors, Anton 3 was designed from the ground up around a new custom chip to best exploit the capabilities offered by new technologies. We present here the main architectural and algorithmic developments that were necessary to achieve such significant advances.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910051","","Drugs;Proteins;Analytical models;High performance computing;Heuristic algorithms;Computational modeling;Biological system modeling","","","","","",51.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Generalizable Coordination of Large Multiscale Workflows: Challenges and Learnings at Scale","H. Bhatia; F. Di Natale; J. Y. Moon; X. Zhang; J. R. Chavez; F. Aydin; C. Stanley; T. Oppelstrup; C. Neale; S. K. Schumacher; D. H. Ahn; S. Herbein; T. S. Carpenter; S. Gnanakaran; P. -T. Bremer; J. N. Glosli; F. C. Lightstone; H. I. Ingolfsson","Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, Livermore, California; Applications, Simulations, and Quality, Lawrence Livermore National Laboratory, Livermore, California; Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, Livermore, California; Lawrence Livermore National Laboratory, Physical and Life Sciences, Livermore, California; Global Security Computing Division, Lawrence Livermore National Laboratory, Livermore, California; Lawrence Livermore National Laboratory, Physical and Life Sciences, Livermore, California; Computational Sciences and Engineering Division, Oak Ridge National Laboratory, Oak Ridge, Tennessee; Lawrence Livermore National Laboratory, Physical and Life Sciences, Livermore, California; Los Alamos National Laboratory, Theoretical Biology and Biophysics, Los Alamos, New Mexico; IBM Thomas J. Watson Research Center, Yorktown Heights, New York; Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, Livermore, California; Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, Livermore, California; Lawrence Livermore National Laboratory, Physical and Life Sciences, Livermore, California; Los Alamos National Laboratory, Theoretical Biology and Biophysics, Los Alamos, New Mexico; Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, Livermore, California; Lawrence Livermore National Laboratory, Physical and Life Sciences, Livermore, California; Lawrence Livermore National Laboratory, Physical and Life Sciences, Livermore, California; Lawrence Livermore National Laboratory, Physical and Life Sciences, Livermore, California","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,16,"The advancement of machine learning techniques and the heterogeneous architectures of most current supercomputers are propelling the demand for large multiscale simulations that can automatically and autonomously couple diverse components and map them to relevant resources to solve complex problems at multiple scales. Nevertheless, despite the recent progress in workflow technologies, current capabilities are limited to coupling two scales. In the first-ever demonstration of using three scales of resolution, we present a scalable and generalizable framework that couples pairs of models using machine learning and in situ feedback. We expand upon the massively parallel Multiscale Machine-Learned Modeling Infrastructure (MuMMI), a recent, award-winning workflow, and generalize the framework beyond its original design. We discuss the challenges and learnings in executing a massive multiscale simulation campaign that utilized over 600,000 node hours on Summit and achieved more than 98% GPU occupancy for more than 83% of the time. We present innovations to enable several orders of magnitude scaling, including simultaneously coordinating 24,000 jobs, and managing several TBs of new data per day and over a billion files in total. Finally, we describe the generalizability of our framework and, with an upcoming open-source release, discuss how the presented framework may be used for new applications.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476210","U.S. Department of Energy (DOE); National Cancer Institute (NCI); National Institutes of Health (NIH); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910090","multiscale simulations;adaptive simulations;massively parallel;heterogenous architecture;machine learning;cancer research","Couplings;Technological innovation;High performance computing;Graphics processing units;Machine learning;Computer architecture;Propulsion","","","",5.0,"",78.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Online Evolutionary Batch Size Orchestration for Scheduling Deep Learning Workloads in GPU Clusters","Z. Bian; S. Li; W. Wang; Y. You","National University of Singapore; National University of Singapore; ByteDance, Singapore; National University of Singapore","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"Efficient GPU resource scheduling is essential to maximize resource utilization and save training costs for the increasing amount of deep learning workloads in shared GPU clusters. Existing GPU schedulers largely rely on static policies to leverage the performance characteristics of deep learning jobs. However, they can hardly reach optimal efficiency due to the lack of elasticity. To address the problem, we propose ONES, an ONline Evolutionary Scheduler for elastic batch size orchestration. ONES automatically manages the elasticity of each job based on the training batch size, so as to maximize GPU utilization and improve scheduling efficiency. It determines the batch size for each job through an online evolutionary search that can continuously optimize the scheduling decisions. We evaluate the effectiveness of ONES with 64 GPUs on TACC's Longhorn supercomputers. The results show that ONES can outperform the prior deep learning schedulers with a significantly shorter average job completion time.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3480859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910081","Deep learning;resource scheduling;evolutionary search;distributed training","Deep learning;Training;Schedules;High performance computing;Heuristic algorithms;Graphics processing units;Elasticity","","","",2.0,"",39.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Whale: Efficient One-to-Many Data Partitioning in RDMA-Assisted Distributed Stream Processing Systems","J. Tan; H. Chen; Y. Wang; H. Jin","National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"To process large-scale real-time data streams, existing distributed stream processing systems (DSPSs) leverage different stream partitioning strategies. The one-to-many data partitioning strategy plays an important role in various applications. With one-to-many data partitioning, an upstream processing instance sends a generated tuple to a potentially large number of downstream processing instances. Existing DSPSs leverage an instance-oriented communication mechanism, where an upstream instance transmits a tuple to different downstream instances separately. However, in one-to-many data partitioning, multiple downstream instances typically run on the same machine to exploit multi-core resources. Therefore, a DSPS actually sends a data item to a machine multiple times, raising significant unnecessary costs for serialization and communication. We show that such a mechanism can lead to serious performance bottleneck due to CPU overload. To address the problem, we design and implement Whale, an efficient RDMA (Remote Direct Memory Access) assisted distributed stream processing system. Two factors contribute to the efficiency of this design. First, we propose a novel RDMA-assisted stream multicast scheme with a self-adjusting non-blocking tree structure to alleviate the CPU workloads of an upstream instance during one-to-many data partitioning. Second, we re-design the communication mechanism in existing DSPSs by replacing the instance-oriented communication with a new worker-oriented communication scheme, which saves significant costs for redundant serialization and communication. We implement Whale on top of Apache Storm and conduct comprehensive experiments to evaluate its performance with large-scale real world datasets. The results show that Whale achieves 56.6x improvement of system throughput and 97% reduction of processing latency compared to existing designs.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476192","National Key Research and Development Program of China(grant numbers:2018YFB1004602); NSFC(grant numbers:61972446,61422202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910125","Distributed stream processing system;one-to-many data partition;remote direct memory access (RDMA)","Costs;Storms;High performance computing;Memory management;Whales;Distributed databases;Throughput","","","","","",27.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Exploiting User Activeness for Data Retention in HPC Systems","W. Zhang; S. Byna; H. Sim; S. Lee; S. Vazhkudai; Y. Chen","Texas Tech University, Lubbock, Texas, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Virginia Tech Blacksburg, Virginia, USA; Oak Ridge National Laboratory, Oak Ridge, Tennessee, USA; Micron Technology Austin, Texas, USA; Texas Tech University, Lubbock, Texas, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"HPC systems typically rely on the fixed-lifetime (FLT) data retention strategy, which only considers temporal locality of data accesses to parallel file systems. However, our extensive analysis based on the leadership-class HPC system traces suggests that the FLT approach often fails to capture the dynamics in users' behavior and leads to undesired data purge. In this study, we propose an activeness-based data retention (ActiveDR) solution, which advocates considering the data retention approach from a holistic activeness-based perspective. By evaluating the frequency and impact of users' activities, ActiveDR prioritizes the file purge process for inactive users and rewards active users with extended file lifetime on parallel storage. Our extensive evaluations based on the traces of the prior Titan supercomputer show that, when reaching the same purge target, ActiveDR achieves up to 37% file miss reduction as compared to the current FLT retention methodology.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476201","National Science Foundation(grant numbers:CCF-1718336,OAC-1835892,CNS-1817094); Lawrence Berkeley National Laboratory(grant numbers:DE-AC02-05CH11231); U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910078","Storage tiering;storage resource management;data retention;user behavior;data management;purge policy","File systems;High performance computing;Storage management;User experience;Supercomputers;Reproducibility of results;Behavioral sciences","","","",1.0,"",49.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Pinpointing Crash-Consistency Bugs in the HPC I/O Stack: A Cross-layer Approach","J. Sun; J. Huang; M. Snir","University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"We present ParaCrash, a testing framework for studying crash recovery in a typical HPC I/O stack, and demonstrate its use by identifying 15 new crash-consistency bugs in various parallel file systems (PFS) and I/O libraries. ParaCrash uses a “golden version” approach to test the entire HPC I/O stack: storage state after recovery from a crash is correct if it matches the state that can be achieved by a partial execution with no crashes. It supports systematic testing of a multilayered I/O stack while properly identifying the layer responsible for the bugs.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476144","NSF(grant numbers:CCF-1763540,CCF-1763540,CCF-1919044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910077","Crash consistency;parallel file systems;I/O library","Cross layer design;Systematics;File systems;High performance computing;Computer bugs;Libraries;Optimization","","","","","",74.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters","Q. Hu; P. Sun; S. Yan; Y. Wen; T. Zhang","S-Lab, Nanyang Technological University; SenseTime; SenseTime; School of Computer Science and Engineering, Nanyang Technological University; School of Computer Science and Engineering, Nanyang Technological University","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Modern GPU datacenters are critical for delivering Deep Learning (DL) models and services in both the research community and industry. When operating a datacenter, optimization of resource scheduling and management can bring significant financial benefits. Achieving this goal requires a deep understanding of the job features and user behaviors. We present a comprehensive study about the characteristics of DL jobs and resource management. First, we perform a large-scale analysis of real-world job traces from SenseTime. We uncover some interesting conclusions from the perspectives of clusters, jobs and users, which can facilitate the cluster system designs. Second, we introduce a general-purpose framework, which manages resources based on historical data. As case studies, we design (1) a Quasi-Shortest-Service-First scheduling service, which can minimize the cluster-wide average job completion time by up to 6.5×; (2) a Cluster Energy Saving service, which improves overall cluster utilization by up to 13%.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910054","GPU Datacenter;Cluster Statistical Analysis;Deep Learning Training;Cluster Management System;Workload Scheduling;Energy Conservation;Time-series Prediction","Deep learning;Industries;Job shop scheduling;Power demand;High performance computing;Graphics processing units;Behavioral sciences","","","","","",84.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Linux vs. Lightweight Multi-kernels for High Performance Computing: Experiences at Pre-Exascale","B. Gerofi; K. Tarumizu; L. Zhang; T. Okamoto; M. Takagi; S. Sumimoto; Y. Ishikawa","RIKEN Center for Computational Science, JAPAN; Fujitsu Ltd., JAPAN; Fujitsu Ltd., JAPAN; Fujitsu Ltd., JAPAN; RIKEN Center for Computational Science, JAPAN; Fujitsu Ltd., JAPAN; RIKEN Center for Computational Science, JAPAN","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"The long standing consensus in the High-Performance Computing (HPC) Operating Systems (OS) community is that lightweight kernel (LWK) based OSes have the potential to outperform Linux at extreme scale. To explore if LWKs live up to their expectation we developed IHK/McKernel, a lightweight multi-kernel OS designed for HPC, and deployed it on two high-end supercomputers to compare its performance against Linux. Oakforest-PACS, an Intel Xeon Phi (x86) based supercomputer, runs a moderately tuned Linux distribution. Fugaku, the world's fastest supercomputer at the time of writing this paper, is based on Fujitsu's A64FX (aarch64) CPU that runs a highly tuned Linux environment. We discuss recent developments in our OS and provide a detailed description on the challenges of tuning Fugaku's Linux for high-end HPC. While in a moderately tuned environment McKernel significantly outperforms Linux (by up to approximately 2X), on Fugaku we observe an average of 4% speedup across all our experiments, with a few exceptions where the LWK outperforms Linux by up to 29%. As part of our evaluation we also disclose a full scale (158,976 compute nodes) noise profile of the Fugaku system.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910114","high-performance computing;operating systems;lightweight kernels;multi kernels;scalability","Linux;High performance computing;Writing;Supercomputers;Kernel;Tuning","","","","","",52.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Revealing Power, Energy and Thermal Dynamics of a 200PF Pre-Exascale Supercomputer","W. Shin; V. Oles; A. M. Karimi; J. A. Ellis; F. Wang","Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,18,"As we approach the exascale computing era, the focused understanding of power consumption and its overall constraint on HPC architectures and applications are becoming increasingly paramount. Summit, located at the Oak Ridge Leadership Computing Facility (OLCF), is one of the fastest and largest pre-exascale platforms in operation today. This paper provides a first-order examination and analysis of power consumption at the component-level, node-level, and system-level, from all 4,626 Summit compute nodes, each with over 100 metrics at 1Hz frequency over the entire year of 2020. We also investigate the power characteristics and energy efficiency of over 840k Summit jobs and 250k GPU failure logs for further operational insights. To the best of our knowledge, this is the first systematic analysis of power data of HPC system at this scale.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910121","HPC;GPU;power;energy;reliability;telemetry;data analysis","Measurement;Data centers;Leadership;Power demand;Systematics;Instruments;Solids","","","",1.0,"",40.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"KAISA: An Adaptive Second-Order Optimizer Framework for Deep Neural Networks","J. G. Pauloski; S. Venkataraman; Q. Huang; K. Chard; Z. Zhang; L. Huang; I. Foster","University of Chicago; University of Wisconsin, Madison; University of Texas at Austin; Argonne National Laboratory, University of Chicago; Texas Advanced Computing Center; Texas Advanced Computing Center; Argonne National Laboratory, University of Chicago","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Kronecker-factored Approximate Curvature (K-FAC) has recently been shown to converge faster in deep neural network (DNN) training than stochastic gradient descent (SGD); however, K-FAC's larger memory footprint hinders its applicability to large models. We present KAISA, a K-FAC-enabled, Adaptable, Improved, and ScAlable second-order optimizer framework that adapts the memory footprint, communication, and computation given specific models and hardware to improve performance and increase scalability. We quantify the tradeoffs between memory and communication cost and evaluate KAISA on large models, including ResNet-50, Mask R-CNN, U-Net, and BERT, on up to 128 NVIDIA A100 GPUs. Compared to the original optimizers, KAISA converges 18.1-36.3% faster across applications with the same global batch size. Under a fixed memory budget, KAISA converges 32.5% and 41.6% faster in ResNet-50 and BERT-Large, respectively. KAISA can balance memory and communication to achieve scaling efficiency equal to or better than the baseline optimizers.","2167-4337","978-1-4503-8442-1","","NSF(grant numbers:OAC-1931354,OAC-1818253); Wisconsin Alumni Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910138","Machine Learning;Distributed Computing;Second-Order Optimization;K-FAC;Data-Parallel Algorithms","Training;Deep learning;Adaptation models;Costs;Computational modeling;Neural networks;Memory management","","","","","",59.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning Workloads","E. Georganas; D. Kalamkar; S. Avancha; M. Adelman; C. Anderson; A. Breuer; J. Bruestle; N. Chaudhary; A. Kundu; D. Kutnick; F. Laub; V. Md; S. Misra; R. Mohanty; H. Pabst; B. Ziv; A. Heinecke","Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Friedrich-Schiller-Universität Jena; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,16,"During the past decade, novel Deep Learning (DL) algorithms/workloads and hardware have been developed to tackle a wide range of problems. Despite the advances in workload/hardware ecosystems, the programming methodology of DL systems is stagnant. DL workloads leverage either highly-optimized, yet platform-specific and inflexible kernels from DL libraries, or in the case of novel operators, reference implementations are built via DL framework primitives with underwhelming performance. This work introduces the Tensor Processing Primitives (TPP), a programming abstraction striving for efficient, portable implementation of DL workloads with high-productivity. TPPs define a compact, yet versatile set of 2D-tensor operators (or a virtual Tensor ISA), which subsequently can be utilized as building-blocks to construct complex operators on high-dimensional tensors. The TPP specification is platform-agnostic, thus code expressed via TPPs is portable, whereas the TPP implementation is highly-optimized and platform-specific. We demonstrate the efficacy of our approach using standalone kernels and end-to-end DL workloads expressed entirely via TPPs that outperform state-of-the-art implementations on multiple platforms.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910046","","Deep learning;Tensors;Codes;High performance computing;Ecosystems;Programming;Libraries","","","","","",58.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Enable Simultaneous DNN Services Based on Deterministic Operator Overlap and Precise Latency Prediction","W. Cui; H. Zhao; Q. Chen; N. Zheng; J. Leng; J. Zhao; Z. Song; T. Ma; Y. Yang; C. Li; M. Guo","Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Qi Zhi Institute; Microsoft Research Asia; Shanghai Qi Zhi Institute; Shanghai Qi Zhi Institute; Alibaba Cloud; Alibaba Cloud; Alibaba Cloud; Shanghai Qi Zhi Institute; Shanghai Qi Zhi Institute","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"While user-facing services experience diurnal load patterns, co-locating services improve hardware utilization. Prior work on co-locating services on GPUs run queries sequentially, as the latencies of the queries are neither stable nor predictable when running simultaneously. The input sensitiveness and the non-deterministic operator overlap are two primary factors of the latency unpredictability. Hence, We propose Abacus, a runtime system that runs multiple services simultaneously. Abacus enables deterministic operator overlap to enforce latency predictability. Abacus composes of an overlap-aware latency predictor, a headroom-based query controller, and segmental model executors. The predictor predicts the latencies of the deterministic operator overlap. The controller determines the appropriate operator overlap for the QoS guarantee of all the services. The executors run the operators as needed to support the deterministic operator overlap. Our evaluation shows that Abacus reduces 51.3% of the QoS violation and improves the throughput by 29.8% on average compared with state-of-the-art solutions.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476143","National Natural Science Foundation of China (NSFC)(grant numbers:62022057,61832006,61632017,61872240,62072297); Program of Shanghai Academic/Technology Research Leader(grant numbers:18XD1401800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910118","DNN services;QoS;latency prediction;GPU;Co-location","Runtime;High performance computing;Graphics processing units;Quality of service;Predictive models;Throughput;Hardware","","","",5.0,"",60.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Distributed Quantum Computing with QMPI","T. Häner; D. S. Steiger; T. Hoefler; M. Troyer","Microsoft Quantum, Switzerland; Microsoft Quantum, Switzerland; ETH Zürich, Switzerland; Microsoft Quantum, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Practical applications of quantum computers require millions of physical qubits and it will be challenging for individual quantum processors to reach such qubit numbers. It is therefore timely to investigate the resource requirements of quantum algorithms in a distributed setting, where multiple quantum processors are inter-connected by a coherent network. We introduce an extension of the Message Passing Interface (MPI) to enable high-performance implementations of distributed quantum algorithms. In turn, these implementations can be used for testing, debugging, and resource estimation. In addition to a prototype implementation of quantum MPI, we present a performance model for distributed quantum computing, SENDQ. The model is inspired by the classical LogP model, making it useful to inform algorithmic decisions when program-ming distributed quantum computers. Specifically, we consider several optimizations of two quantum algorithms for problems in physics and chemistry, and we detail their effects on performance in the SENDQ model.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910065","distributed quantum computing;QMPI","Computers;Quantum algorithm;Program processors;Computational modeling;Message passing;Qubit;Prototypes","","","",3.0,"",66.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"BAASH: Lightweight, Efficient, and Reliable Blockchain-As-A-Service for HPC Systems","A. A. Mamun; F. Yan; D. Zhao","University of Nevada, Reno, Reno, NV, USA; University of Nevada, Reno, Reno, NV, USA; University of Nevada, Reno, Reno, NV, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Distributed resiliency becomes paramount to alleviate the growing costs of data movement and I/Os while preserving the data accuracy in HPC systems. This paper proposes to adopt blockchain-like decentralized protocols to achieve such distributed resiliency. The key challenge for such an adoption lies in the mismatch between blockchain's targeting systems (e.g., shared-nothing, loosely-coupled, TCP/IP stack) and HPC's unique design on storage subsystems, resource allocation, and programming models. We present BAASH, Blockchain-As-A-Service for HPC, deployable in a plug-n-play fashion. BAASH bridges the HPC-blockchain gap with two key components: (i) Lightweight consensus protocols for the HPC's shared-storage architecture, (ii) A new fault-tolerant mechanism compensating for the MPI to guarantee the distributed resiliency. We have implemented a prototype system and evaluated it with more than two million transactions on a 500-core HPC cluster. Results show that the prototype of the proposed techniques significantly outperforms vanilla blockchain systems and exhibits strong reliability with MPI.","2167-4337","978-1-4503-8442-1","","National Science Foundation(grant numbers:CCF-1756013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910142","Blockchain;MPI;fault tolerance;resilience;reproducibility;HPC","Fault tolerance;Costs;High performance computing;Fault tolerant systems;Prototypes;TCPIP;Programming","","","","","",54.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Representation of Women in HPC Conferences","E. Frachtenberg; R. D. Kaner","Reed College, Portland, Oregon, USA; Reed College, Portland, Oregon, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,12,"Women are acutely underrepresented in the HPC workforce. Addressing this gap requires accurate metrics on the representation of women and its associated factors. The goal of this paper is to provide current, broad, and reproducible data on this gender gap. Specifically, this study provides in-depth statistics on women's representation in HPC conferences, especially for authors of peer-reviewed papers, who serve as the keystone for future advances in the field. To this end, we analyzed participant data from nine HPC and HPC-related peer-reviewed conferences. In addition to gender distributions, we looked at post-publication citation statistics of the papers and authors' research experience, country, and work sector. Our main finding is that women represent only 10% of all HPC authors, with large geographical variations and small variations by sector. Representation is particularly low at higher experience levels. This 10% ratio is lower than even the 20-30% ratio in all computer science.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910112","high-performance computing (HPC);women in science;ender representation;bibliometrics","Measurement;Computer science;Codes;High performance computing;Collaboration;Lead;Benchmark testing","","","","","",53.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Preparing an Incompressible-Flow Fluid Dynamics Code for Exascale-Class Wind Energy Simulations","P. Mullowney; R. Li; S. Thomas; S. Ananthan; A. Sharma; J. S. Rood; A. B. Williams; M. A. Sprague","National Renewable Energy Lab, Golden, Colorado, USA; Lawrence Livermore National Lab, Livermore, California, USA; National Renewable Energy Lab, Golden, Colorado, USA; National Renewable Energy Lab, Golden, Colorado, USA; National Renewable Energy Lab, Golden, Colorado, USA; National Renewable Energy Lab, Golden, Colorado, USA; Sandia National Laboratories, Albuquerque, New Mexico, USA; National Renewable Energy Lab, Golden, Colorado, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"The U.S. Department of Energy has identified exascale-class wind farm simulation as critical to wind energy scientific discovery. A primary objective of the ExaWind project is to build high-performance, predictive computational fluid dynamics (CFD) tools that satisfy these modeling needs. GPU accelerators will serve as the computational thoroughbreds of next-generation, exascale-class supercomputers. Here, we report on our efforts in preparing the Exa Wind unstructured mesh solver, Nalu-Wind, for exascale-class machines. For computing at this scale, a simple port of the incompressible-flow algorithms to GPUs is insufficient. To achieve high performance, one needs novel algorithms that are application aware, memory efficient, and optimized for the latest-generation GPU devices. The result of our efforts are unstructured-mesh simulations of wind turbines that can effectively leverage thousands of GPUs. In particular, we demonstrate a first-of-its-kind, incompressible-flow simulation using Algebraic Multigrid solvers that strong scales to more than 4000 GPUs on the Summit supercomputer.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910096","CFD;Overset;Computational Linear Algebra;GPU Computing;Algebraic Multigrid","Performance evaluation;Wind energy;Computational modeling;Computational fluid dynamics;Graphics processing units;Wind farms;Predictive models","","","","","",44.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Symplectic Structure-Preserving Particle-in-Cell Whole-Volume Simulation of Tokamak Plasmas to 111.3 Trillion Particles and 25.7 Billion Grids","J. Xiao; J. Chen; J. Zheng; H. An; S. Huang; C. Yang; F. Li; Z. Zhang; Y. Huang; W. Han; X. Liu; D. Chen; Z. Liu; G. Zhuang; J. Chen; G. Li; X. Sun; Q. Chen","University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; School of Mathematical Sciences, Peking University, Beijing, China; National Supercomputing Center in Wuxi, Wuxi, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; School of Mathematical Sciences, Peking University, Beijing, China; School of Mathematical Sciences, Peking University, Beijing, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; Institute of Plasma Physics, Chinese Academy of Sciences, Hefei, China; Institute of Plasma Physics, Chinese Academy of Sciences, Hefei, China; University of Science and Technology of China, Hefei, China; Zhengzhou Uinversity, Zhengzhou, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"We employ our recently developed explicit 2nd-order charge-conservative symplectic electromagnetic particle-in-cell (PIC) scheme in the cylindrical mesh to simulate the whole-volume magnetic confinement toroidal plasmas on the new Sunway supercomputer. From a large-scale simulation of magneticized toroidal plasma with 111.3 trillion particles and 25.7 billion grids, we have obtained a sustained performance exceeding 201.1 PFLOP/s (double precision) with the fastest iteration step achieving 298.2 PFLOP/s (double precision). For the first time, unprecedented high resolution evolution of 6D electromagnetic fully kinetic plasmas based on 2D equilibrium profiles from Experimental Advanced Superconducting Tokamak (EAST) and designed operation state of China Fusion Engineering Test Reactor (CFETR) are presented, and edge micro-instabilities can be investigated directly. This shows the possibility to study crucial problems and phenomena in the magnetic confinement toroidal plasma directly using the symplectic electromagnetic fully kinetic PIC method on world's leading supercomputers.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3487398","National Key Research and Development Program(grant numbers:2016YFA0400600,2016YFA0400601,2016YFA0400602); National Natural Science Foundation of China (NSFC)(grant numbers:11905220,11805273,11975230); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910124","","Magnetic confinement;Superconducting transmission lines;Toroidal magnetic fields;Superconducting magnets;Tokamak devices;Supercomputers;Plasmas","","","",1.0,"",47.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Scalable Adaptive PDE Solvers in Arbitrary Domains","S. Kumar; M. Ishii; M. Fernando; B. Gao; K. Tan; M. -C. Hsu; A. Krishnamurthy; H. Sundar; B. Ganapathysubramanian","Iowa State University, Ames, Iowa; University of Utah, Salt Lake City, Utah; University of Utah, Salt Lake City, Utah; Iowa State University, Ames, Iowa; Iowa State University, Ames, Iowa; Iowa State University, Ames, Iowa; Iowa State University, Ames, Iowa; University of Utah, Salt Lake City, Utah; Iowa State University, Ames, Iowa","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,18,"Efficiently and accurately simulating partial differential equations (PDEs) in and around arbitrarily defined geometries, especially with high levels of adaptivity, has significant implications for different application domains. A key bottleneck in the above process is the fast construction of a ‘good’ adaptively-refined mesh. In this work, we present an efficient novel octree-based adaptive discretization approach capable of carving out arbitrarily shaped void regions from the parent domain: an essential requirement for fluid simulations around complex objects. Carving out objects produces an incomplete octree. We develop efficient top-down and bottom-up traversal methods to perform finite element computations on incomplete octrees. We validate the framework by (a) showing appropriate convergence analysis and (b) computing the drag coefficient for flow past a sphere for a wide range of Reynolds numbers (0(1-106)) encompassing the drag crisis regime. Finally, we deploy the framework on a realistic geometry on a current project to evaluate COVID-19 transmission risk in classrooms.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910144","Incomplete octrees;matrix-free;finite element method;distributed memory parallelism","Geometry;Three-dimensional displays;Fluids;Computational modeling;Partial differential equations;High performance computing;Octrees","","","","","",67.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"A Next-Generation Discontinuous Galerkin Fluid Dynamics Solver with Application to High-Resolution Lung Airflow Simulations","M. Kronbichler; N. Fehn; P. Munch; M. Bergbauer; K. -R. Wichmann; C. Geitner; M. Allalen; M. Schulz; W. A. Wall","Uppsala University, Uppsala, Sweden; Leibniz Supercomputing Centre, Garching, Germany; Helmholtz-Zentrum hereon GmbH, Geesthacht, Germany; Technical University of Munich, Garching, Germany; Ebenbuild GmbH, Garching, Germany; Technical University of Munich, Garching, Germany; Leibniz Supercomputing Centre, Garching, Germany; Technical University of Munich, Garching, Germany; Technical University of Munich, Garching, Germany","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"We present a novel, highly scalable and optimized solver for turbulent flows based on high-order discontinuous Galerkin discretizations of the incompressible Navier-Stokes equations aimed to minimize time-to-solution. The solver uses explicit-implicit time integration with variable step size. The central algorithmic component is the matrix-free evaluation of discretized finite element operators. The node-level performance is optimized by sum-factorization kernels for tensor-product elements with unique algorithmic choices that reduce the number of arithmetic operations, improve cache usage, and vectorize the arithmetic work across elements and faces. These ingredients are integrated into a framework scalable to the massive parallelism of supercomputers by the use of optimal-complexity linear solvers, such as mixed-precision, hybrid geometric-polynomial-algebraic multigrid solvers for the pressure Poisson problem. The application problem under consideration are fluid dynamical simulations of the human respiratory system under mechanical ventilation conditions, using unstructured/structured adaptively refined meshes for geometrically complex domains typical of biomedical engineering.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476171","German Research Foundation (DFG)(grant numbers:1648); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910083","high-order discontinuous Galerkin;matrix-free algorithms;multigrid;time-to-solution","Adaptation models;Computational modeling;Biological system modeling;Atmospheric modeling;Pulmonary diseases;Ventilation;Supercomputers","","","","","",62.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Understanding, Predicting and Scheduling Serverless Workloads under Partial Interference","L. Zhao; Y. Yang; Y. Li; X. Zhou; K. Li","State Key Laboratory of Communication Content Cognition, Colleage of Intelligence & Computing(CIC), Tianjin University; Tianjin Key Lab. of Advanced Networking (TANKLAB), CIC, Tianjin University; Tianjin Key Lab. of Advanced Networking (TANKLAB), CIC, Tianjin University; Tianjin Key Lab. of Advanced Networking (TANKLAB), CIC, Tianjin University; Tianjin Key Lab. of Advanced Networking (TANKLAB), CIC, Tianjin University","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Interference among distributed cloud applications can be classified into three types: full, partial and zero. While prior research merely focused on full interference, the partial interference that occurs at parts of applications is far more common yet still lacks in-depth study. Serverless computing that structures applications into small-sized, short-lived functions further exacerbate partial interference. We characterize the features of partial interference in serverless as exhibiting high volatility, spatial-temporal variation, and propagation. Given these observations, we propose an incremental learning predictor, named Gsight, which can achieve high precision by harnessing the spatial-temporal overlap codes and profiles of functions via an end-to-end call path. Experimental results show that Gsight can achieve an average error of 1.71%. Its convergence speed is at least 3x faster than that in a serverful system. A scheduling case study shows that the proposed method can improve function density by ≥18.79% while guaranteeing the quality of service (QoS).","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476215","National Key Research and Development Program of China(grant numbers:2016YFB1000205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910093","Serverless;Partial Interference;Performance Prediction;Resource Utilization","Codes;Processor scheduling;Computational modeling;High performance computing;Serverless computing;Interference;Quality of service","","","",2.0,"",65.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"The Hidden Cost of the Edge: A Performance Comparison of Edge and Cloud Latencies","A. Ali-Eldin; B. Wang; P. Shenoy","Chalmers University of Technology; University of Massachusetts Amherst; University of Massachusetts Amherst","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Edge computing has emerged as a popular paradigm for running latency-sensitive applications due to its ability to offer lower network latencies to end-users. In this paper, we argue that despite its lower network latency, the resource-constrained nature of the edge can result in higher end-to-end latency, especially at higher utilizations, when compared to cloud data centers. We study this edge performance inversion problem through an analytic comparison of edge and cloud latencies and analyze conditions under which the edge can yield worse performance than the cloud. To verify our analytic results, we conduct a detailed experimental comparison of the edge and the cloud latencies using a realistic application and real cloud workloads. Both our analytical and experimental results show that even at moderate utilizations, the edge queuing delays can offset the benefits of lower network latencies, and even result in performance inversion where running in the cloud would provide superior latencies. We finally discuss practical implications of our results and provide insights into how application designers and service providers should design edge applications and systems to avoid these pitfalls.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476142","NSF(grant numbers:2105494,1908536,1836752,1763834); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910040","","Data centers;Costs;High performance computing;Tail;Dynamic scheduling;Delays;Resource management","","","",3.0,"",37.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"RIBBON: Cost-Effective and QoS-Aware Deep Learning Model Inference using a Diverse Pool of Cloud Computing Instances","B. Li; R. B. Roy; T. Patel; V. Gadepally; K. Gettings; D. Tiwari","MIT Lincoln Laboratory; MIT Lincoln Laboratory; MIT Lincoln Laboratory; Northeastern University; Northeastern University; MIT Lincoln Laboratory","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,16,"Deep learning model inference is a key service in many businesses and scientific discovery processes. This paper introduces Ribbon, a novel deep learning inference serving system that meets two competing objectives: quality-of-service (QoS) target and cost-effectiveness. The key idea behind Ribbon is to intelligently em-ploy a diverse set of cloud computing instances (heterogeneous instances) to meet the QoS target and maximize cost savings. Rib-bon devises a Bayesian Optimization-driven strategy that helps users build the optimal set of heterogeneous instances for their model inference service needs on cloud computing platforms - and, Ribbon demonstrates its superiority over existing approaches of inference serving systems using homogeneous instance pools. Ribbon saves up to 16% of the inference service cost for different learning models including emerging deep learning recommender system models and drug-discovery enabling models.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476168","NSF(grant numbers:1920020,2124897,1910601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910063","Machine learning;Inference serving;Quality-of-service;Bayesian optimization","Deep learning;Cloud computing;Costs;Computational modeling;High performance computing;Quality of service;Bayes methods","","","",2.0,"",99.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"E.T.: Re-Thinking Self-Attention for Transformer Models on GPUs","S. Chen; S. Huang; S. Pandey; B. Li; G. R. Gao; L. Zheng; C. Ding; H. Liu","Stevens Institute of Technology; University of Connecticut; Stevens Institute of Technology; University of Connecticut; University of Delaware; University of Delaware; University of Connecticut; Stevens Institute of Technology","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Transformer-based deep learning models have become a ubiquitous vehicle to drive a variety of Natural Language Processing (NLP) related tasks beyond their accuracy ceiling. However, these models also suffer from two pronounced challenges, that is, gigantic model size and prolonged turnaround time. To this end, we introduce E.T. that rE-thinks self-attention computation for Transformer models on GPUs with the following contributions: First, we introduce a novel self-attention architecture, which encompasses two tailored self-attention operators with corresponding sequence length-aware optimizations, and operation reordering optimizations. Second, we present an attention-aware pruning design which judiciously uses various pruning algorithms to reduce more computations hence achieves significantly shorter turnaround time. For the pruning algorithms, we not only revamp the existing pruning algorithms, but also tailor new ones for transformer models. Taken together, we evaluate E.T. across a variety of benchmarks for Transformer, BERTBASE and DistilBERT, where E.T. presents superior performance over the mainstream projects, including the popular Nvidia Enterprise solutions, i.e., TensorRT and FasterTransformer.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910069","","Deep learning;Tensors;Computational modeling;High performance computing;Computer architecture;Benchmark testing;Transformers","","","",1.0,"",63.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Parallel Construction of Module Networks","A. Srivastava; S. P. Chockalingam; M. Aluru; S. Aluru","Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Module networks (MoNets) are a parameter-sharing specialization of Bayesian networks that are used for reasoning about multi-dimensional entities with concerted interactions between groups of variables. Construction of MoNets is compute-intensive, with sequential methods requiring months for learning networks with a few thousand variables. In this paper, we present the first scalable distributed-memory parallel solution for constructing MoNets by parallelizing Lemon-Tree, a widely used sequential software. We demonstrate the scalability of our parallel method on a key application of MoNets - the construction of genome-scale gene regulatory networks. Using 4096 cores, our parallel implementation constructs regulatory networks for 5, 716 and 18, 373 genes of two model organisms in 24 minutes and 4.2 hours, compared to an estimated 49 and 1561 days using Lemon-Tree for generating exactly the same networks, respectively. Our method is application-agnostic and broadly applicable to the learning of high-dimensional MoNets for any of its wide array of applications.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476207","National Science Foundation(grant numbers:OAC-1828187,OAC-1854828,CCF-1718479); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910100","Bayesian networks;module networks;score-based learning;parallel machine learning;gene networks","Scalability;High performance computing;Genomics;Software;Organisms;Cognition;Bayes methods","","","","","",62.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines","S. Li; T. Hoefler","Department of Computer Science, ETH, Zurich, Switzerland; Department of Computer Science, ETH, Zurich, Switzerland","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Training large deep learning models at scale is very challenging. This paper proposes Chimera, a novel pipeline parallelism scheme which combines bidirectional pipelines for efficiently training large-scale models. Chimera is a synchronous approach and therefore no loss of accuracy, which is more convergence-friendly than asynchro-nous approaches. Compared with the latest synchronous pipeline approach, Chimera reduces the number of bubbles by up to 50%; ben-efiting from the sophisticated scheduling of bidirectional pipelines, Chimera has a more balanced activation memory consumption. Evaluations are conducted on Transformer based language models. For a GPT-2 model with 1.3 billion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer, Chimera improves the training throughput by 1.16x-2.34x over the state-of-the-art synchronous and asynchronous pipeline approaches.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476145","European Research Council (ERC)(grant numbers:678880,801039,955513); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910071","distributed deep learning;pipeline parallelism;data parallelism;operator parallelism;model parallelism","Training;Computational modeling;High performance computing;Pipelines;Neural networks;Memory management;Parallel processing","","","",4.0,"",60.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Bootstrapping In-Situ Workflow Auto-Tuning via Combining Performance Models of Component Applications","T. Shu; Y. Guo; J. Wozniak; X. Ding; I. Foster; T. Kurc","Southern Illinois University, Carbondale, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; New Jersey Institute of Technology, Newark, NJ, USA; Argonne Natl. Lab and Univ. Chicago, Lemont and Chicago, IL, USA; Stony Brook University, Stony Brook, NY, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"In an in-situ workflow, multiple components such as simulation and analysis applications are coupled with streaming data transfers. The multiplicity of possible configurations necessitates an auto-tuner for workflow optimization. Existing auto-tuning approaches are computationally expensive because many configurations must be sampled by running the whole workflow repeatedly in order to train the auto-tuner surrogate model or otherwise explore the configuration space. To reduce these costs, we instead combine the performance models of component applications by exploiting the analytical workflow structure, selectively generating test configurations to measure and guide the training of a machine learning workflow surrogate model. Because the training can focus on well-performing configurations, the resulting surrogate model can achieve high prediction accuracy for good configurations despite training with fewer total configurations. Experiments with real applications demonstrate that our approach can identify significantly better configurations than other approaches for a fixed computer time budget.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910052","in-situ workflow;auto-tuning;bootstrapping;component model combination","Training;Analytical models;Costs;Computational modeling;High performance computing;Machine learning;Predictive models","","","","","",59.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Meeting the Real-Time Challenges of Ground-Based Telescopes Using Low-Rank Matrix Computations","H. Ltaief; J. Cranney; D. Gratadour; Y. Hong; L. Gatineau; D. Keyes","Extreme Computing Research Center Computer, Electrical and Mathematical Sciences & Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Faculty of Engineering and Built Environment, Research School of Astronomy & Astrophysics College of Science Australian National University Australia School of Electrical Engineering & Computing, University of Newcastle, Australia; Research School of Astronomy & Astrophysics College of Science Australian National University Australia, LESIA Observatoire de Paris PSL University, Sorbonne University, Paris University CNRS, France; Extreme Computing Research Center Computer, Electrical and Mathematical Sciences & Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; HPC Division, NEC Deutschland GmbH, Germany; Extreme Computing Research Center Computer, Electrical and Mathematical Sciences & Engineering Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Adaptive Optics (AO) is a technology that permits to measure and mitigate the distortion effects of atmospheric turbulence on optical beams. AO must operate in real-time by controlling thousands of actuators to shape the surface of deformable mirrors deployed on ground-based telescopes to compensate for these distortions. The command vectors that trigger how each individual actuator should act to bend a portion of the mirror are obtained from Matrix-Vector Multiplications (MVM). We identify and leverage the data sparsity structure of these control matrices coming from the MAVIS instruments for the European Southern Observatory's Very Large Telescope. We provide performance evaluation on x86 and accelerator-based systems. We present the impact of tile low-rank (TLR) matrix approximations on time-to-solution for the MVM and assess the produced image quality. We achieve performance improvement up to two orders of magnitude for TLR-MVM compared to regular dense MVM, while maintaining the image quality.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910107","Ground-Based Telescopes;Real-Time Computational Astronomy;Tile Low-Rank Approximation;Matrix-Vector Multiplication","Image quality;Performance evaluation;Actuators;Atmospheric measurements;Shape;Optical distortion;Telescopes","","","",1.0,"",46.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Closing the “Quantum Supremacy” Gap: Achieving Real-Time Simulation of a Random Quantum Circuit Using a New Sunway Supercomputer","Y. Liu; X. Liu; F. Li; H. Fu; Y. Yang; J. Song; P. Zhao; Z. Wang; D. Peng; H. Chen; C. Guo; H. Huang; W. Wu; D. Chen","National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; Shanghai Research Center for Quantum Sciences, Shanghai, China; Shanghai Research Center for Quantum Sciences, Shanghai, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,12,"We develop a high-performance tensor-based simulator for random quantum circuits(RQCs) on the new Sunway supercomputer. Our major innovations include: (1) a near-optimal slicing scheme, and a path-optimization strategy that considers both complexity and compute density; (2) a three-level parallelization scheme that scales to about 42 million cores; (3) a fused permutation and multiplication design that improves the compute efficiency for a wide range of tensor contraction scenarios; and (4) a mixed-precision scheme to further improve the performance. Our simulator effectively expands the scope of simulatable RQCs to include the 10×10(qubits)X(1 + 40 + l)(depth) circuit, with a sustained performance of 1.2 Eflops (single-precision), or 4.4 Eflops (mixed-precision)as a new milestone for classical simulation of quantum circuits; and reduces the simulation sampling time of Google Sycamore to 304 seconds, from the previously claimed 10,000 years.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3487399","National Key R&D Program of China(grant numbers:2017YFA0604500); National Natural Science Foundation of China(grant numbers:U1839206); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910098","","Technological innovation;Tensors;Computational modeling;High performance computing;Supercomputers;Real-time systems;Internet","","","",5.0,"",35.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"AgEBO-Tabular: Joint Neural Architecture and Hyperparameter Search with Autotuned Data-Parallel Training for Tabular Data","R. Égelé; P. Balaprakash; I. Guyon; V. Vishwanath; F. Xia; R. Stevens; Z. Liu","École polytechnique, Palaiseau, France; Argonne National Laboratory, Lemont, Illinois, USA; CNRS/Inria-LISN, U. Paris-Saclay, France; Argonne National Laboratory, Lemont, Illinois, USA; Argonne National Laboratory, Lemont, Illinois, USA; Argonne National Laboratory, Lemont, Illinois, USA; CNRS/Inria-LISN, U. Paris-Saclay, France","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,16,"Developing high-performing predictive models for large tabular data sets is a challenging task. Neural architecture search (NAS) is an AutoML approach that generates and evaluates multiple neural networks with different architectures concurrently to automatically discover an high performing model. A key issue in NAS, particularly for large data sets, is the large computation time required to evaluate each generated architecture. While data-parallel training has the potential to address this issue, a straightforward approach can result in significant loss of accuracy. To that end, we develop AgEBO-Tabular, which combines Aging Evolution (AE) to search over neural architectures and asynchronous Bayesian optimization (BO) to search over hyperparameters to adapt data-parallel training. We evaluate the efficacy of our approach on two large predictive modeling tabular data sets from the Exascale Computing Project-CANcer Distributed Learning Environment (ECP-CANDLE).","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476203","U.S. Department of Energy (DOE); Office of Science; Advanced Scientific Computing Research(grant numbers:DE-AC02-06CH11357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910086","neural networks;neural architecture search;data-parallelism","Training;Exascale computing;Distance learning;Computational modeling;Neural networks;Distributed databases;Computer architecture","","","","","",37.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Non-Recurring Engineering (NRE) Best Practices: A Case Study with the NERSC/NVIDIA OpenMP Contract","C. S. Daley; A. Southwell; R. Gayatri; S. Biersdorff; C. Toepfer; G. Özen; N. J. Wright","Lawrence Berkeley National Laboratory, Berkeley, California, USA; NVIDIA Corporation, Hillsboro, Oregon, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; NVIDIA Corporation, Hillsboro, Oregon, USA; NVIDIA Corporation, Hillsboro, Oregon, USA; NVIDIA Corporation, Hillsboro, Oregon, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"The NERSC supercomputer, Perlmutter, consists of AMD CPUs and NVIDIA GPUs. NERSC users expect to be able to use OpenMP to take advantage of the highly capable GPUs. This paper describes how NERSC/NVIDIA constructed a Non-Recurring Engineering (NRE) contract to add OpenMP GPU-offload support to the NVIDIA HPC compilers. The paper describes how the contract incorporated the strengths of both parties and encouraged collaboration to improve the quality of the final deliverable. We include our best practices and how this particular contract took into account emerging OpenMP specifications, NERSC workload requirements, and how to use OpenMP most efficiently on GPU hardware. This paper includes OpenMP application performance results obtained with the NVIDIA compilers distributed in the NVIDIA HPC SDK.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910045","NRE;OpenMP target offload;NVIDIA GPU;Supercomputer procurement;Compiler development","High performance computing;Graphics processing units;Collaboration;Supercomputers;Hardware;Contracts;Best practices","","","","","",58.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Minimizing Privilege for Building HPC Containers","R. Priedhorsky; R. S. Canon; T. Randles; A. J. Younge","High Performance Computing Division, Los Alamos National Laboratory, Los Alamos, NM, USA; National Energy Research Scientific Computing Center, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; High Performance Computing Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Center for Computing Research, Sandia National Laboratories, Albuquerque, NM, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"HPC centers face increasing demand for software flexibility, and there is growing consensus that Linux containers are a promising solution. However, existing container build solutions require root privileges and cannot be used directly on HPC resources. This limitation is compounded as supercomputer diversity expands and HPC architectures become more dissimilar from commodity computing resources. Our analysis suggests this problem can best be solved with low-privilege containers. We detail relevant Linux kernel features, propose a new taxonomy of container privilege, and compare two open-source implementations: mostly-unprivileged rootless Podman and fully-unprivileged Charliecloud. We demonstrate that low-privilege container build on HPC resources works now and will continue to improve, giving normal users a better workflow to securely and correctly build containers. Minimizing privilege in this way can improve HPC user and developer productivity as well as reduce support workload for exascale applications.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910066","","Productivity;Linux;High performance computing;Taxonomy;Computer architecture;Containers;Supercomputers","","","",2.0,"",62.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Systematically Inferring I/O Performance Variability by Examining Repetitive Job Behavior","E. Costa; T. Patel; B. Schwaller; J. M. Brandt; D. Tiwari","Northeastern University, USA; Northeastern University, USA; Sandia National Laboratory, USA; Sandia National Laboratory, USA; Northeastern University, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Monitoring and analyzing I/O behaviors is critical to the efficient utilization of parallel storage systems. Unfortunately, with increasing I/O requirements and resource contention, I/O performance variability is becoming a significant concern. This paper investigates I/O behavior and performance variability on a large-scale high-performance computing (HPC) system using a novel methodology that identifies similarity across jobs from the same application leveraging an I/O characterization tool and then, detects potential I/O performance variability across jobs of the same application. We demonstrate and discuss how our unique methodology can be used to perform temporal and feature analyses to detect interesting I/O performance variability patterns in production HPC systems, and their implications for operating/managing large-scale systems.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476186","NSF(grant numbers:1910601,1753840); Northeastern University; U.S. Department of Energy; Sandia National Laboratories(grant numbers:2055483); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910076","Storage systems;HPC;HPC I/O;performance variability;machine learning;distributed computing systems","File systems;High performance computing;Production;Feature extraction;Behavioral sciences;Large-scale systems;Monitoring","","","",3.0,"",62.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"SEEC: Stochastic Escape Express Channel","M. Parasar; N. E. Jerger; P. V. Gratz; J. S. Miguel; T. Krishna","Georgia Institute of Technology; University of Toronto; Texas A & M; University of Wisconsin-Madison; Georgia Institute of Technology","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Allocating a free buffer before moving to the next router is a fundamental tenet for packet movement in NoCs. Often, to solve head of line blocking and avoid deadlock, NoCs are provisioned with significant buffer resources in the form of virtual channels (VC) which consume area and power. We introduce stochastic escape express channels (SEEC) to enhance performance and avoid deadlock with dramatically fewer buffers than state-of-the-art NoCs. The network interfaces in SEEC periodically send special tokens called seekers to find packets destined for them and upgrade them to use a novel flow control called Free-Flow (FF). FF-packets traverse the network minimally from link to link, bypassing routers (bufferlessly) to the destination. As a result, FF-packets bypass regions of congestion in the NoC without needing more buffers. Furthermore, any deadlock that a FF-packet was originally involved in is guaranteed to break, without requiring turn restrictions or extra VCs. We also present an extension called multi-SEEC (mSEEC) that enables multiple simultaneous non-intersecting FF-packet traversals to enhance throughput further. We implement and evaluate SEEC and mSEEC on a mesh over a range of synthetic workloads and real applications and observe 34-40% reduction in average packet latency for real applications and 10- 50% average improvement in throughput for synthetic traffic over the state-of-the-art at 1/6th the area/power budget.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476140","Canada Research Chair; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910122","","Fault tolerance;Art;High performance computing;Fault tolerant systems;System recovery;Throughput;Routing","","","","","",41.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Flare: Flexible In-Network Allreduce","D. De Sensi; S. Di Girolamo; S. Ashkboos; S. Li; T. Hoefler","ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland; ETH Zurich, Zurich, Switzerland","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"The allreduce operation is one of the most commonly used communication routines in distributed applications. To improve its bandwidth and to reduce network traffic, this operation can be accelerated by offloading it to network switches, that aggregate the data received from the hosts, and send them back the aggregated result. However, existing solutions provide limited customization opportunities and might provide suboptimal performance when dealing with custom operators and data types, with sparse data, or when reproducibility of the aggregation is a concern. To deal with these problems, in this work we design a flexible programmable switch by using as a building block PsPIN, a RISC-V architecture implementing the sPIN programming model. We then design, model, and analyze different algorithms for executing the aggregation on this architecture, showing performance improvements compared to state-of-the-art approaches.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910089","In-Network Computing;Programmable Switch;Allreduce","Analytical models;Architecture;High performance computing;Computer architecture;Telecommunication traffic;Switches;Bandwidth","","","",2.0,"",84.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"HatRPC: Hint-Accelerated Thrift RPC over RDMA","T. Li; H. Shi; X. Lu","The Ohio State University, Columbus, USA; The Ohio State University, Columbus, USA; University of California, Merced, Merced, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"In this paper, we propose a novel hint-accelerated Remote Procedure Call (RPC) framework based on Apache Thrift over Remote Direct Memory Access (RDMA) protocols, called HatRPC. HatRPC proposes a hierarchical hint scheme towards optimizing heterogeneous RPC services and functions. The proposed hint design is composed of service-granularity and function-granularity hints for achieving varied optimization goals and reducing design space for further optimizing the underneath RDMA communication engine. We co-design a key-value store called HatKV with HatRPC and LMDB. The effectiveness and efficiency of HatRPC are validated and evaluated with our proposed Apache Thrift Benchmarks (ATB), YCSB, and TPC-H workloads. Performance evaluations show that the proposed HatRPC approach can deliver up to 55% performance improvement for ATB benchmarks and up to 1.51× speedup for TPC-H queries compared with vanilla Thrift over IPoIB. In addition, the co-designed HatKV can achieve up to 85.5% improvement for YCSB workloads.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910058","RDMA;Hint;Code Generation;RPC;Thrift","Performance evaluation;Protocols;High performance computing;Benchmark testing;Optimization;Engines","","","","","",68.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU Tensor Cores","B. Feng; Y. Wang; T. Gena; A. Li; Y. Ding","University of California, Santa Barbara; University of California, Santa Barbara; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; University of California, Santa Barbara","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Over the years, accelerating neural networks with quantization has been widely studied. Unfortunately, prior efforts with diverse precisions (e.g., 1-bit weights and 2-bit activations) are usually restricted by limited precision support on GPUs (e.g., int1 and int4). To break such restrictions, we introduce the first Arbitrary Precision Neural Network framework (APNN-TC)1 to fully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically, APNN-TC first incorporates a novel emulation algorithm to support arbitrary short bit-width computation with int1 compute primitives and XOR/AND Boolean operations. Second, APNN-TC integrates arbitrary precision layer designs to efficiently map our emulation algorithm to Tensor Cores with novel batching strategies and specialized memory organization. Third, APNN-TC embodies a novel arbitrary precision NN design to minimize memory access across layers and further improve performance. Extensive evaluations show that APNN-TC can achieve significant speedup over CUT-LASS kernels and various NN models, such as ResNet and VGG.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476157","NSF(grant numbers:1925717,2124039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910132","GPU Tensor Core;Convolutional Neural Networks;Neural Network Quantization;High-performance Computing","Tensors;Quantization (signal);Computational modeling;High performance computing;Emulation;Memory management;Graphics processing units","","","",7.0,"",47.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Scalable Edge-Based Hyperdimensional Learning System with Brain-Like Neural Adaptation","Z. Zou; Y. Kim; F. Imani; H. Alimohamadi; R. Cammarota; M. Imani","Uiversity of California San Diego, La Jolla, CA, USA; Daegu Gyeongbuk Institute of Science and Technology, South Korea; University of Connecticut, Storrs, CT, USA; Uiversity of California San Diego, La Jolla, CA, USA; Intel Labs, San Diego, CA; University of California Irvine, Irvine, CA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"In the Internet of Things (IoT) domain, many applications are running machine learning algorithms to assimilate the data collected in the swarm of devices. Sending all data to the powerful computing environment, e.g., cloud, poses significant efficiency and scalability issues. A promising way is to distribute the learning tasks onto the IoT hierarchy, often referred to edge computing; however, the existing sophisticated algorithms such as deep learning are often overcomplex to run on less-powerful and unreliable embedded IoT devices. Hyperdimensional Computing (HDC) is a brain-inspired learning approach for efficient and robust learning on today's embedded devices. Encoding, or transforming the input data into high-dimensional representation, is the key first step of HDC before performing a learning task. All existing HDC approaches use a static encoder; thus, they still require very high dimensionality, resulting in significant efficiency loss for the edge devices with limited resources. In this paper, we have developed NeuralHD, a new HDC approach with a dynamic encoder for adaptive learning. Inspired by human neural regeneration study in neuroscience, NeuralHD identifies insignificant dimensions and regenerates those dimensions to enhance the learning capability and robustness. We also present a scalable learning framework to distribute NeuralHD computation over edge devices in IoT systems. Our solution enables edge devices capable of real-time learning from both labeled and unlabeled data. Our evaluation on a wide range of practical classification tasks shows that NeuralHD provides $5.7 \times$ and $6.1\times(12.3\times$ and $14.1 \times$) faster and more energy-efficient training compared to the HD-based algorithms (DNNs) running on the same platform. NeuralHD also provide $4.2\times$ and $11.6\times$ higher robustness to noise in the unreliable network and hardware of IoT environments as compared to DNNs.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3480958","National Science Foundation (NSF)(grant numbers:2127780); Semiconductor Research Corporation (SRC)(grant numbers:2988.001); Office of Naval Research(grant numbers:N00014-21-1-2225); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910079","","Training;Learning systems;Adaptive learning;Scalability;Robustness;Real-time systems;Energy efficiency","","","",7.0,"",87.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Dr. Top-k: Delegate-Centric Top-k on GPUs","A. Gaihre; D. Zheng; S. Weitze; L. Li; S. L. Song; C. Ding; X. S. Li; H. Liu","Stevens Institute of Technology; Johns Hopkins University; Stevens Institute of Technology; Brookhaven National Laboratory; University of Sydney and UW Seattle; University of Connecticut; Lawrence Berkeley National Laboratory; Stevens Institute of Technology","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Recent top-k computation efforts explore the possibility of revising various sorting algorithms to answer top-k queries on GPUs. These endeavors, unfortunately, perform significantly more work than needed. This paper introduces Dr. Top-k, a Delegate-centric top-k system on GPUs that can reduce the top-k workloads significantly. Particularly, it contains three major contributions: First, we introduce a comprehensive design of the delegate-centric concept, including maximum delegate, delegate-based filtering, and $\beta$ delegate mechanisms to help reduce the workload for top-k up to more than 99%. Second, due to the difficulty and importance of deriving a proper subrange size, we perform a rigorous theoretical analysis, coupled with thorough experimental validations to identify the desirable subrange size. Third, we introduce four key system optimizations to enable fast multi-GPU top-k computation. Taken together, this work constantly outperforms the state-of-the-art.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910070","","Filtering;High performance computing;Optimization;Sorting","","","","","",55.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Billion atom molecular dynamics simulations of carbon at extreme conditions and experimental time and length scales","K. Nguyen–Cong; J. T. Willman; S. G. Moore; A. B. Belonoshko; R. Gayatri; E. Weinberg; M. A. Wood; A. P. Thompson; I. I. Oleynik","University of South Florida, Tampa, FL, USA; University of South Florida, Tampa, FL, USA; Sandia National Laboratories, Albuquerque, NM, USA; Royal Institute of Technology (KTH), Stockholm, Sweden; NERSC, Berkeley, CA, USA; NVIDIA Corporation, Santa Clara, CA, USA; Royal Institute of Technology (KTH), Stockholm, Sweden; Royal Institute of Technology (KTH), Stockholm, Sweden; University of South Florida, Tampa, FL, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,12,"Billion atom molecular dynamics (MD) using quantum-accurate machine-learning Spectral Neighbor Analysis Potential (SNAP) observed long-sought high pressure BC8 phase of carbon at extreme pressure (12 Mbar) and temperature (5,000 K). 24-hour, 4650 node production simulation on OLCF Summit demonstrated an unprecedented scaling and unmatched real-world performance of SNAP MD while sampling 1 nanosecond of physical time. Efficient implementation of SNAP force kernel in LAMMPS using the Kokkos CUDA backend on NVIDIA GPUs combined with excellent strong scaling (better than 97% parallel efficiency) enabled a peak computing rate of 50.0 PFLOPs (24.9% of theoretical peak) for a 20 billion atom MD simulation on the full Summit machine (27,900 GPUs). The peak MD performance of 6.21 Matom-steps/node-s is 22.9 times greater than a previous record for quantum-accurate MD. Near perfect weak scaling of SNAP MD highlights its excellent potential to advance the frontier of quantum-accurate MD to trillion atom simulations on upcoming exascale platforms. KEYWORDS molecular dynamics, machine-learning interatomic potentials, car-bon, extreme conditions","2167-4337","978-1-4503-8442-1","","DOE; NNSA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910135","","Analytical models;Quantum computing;Computational modeling;High performance computing;Force;Graphics processing units;Machine learning","","","","","",52.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Enabling Large-Scale Correlated Electronic Structure Calculations: Scaling the RI-MP2 Method on Summit","G. M. J. Barca; J. L. G. Vallejo; D. L. Poole; M. Alkan; R. Stocks; A. P. Rendell; M. S. Gordon","Australian National University, Canberra, Australia; Iowa State University, Ames, United States; Iowa State University, Ames, United States; Iowa State University, Ames, United States; Australian National University, Canberra, Australia; Flinders University, Adelaide, Australia; Australian National University, Canberra, Australia","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Second-order Møller-Plesset perturbation theory using the Resolution-of-the-Identity approximation (RI-MP2) is a state-of-the-art approach to accurately estimate many-body electronic correlation effects. This is critical for predicting the physicochemical properties of complex molecular systems; however, the scale of these calculations is limited by their extremely high computational cost. In this paper, a novel many-GPU algorithm and implementation of a molecular-fragmentation-based RI-MP2 method are presented that enable correlated calculations on over 180,000 electrons and 45,000 atoms using up to the entire Summit supercomputer in 12 minutes. The implementation demonstrates remarkable speedups with respect to other current GPU and CPU codes, excellent strong scalability on Summit achieving 89.1% parallel efficiency on 4600 nodes, and shows nearly-ideal weak scaling up to 612 nodes. This work makes feasible ab initio correlated quantum chemistry calculations on significantly larger molecular scales than before on both large supercomputing systems and on commodity clusters, with a potential for major impact on progress in chemical, physical, biological and engineering sciences.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476222","Air Force Office of Scientific Research(grant numbers:FA9550-18-1-0321); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910120","quantum chemistry;SCF;MP2;GPU;Summit","Codes;Tensors;Quantum chemistry;Heuristic algorithms;Graphics processing units;Clustering algorithms;Supercomputers","","","",1.0,"",40.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Accelerating All-Electron Ab initio Simulation of Raman Spectra for Biological Systems","H. Shang; F. Li; Y. Zhang; Y. Liu; L. Zhang; M. Wu; Y. Wu; D. Wei; H. Cui; X. Liu; F. Wang; Y. Ye; Y. Gao; S. Ni; X. Chen; D. Chen","SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Supercomputer Center in Wuxi, Wuxi, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Supercomputer Center in Wuxi, Wuxi, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Tsinghua University, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Supercomputer Center in Wuxi, Wuxi, China; Tsinghua University, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, UCAS, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, UCAS, Beijing, China; University of Science and Technology of China, Hefei; National Supercomputer Center in Wuxi, Wuxi, China; Institute of Computing Technology, Chinese Academy of Sciences, UCAS, Beijing, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Raman spectroscopy provides chemical and compositional information that can serve as a structural fingerprint for various materials. Therefore, simulations of Raman spectra, including both quantum perturbation analyses and ground-state calculations are of significant interest. However, highly accurate full quantum mechanical (QM) simulations of Raman spectra have previously been confined to small systems. For large systems such as biological materials, the computational cost of full QM simulations is extremely high, and their extension to such systems remains challenging. In the work described here, by employing robust new algorithms and advances in implementation for the many-core architectures, we are able to perform fast, accurate, and massively parallel full ab initio simulations of the Raman spectra of biological systems with excellent strong and weak scaling, thereby providing a starting point for applying QM approaches to structural studies of such systems.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476160","The National Natural Science Foundation of China(grant numbers:22003073,62090024,61872043,61802368); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910108","Quantum mechanics;All-electron;Many-core processor;Scalability;Biological systems","Proteins;Technological innovation;Codes;Sensitivity;Biological system modeling;Computational modeling;Perturbation methods","","","","","",34.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"lMFF: Efficient and Scalable layered Materials Force Field on Heterogeneous Many-Core Processors","P. Gao; X. Duan; J. Guo; J. Wang; Z. Song; L. Cui; X. Meng; X. Liu; W. Zhang; M. Ma; G. Li; D. Chen; H. Fu; W. Xue; W. Liu; G. Yang","National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; Department of Engineering Mechanics, Center for Nano and Micro Mechanics, Tsinghua University, Beijing, China; Ministry of Natural Resources, First Institute of Oceanography and Key Laboratory of Marine Science and Numerical Modeling, Qingdao, China; Joint SDU-NTU Centre for Artificial Intelligence Research (C-FAIR), Jinan, China; Engineering Research Center of Digital Media Technology, Ministry of Education, Jinan, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; Department of Engineering Mechanics, Center for Nano and Micro Mechanics, Tsinghua University, Beijing, China; Dalian Institute of Chemical Physics, Chinese Academy of Sciences, Dalian, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,16,"LAMMPS is one of the most popular Molecular Dynamic (MD) packages and is widely used in the field of physics, chemistry and materials simulation. Layered Materials Force Field (LMFF) is our expansion of the LAMMPS potential function based on the Tersoff potential and inter-layer potential (ILP) in LAMMPS. LMFF is designed to study layered materials such as graphene and boron hexanitride. It is universal and does not depend on any platform. We have also carried out a series of optimizations on LMFF and the optimization work is carried out on the new generation of Sunway supercomputer, called SWLMFF. Experiments show that our implementation is efficient, scalable and portable. When generic LMFF is ported to Intel Xeon Gold 6278C, $2\times$ performance improvement is achieved. For the optimized SWLMFF, the overall performance improvement is nearly $200-330\times$ compared to the original ILP and Tersoff potentials. And SWLMFF has good parallel efficiency of 95%-100% under weak scaling with 2.7 million atoms on a single process. The maximum atomic system simulated by SWLMFF is close to $2^{31}$ atoms. And nanosecond simulations in one day can be realized.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910048","High Performance Computing;Molecular Dynamics;LAMMPS;Inter-Layer Potential (ILP);Tersoff;Hybrid Potentials Simulation;Computational Science;next-generation Sunway Supercomputer","Program processors;Atomic layer deposition;High performance computing;Force;Graphene;Supercomputers;Optimization","","","","","",41.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Hardware Acceleration of Tensor-Structured Multilevel Ewald Summation Method on MDGRAPE-4A, a Special-Purpose Computer System for Molecular Dynamics Simulations","G. Morimoto; Y. M. Koyama; H. Zhang; T. S. Komatsu; Y. Ohno; K. Nishida; I. Ohmura; H. Koyama; M. Taijit","RIKEN BDR, Osaka, Japan; RIKEN BDR, Osaka, Japan; Gusu Laboratory of Materials, Suzhou, China; RIKEN BDR, Osaka, Japan; RIKEN BDR, Osaka, Japan; RIKEN BDR, Osaka, Japan; RIKEN BDR, Osaka, Japan; RIKEN BDR, Osaka, Japan; RIKEN BDR, Osaka, Japan","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"We developed MDGRAPE-4A, a special-purpose computer system for molecular dynamics simulations, consisting of 512 nodes of custom system-on-a-chip LSIs with dedicated processor cores and interconnects designed to achieve strong scalability for biomolecular simulations. To reduce the global communications required for the evaluation of Coulomb interactions, we conducted a co-design of the MDGRAPE-4A and the novel algorithm, tensor-structured multilevel Ewald summation method (TME), which produced hardware modules on the custom LSI circuit for particle-grid operations and for grid-grid separable convolutions on a 3D torus network. We implemented the convolution for the top-level grid potentials by using 3D FFTs on an FPGA, along with an FPGA-based octree network to gather grid charges. The elapsed time for the long-range part of Coulomb is 50 $\mu\mathrm{s}$, which can mostly overlap with those for the short-range part, and the additional cost is approximately 10 $\mu\mathrm{s}/\text{step}$, which is only a 5% performance loss.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476190","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910075","","Three-dimensional displays;Costs;Computational modeling;Biological system modeling;Scalability;Octrees;Large scale integration","","","","","",40.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Accelerating Bandwidth-Bound Deep Learning Inference with Main-Memory Accelerators","B. Y. Cho; J. Jung; M. Erez","The University of Texas at Austin, Austin, Texas, USA; The University of Texas at Austin, Austin, Texas, USA; The University of Texas at Austin, Austin, Texas, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Matrix-matrix multiplication operations (GEMMs) are important in many HPC and machine-learning applications. They are often mapped to discrete accelerators (e.g., GPUs) to improve performance. However, we find that large tall/skinny and fat/short matrices benefit little from discrete acceleration and also do not perform well on a CPU. Such matrices are prevalent in important workloads, such as deep-learning inference within large-scale datacenters. We demonstrate the large potential of accelerating these GEMMs with processing in the main CPU memory, where processing in memory units (PIMs) take advantage of otherwise untapped bandwidth without requiring data copies. We develop a novel GEMM execution flow and corresponding memory-side address-generation logic that exploits GEMM locality and enables long-running PIM kernels despite the complex address-mapping functions employed by the CPU. Our evaluation of StepS tone variants at the channel, device, and within-device PIM levels demonstrate 12x better minimum latency than a CPU and 2.8x greater throughput for strict query latency constraints. End-to-end performance analysis of recent recommendation and language models shows that StepStone outperforms a fast CPU by up to 16x and also the best prior main-memory acceleration approaches by up to 2.4×.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910088","","Deep learning;Technological innovation;Simulation;High performance computing;Memory management;Random access memory;Throughput","","","","","",53.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"LCCG: A Locality-Centric Hardware Accelerator for High Throughput of Concurrent Graph Processing","J. Zhao; Y. Zhang; X. Liao; L. He; B. He; H. Liu; H. Jin","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; University of Warwick, United Kingdom; National University of Singapore, Singapore; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"In modern data centers, massive concurrent graph processing jobs are being processed on large graphs. However, existing hardware/-software solutions suffer from irregular graph traversal and intense resource contention. In this paper, we propose LCCG, a Locality-entric programmable accelerator that augments the many-core processor for achieving higher throughput of Concurrent Graph processing jobs. Specifically, we develop a novel topology-aware execution approach into the accelerator design to regularize the graph traversals for multiple jobs on-the-fly according to the graph topology, which is able to fully consolidate the graph data accesses from concurrent jobs. By reusing the same graph data among more jobs and coalescing the accesses of the vertices' states for these jobs, LCCG can improve the core utilization. We conduct extensive experiments on a simulated 64-core processor. The results show that LCCG improves the throughput of the cutting-edge software system by 11.3~23.9 times with only 0.5% additional area cost. More-over, LCCG gains the speedups of 4.7~10.3, 5.5~13.2, and 3.8~8.4 times over state-of-the-art hardware graph processing accelerators (namely, HATS, Minnow, and PHI, respectively).","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910049","","Data centers;Costs;High performance computing;Throughput;Software systems;Topology;Hardware acceleration","","","","","",75.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Simurgh: A Fully Decentralized and Secure NVMM User Space File System","N. Moti; F. Schimmelpfennig; R. Salkhordeh; D. Klopp; T. Cortes; U. Rückert; A. Brinkmann","Johannes Gutenberg University Mainz, Mainz, Germany; Johannes Gutenberg University Mainz, Mainz, Germany; Johannes Gutenberg University Mainz, Mainz, Germany; Johannes Gutenberg University Mainz, Mainz, Germany; Universitat Politécnica de Catalunya, Barcelona, Spain; Bielefeld University, Bielefeld, Germany; Johannes Gutenberg University Mainz, Mainz, Germany","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,16,"The availability of non-volatile main memory (NVMM) has started a new era for storage systems and NVMM specific file systems can support extremely high data and metadata rates, which are required by many HPC and data-intensive applications. Scaling metadata performance within NVMM file systems is nevertheless often restricted by the Linux kernel storage stack, while simply moving metadata management to the user space can compromise security or flexibility. This paper introduces Simurgh, a hardware-assisted user space file system with decentralized metadata management that allows secure metadata updates from within user space. Simurgh guarantees consistency, durability, and ordering of updates without sacrificing scalability. Security is enforced by only allowing NVMM access from protected user space functions, which can be implemented through two proposed instructions. Comparisons with other NVMM file systems show that Simurgh improves metadata performance up to 18x and application performance up to 89% compared to the second-fastest file system.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476180","Carl Zeiss Foundation(grant numbers:P2018-02-003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910101","phase change memory;shared memory;file system","Nonvolatile memory;File systems;Scalability;Linux;High performance computing;Memory management;Metadata","","","",2.0,"",83.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"lunule: An Agile and Judicious Metadata Load Balancer for CephFS","Y. Wang; C. Li; X. Shao; Y. Chen; F. Yan; Y. Xu","University of Science and Technology of China, Hefei, Anhui, China; Anhui Province Key Laboratory of High Performance Computing, USTC, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; University of Nevada, Reno, Reno, Nevada, USA; Anhui Province Key Laboratory of High Performance Computing, USTC, Hefei, Anhui, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"For a decade, the Ceph distributed file system (CephFS) has been widely used to serve the ever-growing big data in many key fields ranging from Internet services to AI computing. To scale out the massive metadata access, CephFS adopts a dynamic subtree partitioning method, splitting the hierarchical namespace and distributing subtrees across multiple metadata servers. However, this method suffers from a severe imbalance problem that may result in poor performance due to its inaccurate imbalance prediction, ignorance of workload characteristics, and unnecessary/invalid migration ac-tivities. To eliminate these inefficiencies, we propose Lunule, a novel CephFS metadata load balancer, which employs an imbalance fac-tor model for accurately determining when to trigger re-balance and tolerate benign imbalanced situations. Lunule further adopts a workload-aware migration planner to appropriately select sub-tree migration candidates. Compared to baselines, Lunule achieves better load balance, increases the metadata throughput by up to 315.8%, and shortens the tail job completion time by up to 64.6% for five real-world workloads and their mixture, respectively. Be-sides, Lunule is capable of handling the metadata cluster expansion and the client workload growth, and scales linearly on a cluster of 16 MDSs.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476196","National Science Foundation(grant numbers:CAREER-2048044,IIS-1838024,CCF-1756013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910055","","File systems;System performance;High performance computing;Web and internet services;Tail;Metadata;Throughput","","","",2.0,"",49.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"DeltaFS: A Scalable No-Ground-Truth Filesystem For Massively-Parallel Computing","Q. Zheng; C. D. Cranor; G. R. Ganger; G. A. Gibson; G. Amvrosiadis; B. W. Settlemyer; G. A. Grider","Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Los Alamos National Lab, Los Alamos, NM, USA; Los Alamos National Lab, Los Alamos, NM, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"High-Performance Computing (HPC) is known for its use of massive concurrency. But it can be challenging for a parallel filesystem's control plane to utilize cores when every client process must globally synchronize and serialize its metadata mutations with those of other clients. We present DeltaFS, a new paradigm for distributed file system metadata. DeltaFS allows jobs to self-commit their namespace changes to logs, avoiding the cost of global synchronization. Followup jobs selectively merge logs produced by previous jobs as needed, a prin-ciple we term No Ground Truth which allows for efficient data sharing. By avoiding unnecessary synchronization of metadata operations, DeltaFS improves metadata operation throughput up to 98× leveraging parallelism on the nodes where job processes run. This speedup grows as job size increases. DeltaFS enables efficient inter-job communication, reducing overall workflow runtime by significantly improving client metadata operation latency up to 49× and resource usage up to 52×.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910062","","Performance evaluation;Runtime;File systems;High performance computing;Process control;Metadata;Parallel processing","","","","","",86.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Distributed Multigrid Neural Solvers on Megavoxel Domains","A. Balu; S. Botelho; B. Khara; V. Rao; S. Sarkar; C. Hegde; A. Krishnamurthy; S. Adavani; B. Ganapathysubramanian","Iowa State University Ames, Iowa, USA; Vinay Rao Rocket ML Inc. Portland, Oregon, USA; Iowa State University Ames, Iowa, USA; Vinay Rao Rocket ML Inc. Portland, Oregon, USA; Iowa State University Ames, Iowa, USA; New York University, New York City, New York, USA; Iowa State University Ames, Iowa, USA; Vinay Rao Rocket ML Inc. Portland, Oregon, USA; Iowa State University Ames, Iowa, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"We consider the distributed training of large scale neural networks that serve as PDE (partial differential equation) solvers producing full field outputs. We specifically consider neural solvers for the generalized 3D Poisson equation over megavoxel domains. A scalable framework is presented that integrates two distinct advances. First, we accelerate training a large model via a method analogous to the multigrid technique used in numerical linear algebra. Here, the network is trained using a hierarchy of increasing resolution inputs in sequence, analogous to the ‘V’, ‘W’, ‘F’ and ‘Half-V’ cycles used in multigrid approaches. In conjunction with the multi-grid approach, we implement a distributed deep learning framework which significantly reduces the time to solve. We show scalability of this approach on both GPU (Azure VMs on Cloud) and CPU clusters (PSC Bridges2). This approach is deployed to train a generalized 3D Poisson solver that scales well to predict output full field solutions up to the resolution of 512 × 512 × 512 for a high dimensional family of inputs. This strategy opens up the possibility of fast and scalable training of neural PDE solvers on heterogeneous clusters.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476218","National Science Foundation(grant numbers:2019574,1954556,1644441); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910116","Physics aware neural networks;Distributed training;Multigrid;Neural PDE solvers","Training;Three-dimensional displays;Poisson equations;Scalability;High performance computing;Neural networks;Linear algebra","","","","","",52.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"A 400 trillion-grid Vlasov Simulation on Fugaku Supercomputer: large-scale Distribution of Cosmic Relic Neutrinos in a Six-dimensional Phase Space","K. Yoshikawa; S. Tanaka; N. Yoshida","Center for Compuational Sciences, University of Tsukuba, Tsukua, Japan; Yukawa Institute for Theoretical Physics, Kyoto University, Kyoto, Japan; Kavli Institute for the Physics and Mathematics of the Universe, The University of Tokyo, Kashiwa, Japan","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,11,"We report a Vlasov simulation of cosmic relic neutrinos combined with N -body simulation of cold dark matter in the context of large-scale structure formation in the Universe performed on Fugaku supercomputer. Gravitational dynamics of the neutrinos is followed, for the first time, by directly integrating the Vlasov equation in a six-dimensional phase space. Our largest simulation combines the Vlasov simulation on 400 trillion grids and 330 billion-body calculations in a self-consistent manner, and reproduces accurately the nonlinear dynamics of neutrinos in the Universe. The novel high-order Vlasov solver is optimized by combining an array of state-of-the-art numerical schemes and fully utilizing the SIMD instructions on the A64FX processors. Time-To-Solution of our simulation is an order of magnitude shorter than the largest N-body simulations. The performance scales excellently with up to 147,456 nodes (7 million CPU cores) on Fugaku; the weak and strong scaling efficiencies are 82% - 96% and 82% - 93%, respectively.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3487401","MEXT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910059","large-scale structure in the universe;comic relic neutrino;Vlasov simulation;Fugaku","Neutrino sources;Program processors;High performance computing;Dark matter;Computational modeling;Mathematical models;Supercomputers","","","","","",27.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"EIGA: Elastic and Scalable Dynamic Graph Analysis","K. Gabert; K. Sancak; M. Y. Ozkaya; A. Pinar; U. V. Catalyurek","Georgia Institute of Technology, Atlanta, Georgia, USA; Georgia Institute of Technology, Atlanta, Georgia, USA; Georgia Institute of Technology, Atlanta, Georgia, USA; Sandia National Laboratories, Livermore, California, USA; Georgia Institute of Technology, Atlanta, Georgia, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Modern graphs are not only large, but rapidly changing. The rate of change can vary significantly along with the computational cost. Existing distributed graph analysis systems have largely been designed to operate on static graphs. Infrastructure changes in these systems need to occur when the system is idle, which can result in significant wasted resources or the inability to cope with changes. We present EIGA, an elastic and scalable dynamic graph analysis system. Using a shared-nothing architecture and consistent hashing, E I GA can scale elastically as the graph grows or more computation is required. By applying sketches, we perform an edge partitioning of the graph where high degree vertices can be split among multiple nodes. EIGA supports both synchronous and asyn-chronous vertex-centric applications that operate in batches on a continuously changing graph. We experimentally demonstrate that EIGA outperforms state-of-the-art static systems while supporting client queries, elastic infrastructure changes, and dynamic algorithms.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3480857","NSF(grant numbers:CCF-1919021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910105","Distributed graphs systems;dynamic graphs;elastic computing","Runtime;Heuristic algorithms;High performance computing;Computer architecture;Dynamic scheduling;Partitioning algorithms;Computational efficiency","","","","","",90.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Krill: A Compiler and Runtime System for Concurrent Graph Processing","H. Chen; M. Shen; N. Xiao; Y. Lu","Sun Yat-sen University; Sun Yat-sen University; Sun Yat-sen University; Sun Yat-sen University","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"As a large number of emerging graph applications spread across different domains, the need for processing massive concurrent graph jobs (CGJs) is increasing. However, existing graph processing systems designed for a single job cannot efficiently tackle multiple CGJs, where they suffer from interfering memory access patterns and inefficient property management. In this paper, we introduce Krill, a compiler and runtime system for processing concurrent graph jobs. We propose an SAP model, which decouples graph structure, algorithm, and property. In the compiler, we propose leveraging the property buffer to easily write and manage property data. In the runtime system, we propose a novel technique named graph kernel fusion to reduce memory accesses, which fuses all the jobs and processes them as a whole. Experimental results show our system significantly reduces the number of memory accesses for CGJs by more than $6\mathrm{x}$ compared with the baseline, and achieves up to $6.76\mathrm{x}$ speedup with $3. 84\mathrm{x}$ shorter response latency than GraphM, the state-of-the-art concurrent graph processing system.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476159","Natural Science Foundation of China(grant numbers:U1811461,62072479); Guangdong Natural Science Foundation(grant numbers:2018B030312002,2021A1515011836); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910087","Concurrent Graph Processing;Domain-Specific Compilers;Runtime Systems","Runtime;Fuses;Scalability;High performance computing;Memory management;Programming;Hardware","","","","","",66.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Pilgrim: Scalable and (near) lossless MPI Tracing","C. Wang; P. Balaji; M. Snir","University of Illinois at Urbana-Champaign Champaign, Illinois, USA; Facebook Inc. Menlo Park, CA, USA; University of Illinois at Urbana-Champaign Champaign, Illinois, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"Traces of MPI communications are used by many performance analysis and visualization tools. Storing exhaustive traces of large scale MPI applications is infeasible, due to their large volume. Aggregated or lossy MPI traces are smaller, but provide much less information. In this paper, we present Pilgrim, a near lossless MPI tracing tool that incurs moderate overheads and generates small trace files at large scales, by using sophisticated compression techniques. Furthermore, for codes with regular communication patterns, Pilgrim can store their traces in constant space regardless of the problem size, the number of processors, and the number of iterations. In comparison with existing tools, Pilgrim preserves more information with less space in all the programs we tested.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910117","Communication tracing;Lossless MPI tracing;Trace compression","Visualization;Codes;Instruction sets;High performance computing;Generators;Performance analysis;Decoding","","","","","",37.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Hybrid, Scalable, Trace-Driven Performance Modeling of GPGPUs","Y. Arafa; A. -H. Badawy; A. ElWazir; A. Barai; A. Eker; G. Chennupati; N. Santhi; S. Eidenbenz","Klipsch School of ECE, New Mexico State University; Los Alamos National Laboratory; Klipsch School of ECE, New Mexico State University; Klipsch School of ECE, New Mexico State University; Binghamton University; Amazon Alexa; Los Alamos National Laboratory; Los Alamos National Laboratory","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"In this paper, we present PPT-GPU, a scalable performance prediction toolkit for GPUs. PPT-GPU achieves scalability through a hybrid high-level modeling approach where some computations are extrapolated and multiple parts of the model are parallelized. The tool primary prediction models use pre-collected memory and instructions traces of the workloads to accurately capture the dynamic behavior of the kernels. PPT-GPU reports an extensive array of GPU performance metrics accurately while being easily extensible. We use a broad set of benchmarks to verify predictions accuracy. We compare the results against hardware metrics collected using vendor profiling tools and cycle-accurate simulators. The results show that the performance predictions are highly correlated to the actual hardware (MAPE: < 16% and Correlation: > 0.98). Moreover, PPT-GPU is orders of magnitude faster than cycle-accurate simulators. This comprehensiveness of the collected metrics can guide architects and developers to perform design space explorations. Moreover, the scalability of the tool enables conducting efficient and fast sensitivity analyses for performance-critical applications.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910064","NVIDIA GPUs;Modeling and Simulation;Design Space Exploration;Performance Prediction;PTX;SASS","Measurement;Sensitivity analysis;Computational modeling;Scalability;High performance computing;Graphics processing units;Predictive models","","","",1.0,"",63.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"G-SEPM: Building an Accurate and Efficient Soft Error Prediction Model for GPGPUs","H. Yue; X. Wei; G. Li; J. Zhao; N. Jiang; J. Tan","College of Computer Science and Technology, Jilin University, Changchun, Jilin, China; College of Computer Science and Technology, Jilin University, Changchun, Jilin, China; College of Computer Science and Technology, Jilin University, Changchun, Jilin, China; College of Computer Science and Technology, Jilin University, Changchun, Jilin, China; College of Computer Science and Technology, Jilin University, Changchun, Jilin, China; SKL Computer Architecture, ICT, CAS University of Chinese Academy of Sciences, Beijing, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"As GPUs become ubiquitous in large-scale general purpose HPC systems (GPGPUs), ensuring the reliable execution of such systems in the presence of soft errors is increasingly essential. To provide insights into how resilient GPU programs are toward soft errors, researchers typically rely on random Fault Injection (FI) to evaluate the tolerance of programs. However, it is expensive to obtain a statistically significant resilience profile and not suitable to identify all the error-critical fault sites of GPU programs. To address the above challenges, in this work, we build a GPGPU-based Soft Error Prediction Model (G-SEPM) that can replace FI to estimate the resilience characteristics of individual fault sites accurately and efficiently. We observe that the instruction-type, bit-position, bit-flip direction, and error propagation information have capabilities to characterize fault site resiliency. Leveraging these heuristic features, G-SEPM drives the machine learning model to reveal the hidden interactions among fault site resiliency and our observed features. Experimental results demonstrate that G-SEPM achieves high accuracy for fault site error estimation and critical fault site identification while introducing negligible overhead. In addition, G-SEPM can provide essential insight for programmers/architects to design more cost-effective soft error mitigation solutions.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476170","National Natural Science Foundation of China (NSFC)(grant numbers:U19A2061,61772228,61802143); National key research and development program of China(grant numbers:2017YFC1502306); Jilin Scientific and Technological Development Program(grant numbers:20190701016GH); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910126","GPGPU;Soft Error;Fault Injection;Error Resilience;Machine Learning","Fault diagnosis;Support vector machines;Error analysis;High performance computing;Graphics processing units;Estimation;Predictive models","","","",1.0,"",45.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Single-Node Partitioned-Memory for Huge Graph Analytics: Cost and Performance Trade-Offs","S. Ghosh; N. R. Tallent; M. Minutoli; M. Halappanavar; R. Peri; A. Kalyanaraman","Pacific Northwest National Lab Richland, Washington, USA; Pacific Northwest National Lab Richland, Washington, USA; Pacific Northwest National Lab Richland, Washington, USA; Pacific Northwest National Lab Richland, Washington, USA; Facebook, Austin, TX, USA; Washington State University Pullman, Washington, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,16,"Because of cost, non-volatile memory NVDIMMs such as Intel Optane are attractive in single-node big-memory systems. We evaluate performance and cost trade-offs when using Optane as volatile memory for huge-graph analytics. We study two scalable graph applications with different work locality, access patterns, and parallelism. We evaluate single and partitioned address spaces-Memory and AppDirect modes-and compare with distributed executions on GPU-accelerated and CPU-based supercomputers. We show that AppDIRECT can perform and scale better than Memory for the largest working sets (12%), even when dominated by irregular access patterns, if most accesses are NUMA-local and Optane accesses are frequently reads. Surprisingly, between Memory and AppDirect, processor-cache performance can change due to line invalidations; updates to the caching policy (via non-temporal hints) can make a 25% improvement. We observe that single-node graph analytics frequently has >4-10× cost/performance advantages over distributed-memory executions on supercomputers.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910140","non-volatile memory;graph analytics;performance evaluation","Costs;Nonvolatile memory;High performance computing;Parallel processing;Supercomputers","","","","","",56.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Accelerating Applications using Edge Tensor Processing Units","K. -C. Hsu; H. -W. Tseng","University of California, Riverside, Riverside, California, USA; University of California, Riverside, Riverside, California, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,16,"Neural network (NN) accelerators have been integrated into a wide-spectrum of computer systems to accommodate the rapidly growing demands for artificial intelligence (AI) and machine learning (ML) applications. NN accelerators share the idea of providing native hardware support for operations on multidimensional tensor data. Therefore, NN accelerators are theoretically tensor processors that can improve system performance for any problem that uses tensors as inputs/outputs. Unfortunately, commercially available NN accelerators only expose computation capabilities through AI/ML-specific interfaces. Furthermore, NN accelerators reveal very few hardware design details, so applications cannot easily leverage the tensor operations NN accelerators provide. This paper introduces General-Purpose Computing on Tensor Processing Units (GPTPU), an open-source, open-architecture framework that allows the developer and research communities to discover opportunities that NN accelerators enable for applications. GPTPU includes a powerful programming interface with efficient runtime system-level support-similar to that of CUDA/OpenCL in GPGPU computing-to bridge the gap between application demands and mismatched hardware/software interfaces. We built GPTPU machine uses Edge Tensor Processing Units (Edge TPUs), which are widely available and representative of many commercial NN accelerators. We identified several novel use cases and revisited the algorithms. By leveraging the underlying Edge TPUs to perform tensor-algorithm-based compute kernels, our results reveal that GPTPU can achieve a 2.46x speedup over high-end CPUs and reduce energy consumption by 40%.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476177","National Science Foundation (NSF)(grant numbers:2007124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910092","","Tensors;Runtime;Protocols;System performance;Scalability;Reverse engineering;Prototypes","","","","","",110.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Enabling and Scaling the HPCG Benchmark on the Newest Generation Sunway Supercomputer with 42 Million Heterogeneous Cores","Q. Zhu; H. Luo; C. Yang; M. Ding; W. Yin; X. Yuan","Center for Data Science, Peking University, Beijing, China; School of Mathematical Sciences, Peking University, Beijing, China; National Engineering Laboratory for Big Data Analysis and Applications, Peking University, Beijing, China; School of Electronic Engineering and Computer Science, Peking University, Beijing, China; National Research Center of Parallel Computer Engineering & Technology, Beijing, China; National Research Center of Parallel Computer Engineering & Technology, Beijing, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"We study and evaluate performance optimization techniques for the HPCG benchmark on the newest generation Sunway super-computer. Specifically, a two-level blocking scheme is proposed to expose adequate parallelism in the symmetric Gauss-Seidel kernel while keeping a fast convergence rate, a fine-grained kernel fusion technique is developed to alleviate the bandwidth load on local storage with small capacity, and a low overhead thread collaboration method is presented to efficiently move data between threads and hide its cost with data transfer operations. Test results show that the optimized HPCG code is able to exploit 73.0% of the theoretical memory bandwidth, and scale to over 42 million heterogeneous cores with 95.5% weak-scaling efficiency and 5.91 Pflop/s performance. We also study how the performance can be improved if the specific rules of HPCG are not fully obeyed, and design dependency preserving parallelization and vectorization methods, further boosting performance to 27.6 Pflop/ s.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476158","Natural Science Foundation of Beijing Municipality(grant numbers:JQ18001); National Key R&D Program of China(grant numbers:2016YFB0200603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910099","HPCG;sparse matrix computation;the newest generation Sunway supercomputer","Instruction sets;High performance computing;Memory management;Bandwidth;Benchmark testing;Parallel processing;Data transfer","","","",3.0,"",48.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM","D. Narayanan; M. Shoeybi; J. Casper; P. LeGresley; M. Patwary; V. Korthikanti; D. Vainbrand; P. Kashinkunti; J. Bernauer; B. Catanzaro; A. Phanishayee; M. Zaharia","Stan ford University; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; NVIDIA; Microsoft Research; Stan ford University","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Large language models have led to state-of-the-art accuracies across several tasks. However, training these models efficiently is challenging because: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to scaling issues at thousands of GPUs. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52% of theoretical peak).","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910057","","Training;Schedules;Tensors;Computational modeling;Memory management;Graphics processing units;Throughput","","","","","",47.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning","S. Rajbhandari; O. Ruwase; J. Rasley; S. Smith; Y. He","NA; NA; NA; NA; NA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model. In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs (40% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed 11DeepSpeed (https://www.deepspeed.ai/) is a deep learning optimization library designed to make distributed training easy, efficient, and effective. DeepSpeed has been extensively adopted by the DL community..","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910082","","Training;Deep learning;Technological innovation;Nonvolatile memory;Scalability;Graphics processing units;Parallel processing","","","",10.0,"",45.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Extreme-Scale Ab initio Quantum Raman Spectra Simulations on the Leadership HPC System in China","H. Shang; F. Li; Y. Zhang; L. Zhang; Y. Fu; Y. Gao; Y. Wu; X. Duan; R. Lin; X. Liu; Y. Liu; D. Chen","SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Supercomputing Center in Wuxi, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Supercomputing Center in Wuxi, China; Shangdong University of Science and Technology, Qingdao, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; National Supercomputing Center in Wuxi, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Tsinghua University, Beijing, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"Raman spectroscopy provides chemical and compositional information that can serve as a structural fingerprint for various materials. Therefore, simulations of Raman spectra, including both quantum perturbation analyses and ground-state calculations, are of significant interest. However, highly accurate full quantum mechanical (QM) simulations of Raman spectra have previously been confined to small systems. For large systems such as biological materials, full QM simulations have an extremely high computational cost and remain challenging. In this work, robust new algorithms and advanced implementations on many-core architectures are employed to enable fast, accurate, and massively parallel full ab initio simulations of the Raman spectra of realistic biological systems containing up to 3006 atoms, with excellent strong and weak scaling. Up to a performance of 468.5 PFLOP / s in double-precision and 813.7 PLOPS/s in mixed-half precision is achieved on the new-generation Sunway high-performance computing system, suggesting the potential for new applications of the QM approach to biological systems.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3487402","National Natural Science Foundation of China(grant numbers:22003073); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910094","Quantum mechanics;Raman spectra;Massively parallel and high-performance simulations;All-electron;Many-core processor;Biological systems","Analytical models;Leadership;Quantum computing;Computational modeling;Biological system modeling;High performance computing;Perturbation methods","","","",2.0,"",47.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"FedAT: A High-Performance and Communication-Efficient Federated Learning System with Asynchronous Tiers","Z. Chai; Y. Chen; A. Anwar; L. Zhao; Y. Cheng; H. Rangwala","George Mason University, Fairfax, VA, USA; George Mason University, Fairfax, VA, USA; IBM Research - Almaden, San Jose, CA, USA; Emory University, Atlanta, GA, USA; George Mason University, Fairfax, VA, USA; George Mason University, Fairfax, VA, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,17,"Federated learning (FL) involves training a model over massive distributed devices, while keeping the training data localized and private. This form of collaborative learning exposes new tradeoffs among model convergence speed, model accuracy, balance across clients, and communication cost, with new challenges including: (1) straggler problem-where clients lag due to data or (computing and network) resource heterogeneity, and (2) communication bottleneck-where a large number of clients communicate their local updates to a central server and bottleneck the server. Many existing FL methods focus on optimizing along only one single dimension of the tradeoff space. Existing solutions use asynchronous model updating or tiering-based, synchronous mechanisms to tackle the straggler problem. However, asynchronous methods can easily create a communication bottleneck, while tiering may introduce biases that favor faster tiers with shorter response latencies. To address these issues, we present FedAT, a novel Federated learning system with Asynchronous Tiers under Non-i.i.d. training data. FedAT synergistically combines synchronous, intra-tier training and asynchronous, cross-tier training. By bridging the synchronous and asynchronous training through tiering, FedAT minimizes the straggler effect with improved convergence speed and test accuracy. FedAT uses a straggler-aware, weighted aggregation heuristic to steer and balance the training across clients for further accuracy improvement. FedAT compresses uplink and downlink communications using an efficient, polyline-encoding-based compression algorithm, which minimizes the communication cost. Results show that FedAT improves the prediction performance by up to 21.09% and reduces the communication cost by up to 8.5×, compared to state-of-the-art FL methods.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476211","National Science Foundation (NSF)(grant numbers:CCF-1919075,CCF-1919113,CMMI-2134689,IIS-1755850,CNS-1841520,IIS-2007716,OAC-2007976,IIS-1942594,IIS-1907805); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910131","federated learning;asynchronous distributed learning;communication efficiency;tiering;weighted aggregation","Training;Costs;Federated learning;Computational modeling;Training data;Predictive models;Data models","","","",7.0,"",53.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Reverse-Mode Automatic Differentiation and Optimization of GPU Kernels via Enzyme","W. S. Moses; V. Churavy; L. Paehler; J. Hückelheim; S. H. K. Narayanan; M. Schanen; J. Doerfert","MIT CSAIL; MIT CSAIL; Technical University of Munich; Argonne National Laboratory; Argonne National Laboratory; Argonne National Laboratory; Argonne National Laboratory","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,18,"Computing derivatives is key to many algorithms in scientific computing and machine learning such as optimization, uncertainty quantification, and stability analysis. Enzyme is a LL VM compiler plugin that performs reverse-mode automatic differentiation (AD) and thus generates high performance gradients of programs in languages including $\mathrm{C}/\mathrm{C}++$, Fortran, Julia, and Rust. Prior to this work, Enzyme and other AD tools were not capable of generating gradi-ents of GPU kernels. Our paper presents a combination of novel techniques that make Enzyme the first fully automatic reverse-mode AD tool to generate gradients of GPU kernels. Since unlike other tools Enzyme performs automatic differentiation within a general-purpose compiler, we are able to introduce several novel GPU and AD-specific optimizations. To show the generality and efficiency of our approach, we compute gradients office GPU-based HPC applications, executed on NVIDIA and AMD GPUs. All bench-marks run within an order of magnitude of the original program's execution time. Without GPU and AD-specific optimizations, gra-dients of GPU kernels either fail to run from a lack of resources or have infeasible overhead. Finally, we demonstrate that increasing the problem size by either increasing the number of threads or increasing the work per thread, does not substantially impact the overhead from differentiation.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476165","Defense Advanced Research Projects Agency (DARPA)(grant numbers:HR0011-20-9-0016); NSF(grant numbers:OAC-1835443); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910056","Automatic Differentiation;AD;CUDA;ROCm;GPU;LL VM;HPC","Schedules;Uncertainty;Scientific computing;Scalability;Graphics processing units;Parallel processing;Biochemistry","","","",2.0,"",61.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Overcoming Barriers to Scalability in Variational Quantum Monte Carlo","T. Zhao; S. De; B. Chen; J. Stokes; S. Veerapaneni","Department of Mathematics, University of Michigan, Ann Arbor, MI, USA; Department of Mathematics, University of Michigan, Ann Arbor, MI, USA; Department of Mathematics, University of Michigan, Ann Arbor, MI, USA; Flatiron Institute Simons Foundation, New York, NY, USA; Department of Mathematics, University of Michigan, Ann Arbor, MI, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,12,"The variational quantum Monte Carlo (VQMC) method received significant attention in the recent past because of its ability to overcome the curse of dimensionality inherent in many-body quantum systems. Close parallels exist between VQMC and the emerging hybrid quantum-classical computational paradigm of variational quantum algorithms. VQMC overcomes the curse of dimensionality by performing alternating steps of Monte Carlo sampling from a parametrized quantum state followed by gradient-based optimization. While VQMC has been applied to solve high-dimensional problems, it is known to be difficult to parallelize, primarily owing to the Markov Chain Monte Carlo (MCMC) sampling step. In this work, we explore the scalability of VQMC when autoregressive models, with exact sampling, are used in place of MCMC. This approach can exploit distributed-memory, shared-memory and/or GPU parallelism in the sampling task without any bottlenecks. In particular, we demonstrate GPU-scalability of VQMC for solving up to ten-thousand dimensional combinatorial optimization problems.","2167-4337","978-1-4503-8442-1","","NSF(grant numbers:DMS-2038030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910128","variational inference;density estimation;normalizing flows;generative models;neural networks;GPU parallelization","Training;Quantum system;Monte Carlo methods;Scalability;Computational modeling;Graphics processing units;Markov processes","","","","","",31.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"3D Acoustic-Elastic Coupling with Gravity: The Dynamics of the 2018 Palu, Sulawesi Earthquake and Tsunami","L. Krenz; C. Uphoff; T. Ulrich; A. -A. Gabriel; L. S. Abrahams; E. M. Dunham; M. Bader","Technical University of Munich, Garching, Germany; Ludwig-Maximilians-Universität München, Munich, Germany; Ludwig-Maximilians-Universität München, Munich, Germany; Ludwig-Maximilians-Universität München, Munich, Germany; Stanford University, Stanford, CA, USA; Stanford University, Stanford, CA, USA; Technical University of Munich, Garching, Germany","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,17,"We present a highly scalable 3D fully-coupled Earth & ocean model of earthquake rupture and tsunami generation and perform the first fully coupled simulation of an actual earthquake-tsunami event and a 3D benchmark problem of tsunami generation by a megathrust dynamic earthquake rupture. Multi-petascale simulations, with excellent performance demonstrated on three different platforms, allow high-resolution forward modeling. Our largest mesh has ≈261 billion degrees of freedom, resolving at least 15 Hz of the acoustic wave field. We self-consistently model seismic, acoustic and surface gravity wave propagation in elastic (Earth) and acoustic (ocean) materials sourced by physics-based non-linear earthquake dynamic rupture, thereby gaining insight into the tsunami generation process without relying on approximations that have previously been applied to permit solution of this challenging problem. Complicated geometries, including high-resolution bathymetry, coastlines and segmented earthquake faults are discretized by adaptive unstructured tetrahedral meshes. This inevitably leads to large differences in element sizes and wave speeds which can be mitigated by ADER local time-stepping and a Discontinuous Galerkin discretization yielding high-order accuracy in time and space.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476173","German Research Foundation (DFG)(grant numbers:GA 2465/2-1,GA 2465/3-1); National Science Foundation(grant numbers:DGE-1656518); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910041","ADER-DG;Elastic-Acoustic-Coupling;Earthquake Simulation;Computational Seismology;SeisSol;Tsunami Generation","Earth;Solid modeling;Sea surface;Three-dimensional displays;Propagation;Earthquakes;Surface acoustic waves","","","",1.0,"",67.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"In-Depth Analyses of Unified Virtual Memory System for GPU Accelerated Computing","T. Allen; R. Ge","School of Computing Clemson University; School of Computing Clemson University","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"The abstraction of a shared memory space over separate CPU and GPU memory domains has eased the burden of portability for many HPC codebases. However, users pay for the ease of use provided by systems-managed memory space with a moderate-to-high performance overhead. NVIDIA Unified Virtual Memory (UVM) is presently the primary real-world implementation of such abstraction and offers a functionally equivalent testbed for a novel indepth performance study for both UVM and future Linux Heterogeneous Memory Management (HMM) compatible systems. The continued advocation for UVM and HMM motivates the improvement of the underlying system. We focus on a UVM-based system and investigate the root causes of the UVM overhead, which is a non-trivial task due to the complex interactions of multiple hardware and software constituents and the requirement of targeted analysis methodology. In this paper, we take a deep dive into the UVM system architecture and the internal behaviors of page fault generation and servicing. We reveal specific GPU hardware limitations using targeted benchmarks to uncover driver functionality as a real-time system when processing the resultant workload. We further provide a quantitative evaluation of fault handling for various applications under different scenarios, including prefetching and oversubscription. We find that the driver workload is dependent on the interactions among application access patterns, GPU hardware constraints, and Host OS components. We determine that the cost of host OS components is significant and present across implementations, warranting close attention. This study serves as a proxy for future shared memory systems such as those that interface with HMM.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3480855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910109","UVM;NVIDIA;GPU;virtual memory;GPGPU;HMM","Prefetching;Linux;High performance computing;Memory management;Graphics processing units;Hidden Markov models;Systems architecture","","","",1.0,"",39.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Paths to OpenMP in the Kernel","J. Ma; W. Wang; A. Nelson; M. Cuevas; B. Homerding; C. Liu; Z. Huang; S. Campanoni; K. Hale; P. Dinda","Northwestern University, United States; Northwestern University, United States; Northwestern University, United States; Northwestern University, United States; Argonne National Laboratory, Northwestern University, United States; Illinois Institute of Technology, United States; Northwestern University, United States; Northwestern University, United States; Illinois Institute of Technology, United States; Northwestern University, United States","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"OpenMP implementations make increasing demands on the kernel. We take the next step and consider bringing OpenMP into the kernel. Our vision is that the entire OpenMP application, run-time system, and a kernel framework is interwoven to become the kernel, allowing the OpenMP implementation to take full advantage of the hardware in a custom manner. We compare and contrast three approaches to achieving this goal. The first, runtime in kernel (RTK), ports the OpenMP runtime to the kernel, allowing any kernel code to use OpenMP pragmas. The second, process in kernel (PIK) adds a specialized process abstraction for running user-level OpenMP code within the kernel. The third, custom compilation for kernel (CCK), compiles OpenMP into a form that leverages the kernel framework without any intermediaries. We describe the design and implementation of these approaches, and evaluate them using NAS and other benchmarks.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910061","parallelism;OpenMP;operating systems","Runtime;Codes;Linux;High performance computing;Benchmark testing;Performance gain;Hardware","","","","","",82.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Index Launches: Scalable, Flexible Representation of Parallel Task Groups","R. Soi; M. Bauer; S. Treichler; M. Papadakis; W. Lee; P. McCormick; A. Aiken; E. Slaughter","BITS Pilani - Hyderabad Campus, India; NVIDIA, USA; NVIDIA, USA; NVIDIA, USA; NVIDIA, USA; Los Alamos National Laboratory, USA; Stanford University, USA; SLAC National Accelerator Laboratory, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"It's common to see specialized language constructs in modern task-based programming systems for reasoning about groups of indepen-dent tasks intended for parallel execution. However, most systems use an ad-hoc representation that limits expressiveness and of-ten overfits for a given application domain. We introduce index launches, a scalable and flexible representation of a group of tasks. Index launches use a flexible mechanism to indicate the data required for a given task, allowing them to be used for a much broader set of use cases while maintaining an efficient representation. We present a hybrid design for index launches, involving static and dynamic program analyses, along with a characterization of how they're used in Legion and Regent, and show how they generalize constructs found in other task-based systems. Finally, we present re-sults of scaling experiments which demonstrate that index launches are crucial for the efficient distributed execution of several scientific codes in Regent.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910047","index launches;task-based parallelism;runtime systems;regions","Runtime;Program processors;Codes;Parallel programming;High performance computing;Supercomputers;Cognition","","","",1.0,"",32.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"TriPoll: Computing Surveys of Triangles in Massive-Scale Temporal Graphs with Metadata","T. Steil; T. Reza; K. Iwabuchi; B. W. Priest; G. Sanders; R. Pearce","Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory (LLNL), Livermore, CA, USA; Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory (LLNL), Livermore, CA, USA; Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory (LLNL), Livermore, CA, USA; Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory (LLNL), Livermore, CA, USA; Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory (LLNL), Livermore, CA, USA; Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory (LLNL), Livermore, CA, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Understanding the higher-order interactions within network data is a key objective of network science. Surveys of metadata triangles (or patterned 3-cycles in metadata-enriched graphs) are often of interest in this pursuit. In this work, we develop TriPoll, a prototype distributed HPC system capable of surveying triangles in massive graphs containing metadata on their edges and vertices. We contrast our approach with much of the prior effort on triangle analysis, which often focuses on simple triangle counting, usually in simple graphs with no metadata. We assess the scalability of TriPoll when surveying triangles involving metadata on real and synthetic graphs with up to hundreds of billions of edges. We utilize communication-reducing optimizations to demonstrate a triangle counting task on a 224 billion edge web graph in approximately half of the time of competing approaches, while additionally supporting metadata-aware capabilities.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910134","distributed graph processing;asynchronous communication","Social networking (online);Scalability;Image edge detection;Prototypes;Machine learning;Metadata;Software","","","",2.0,"",64.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Discovering and Balancing Fundamental Cycles in Large Signed Graphs","G. Alabandi; J. Tesic; L. Rusnak; M. Burtscher","Texas State University, San Marcos, TX, U.S.A; Texas State University, San Marcos, TX, U.S.A; Texas State University, San Marcos, TX, U.S.A; Texas State University, San Marcos, TX, U.S.A","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Computing consensus states via global sign balancing is a key step in social network analysis. This paper presents graphB+, a fast algorithm for balancing signed graphs based on a new vertex and edge labeling technique, and a parallel implementation thereof for rapidly detecting and balancing all fundamental cycles. The main benefits of graphB+ are that the labels can be computed with linear time complexity, only require a linear amount of memory, and that the running time for balancing a cycle is linear in the length of the cycle times the vertex degrees but independent of the size of the graph. We parallelized graphB+ using OpenMP and CUDA. It takes 0.85 seconds on a Titan V GPU to balance the signs on the edges of an Amazon graph with 10 million vertices and 22 million edges, amounting to over 14 million fundamental cycles identified, traversed, and balanced per second.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476153","National Science Foundation(grant numbers:1955367); Department of Energy; National Nuclear Security Administration(grant numbers:DE-NA0003969); NVIDIA Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910113","Fundamental cycles;Signed-graph balancing;Parallelization;GPU computing","Social networking (online);Image edge detection;High performance computing;Memory management;Graphics processing units;Labeling;Time complexity","","","","","",45.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"cuTS: Scaling Subgraph Isomorphism on Distributed Multi-GPU Systems Using Trie Based Data Structure","L. Xiang; A. Khan; E. Serra; M. Halappanavar; A. Sukumaran-Rajam","Washington State University, Pullman, Washington, USA; Pacific Northwest National Lab, Richland, Washington, USA; Boise State University, PNNL Boise, Idaho, USA; Pacific Northwest National Lab, Richland, Washington, USA; Washington State University, Pullman, Washington, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"Subgraph isomorphism is a pattern-matching algorithm widely used in many domains such as chem-informatics, bioinformatics, databases, and social network analysis. It is computationally expensive and is a proven NP-hard problem. The massive parallelism in GPUs is well suited for solving subgraph isomorphism. However, current GPU implementations are far from the achievable performance. Moreover, the enormous memory requirement of current approaches limits the problem size that can be handled. This work analyzes the fundamental challenges associated with processing subgraph isomorphism on GPUs and develops an efficient GPU implementation. We also develop a GPU-friendly trie-based data structure to drastically reduce the intermediate storage space requirement, enabling large benchmarks to be processed. We also develop the first distributed sub-graph isomorphism algorithm for GPUs. Our experimental evaluation demonstrates the efficacy of our approach by comparing the execution time and number of cases that can be handled against the state-of-the-art GPU implementations.","2167-4337","978-1-4503-8442-1","","National Science Foundation(grant numbers:1816793,1919211); Washington State University(grant numbers:17-SC-20-SC); U.S. Department of Energy (DOE); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910074","","Social networking (online);NP-hard problem;Databases;Scalability;High performance computing;Memory management;Graphics processing units","","","","","",19.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Accelerating Large Scale de novo Metagenome Assembly Using GPUs","M. G. Awan; S. Hofmeyr; R. Egan; N. Ding; A. Buluc; J. Deslippe; L. Oliker; K. Yelick","Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; Lawrence Berkeley National Laboratory, Berkeley, California, USA; USA Lawrence Berkeley National Laboratory, University of California Berkeley, California, Berkeley, California, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,11,"Metagenomic workflows involve studying uncultured microorganisms directly from the environment. These environmental samples when processed by modern sequencing machines yield large and complex datasets that exceed the capabilities of metagenomic software. The increasing sizes and complexities of datasets make a strong case for exascale-capable metagenome assemblers. However, the underlying algorithmic motifs are not well suited for GPUs. This poses a challenge since the majority of next-generation supercomputers will rely primarily on GPUs for computation. In this paper we present the first of its kind GPU-accelerated implementation of the local assembly approach that is an integral part of a widely used large-scale metagenome assembler, MetaHipMer. Local assembly uses algorithms that induce random memory accesses and non-deterministic workloads, which make GPU offloading a challenging task. Our GPU implementation outperforms the CPU version by about $7\mathrm{x}$ and boosts the performance of MetaHipMer by 42% when running on 64 Summit nodes.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910050","metagenomic;genomic;GPU;CUDA;sequence assembly;sparse data structures;graph algorithms","Sequential analysis;Microorganisms;High performance computing;Software algorithms;Graphics processing units;Supercomputers;Software","","","","","",31.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"On the Parallel I/O Optimality of Linear Algebra Kernels: Near-Optimal Matrix Factorizations","G. Kwasniewski; M. Kabic; T. Ben-Nun; A. N. Ziogas; J. E. Saethre; A. Gaillard; T. Schneider; M. Besta; A. Kozhevnikov; J. VandeVondele; T. Hoefler","Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland; Department of Computer Science, ETH Zurich Swiss National Computing Center, Switzerland","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Matrix factorizations are among the most important building blocks of scientific computing. However, state-of-the-art libraries are not communication-optimal, underutilizing current parallel architectures. We present novel algorithms for Cholesky and LU factorizations that utilize an asymptotically communication-optimal 2.5D decomposition. We first establish a theoretical framework for deriving parallel I/O lower bounds for linear algebra kernels, and then utilize its insights to derive Cholesky and LU schedules, both communicating $N^{3}/(P\sqrt{M})$ elements per processor, where M is the local memory size. The empirical results match our theoretical analysis: our implementations communicate significantly less than Intel MKL, SLATE, and the asymptotically communication-optimal CANDMC and CAPITAL libraries. Our code outperforms these state-of-the-art libraries in almost all tested scenarios, with matrix sizes ranging from 2,048 to 524,288 on up to 512 CPU nodes of the Piz Daint supercomputer, decreasing the time-to-solution by up to three times. Our code is ScaLAPAck-compatible and available as an open-source library.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476167","European Research Council (ERC); Swiss National Science Foundation(grant numbers:185778); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910123","Distributed linear algebra algorithms;communication complexity;matrix factorization","Schedules;Codes;Three-dimensional displays;Scientific computing;Layout;Libraries;Supercomputers","","","",5.0,"",66.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"STM-Multifrontal QR: Streaming Task Mapping Multifrontal QR Factorization Empowered by GCN","S. Lin; W. Yang; H. Wang; Q. Tsai; K. Li","Hunan University, Changsha, China; Hunan University, Changsha, China; Hunan University, Changsha, China; Hunan University, Changsha, China; Hunan University, Changsha, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Multifrontal QR algorithm, which consists of symbolic analysis and numerical factorization, is a high-performance algorithm for orthogonal factorizing sparse matrix. In this work, a graph convolutional network (GCN) for adaptively selecting the optimal reordering algorithm is proposed in symbolic analysis. Using our GCN adaptive classifier, the average numerical factorization time is reduced by 20.78% compared with the default approach, and the additional memory overhead is approximately 4% higher than that of prior work. Moreover, for numerical factorization, an optimized tasks stream parallel processing strategy is proposed and a more efficient computing task mapping framework for NUMA architecture is adopted in this paper, which called STM-Multifrontal QR factorization. Numerical experiments on the TaiShan Server show average 1.22x performance gains over the original SuiteSparseQR. Nearly 80% of datasets have achieved better performance compared with the MKL sparse QR on Intel Xeon 6248.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476199","National Key R&D Program of China(grant numbers:2018YFB0204302); National Natural Science Foundation of China(grant numbers:92055213,61872127,61751204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910129","Multifrontal QR Factorization;Graph Convolutional Network;Task Stream Processing;Task Mapping Optimization;NUMA Multicore Architecture","Multicore processing;High performance computing;Computer architecture;Performance gain;Parallel processing;Approximation algorithms;Classification algorithms","","","","","",38.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Libshalom: Optimizing Small and Irregular-Shaped Matrix Multiplications on ARMv8 Multi-Cores","W. Yang; J. Fang; D. Dong; X. Su; Z. Wang","College of Computer Science, National University of Defense Technology, China; College of Computer Science, National University of Defense Technology, China; College of Computer Science, National University of Defense Technology, China; College of Computer Science, National University of Defense Technology, China; School of Computing, University of Leeds, United Kingdom","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"General Matrix Multiplication (GEMM) is a key subroutine in highperformance computing. While the mainstream linear algebra libraries can deliver high performance on large and regular-shaped GEMM, they are inadequate for optimizing small and irregular-shaped GEMMs, which are commonly seen in new HPC applications. Some of the recent works in this direction have made promising progress on x86 architectures and GPUs but still leave much room for improvement on emerging HPC hardware built upon the ARMv8 architecture. We present Libshalom, an open-source library for optimizing small and irregular-shaped GEMMs, explicitly targeting the ARMv8 architecture. Libshalom builds upon the classical Goto algorithm but tailors it to minimize the expensive memory accessing overhead for data packing and processing small matrices. It uses analytic methods to determine GEMM kernel optimization parameters, enhancing the computation and parallelization efficiency of the GEMM kernels. We evaluate Libshalom by applying it to three ARMv8 multi-core architectures and comparing it against five mainstream linear algebra libraries. Experimental results show that Libshalom can consistently outperform existing solutions across GEMM workloads and hardware architectures.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476217","National Key R&D program of China(grant numbers:2018YFB0204300); National Natural Science Foundation of China (NSFC)(grant numbers:61972408,61872294); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910139","Matrix Multiplication;Small and Irregular-Shaped;ARMv8 Multi-Core;Performance Optimization","Algorithms;High performance computing;Computer architecture;Linear algebra;Libraries;Hardware;Computational efficiency","","","",3.0,"",59.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"TensorKMC: Kinetic Monte Carlo Simulation of 50 Trillion Atoms Driven by Deep Learning on a New Generation of Sunway Supercomputer","H. Shang; X. Chen; X. Gao; R. Lin; L. Wang; F. Li; Q. Xiao; L. Xu; Q. Sun; L. Zhu; F. Wang; Y. Zhang; H. Song","SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Applied Physics and Computational Mathematics, Beijing, China; Institute of Applied Physics and Computational Mathematics, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Wuxin, China; Institute of Applied Physics and Computational Mathematics, Beijing, China; National Supercomputer Center in Wuxi, Wuxi, China; National Supercomputer Center in Wuxi, Wuxi, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Supercomputer Center in Wuxi, Wuxi, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Wuxin, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Applied Physics and Computational Mathematics, Beijing, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"The atomic kinetic Monte Carlo method plays an important role in multi-scale physical simulations because it bridges the micro and macro worlds. However, its accuracy is limited by empirical potentials. We therefore propose herein a triple-encoding algorithm and vacancy-cache mechanism to efficiently integrate ab initio neural network potentials (NNPs) with AKMC and implement them in our TensorKMC codes. We port our program to SW26010-pro and innovate a fast feature operator and a big fusion operator for the NNPs for fully utilizing the powerful heterogeneous computing units of the new-generation Sunway supercomputer. We further optimize memory usage. With these improvements, TensorKMC can simulate up to 54 trillions of atoms and achieve excellent strong and weak scaling performance up to 27,456,000 cores.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476174","The National Key Research and Development Program of China(grant numbers:2016YFB0201203); Science Challenge Project(grant numbers:TZZT2019,TZ2018002); National Natural Science Foundation of China(grant numbers:12004046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910110","Kinetic Monte Carlo;Neural Network Potentials;Many-core processor;Scalability","Technological innovation;Monte Carlo methods;Codes;Quantum chemistry;High performance computing;Optimization methods;Artificial neural networks","","","",1.0,"",63.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"High-Throughput Virtual Screening of Small Molecule Inhibitors for SARS-CoV-2 Protein Targets with Deep Fusion Models","G. A. Stevenson; D. Jones; H. Kim; W. F. Drew Bennett; B. J. Bennion; M. Borucki; F. Bourguet; A. Epstein; M. Franco; B. Harmon; S. He; M. P. Katz; D. Kirshner; V. Lao; E. Y. Lau; J. Lo; K. McLoughlin; R. Mosesso; D. K. Murugesh; O. A. Negrete; E. A. Saada; B. Segelke; M. Stefan; M. W. Torres; D. Weilhammer; S. Wong; Y. Yang; A. Zemla; X. Zhang; F. Zhu; F. C. Lightstone; J. E. Allen","Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Sandia National Laboratories Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; NVIDIA Corporation, Santa Clara, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Sandia National Laboratories Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Sandia National Laboratories Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Sandia National Laboratories Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA; Lawrence Livermore National Laboratory Livermore, California, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Structure-based Deep Fusion models were recently shown to outperform several physics- and machine learning-based protein-ligand binding affinity prediction methods. As part of a multi-institutional COVID-19 pandemic response, over 500 million small molecules were computationally screened against four protein structures from the novel coronavirus (SARS-CoV-2), which causes COVID-19. Three enhancements to Deep Fusion were made in order to evaluate more than 5 billion docked poses on SARS-CoV-2 protein targets. First, the Deep Fusion concept was refined by formulating the architecture as one, coherently backpropagated model (Coherent Fusion) to improve binding-affinity prediction accuracy. Secondly, the model was trained using a distributed, genetic hyper-parameter optimization. Finally, a scalable, high-throughput screening capability was developed to maximize the number of ligands evaluated and expedite the path to experimental evaluation. In this work, we present both the methods developed for machine learning-based high-throughput screening and results from using our computational pipeline to find SARS-CoV-2 inhibitors.","2167-4337","978-1-4503-8442-1","","DTRA(grant numbers:HDTRA1036045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910141","deep learning;hyper-parameter optimization;SARS-CoV-2;COVID-19;HPC;GPU;AI","Proteins;COVID-19;Inhibitors;Pandemics;Computational modeling;High performance computing;Pipelines","","","","","",72.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"High Performance Uncertainty Quantification with Parallelized Multilevel Markov Chain Monte Carlo","L. Seelinger; A. Reinarz; L. Rannabauer; M. Bader; P. Bastian; R. Scheichl","Institute for Scientific Computing, Heidelberg University, Heidelberg, Germany; Department of Computer Science, Durham University, Durham, United Kingdom; Department of Informatics, Technical University of Munich, Garching, Germany; Department of Informatics, Technical University of Munich, Garching, Germany; Institute for Scientific Computing, Heidelberg University, Heidelberg, Germany; Institute for Applied Mathematics, Heidelberg University, Heidelberg, Germany","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"Numerical models of complex real-world phenomena often necessitate High Performance Computing (HPC). Uncertainties increase problem dimensionality further and pose even greater challenges. We present a parallelization strategy for multilevel Markov chain Monte Carlo, a state-of-the-art, algorithmically scalable Uncertainty Quantification (UQ) algorithm for Bayesian inverse problems, and a new software framework allowing for large-scale parallelism across forward model evaluations and the UQ algorithms themselves. The main scalability challenge presents itself in the form of strong data dependencies introduced by the MLMCMC method, prohibiting trivial parallelization. Our software is released as part of the modular and open-source MIT Uncertainty Quantification Library (MUQ), and can easily be coupled with arbitrary user codes. We demonstrate it using the Distributed and Unified Numerics Environment (DUNE) and the ExaHyPE Engine. The latter provides a realistic, large-scale tsunami model in which we identify the source of a tsunami from buoy-elevation data.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476150","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910095","Multilevel Methods;ADER-DG;Bayesian inverse problems;Tsunami simulation","Uncertainty;Monte Carlo methods;Inverse problems;Computational modeling;High performance computing;Software algorithms;Markov processes","","","","","",33.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks","V. Md; S. Misra; G. Ma; R. Mohanty; E. Georganas; A. Heinecke; D. Kalamkar; N. K. Ahmed; S. Avancha","Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Full-batch training on Graph Neural Networks (GNN) to learn the structure of large graphs is a critical problem that needs to scale to hundreds of compute nodes to be feasible. It is challenging due to large memory capacity and bandwidth requirements on a single compute node and high communication volumes across multiple nodes. In this paper, we present DistGNN that optimizes the well-known Deep Graph Library (DGL) for full-batch training on CPU clusters via an efficient shared memory implementation, communication reduction using a minimum vertex-cut graph partitioning algorithm and communication avoidance using a family of delayed-update algorithms. Our results on four common GNN benchmark datasets: Reddit, OGB-Products, OGB-Papers and Proteins, show up to 3.7× speed-up using a single CPU socket and up to 97× speed-up using 128 CPU sockets, respectively, over baseline DGL implementations running on a single CPU socket.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3480856","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910067","Graph Neural Networks;Graph Partition;Distributed Algorithm;Deep Learning;Deep Graph Library","Training;Proteins;Social networking (online);Sockets;High performance computing;Memory management;Clustering algorithms","","","",8.0,"",38.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Efficient Scaling of Dynamic Graph Neural Networks","V. T. Chakaravarthy; S. S. Pandian; S. Raje; Y. Sabharwal; T. Suzumura; S. Ubaru","IBM Research India; IBM Research India; IBM Research India; IBM Research India; IBM T.J. Watson Research Center, USA; IBM T.J. Watson Research Center, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"We present distributed algorithms for training dynamic Graph Neural Networks (GNN) on large scale graphs spanning multi-node, multi-GPU systems. To the best of our knowledge, this is the first scaling study on dynamic GNN. We devise mechanisms for reducing the GPU memory usage and identify two execution time bottlenecks: CPU-GPU data transfer; and communication volume. Exploiting properties of dynamic graphs, we design a graph difference-based strategy to significantly reduce the transfer time. We develop a simple, but effective data distribution technique under which the communication volume remains fixed and linear in the input size, for any number of GPUs. Our experiments using billion-size graphs on a system of 128 GPUs shows that: (i) the distribution scheme achieves up to 30x speedup on 128 GPUs; (ii) the graph-difference technique reduces the transfer time by a factor of up to 4.1x and the overall execution time by up to 40%.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3480858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910072","Graph neural networks;dynamic graphs;learning","Training;High performance computing;Heuristic algorithms;Graphics processing units;Data transfer;Graph neural networks;Distributed algorithms","","","",2.0,"",31.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Efficient Tensor Core-Based GPU Kernels for Structured Sparsity under Reduced Precision","Z. Chen; Z. Qu; L. Liu; Y. Ding; Y. Xie","University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"The success of DNN comes at the expense of excessive memory/-computation cost, which can be addressed by exploiting reduced precision and sparsity jointly. Existing sparse GPU kernels, however, fail to achieve practical speedup over cuBLASHgemm under half-precision. Those for fine-grained sparsity suffer from low data reuse, and others for coarse-grained sparsity are limited by the wrestling between kernel performance and model quality under different grain sizes. We propose column-vector-sparse-encoding that has a smaller grain size under the same reuse rate compared with block sparsity. Column-vector-sparse-encoding can be applied to both SpMM & SDDMM, two major sparse DNN operations. We also introduce the Tensor-Core-based 1D Octet Tiling that has efficient memory access and computation patterns under small grain size. Based on these, we design SpMM and SDDMM kernels and achieve 1.71-7.19x speedup over cuSPARSE. Practical speedup is achieved over cuBLASHgemm under >70% and >90% sparsity with 4×1 grain size and half-precision.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476182","NSF(grant numbers:1925717); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910106","Neural Networks;Sparse Matrices;GPGPU;Tensor Core","Grain size;Neural networks;Memory management;Graphics processing units;Transformers;Encoding;Mathematical models","","","",4.0,"",31.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Arithmetic-Intensity-Guided Fault Tolerance for Neural Network Inference on GPUs","J. Kosaian; K. V. Rashmi","Carnegie Mellon University; Carnegie Mellon University","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Neural networks (NNs) are increasingly employed in safety-critical domains and in environments prone to unreliability (e.g., soft errors), such as on spacecraft. Therefore, it is critical to impart fault tolerance to NN inference. Algorithm-based fault tolerance (ABFT) is emerging as an efficient approach for fault tolerance in NNs. We propose an adaptive approach to ABFT for NN inference that exploits untapped opportunities in emerging deployment scenarios. GPUs have high compute-to-memory-bandwidth ratios, while NN layers have a wide range of arithmetic intensities. This leaves some layers compute bound and others memory-bandwidth bound, but current approaches to ABFT do not consider these differences. We first investigate ABFT schemes best suited for each of these scenarios. We then propose intensity-guided ABFT, an adaptive, arithmetic-intensity-guided approach that selects the most efficient ABFT scheme for each NN layer. Intensity-guided ABFT reduces execution-time overhead by 1.09-5.3× across many NNs compared to traditional approaches to ABFT.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476184","Amazon Web Services; European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910043","fault tolerance;neural networks;arithmetic intensity","Space vehicles;Fault tolerance;Instruction sets;High performance computing;Fault tolerant systems;Graphics processing units;Artificial neural networks","","","",2.0,"",91.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"FastZ: Accelerating Gapped Whole Genome Alignment on GPUs","S. C. Gundabolu; T. N. Vijaykumar; M. Thottethodi","Purdue University West Lafayette, Indiana, USA; Purdue University West Lafayette, Indiana, USA; Purdue University West Lafayette, Indiana, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"Recognizing the importance of whole genome alignment (WGA), the National Institutes for Health maintains LASTZ, a sequential WGA application. As genomic data grows, there is a compelling need for scalable, high-performance WGA. Unfortunately, high-sensitivity, ‘gapped’ alignment which uses dynamic programming (DP) is slow, whereas faster alignment with ungapped filtering is often less sensitive. We develop FastZ, a GPU-accelerated, gapped WGA software which matches gapped LASTZ in sensitivity. FastZ employs a novel inspector-executor scheme in which (a) the lightweight inspector elides DP traceback except in common, extremely short alignments, where the inspector performs limited, eager traceback to eliminate the executor, and (b) executor trimming avoids unnecessary work. Further, FastZ employs register-based cyclic-buffering to drastically reduce memory traffic, and groups DP problems by size for load balance. FastZ running on an RTX 3080 GPU and our multicore implementation of LASTZ achieve 111x and 20x speedups over the sequential LASTZ, respectively.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910115","","Sensitivity;Multicore processing;Heuristic algorithms;Memory management;Genomics;Graphics processing units;Dynamic scheduling","","","","","",40.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"PEPPA-X: Finding Program Test Inputs to Bound Silent Data Corruption Vulnerability in HPC Applications","M. H. Rahman; A. Shamji; S. Guo; G. Li","University of Iowa, Iowa City, IA, USA; University of Iowa, Iowa City, IA, USA; Baidu Security, Sunnyvale, CA, USA; University of Iowa, Iowa City, IA, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Transient hardware faults have become prevalent due to the shrinking size of transistors, leading to silent data corruptions (SDCs). Therefore, HPC applications need to be evaluated (e.g., via fault injections) and protected to meet the reliability target. In the evaluation, the target programs exercise with a set of given inputs which are usually from program benchmark suite. However, these inputs rarely manifest the SDC vulnerabilities, leading to over-optimistic assessment and unexpectedly higher failure rates in production. We propose PEPPA-X, which efficiently identifies the test inputs that estimate the bound of program SDC resiliency. Our key insight is that the SDC sensitivity distribution in a program often remains stationary across input space. Thereby, we can guide the search of SDC-bound inputs by a sampled distribution. Our evaluation shows that PEPPA-X can identify the SDC-bound input of a program that existing methods cannot find even with 5x more search time.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910044","Error Resilience;Fault Injection;Silent Data Corruption;Software Testing;Input Fuzzing;Program Analysis;Error Propagation;High Performance Computing","Sensitivity;High performance computing;Production;Benchmark testing;Hardware;Transistors;Transient analysis","","","",2.0,"",60.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Cuttlefish: Library for Achieving Energy Efficiency in Multicore Parallel Programs","S. Kumar; A. Gupta; V. Kumar; S. Bhalachandra","IIIT-Delhi, India; IIIT-Delhi, India; IIIT-Delhi, India; Lawrence Berkeley National Laboratory, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"A low-cap power budget is challenging for exascale computing. Dy-namic Voltage and Frequency Scaling (DVFS) and Uncore Frequency Scaling (UFS) are the two widely used techniques for limiting the HPC application's energy footprint. However, existing approaches fail to provide a unified solution that can work with different types of parallel programming models and applications. This paper proposes Cuttlefish, a programming model oblivious C/C++ library for achieving energy efficiency in multicore parallel programs running over Intel processors. An online profiler periodi-cally profiles model-specific registers to discover a running appli-cation's memory access pattern. Using a combination of DVFS and UFS, Cuttlefish then dynamically adapts the processor's core and uncore frequencies, thereby improving its energy efficiency. The evaluation on a 20-core Intel Xeon processor using a set of widely used OpenMP benchmarks, consisting of several irregular-tasking and work-sharing pragmas, achieves geometric mean energy savings of 19.4% with a 3.6% slowdown.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910060","Multicore parallelism;DVFS;UFS;energy efficiency","Adaptation models;Limiting;Multicore processing;Parallel programming;Semantics;Power control;Production","","","",1.0,"",52.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Temporal Vectorization for Stencils","L. Yuan; H. Cao; Y. Zhang; K. Li; P. Lu; Y. Yue","SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Stencil computations represent a very common class of nested loops in scientific and engineering applications. Exploiting vector units in modern CPUs is crucial to achieving peak performance. Previous vectorization approaches often consider the data space, in particular the innermost unit-strided loop. It leads to the well-known data alignment conflict problem that vector loads are overlapped due to the data sharing between continuous stencil computations. This paper proposes a novel temporal vectorization scheme for stencils. It vectorizes the stencil computation in the iteration space and assembles points with different time coordinates in one vector. The temporal vectorization leads to a small fixed number of vector reorganizations that is irrelevant to the vector length, stencil order, and dimension. Furthermore, it is also applicable to Gauss-Seidel stencils, whose vectorization is not well-studied. The effectiveness of the temporal vectorization is demonstrated by various Jacobi and Gauss-Seidel stencils.","2167-4337","978-1-4503-8442-1","","National Key Research & Development Program of China(grant numbers:2016YFB0200800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910136","Stencil Computation;Vectorization;Data Alignment Conflicts","Jacobian matrices;Codes;High performance computing;Design methodology;Time-domain analysis;Finite difference methods;Lattice Boltzmann methods","","","","","",55.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"PAGANI: A Parallel Adaptive GPU Algorithm for Numerical Integration","I. Sakiotis; K. Arumugam; M. Paterno; D. Ranjan; B. Terzić; M. Zubair","Old Dominion University Norfolk, Virginia, USA; NVIDIA Santa Clara, California, USA; Fermi National Accelerator Laboratory Batavia, Illinois, USA; Old Dominion University Norfolk, Virginia, USA; Old Dominion University Norfolk, Virginia, USA; Old Dominion University Norfolk, Virginia, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"We present a new adaptive parallel algorithm for the challenging problem of multi-dimensional numerical integration on massively parallel architectures. Adaptive algorithms have demonstrated the best performance, but efficient many-core utilization is difficult to achieve because the adaptive work-load can vary greatly across the integration space and is impossible to predict a priori. Existing parallel algorithms utilize sequential computations on independent processors, which results in bottlenecks due to the need for data redistribution and processor synchronization. Our algorithm employs a high-throughput approach in which all existing sub-regions are processed and sub-divided in parallel. Repeated sub-region classification and filtering improves upon a brute-force approach and allows the algorithm to make efficient use of computation and memory resources. A CUDA implementation shows orders of magnitude speedup over the fastest open-source CPU method and extends the achievable accuracy for difficult integrands. Our algorithm typically outperforms other existing deterministic parallel methods.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476198","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910080","adaptive;multi-dimensional;integration;deterministic","Memory management;Graphics processing units;Prototypes;Programming;Prediction algorithms;Classification algorithms;Parallel architectures","","","","","",29.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Reducing Redundancy in Data Organization and Arithmetic Calculation for Stencil Computations","K. Li; L. Yuan; Y. Zhang; Y. Yue","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Scienceslice, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Scienceslice, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Stencil computation is one of the most important kernels in various scientific and engineering applications. A variety of work has focused on vectorization techniques, aiming at exploiting the in-core data parallelism. However, they either incur spatial data conflicts or hurt the data locality when integrated with tiling. In this paper, a novel spatial computation folding is devised to reduce the data reorganization overhead for vectorization and preserve the data locality for tiling in the data space simultaneously. We then propose an approach of temporal computation folding enhanced with shifts reusing, tessellate tiling, and semi-automatic code generation. It aims to further reduce the redundancy of arithmetic calculations and exploit the register reuse along the time dimension. Experimental results on the AVX2 and AVX-512 CPUs show that our approach obtains significant performance improvements compared with state-of-the-art techniques.","2167-4337","978-1-4503-8442-1","","National Key Research & Development Program of China(grant numbers:2016YFB0200800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910143","Stencil;Vectorization;Register reuse;Data locality","Codes;High performance computing;Redundancy;Organizations;Parallel processing;Spatial databases;Computational efficiency","","","","","",50.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"CAKE: Matrix Multiplication Using Constant-Bandwidth Blocks","H. T. Kung; V. Natesh; A. Sabot","Harvard University, Cambridge, MA, USA; Harvard University, Cambridge, MA, USA; Harvard University, Cambridge, MA, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"We offer a novel approach to matrix-matrix multiplication computation on computing platforms with memory hierarchies. Constant-bandwidth (CB) blocks improve computation throughput for architectures limited by external memory bandwidth. Configuring the shape and size of CB blocks operating from within any memory hierarchy level (e.g., internal SRAM), we achieve high throughput while holding external bandwidth (e.g., with DRAM) constant. We explain how, surprisingly, CB blocks can maintain constant external bandwidth as computation throughput increases. Analogous to partitioning a cake into pieces, we dub our CB-partitioned system CAKE. We show CAKE outperforms state-of-the-art libraries in compu-tation time on real-world systems where external bandwidth represents a bottleneck, demonstrating CAKE's ability to address the memory wall. CAKE achieves superior performance by directly using theoretically optimal CB-partitioned blocks in tiling and scheduling, obviating the need for extensive design search.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476166","Air Force Research Laboratory(grant numbers:FA87S0-18-1-0112); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910104","memory management;computation for deep neural network (DNN);parallel processing;parallel architectures;optimal block scheduling;arithmetic intensity;matrix multiplication;memory wall","Power demand;Shape;Processor scheduling;Memory management;Neural networks;Random access memory;Bandwidth","","","",1.0,"",31.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"HPAC: Evaluating Approximate Computing Techniques on HPC OpenMP Applications","K. Parasyris; G. Georgakoudis; H. Menon; J. Diffenderfer; I. Laguna; D. Osei-Kuffuor; M. Schordan","Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"As we approach the limits of Moore's law, researchers are exploring new paradigms for future high-performance computing (HPC) systems. Approximate computing has gained traction by promising to deliver substantial computing power. However, due to the stringent accuracy requirements of HPC scientific applications, the broad adoption of approximate computing methods in HPC requires an in-depth understanding of the application's amenability to approximations. We develop HPAC, a framework with compiler and runtime support for code annotation and transformation, and accuracy vs. performance trade-off analysis of OpenMP HPC applications. We use HPAC to perform an in-depth analysis of the effectiveness of approximate computing techniques when applied to HPC applications. The results reveal possible performance gains of approximation and its interplay with parallel execution. For instance, in the LULESH proxy application approximation provides substantial performance gains due to the reduction of memory accesses. However, in the leukocyte benchmark approximation induces load imbalance in the parallel execution and thus limiting the performance gains.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476216","LLNL(grant numbers:DE-AC52-07NA27344,LLNL-CONF-821216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910091","Approximate Computing;Programming Models;Runtime Systems;OpenMP","Semiconductor device modeling;Codes;Runtime;High performance computing;Moore's Law;Approximate computing;Performance gain","","","",2.0,"",49.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Accelerating XOR-Based Erasure Coding using Program Optimization Techniques","Y. Uezato","Dwango, Co., Ltd., Japan","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Erasure coding (EC) affords data redundancy for large-scale systems. XOR-based EC is an easy-to-implement method for optimizing EC. This paper addresses a significant performance gap between the state-of-the-art XOR-based EC approach (~4.9 GB/s coding through-put) and Intel's high-performance EC library based on another approach (~6.7 GB/s). We propose a novel approach based on our observation that XOR-based EC virtually generates programs of a Domain Specific Language for XORing byte arrays. We formalize such programs as straight-line programs (SLPs) of compiler construction and optimize SLPs using various program optimization techniques. Our optimization flow is three-fold: 1) reducing the number of XORs using grammar compression algorithms; 2) reducing memory accesses using deforestation, a functional program optimization method; and 3) reducing cache misses using the (red-blue) pebble game of program analysis. We provide an experimentallibrary, which outperforms Intel's library with an $\sim 8.92$ GB/s throughput.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910053","","Program processors;High performance computing;Redundancy;Optimization methods;Games;Throughput;Encoding","","","","","",104.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Error-Controlled, Progressive, and Adaptable Retrieval of Scientific Data with Multilevel Decomposition","X. Liang; Q. Gong; J. Chen; B. Whitney; L. Wan; Q. Liu; D. Pugmire; R. Archibald; N. Podhorszki; S. Klasky","Missouri S&T, Rolla, MO, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; New Jersey Institute of Technology, New York, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Extreme-scale simulations and high-resolution instruments have been generating an increasing amount of data, which poses significant challenges to not only data storage during the run, but also post-processing where data will be repeatedly retrieved and analyzed for a long period of time. The challenges in satisfying a wide range of post-hoc analysis needs while minimizing the I/O overhead caused by inappropriate and/or excessive data retrieval should never be left unmanaged. In this paper, we propose a data refactoring, compressing, and retrieval framework capable of 1) fine-grained data refactoring with regard to precision; 2) incrementally retrieving and recomposing the data in terms of various error bounds; and 3) adaptively retrieving data in multi-precision and multi-resolution with respect to different analysis. With the progressive data re-composition and the adaptable retrieval algorithms, our framework significantly reduces the amount of data retrieved when multiple incremental precision are requested and/or the downstream analysis time when coarse resolution is used. Experiments show that the amount of data retrieved under the same progressively requested error bound using our framework is 64% less than that using state-of-the-art single-error-bounded approaches. Parallel experiments with up to 1, 024 cores and $\sim\ 600$ GB data in total show that our approach yields $1.36\times$ and $2.52\times$ performance over existing approaches in writing to and reading from persistent storage systems, respectively.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476179","U.S. Department of Energy Office of Science(grant numbers:17-SC-20-SC); National Nuclear Security Administration; Office of Science; Office of Advanced Scientific Computing Research (ASCR); Oak Ridge National Laboratory (ORNL); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910085","Data compression;error control;storage and I/O;data retrieval","Measurement;Analytical models;Scientific computing;Instruments;High performance computing;Memory;Writing","","","","","",50.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"LogECMem: Coupling Erasure-Coded In-Memory Key-Value Stores with Parity Logging","L. Cheng; Y. Hu; Z. Ke; J. Xu; Q. Yao; D. Feng; W. Wang; W. Chen","Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; Huazhong University of Science and Technology, Wuhan, China; HIKVISION, Hangzhou, China; HIKVISION, Hangzhou, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"In-memory key-value stores are often used to speed up Big Data workloads on modern HPC clusters. To maintain their high availability, erasure coding has been recently adopted as a low-cost redundancy scheme instead of replication. Existing erasure-coded update schemes, however, have either low performance or high memory overhead. In this paper, we propose a novel parity logging-based architecture, HybridPL, which creates a hybrid of in-place update (for data and XOR parity chunks) and log-based update (for the remaining parity chunks), so as to balance the update performance and memory cost, while maintaining efficient single-failure repairs. We realize HybridPL as an in-memory key-value store called LogECMem, and further design efficient repair schemes for multiple failures. We prototype LogECMem and conduct experiments on different workloads. We show that LogECMem achieves better up-date performance over existing erasure-coded update schemes with low memory overhead, while maintaining high basic I/O and repair performance.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3480852","National Natural Science Foundation of China(grant numbers:61872414); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910127","Erasure coding;Key-value stores;Update;Parity logging","Couplings;Costs;High performance computing;Redundancy;Memory management;Prototypes;Maintenance engineering","","","","","",63.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Scalable FBP Decomposition for Cone-Beam CT Reconstruction","P. Chen; M. Wahib; X. Wang; T. Hirofuchi; H. Ogawa; A. Biguri; R. Boardman; T. Blumensath; S. Matsuoka","RIKEN CCS, National Institute of Advanced Industrial Science and Technology, Hyogo, Japan; RIKEN CCS, National Institute of Advanced Industrial Science and Technology, Hyogo, Japan; Oak Ridge National Laboratory, US Boston Children's Hospital, US; RIKEN CCS, National Institute of Advanced Industrial Science and Technology, Hyogo, Japan; RIKEN CCS, National Institute of Advanced Industrial Science and Technology, Hyogo, Japan; Institute of Nuclear Medicine, University College, London, UK; μ-VIS X-Ray Imaging Centre, University of Southampton, UK; μ-VIS X-Ray Imaging Centre, University of Southampton, UK; Tokyo Institute of Technology, Japan RIKEN CCS, Hyogo, Japan","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,16,"Filtered Back-Projection (FBP) is a fundamental compute intense algorithm used in tomographic image reconstruction. Cone-Beam Computed Tomography (CBCT) devices use a cone-shaped X-ray beam, in comparison to the parallel beam used in older CT generations. Distributed image reconstruction of cone-beam datasets typically relies on dividing batches of images into different nodes. This simple input decomposition, however, introduces limits on input/ output sizes and scalability. We propose a novel decomposition scheme and reconstruction algorithm for distributed FPB. This scheme enables arbitrarily large input/output sizes, eliminates the redundancy arising in the end-to-end pipeline and improves the scalability by replacing two communication collectives with only one segmented reduction. Finally, we implement the proposed decomposition scheme in a framework that is useful for all current-generation CT devices (7thgen). In our experiments using up to 1024 GPUs, our framework can construct 40963 volumes, for real-world datasets, in under 16 seconds (including I/O).","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476139","EPSRC(grant numbers:EP/R002495/1); EURAMET(grant numbers:17IND08); PRESTO(grant numbers:JPMJPR20MA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910103","HPC;Image Reconstruction;FBP;GPU","Computed tomography;Scalability;Memory management;Redundancy;Pipelines;Graphics processing units;Reconstruction algorithms","","","","","",73.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Online Optimization of File Transfers in High-Speed Networks","M. Arifuzzaman; E. Arslan","University of Nevada, Reno, Reno, Nevada, USA; University of Nevada, Reno, Reno, Nevada, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"File transfers in high-speed networks require network and I/O parallelism to reach high speeds, however, creating arbitrarily large numbers of I/O and network threads overwhelms system resources and causes fairness issues. In this paper, we introduce Falcon that combines a novel utility function with state-of-the-art online op-timization algorithms to discover the degree of I/O and network parallelism for file transfer that can maximize the throughput while keeping system overhead low and ensuring fairness among competing transfers. Our extensive evaluations in several dedicated and production high-speed networks show that Falcon can find near-optimal solution in as little as 20 seconds and outperforms existing transfer application by 2-6x. Moreover, unlike other file transfer optimization solutions that fail to ensure fair resource allocation between competing transfers, Falcon is guaranteed to converge to Nash Equilibrium when multiple Falcon agents compete for the network resources with the help of its game theory-inspired utility function.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476208","National Science Foundation (NSF)(grant numbers:1850353,2007789); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910102","","High-speed networks;Instruction sets;High performance computing;Production;Games;Parallel processing;Throughput","","","",1.0,"",37.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Hardware-Supported Remote Persistence for Distributed Persistent Memory","Z. Duan; H. Lu; H. Liu; X. Liao; H. Jin; Y. Zhang; S. Wu","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computing Science and Technology, Huazhong University of Science and Technology, Wuhan, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"The advent of Persistent Memory (PM) necessitates an evolution of Remote Direct Memory Access (RDMA) technologies for supporting remote data persistence. Previous software-based solutions require remote CPU intervention and postpone the visibility of remote persistence. In this paper, we design several hardware-supported RDMA primitives to flush data from the volatile cache of RDMA Network Interface Cards (RNICs) to the PM. We also propose durable RPCs based on the proposed RDMA Flush primitives to support remote data persistence and fast failure recovery. We emulate the performance of RDMA Flush primitives through other RDMA prim-itives, and compare our proposals with several state-of-the-art RPCs in a real testbed equipped with PM and InfiniBand networks. Experimental results show that our proposals can improve the throughput of RPCs by up to 90%, and reduce the 99th percentile latency by up to 49%. The experimental studies also provide instructive guidelines for designing RDMA-based distributed PM systems.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476194","National Key Research and Development Program of China(grant numbers:2017YFBI001603); National Natural Science Foundation of China(grant numbers:62072198,61732010,61825202,62032008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910097","RDMA;PM;RPC;Data Persistence","High performance computing;Throughput;Proposals;Network interfaces;Guidelines","","","","","",54.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Clairvoyant Prefetching for Distributed Machine Learning I/O","N. Dryden; R. Böhringer; T. Ben-Nun; T. Hoefler","Department of Computer Science, ETH Zürich, Switzerland; Department of Computer Science, ETH Zürich, Switzerland; Department of Computer Science, ETH Zürich, Switzerland; Department of Computer Science, ETH Zürich, Switzerland","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"I/O is emerging as a major bottleneck for machine learning training, especially in distributed environments. Indeed, at large scale, I/O takes as much as 85% of training time. Addressing this I/O bottle-neck necessitates careful optimization, as optimal data ingestion pipelines differ between systems, and require a delicate balance between access to local storage, external filesystems, and remote nodes. We introduce NoPFS, a machine learning I/O middleware, which provides a scalable, flexible, and easy-to-use solution to the I/O bottleneck. NoPFS uses clairvoyance: Given the seed generating the random access pattern for training with SGD, it can exactly predict when and where a sample will be accessed. We combine this with an analysis of access patterns and a performance model to provide distributed caching policies that adapt to different datasets and storage hierarchies. NoPFS reduces I/O times and improves end-to-end training by up to 5.4× on the ImageNet-1k, ImageNet-22k, and CosmoFlow datasets.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910068","Deep learning;high-performance computing;I/O","Training;Analytical models;Runtime;Prefetching;Pipelines;Machine learning;Throughput","","","",4.0,"",79.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"ndzip-gpu: Efficient lossless Compression of Scientific Floating-Point Data on GPUs","F. Knorr; P. Thoman; T. Fahringer","University of Innsbruck, Austria; University of Innsbruck, Austria; University of Innsbruck, Austria","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"Lossless data compression is a promising software approach for reducing the bandwidth requirements of scientific applications on accelerator clusters without introducing approximation errors. Suitable compressors must be able to effectively compact floating-point data while saturating the system interconnect to avoid introducing unnecessary latencies. We present ndzip-gpu, a novel, highly-efficient GPU parallelization scheme for the block compressor ndzip, which has recently set a new milestone in CPU floating-point compression speeds. Through the combination of intra-block parallelism and efficient memory access patterns, ndzip-gpu achieves high resource utilization in decorrelating multi-dimensional data via the Integer Lorenzo Transform. We further introduce a novel, efficient warp-cooperative primitive for vertical bit packing, providing a high-throughput data reduction and expansion step. Using a representative set of scientific data, we compare the performance of ndzip-gpu against five other, existing GPU compressors. While observing that effectiveness of any compressor strongly depends on characteristics of the dataset, we demonstrate that ndzip-gpu offers the best average compression ratio for the examined data. On Nvidia Turing, Volta and Ampere hardware, it achieves the highest single-precision throughput by a significant margin while maintaining a favorable trade-off between data reduction and throughput in the double-precision case.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910073","accelerator;gpgpu;data compression;floating-point","Source coding;Graphics processing units;Bandwidth;Transforms;Throughput;Compressors;Supercomputers","","","",1.0,"",25.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Resilient Error-Bounded Lossy Compressor for Data Transfer","S. Li; S. Di; K. Zhao; X. Liang; Z. Chen; F. Cappello","University of California Riverside, Riverside, CA, USA; Argonne National Laboratory, Lemont, IL, USA; University of California Riverside, Riverside, CA, USA; Missouri University of Science and Technology, Rolla, MO, USA; University of California Riverside, Riverside, CA, USA; Argonne National Laboratory, Lemont, IL, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"Today's exa-scale scientific applications or advanced instruments are producing vast volumes of data, which need to be shared/transferred through the network/devices with relatively low bandwidth (e.g., data sharing on WAN or transferring from edge devices to supercomputers). Lossy compression is one of the candidate strategies to address the big data issue. However, little work was done to make it resilient against silent errors, which may happen during the stage of compression or data transferring. In this paper, we propose a resilient error-bounded lossy compressor based on the SZ compression framework. Specifically, we design a new independent-block-wise model that decomposes the entire dataset into many independent sub-blocks to compress. Then, we design and implement a series of error detection/correction strategies elaboratively for each stage of SZ. Our method is arguably the first algorithm-based fault tolerance (ABFT) solution for lossy compression. Our proposed solution incurs negligible execution overhead in the fault-free situation. Upon soft errors happening, it ensures decompressed data strictly bounded within user's requirement with a very limited degradation of compression ratio and low overhead.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476195","U.S. Department of Energy; Office of Science; Office of Advanced Scientific Computing Research(grant numbers:DE-AC02-06CHl1357); National Science Foundation(grant numbers:1617488,1619253,2003709); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910111","Lossy compression;Algorithm Based Fault Tolerance;data transfer","Degradation;Wide area networks;Fault tolerance;Computational modeling;Fault tolerant systems;Writing;Data models","","","",4.0,"",55.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Productivity, Portability, Performance: Data-Centric Python","A. N. Ziogas; T. Schneider; T. Ben-Nun; A. Calotoiu; T. De Matteis; J. de Fine Licht; L. Lavarini; T. Hoefler","Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Python has become the de facto language for scientific computing. Programming in Python is highly productive, mainly due to its rich science-oriented software ecosystem built around the NumPy module. As a result, the demand for Python support in High Performance Computing (HPC) has skyrocketed. However, the Python language itself does not necessarily offer high performance. In this work, we present a workflow that retains Python's high productivity while achieving portable performance across different architectures. The workflow's key features are HPC-oriented language extensions and a set of automatic optimizations powered by a data-centric intermediate representation. We show performance results and scaling across CPU, GPU, FPGA, and the Piz Daint supercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over previous-best solutions, first-ever Xilinx and Intel FPGA results of annotated Python, and up to 93.16% scaling efficiency on 512 nodes.","2167-4337","978-1-4503-8442-1","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910042","Data-Centric;High Performance Computing;Python;NumPy","Productivity;Codes;Scientific computing;High performance computing;Semantics;Computer architecture;Supercomputers","","","","","",75.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Empirical Evaluation of Circuit Approximations on Noisy Quantum Devices","E. Wilson; F. Mueller; L. Bassman; C. Iancu","North Carolina State University; North Carolina State University; Lawrence Berkeley National Lab; Lawrence Berkeley National Lab","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,12,"Noisy Intermediate-Scale Quantum (NISQ) devices fail to produce outputs with sufficient fidelity for deep circuits with many gates today. Such devices suffer from read-out, multi-qubit gate and cross-talk noise combined with short decoherence times limiting circuit depth. This work develops a methodology to generate shorter circuits with fewer multi-qubit gates whose unitary transformations approximate the original reference one. It explores the benefit of such generated approximations under NISQ devices. Experimental results with Grover's algorithm, multiple-control Toffoli gates, and the Transverse Field Ising Model show that such approximate circuits produce higher fidelity results than longer, theoretically precise circuits on NISQ devices, especially when the reference circuits have many CNOT gates to begin with. With this ability to fine-tune circuits, it is demonstrated that quantum computations can be performed for more complex problems on today's devices than was feasible before, sometimes even with a gain in overall precision by up to 60%.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476189","U.S. Department of Energy (DOE)(grant numbers:DE-AC02-05CH11231); NSF(grant numbers:DMR-1747426,PHY-1818914,OAC-1917383,MPS-2120757); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910130","Quantum Computing;Circuit Approximation;Error Mitigation;Quantum Compilation","Performance evaluation;Quantum computing;Limiting;High performance computing;Quantum mechanics;Logic gates;Noise measurement","","","",1.0,"",37.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"SV-Sim: Scalable PGAS-Based State Vector Simulation of Quantum Circuits","A. Li; B. Fang; C. Granade; G. Prawiroatmodjo; B. Heim; M. Roetteler; S. Krishnamoorthy","Quantum Science Center, Oak Ridge, TN, USA; Quantum Science Center, Oak Ridge, TN, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA; Quantum Science Center, Oak Ridge, TN, USA; Quantum Science Center, Oak Ridge, TN, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,14,"High-performance quantum circuit simulation in a classic HPC is still imperative in the NISQ era. Observing that the major obstacle of scalable state-vector quantum simulation arises from the massively fine-grained irregular data-exchange with remote nodes, in this paper we present SV-Sim to apply the PGAS-based communication models (i.e., direct peer access for intra-node CPUs/GPUs and SHMEM for inter-node CPU/GPU clusters) for efficient general-purpose quantum circuit simulation. Through an orchestrated design based on device functional pointer, SV-Sim is able to abstract various quantum gates across multiple heterogeneous backends, including IBM/Intel/AMD CPUs, NVIDIA/AMD GPUs, and Intel Xeon Phi, in a unified framework, but still asserting outstanding performance and tractable interface to higher-level quantum programming environments, such as IBM Qiskit, Microsoft Q# and Google Cirq. Circumventing the obstacle from the lack of polymorphism in GPUs and leveraging the device-initiated one-sided communication, SV-Sim can process circuit that are dynamically generated in Python using a single GPU/CPU kernel without the need of expensive JIT or runtime parsing, significantly simplifying the programming complexity and improving performance for QC simulation. This is especially appealing for the variational quantum algorithms given the circuits are synthesized online per iteration. Evaluations on the latest NVIDIA DGX-A100, V100-DGX-2, ALCF Theta, OLCF Spock, and OLCF Summit HPCs show that SV-Sim can deliver scalable performance on various state-of-the-art HPC platforms, offering a useful tool for quantum algorithm validation and verification. SV-Sim has been released at http://github.com/pnnl/sv-sim. A version specially tweaked for Q#/QDK is also provided.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476169","U.S. Department of Energy(grant numbers:DE-AC05-76RL01830); Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910084","Quantum Simulation;GPU;NVSHMEM;OpenSHMEM","Performance evaluation;Quantum algorithm;Runtime;Scalability;Peer-to-peer computing;Integrated circuit modeling;Quantum circuit","","","",3.0,"",60.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"SW_Qsim: A Minimize-Memory Quantum Simulator with High-Performance on a New Sunway Supercomputer","F. Li; X. Liu; Y. Liu; P. Zhao; Y. Yang; H. Shang; W. Sun; Z. Wang; E. Dong; D. Chen","National Supercompuer Center, Wuxi, China; National Supercompuer Center, Wuxi, China; National Supercompuer Center, Wuxi, China; National Supercompuer Center, Wuxi, China; National Supercompuer Center, Wuxi, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Supercompuer Center, Wuxi, China; National Supercompuer Center, Wuxi, China; National Supercompuer Center, Wuxi, China; Tsinghua University, Beijing, China","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,13,"Classical simulation of quantum computation plays a critical role in numerical studies of quantum algorithms and the validation of quantum devices. Here, we introduce SW_Qsim, a tensor-network-based quantum simulator, which is designed with a two-level parallel structure for efficient implementation on the many-core New Sunway Supercomputer. We propose a minimize-memory contraction path algorithm for rectangular quantum grids to reduce the memory overhead, and provide the memory-limited simulation capacity of SW26010pro. Moreover, tensor operations are carefully optimized on the SW processor to achieve high performance. We design a fault tolerance mechanism to improve the extreme-scale parallel stability. We benchmark SW_Qsim's simulation of RQCs up to 400-qubits, achieving near-linear strong and weak scaling with up to 28.75 million cores, far beyond the previous state of the art. Our work sheds light on the development of efficient quantum algorithms for use in the physical, chemical, and engineering science fields.","2167-4337","978-1-4503-8442-1","","National Key Research & Development Program of China(grant numbers:2020YFB0204800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910137","","Tensors;Quantum algorithm;Computational modeling;Qubit;Power system stability;Supercomputers;Stability analysis","","","","","",36.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"MAPA: Multi-Accelerator Pattern Allocation Policy for Multi-Tenant GPU Servers","K. Ranganath; J. D. Suetterlein; J. B. Manzano; S. L. Song; D. Wong","Department of Electrical and Computer Engineering, University of California, Riverside, CA, USA; High-Performance Computing Group Pacific Northwest National Lab, WA, USA; High-Performance Computing Group Pacific Northwest National Lab, WA, USA; Future System Architecture Lab, School of Computer Science University of Sydney, Sydney, Australia; Department of Electrical and Computer Engineering, University of California, Riverside, CA, USA","SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022",2021,"","",1,15,"Multi-accelerator servers are increasingly being deployed in shared multi-tenant environments (such as in cloud data centers) in order to meet the demands of large-scale compute-intensive workloads. In addition, these accelerators are increasingly being inter-connected in complex topologies and workloads are exhibiting a wider variety of inter-accelerator communication patterns. However, existing allocation policies are ill-suited for these emerging use-cases. Specifically, this work identifies that multi-accelerator workloads are commonly fragmented leading to reduced bandwidth and increased latency for inter-accelerator communication. We propose Multi-Accelerator Pattern Allocation (MAPA), a graph pattern mining approach towards providing generalized allocation support for allocating multi-accelerator workloads on multi-accelerator servers. We demonstrate that MAPA is able to improve the execution time of multi-accelerator workloads and that MAPA is able to provide generalized benefits across various accelerator topologies. Finally, we demonstrate a speedup of 12.4% for 75th percentile of jobs with the worst case execution time reduced by up to 35% against baseline policy using MAPA.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3480853","NSF(grant numbers:1815643,1955650,2047521); University of Sydney; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910133","","Network topology;Processor scheduling;High performance computing;Graphics processing units;Throughput;Hardware;Topology","","","",1.0,"",75.0,"","18 Oct 2022","","","IEEE","IEEE Conferences"
"Message from the Steering Chair","V. K. Prasanna","University of Southern California","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","i","i","Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139769","","","","","","","","","IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"DozzNoC: Reducing Static and Dynamic Energy in NoCs with Low-latency Voltage Regulators using Machine Learning","M. Clark; Y. Chen; A. Karanth; B. Ma; A. Louri","School of Electrical Engineering and Computer Science, Ohio University, Athens, OH; Department of Electrical and Computer Engineering, University of Texas at Dallas, Dallas, TX; School of Electrical Engineering and Computer Science, Ohio University, Athens, OH; Department of Electrical and Computer Engineering, University of Texas at Dallas, Dallas, TX; Department of Electrical and Computer Engineering, George Washington University, Washington, DC","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1","11","Network-on-chips (NoCs) continues to be the choice of communication fabric in multicore architectures because the NoC effectively combines the resource efficiency of the bus with the parallelizability of the crossbar. As NoC suffers from both high static and dynamic energy consumption, power-gating and dynamic voltage and frequency scaling (DVFS) have been proposed in the literature to improve energy-efficiency. In this work, we propose DozzNoC, an adaptable power management technique that effectively combines power-gating and DVFS techniques to target both static power and dynamic energy reduction with a single inductor multiple output (SIMO) voltage regulator. The proposed power management design is further enhanced by machine learning techniques that predict future traffic load for proactive DVFS mode selection. DozzNoC utilizes a SIMO voltage regulator scheme that allows for fast, low-powered, and independently power-gated or voltage scaled routers such that each router and its outgoing links share the same voltage/frequency domain. Our simulation results using PARSEC and Splash-2 benchmarks on an 8 × 8 mesh network show that for a decrease of 7% in throughput, we can achieve an average dynamic energy savings of 25% and an average static power reduction of 53%.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139843","","Voltage control;Regulators;Switches;Machine learning;Network topology;Throughput;Predictive models","energy conservation;learning (artificial intelligence);low-power electronics;multivariable systems;network routing;network-on-chip;voltage control;voltage regulators","DozzNoC;low-latency voltage regulators;machine learning;network-on-chips;multicore architectures;power-gating;frequency scaling;adaptable power management technique;DVFS techniques;dynamic energy reduction;single inductor multiple output voltage regulator;power management design;SIMO voltage regulator scheme;voltage scaled routers;dynamic energy comsumption;static energy comsumption;dynamic voltage scaling;ndependently power-gated router;dynamic energy savings;static power reduction","",4.0,"",36.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Neksus: An Interconnect for Heterogeneous System-In-Package Architectures","V. Goyal; X. Wang; V. Bertacco; R. Das","Department of Computer Science and Engineering, University of Michigan, Ann Arbor, USA; Department of Computer Science and Engineering, University of Michigan, Ann Arbor, USA; Department of Computer Science and Engineering, University of Michigan, Ann Arbor, USA; Department of Computer Science and Engineering, University of Michigan, Ann Arbor, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","12","21","In the embedded systems industry today, skyrocketing design and manufacturing costs of Systems-on-Chip (SoCs) are key limiting factors for growth. Emerging 2.5D-based System-In-Package (SiP) architectures show potential to lower these costs by enabling the reuse of hard core units and providing higher manufacturing yields due to small chiplet sizes.In this paper, we present Neksus, a novel architecture designed to lower SiP manufacturing costs, support modular ""plug-and-play"" chiplet integration, and leverage the unique properties of interposers. Key to Neksus is a new dedicated interconnect chiplet that addresses the limitations of SiP packaging technology by leveraging direct communication over a mini-chain IP-connection topology. In addition to satisfying SiP technology constraints, because our mini-chain design provides high-bandwidth IP-to-IP communication, it is particularly well-suited for bandwidth-intensive mobile applications. Our evaluation shows Neksus provides up to 28% performance improvement and 31% energy savings over recent SiP architecture.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139857","2.5D stacking;interposer;chiplets;Mobile applications","IP networks;Silicon;Bandwidth;Substrates;Industries;Topology;Stacking","embedded systems;integrated circuit interconnections;integrated circuit manufacture;mobile computing;multiprocessor interconnection networks;performance evaluation;system-in-package;system-on-chip","Neksus;embedded systems industry;system-on-chip;hard core units;manufacturing yields;chiplet sizes;SiP manufacturing costs;SiP packaging technology;mini-chain IP-connection topology;mini-chain design;high-bandwidth IP-to-IP communication;SiP technology constraints;interconnect chiplet;modular plug-and-play chiplet integration;heterogeneous system-in-package architectures;SoC;2.5D-based SiP architectures;interposers;bandwidth-intensive mobile applications;performance improvement","",1.0,"",38.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Accelerated Reply Injection for Removing NoC Bottleneck in GPGPUs","Y. Li; L. Chen","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","22","31","The high level of parallelism in GPGPUs has resulted in significantly changed on-chip data traffic behaviors. This demands new research to identify and address the limiting factors of networks-on-chip (NoCs) in the context of GPGPUs. In this paper, we quantitatively analyze the performance of on-chip networks in GPGPUs, and address a serious NoC bottleneck where the reply data from memory controllers experience large contention when being injected to the reply network. To remove this reply injection bottleneck, we propose Accelerated Reply Injection (ARI), a very effective scheme that can supply a fast rate of data traffic from memory controllers to feed the reply injection points, and accelerates the consumption of the injected packets by quickly transferring the packets out of the injection points, thus increasing both supply and consumption of reply traffic injection. Simulation results on a wide range of benchmarks show that the proposed ARI reduces the data stall time in memory controllers by 67.8% on average, and increases IPC by more than 15.4% on average, with less than 1% area overhead.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139870","","System-on-chip;Acceleration;Memory management;Bandwidth;Graphics processing units;Limiting","graphics processing units;network-on-chip","Accelerated Reply Injection;GPGPUs;on-chip data traffic behaviors;networks-on-chip;on-chip networks;serious NoC bottleneck;reply data;memory controllers;reply network;reply injection bottleneck;reply injection points;injected packets;reply traffic injection;data stall time","","","",40.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Machine-agnostic and Communication-aware Designs for MPI on Emerging Architectures","J. M. Hashmi; S. Xu; B. Ramesh; M. Bayatpour; H. Subramoni; D. K. D. Panda","The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","32","41","Modern multi-/many-cores offer higher core-density, hardware multi-threading, deeper memory hierarchies, and diverse architectural capabilities. While emerging cloud-based HPC systems are able to deliver near-native performance, they bring more diversity to the architectures. The Message Passing Interface (MPI) offers the flexibility to arbitrarily bind application processes to CPU cores, however the static nature of these binding policies typically does not take applications' communication patterns and underlying machine architecture into consideration. This lack of association between the dynamic nature of applications and architectural diversity offered by modern processors makes it difficult for the application developers and MPI designers to exploit modern multi-/many-core systems to their full potential. In this paper, we propose a set of low-level benchmarking based approaches and MPI-level designs to infer vendor-specific machine characteristics e.g., physical to virtual machine topologies, and dynamic communication patterns of the applications. By utilizing this information, we propose two novel algorithms to construct efficient MPI mappings for any given architecture and application communication pattern. The proposed designs are implemented in the MVAPICH2 MPI library and are evaluated on three different architectures using various micro-benchmarks and application kernels. We demonstrate up to 2X performance improvement for MPI collectives, and up to 3.5X and 26% improvement for NAS-CG and miniAMR application kernels, respectively.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139834","Multi-/Many-core;SMP;MPI;HPC;Azure;Cloud;Process Placement;Communication Graphs;Topology","Topology;Hardware;Kernel;Computer architecture;Runtime;Venus;Libraries","application program interfaces;message passing;microprocessor chips;multiprocessing systems;multi-threading;virtual machines","machine-agnostic;communication-aware designs;hardware multithreading;deeper memory hierarchies;diverse architectural capabilities;cloud-based HPC systems;near-native performance;Message Passing Interface;application processes;CPU cores;static nature;binding policies;architectural diversity;application developers;MPI designers;low-level benchmarking based approaches;MPI-level designs;virtual machine topologies;dynamic communication patterns;efficient MPI mappings;application communication pattern;MVAPICH2 MPI library;application kernels;performance improvement;MPI collectives;miniAMR application;vendor-specific machine characteristics;core-density","",2.0,"",30.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"ClusterSR: Cluster-Aware Scattered Repair in Erasure-Coded Storage","Z. Shen; J. Shu; Z. Huang; Y. Fu","School of Informatics, Xiamen University; Department of Computer Science and Technology, Tsinghua University; Department of Computer Science and Engineering, The University of Texas at Arlington; School of Computer Science, North China University of Technology","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","42","51","Erasure coding is a storage-efficient means to guarantee data reliability in today's commodity storage systems, yet its repair performance is seriously hindered by the substantial repair traffic. Repair in clustered storage systems is even complicated because of the scarcity of the cross-cluster bandwidth. We present ClusterSR, a cluster-aware scattered repair approach. ClusterSR minimizes the cross-cluster repair traffic by carefully choosing the clusters for reading and repairing chunks. It further balances the cross-cluster repair traffic by scheduling the repair of multiple chunks. Large-scale simulation and Alibaba Cloud ECS experiments show that ClusterSR can reduce 6.7-52.7% of the cross-cluster repair traffic and improve 14.1-68.8% of the repair throughput.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139879","","Maintenance engineering;Encoding;Bandwidth;Fault tolerance;Fault tolerant systems;Computer architecture;Computer science","cloud computing;encoding;storage management","ClusterSR;erasure-coded storage;erasure coding;storage-efficient;commodity storage systems;repair performance;substantial repair traffic;clustered storage systems;cross-cluster bandwidth;cluster-aware scattered repair approach;cross-cluster repair traffic;repair throughput;large-scale simulation;Alibaba Cloud ECS experiments","",7.0,"",34.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Stitch It Up: Using Progressive Data Storage to Scale Science","J. Lofstead; J. Mitchell; E. Chen","Sandia National Labs; Sandia National Labs; University of California, Berkeley","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","52","61","Generally, scientific simulations load the entire simulation domain into memory because most, if not all, of the data changes with each time step. This has driven application structures that have, in turn, affected the design of popular IO libraries, such as HDF-5, ADIOS, and NetCDF. This assumption makes sense for many cases, but there is also a significant collection of simulations where this approach results in vast swaths of unchanged data written each time step. This paper explores a new IO approach that is capable of stitching together a coherent global view of the total simulation space at any given time. This benefit is achieved with no performance penalty compared to running with the full data set in memory, at a radically smaller process requirement, and results in radical data reduction with no fidelity loss. Additionally, the structures employed enable online simulation monitoring.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139788","storage;io;database;python;analytics;spparks;kinetic monte carlo","Computational modeling;Data models;Welding;Libraries;Writing;Analytical models;Microstructure","data reduction;storage management","IO approach;coherent global view;total simulation space;radically smaller process requirement;radical data reduction;online simulation monitoring;progressive data storage;scientific simulations;simulation domain;application structures;HDF-5;ADIOS;NetCDF;unchanged data;IO libraries","",1.0,"",24.0,"USGov","14 Jul 2020","","","IEEE","IEEE Conferences"
"HFetch: Hierarchical Data Prefetching for Scientific Workflows in Multi-Tiered Storage Environments","H. Devarajan; A. Kougkas; X. -H. Sun","Department of Computer Science, Illinois Institute of Technology; Department of Computer Science, Illinois Institute of Technology; Department of Computer Science, Illinois Institute of Technology","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","62","72","In the era of data-intensive computing, accessing data with a high-throughput and low-latency is more imperative than ever. Data prefetching is a well-known technique for hiding read latency. However, existing solutions do not consider the new deep memory and storage hierarchy and also suffer from under-utilization of prefetching resources and unnecessary evictions. Additionally, existing approaches implement a client-pull model where understanding the application's I/O behavior drives prefetching decisions. Moving towards exascale, where machines run multiple applications concurrently by accessing files in a workflow, a more data-centric approach can resolve challenges such as cache pollution and redundancy. In this study, we present HFetch, a truly hierarchical data prefetcher that adopts a server-push approach to data prefetching. We demonstrate the benefits of such an approach. Results show 10-35% performance gains over existing prefetchers and over 50% when compared to systems with no prefetching.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139852","hierarchical;multi-tiered;prefetching;middle-ware;server-push;data-centric","Prefetching;Hidden Markov models;Random access memory;HEMTs;MODFETs;Optimization;Pollution","cache storage;content management;Internet;scientific information systems","data-centric approach;HFetch;truly hierarchical data prefetcher;server-push approach;hierarchical data prefetching;scientific workflows;multitiered storage environments;data-intensive computing;deep memory;storage hierarchy;prefetching resources;client-pull model;prefetching decisions;application I/O behavior","",8.0,"",51.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"CanarIO: Sounding the Alarm on IO-Related Performance Degradation","M. R. Wyatt; S. Herbein; K. Shoga; T. Gamblin; M. Taufer","UTK, Knoxville, TN, USA; LLNL, Livermore, CA, USA; LLNL, Livermore, CA, USA; LLNL, Livermore, CA, USA; UTK, Knoxville, TN, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","73","83","Users interact with High Performance Computing (HPC) machines through batch systems, which take user job submissions and allocate them to computing resources. While some resource managers have a generalized resource model, in nearly all modern systems, nodes are the only resource managed. Other resources, such as parallel file systems, are also necessary for jobs to make progress, but schedulers are blind to these resources. Facility staff can manually detect critical problems and manually hold jobs that need particular file systems, but this requires manual monitoring. Without human intervention, modern schedulers will happily run jobs whose required resources are not available. As a result, resources are wasted when IO-intensive jobs are scheduled on file systems with degraded performance.We introduce CanarIO, a tool for predicting the IO-sensitivity of HPC jobs and detecting IO-related performance degradation on HPC systems. CanarIO uses a set of ""canary"" IO probes run at regular intervals on the system. Using performance measurements from these jobs, CanarIO builds classifiers that can determine which jobs are IO-sensitive and when file system performance is degraded. We demonstrate the accuracy of our tool with a simulation of system execution using real HPC data. Specifically, we detect 37.5% of IO degradation events and correctly identify >90% of IO-sensitive jobs. We show that with CanarIO predictions we recover >1,500 node-hours in 10 days, with a potential maximum of nearly 10,000 node-hours. CanarIO is the first step necessary for augmenting schedulers to be resource-aware.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139855","","Degradation;Probes;Runtime;Monitoring;Tools;Bandwidth;Measurement","batch processing (computers);input-output programs;parallel processing;resource allocation;scheduling","IO-intensive jobs;HPC systems;file system performance;CanarIO predictions;resource-aware;IO-related Performance degradation;High Performance Computing machines;batch systems;user job submissions;resource allocation;system execution simulation","",2.0,"",16.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"A Study of Graph Analytics for Massive Datasets on Distributed Multi-GPUs","V. Jatala; R. Dathathri; G. Gill; L. Hoang; V. K. Nandivada; K. Pingali","The University of Texas at Austin, Austin, Texas, USA; The University of Texas at Austin, Austin, Texas, USA; The University of Texas at Austin, Austin, Texas, USA; The University of Texas at Austin, Austin, Texas, USA; IIT Madras, Chennai, Tamil Nadu, India; The University of Texas at Austin, Austin, Texas, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","84","94","There are relatively few studies of distributed GPU graph analytics systems in the literature and they are limited in scope since they deal with small data-sets, consider only a few applications, and do not consider the interplay between partitioning policies and optimizations for computation and communication.In this paper, we present the first detailed analysis of graph analytics applications for massive real-world datasets on a distributed multi-GPU platform and the first analysis of strong scaling of smaller real-world datasets. We use D-IrGL, the state-of-the-art distributed GPU graph analytical framework, in our study. Our evaluation shows that (1) the Cartesian vertex-cut partitioning policy is critical to scale computation out on GPUs even at a small scale, (2) static load imbalance is a key factor in performance since memory is limited on GPUs, (3) device-host communication is a significant portion of execution time and should be optimized to gain performance, and (4) asynchronous execution is not always better than bulk-synchronous execution.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139771","Graphics Processing Units;Graph Processing;Distributed Systems;Performance","Graphics processing units;Performance evaluation;Optimization;Computer architecture;Mirrors;Synchronization;Computational modeling","data handling;distributed processing;graph theory;graphics processing units","massive datasets;distributed multiGPUs;distributed GPU graph analytics systems;Cartesian vertex-cut partitioning policy;real-world datasets;D-IrGL;device-host communication","",5.0,"",34.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"A Highly Efficient Dynamical Core of Atmospheric General Circulation Model based on Leap-Format","H. Cao; L. Yuan; H. Zhang; B. Wu; S. Li; P. Lu; Y. Zhang; Y. Xu; M. Zhang","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences; Institute of Atmospheric Physics, Chinese Academy of Sciences; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences; Department of Computer Science, ETH Zurich; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Atmospheric Physics, Chinese Academy of Sciences","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","95","104","The finite-difference dynamical core based on the equal-interval latitude-longitude mesh has been widely used for numerical simulations of the Atmospheric General Circulation Model (AGCM). Previous work utilizes different filtering schemes to alleviate the instability problem incurred by the unequal physical spacing at different latitudes, but they all incur high communication and computation overhead and become a scaling bottleneck. This paper proposes a new leap-format finite-difference computing scheme. It generalizes the usual finite-difference format with adaptive wider intervals and is able to maintain the computational stability in the grid updating. Therefore, the costly filtering scheme is eliminated. The new scheme is parallelized with a shifting communication method and implemented with fine communication optimizations based on a 3D decomposition. With the proposed leap-format computation scheme, the communication overhead of the AGCM is significantly reduced and good load balance is exhibited. The simulation results verify the correctness of the new leap-format scheme. The new scheme achieves the speed of 16.6 simulation-year-per-day (SYPD) and up to 3.3x speedup over the latest implementation.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139804","dynamical core;leap-format finite-difference;shifting communication scheme;polar regions;filtering module","Atmospheric modeling;Computational modeling;Mathematical model;Three-dimensional displays;Power system stability;Stability analysis;Two dimensional displays","atmospheric movements;finite difference methods;geophysics computing;grid computing;numerical analysis;optimisation;parallel processing","numerical simulations;atmospheric general circulation model;AGCM;leap-format finite-difference computing scheme;shifting communication method;leap-format computation scheme;communication overhead;finite-difference dynamical core;equal-interval latitude-longitude mesh;grid updating;3D decomposition","",1.0,"",23.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Understanding GPU-Based Lossy Compression for Extreme-Scale Cosmological Simulations","S. Jin; P. Grosset; C. M. Biwer; J. Pulido; J. Tian; D. Tao; J. Ahrens","The University of Alabama, AL, USA; Los Alamos National Laboratory, NM, USA; Los Alamos National Laboratory, NM, USA; Los Alamos National Laboratory, NM, USA; The University of Alabama, AL, USA; The University of Alabama, AL, USA; Los Alamos National Laboratory, NM, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","105","115","To help understand our universe better, researchers and scientists currently run extreme-scale cosmology simulations on leadership supercomputers. However, such simulations can generate large amounts of scientific data, which often result in expensive costs in data associated with data movement and storage. Lossy compression techniques have become attractive because they significantly reduce data size and can maintain high data fidelity for post-analysis. In this paper, we propose to use GPU-based lossy compression for extreme-scale cosmological simulations. Our contributions are threefold: (1) we implement multiple GPU-based lossy compressors to our open-source compression benchmark and analysis framework named Foresight; (2) we use Foresight to comprehensively evaluate the practicality of using GPU-based lossy compression on two real-world extreme-scale cosmology simulations, namely HACC and Nyx, based on a series of assessment metrics; and (3) we develop a general optimization guideline on how to determine the best-fit configurations for different lossy compressors and cosmological simulations. Experiments show that GPU-based lossy compression can provide necessary accuracy on post-analysis for cosmological simulations and high compression ratio of 5 ~ 15× on the tested datasets, as well as much higher compression and decompression throughput than CPU-based compressors.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139874","","Compressors;Measurement;Supercomputers;Data models;Computational modeling;Adaptation models;Graphics processing units","astronomy computing;computer simulation;cosmology;data compression;graphics processing units;mainframes;parallel machines;public domain software;storage management","extreme-scale cosmological simulations;extreme-scale cosmology simulations;data movement;GPU-based lossy compression;open-source compression benchmark;data storage;Foresight;HACC;Nyx;leadership supercomputers","",18.0,"",44.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Optimizing High Performance Markov Clustering for Pre-Exascale Architectures","O. Selvitopi; M. T. Hussain; A. Azad; A. Buluç","Lawrence Berkeley National Laboratory, Berkeley, CA; Indiana University, Bloomington, IN; Indiana University, Bloomington, IN; Lawrence Berkeley National Laboratory, Berkeley, CA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","116","126","HipMCL is a high-performance distributed memory implementation of the popular Markov Cluster Algorithm (MCL) and can cluster large-scale networks within hours using a few thousand CPU-equipped nodes. It relies on sparse matrix computations and heavily makes use of the sparse matrix-sparse matrix multiplication kernel (SpGEMM). The existing parallel algorithms in HipMCL are not scalable to Exascale architectures, both due to their communication costs dominating the runtime at large concurrencies and also due to their inability to take advantage of accelerators that are increasingly popular. In this work, we systematically remove scalability and performance bottlenecks of HipMCL. We enable GPUs by performing the expensive expansion phase of the MCL algorithm on GPU. We propose a CPU-GPU joint distributed SpGEMM algorithm called pipelined Sparse SUMMA and integrate a probabilistic memory requirement estimator that is fast and accurate. We develop a new merging algorithm for the incremental processing of partial results produced by the GPUs, which improves the overlap efficiency and the peak memory usage. We also integrate a recent and faster algorithm for performing SpGEMM on CPUs. We validate our new algorithms and optimizations with extensive evaluations. With the enabling of the GPUs and integration of new algorithms, HipMCL is up to 12.4× faster, being able to cluster a network with 70 million proteins and 68 billion connections just under 15 minutes using 1024 nodes of ORNL's Summit supercomputer.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139868","Markov clustering;HipMCL;SpGEMM","Clustering algorithms;Sparse matrices;Markov processes;Memory management;Merging;Optimization;Graphics processing units","distributed memory systems;graphics processing units;Markov processes;matrix multiplication;parallel processing;pattern clustering;pipeline processing;sparse matrices","HipMCL;MCL algorithm;CPU-GPU joint distributed SpGEMM algorithm;probabilistic memory requirement estimator;merging algorithm;high performance Markov clustering;pre-exascale architectures;high-performance distributed memory implementation;Markov cluster algorithm;sparse matrix computations;sparse matrix-sparse matrix multiplication kernel;parallel algorithms;cluster large-scale networks;CPU-equipped nodes;pipelined sparse SUMMA;incremental processing;ORNL Summit supercomputer","",7.0,"",30.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Tightening Up the Incentive Ratio for Resource Sharing Over the Rings","Y. Cheng; X. Deng; Y. Li","Suzhou Key Laboratory for Big Data and Information Service, School of Business, Suzhou University of Science and Technology, Suzhou, China; Center of Frontier of Computer Studies, Peking University, Beijing, China; Center of Frontier of Computer Studies, Peking University, Beijing, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","127","136","Fundamental issues in resource sharing over large scale networks have gained much attention from the research community, in response to the growth of sharing economy over the Internet and mobile networks. We are particularly interested in the fundamental file sharing and subsequently P2P network bandwidth sharing developed by BitTorrent and later formalized by Wu and Zhang [15] as the proportional response protocol. It is of practical importance in the design to provide agent incentives to follow the distributed protocol out of their own rationality. We study the robustness of the distributed protocol in this incentive issue against a Sybil attack, a common type of grave threat in P2P network. For the resource sharing on rings, and we characterize the utility gain from a Sybil attack in the concept of incentive ratio. Previous works proved the incentive ratio is lower bounded by two and upper bounded by four, and later the upper bound is improved to three. It has been listed in [5] and [9] as an open problem to tighten them. In this paper, we completely resolve this open problem with a better understanding on the influence from different class agents to the resource allocation under the distributed protocol.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139833","Resource sharing;Proportional response dynamics;Incentive ratio;Sybil attack;Distributed protocol","Resource management;Protocols;Dynamic scheduling;Peer-to-peer computing;Robustness;Heuristic algorithms;Internet","incentive schemes;peer-to-peer computing;protocols;resource allocation","P2P network;incentive issue;agent incentives;proportional response protocol;P2P network bandwidth sharing;fundamental file sharing;mobile networks;sharing economy;distributed protocol;resource allocation;incentive ratio;resource sharing;Sybil attack","",1.0,"",15.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Communication-Efficient String Sorting","T. Bingmann; P. Sanders; M. Schimek","Karlsruhe Institute of Technology, Karlsruhe, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","137","147","There has been surprisingly little work on algorithms for sorting strings on distributed-memory parallel machines. We develop efficient algorithms for this problem based on the multi-way merging principle. These algorithms inspect only characters that are needed to determine the sorting order. Moreover, communication volume is reduced by also communicating (roughly) only those characters and by communicating repetitions of the same prefixes only once. Experiments on up to 1280 cores reveal that these algorithm are often more than five times faster than previous algorithms.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139805","distributed-memory algorithm;string sorting;communication-efficient algorithm","Sorting;Partitioning algorithms;Arrays;Clustering algorithms;Merging;Buildings","distributed memory systems;parallel machines;sorting;tree data structures","sorting order;communication volume;communication-efficient string;surprisingly little work;distributed-memory parallel machines;multiway merging principle","",2.0,"",32.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"SCSL: Optimizing Matching Algorithms to Improve Real-time for Content-based Pub/Sub Systems","T. Ding; S. Qian; J. Cao; G. Xue; M. Li","Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","148","157","Although many matching algorithms have been proposed to improve the matching efficiency of the content-based publish/subscribe system, existing work seldom consider the real-time of event dissemination from the perspective of event matching. On the basis of two existing matching algorithms, in this paper, we propose a subscription-classifying and structure-layering (SCSL) optimization method for matching algorithms, aiming to improve real-time by shortening the determining time of matching subscriptions. The basic idea of SCSL is that subscriptions with high matching probabilities should be processed first in the process of event matching and their storage positions in the data structure should be adjusted in line with changing probabilities. One challenge of SCSL is the trade-off that needs to be made between the gains of improving real-time performance by identifying matching subscriptions earlier and the cost of increasing matching time due to subscription classification and adjustment. We design a concise scheme to classify subscriptions, establish a lightweight adjustment mechanism to deal with dynamics and propose an efficient greedy algorithm to compute the adjustment solution, which alleviates the impact of SCSL on matching performance. The experiment results show that the 95th percentile of the determining time of matching subscriptions is improved by about 70%. Furthermore, we integrate SCSL into Apache Kafka to augment it as a content-based publish/subscribe system and test the effect of SCSL based on real-world stock trace data, which witnesses about 40% improvement on the average event transfer latency and confirms that SCSL can effectively improve the real-time performance of content-based publish/subscribe systems.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139781","publish/subscribe;real-time;matching;event transfer latency;QoS","Real-time systems;Data structures;Data models;Optimization;Search problems;Indexing;Heuristic algorithms","data structures;greedy algorithms;message passing;middleware;optimisation;pattern classification;probability","SCSL;event dissemination;event matching;matching algorithms;matching probabilities;subscription classification;greedy algorithm;content-based pub/sub systems;content-based publish/subscribe system;subscription-classifying and structure-layering optimization;storage positions;data structure;matching subscriptions;Apache Kafka","",6.0,"",21.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Distributed Graph Realizations","J. Augustine; K. Choudhary; A. Cohen; D. Peleg; S. Sivasubramaniam; S. Sourav","Indian Institute of Technology Madras; Weizmann Institute of Science; Weizmann Institute of Science; Weizmann Institute of Science; Indian Institute of Technology Madras; National University of Singapore","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","158","167","We study graph realization problems from a distributed perspective. The problem is naturally applicable to the distributed construction of overlay networks that must satisfy certain degree or connectivity properties, and we study it in the node capacitated clique (NCC) model of distributed computing, recently introduced for representing peer-to-peer networks.We focus on two central variants, degree-sequence realization and minimum threshold-connectivity realization. In the degree sequence problem, each node v is associated with a degree d(v), and the resulting degree sequence is realizable if it is possible to construct an overlay network in which the degree of each node v is d(v). The minimum threshold-connectivity problem requires us to construct an overlay network that satisfies connectivity constraints specified between every pair of nodes.Overlay network realizations can be either explicit or implicit. Explicit realizations require both endpoints of any edge in the realized graph to be aware of the edge. In implicit realizations, on the other hand, at least one endpoint of each edge of the realized graph needs to be aware of the edge.The main realization algorithms we present are the following. (1) A $\tilde O(\min \{ \sqrt m ,\Delta \} )$ time algorithm for implicit realization of a degree sequence. Here, Δ = maxv d(v) is the maximum degree and m = (1/2) v d(v) is the number of edges in the final realization. (2) A $\tilde O\left( \Delta \right)$ time algorithm for an explicit realization of a degree sequence. We first compute an implicit realization and then transform it into an explicit one in $\tilde O\left( \Delta \right)$ additional rounds. (3) A $\tilde O\left( \Delta \right)$ time algorithm for the threshold connectivity problem that obtains an explicit solution and an improved $\tilde O\left( 1 \right)$ algorithm for implicit realization when all nodes know each other’s IDs. These algorithms are 2-approximations w.r.t. the number of edges. Our algorithms are complemented by lower bounds showing tightness up to log n factors. Additionally, we provide algorithms for realizing trees and an $\tilde O\left( 1 \right)$ round algorithm for approximate degree sequence realization.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139848","","Peer-to-peer computing;Overlay networks;Graphics;Approximation algorithms;Computational modeling;IP networks;Distributed processing","approximation theory;computational complexity;graph theory;peer-to-peer computing","node capacitated clique model;distributed computing;peer-to-peer networks;degree-sequence realization;degree sequence problem;resulting degree sequence;minimum threshold-connectivity problem;overlay network realizations;explicit realization;implicit realization;realized graph needs;main realization algorithms;maximum degree;threshold connectivity problem;approximate degree sequence realization;distributed graph realizations;graph realization problems;distributed construction","",4.0,"",33.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Transaction-Based Core Reliability","S. Wook Stephen Do; M. Dubois","IC LAB, Futurewei Technologies, Santa Clara, US; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, US","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","168","179","Modern microprocessor designs are becoming more vulnerable to transient faults leading to transient errors due to design trends mandating low supply voltage and reduced noise margins, shrinking feature sizes and increased transistor density for fast, low power circuits. Detecting and correcting transient errors in random logic in a processor core has become an important design goal and confronts design challenges such as input replication, error confinement and overheads minimization. Transactional Memory (TM) is a recent paradigm to improve the programmability and performance of parallel programs. TM has appeared in industry, providing hardware mechanisms for conflict detection and resolution, and checkpointing and rollback. In this paper, we leverage the features of Hardware TM (HTM) to provide processor cores with transient error detection and recovery at low hardware cost. We contribute to the current state of the art of dependable systems by proposing a novel microarchitecture.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139811","transient error detection and correction;hardware transactional memory;reliability;core dependability","Hardware;Transient analysis;Fingerprint recognition;Registers;Feature extraction;Redundancy","error detection;fault tolerance;integrated circuit design;integrated circuit noise;integrated circuit reliability;low-power electronics;microprocessor chips;parallel programming","transient errors;random logic;processor core;error confinement;overhead minimization;Transactional Memory;hardware mechanisms;conflict detection;Hardware TM;transient error detection;hardware cost;transaction-based core reliability;microprocessor designs;transient faults;noise margins;transistor density;low power circuits;HTM","","","",46.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Understanding the Interplay between Hardware Errors and User Job Characteristics on the Titan Supercomputer","S. -H. Lim; R. G. Miller; S. S. Vazhkudai","Oak Ridge National Laboratory, Oak Ridge, TN; Oak Ridge National Laboratory, Oak Ridge, TN; Oak Ridge National Laboratory, Oak Ridge, TN","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","180","190","Designing dependable supercomputers begins with an understanding of errors in real-world, large-scale systems. The Titan supercomputer at Oak Ridge National Laboratory provides a unique opportunity to investigate errors when an actual system is actively used by multiple concurrent users and workloads from diverse domains at varying scales. This study presents a thorough analysis of 6, 908, 497 hardware errors from 18, 688 compute nodes of Titan for 312, 215 user jobs over a 3-year time period. Through careful joining of two system logs – the Machine Check Architecture (MCA) log and the job scheduler log – we show the correlated pattern of hardware errors for each job and user, in addition to individual descriptive statistics of errors, jobs, and users. Since the majority of hardware errors are memory errors, this study also shows the importance of error correcting in memory systems.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139888","","Hardware;Supercomputers;Reliability;Computer architecture;Graphics processing units;Large-scale systems;Government","concurrency (computers);mainframes;parallel machines;processor scheduling;system monitoring","user job characteristics;Titan supercomputer;large-scale systems;Oak Ridge National Laboratory;concurrent users;hardware errors;memory errors;error correction;machine check architecture;job scheduler log;system logs;memory systems","",1.0,"",22.0,"USGov","14 Jul 2020","","","IEEE","IEEE Conferences"
"EC-Fusion: An Efficient Hybrid Erasure Coding Framework to Improve Both Application and Recovery Performance in Cloud Storage Systems","H. Qiu; C. Wu; J. Li; M. Guo; T. Liu; X. He; Y. Dong; Y. Zhao","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer and Information Sciences, Temple University, Philadelphia, United States; Department of Computer and Information Sciences, Temple University, Philadelphia, United States; Alibaba Group, Hangzhou, China; Alibaba Group, Hangzhou, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","191","201","Nowadays erasure coding is one of the most significant techniques in cloud storage systems, which provides both quick parallel I/O processing and high capabilities of fault tolerance on massive data accesses. In these systems, triple disk failure tolerant arrays (3DFTs) is a typical configuration, which is supported by several classic erasure codes like Reed-Solomon (RS) codes, Local Reconstruction Codes (LRC), Minimum Storage Regeneration (MSR) codes, etc. For an online recovery process, the foreground application workloads and the background recovery workloads are handled simultaneously, which requires a comprehensive understanding on both two types of workload characteristics. Although several techniques have been proposed to accelerate the I/O requests of online recovery processes, they are typically unilateral due to the fact that the above two workloads are not combined together to achieve high cost-effective performance.To address this problem, we propose Erasure Codes Fusion (EC-Fusion), an efficient hybrid erasure coding framework in cloud storage systems. EC-Fusion is a combination of RS and MSR codes, which dynamically selects the appropriate code based on its properties. On one hand, for write-intensive application workloads or low risk on data loss in recovery workloads, EC-Fusion uses RS code to decrease the computational overhead and storage cost concurrently. On the other hand, for read-intensive or frequent reconstruction in workloads, MSR code is a proper choice. Therefore, a better overall application and recovery performance can be achieved in a cost-effective fashion. To demonstrate the effectiveness of EC-Fusion, several experiments are conducted in hadoop systems. The results show that, compared with the traditional hybrid erasure coding techniques, EC-Fusion accelerates the response time for application by up to 1.77×, and reduces the reconstruction time by up to 69.10%.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139819","Erasure Coding;Storage Fusion;High Reliability;Reconstruction;Storage Efficiency","Encoding;Heart;Microwave integrated circuits;Cloud computing;Redundancy;Reed-Solomon codes","cloud computing;data handling;error correction codes;fault tolerance;fault tolerant computing;parallel processing;RAID;Reed-Solomon codes;storage management","efficient hybrid erasure coding framework;erasure codes fusion;minimum storage regeneration codes;recovery performance;traditional hybrid erasure coding techniques;hadoop systems;MSR code;storage cost;computational overhead;RS code;write-intensive application workloads;appropriate code;workload characteristics;background recovery workloads;foreground application workloads;online recovery process;local reconstruction codes;Reed-Solomon codes;classic erasure codes;triple disk failure tolerant arrays;massive data accesses;cloud storage systems;EC-Fusion","",9.0,"",44.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Learning an Effective Charging Scheme for Mobile Devices","T. Liu; B. Wu; W. Xu; X. Cao; J. Peng; H. Wu","Visual Computing and Virtual Reality Key Laboratory of Sichuan Province, Sichuan Normal University, Chengdu, China; University of Louisiana at Lafayette, Lafayette, LA, USA; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; Center for Cybersecurity Education and Research, Dominion University, Norfolk, VA, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","202","211","Wireless charging has been demonstrated as a promising technology for prolonging device operational lifetimes in Wireless Rechargeable Networks (WRNs). To schedule a mobile charger to move along a predesigned trajectory to charge devices, most existing studies assume that the precise location information of devices is already known. Unfortunately, this assumption does not always hold in real mobile application, because the activities of vast majority of mobile devices carried by mobile agents appear dynamic and random. To the best of our knowledge, this is the first work to study how to wirelessly charge mobile devices with non-deterministic mobility. We aim to provide effective charging service to them, subject to the energy capacity of the mobile charger. Then, we formalize the effective charging problem as a charging reward maximization problem (CRMP), where the amount of reward obtained by charging a de-vice is inversely proportional to the residual lifetime of the device. To derive an effective charging heuristic, an algorithm based on Reinforcement Learning (RL) is proposed. The evaluation results show that the RL-based charging algorithm achieves excellent charging effectiveness. We further interpret the learned heuristic to gain deep and valuable insights into the design options.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139774","Wireless Rechargeable Networks;Mobile Devices;Reinforcement Learning","Mobile handsets;Heuristic algorithms;Batteries;Trajectory;Wireless communication;Energy consumption;Wireless sensor networks","battery chargers;electronic engineering computing;learning (artificial intelligence);mobile agents;mobile computing;mobile handsets;power aware computing;scheduling;wireless sensor networks","wireless rechargeable networks;mobile charger;mobile application;mobile devices;mobile agents;nondeterministic mobility;effective charging service;effective charging problem;charging reward maximization problem;effective charging heuristic;effective charging scheme;wireless charging;device operational lifetimes;charging effectiveness;residual lifetime;reinforcement learning;RL-based charging algorithm","",4.0,"",32.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Optimize Scheduling of Federated Learning on Battery-powered Mobile Devices","C. Wang; X. Wei; P. Zhou","Dept. of Computer Science, Old Dominion University, Norfolk, VA, USA; Dept. of Computer Science, Old Dominion University, Norfolk, VA, USA; Dept. of Electrical and Computer Engineering, Stony Brook University, NY, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","212","221","Federated learning learns a collaborative model by aggregating locally-computed updates from mobile devices for privacy preservation. While current research typically prioritizing the minimization of communication overhead, we demonstrate from an empirical study, that computation heterogeneity is a more pronounced bottleneck on battery-powered mobile devices. Moreover, if class is unbalanced among the mobile devices, inappropriate selection of participants may adversely cause gradient divergence and accuracy loss. In this paper, we utilize data as a tunable knob to schedule training and achieve near-optimal solutions of computation time and accuracy loss. Based on the offline profiling, we formulate optimization problems and propose polynomial-time algorithms when data is class-balanced or unbalanced. We evaluate the optimization framework extensively on a mobile testbed with two datasets. Compared with common benchmarks of federated learning, our algorithms achieve 210× speedups with negligible accuracy loss. They also mitigate the impact from mobile stragglers and improve parallelism for federated learning.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139810","Federated learning;on-device deep learning;scheduling optimization;non-IID data","Mobile handsets;Computational modeling;Training;Optimization;Task analysis;Servers;Time-frequency analysis","computational complexity;data privacy;learning (artificial intelligence);mobile computing;optimisation;power aware computing;scheduling","mobile stragglers;federated learning;scheduling optimization;battery-powered mobile devices;computational time;mobile testbed;collaborative model;polynomial-time algorithms;privacy preservation","",11.0,"",30.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Harnessing Deep Learning via a Single Building Block","E. Georganas; K. Banerjee; D. Kalamkar; S. Avancha; A. Venkat; M. Anderson; G. Henry; H. Pabst; A. Heinecke","Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation; Intel Corporation","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","222","233","Deep learning (DL) is one of the most prominent branches of machine learning. Due to the immense computational cost of DL workloads, industry and academia have developed DL libraries with highly-specialized kernels for each workload/architecture, leading to numerous, complex code-bases that strive for performance, yet they are hard to maintain and do not generalize. In this work, we introduce the batch-reduce GEMM kernel and show how the most popular DL algorithms can be formulated with this kernel as the basic building-block. Consequently, the DL library-development degenerates to mere (potentially automatic) tuning of loops around this sole optimized kernel. By exploiting our new kernel we implement Recurrent Neural Networks, Convolution Neural Networks and Multilayer Perceptron training and inference primitives in just 3K lines of high-level code. Our primitives outperform vendor-optimized libraries on multi-node CPU clusters, and we also provide proof-of-concept CNN kernels targeting GPUs. Finally, we demonstrate that the batch-reduce GEMM kernel within a tensor compiler yields high-performance CNN primitives, further amplifying the viability of our approach.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139809","","Kernel;Tensile stress;Libraries;Convolution;Training;Registers;Optimization","convolutional neural nets;learning (artificial intelligence);multilayer perceptrons;neural net architecture;operating system kernels;program compilers;recurrent neural nets;software libraries;tensors","deep learning;building block;machine learning;batch-reduce GEMM kernel;sole optimized kernel;recurrent neural networks;convolution neural networks;inference primitives;vendor-optimized libraries;proof-of-concept CNN kernels;tensor compiler;DL library;multilayer perceptron training","",12.0,"",50.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Experience-Driven Computational Resource Allocation of Federated Learning by Deep Reinforcement Learning","Y. Zhan; P. Li; S. Guo","Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","234","243","Federated learning is promising in enabling large-scale machine learning by massive mobile devices without exposing the raw data of users with strong privacy concerns. Existing work of federated learning struggles for accelerating the learning process, but ignores the energy efficiency that is critical for resource-constrained mobile devices. In this paper, we propose to improve the energy efficiency of federated learning by lowering CPU-cycle frequency of mobile devices who are faster in the training group. Since all the devices are synchronized by iterations, the federated learning speed is preserved as long as they complete the training before the slowest device in each iteration. Based on this idea, we formulate an optimization problem aiming to minimize the total system cost that is defined as a weighted sum of training time and energy consumption. Due to the hardness of nonlinear constraints and unawareness of network quality, we design an experience-driven algorithm based on the Deep Reinforcement Learning (DRL), which can converge to the near-optimal solution without knowledge of network quality. Experiments on a small-scale testbed and large-scale simulations are conducted to evaluate our proposed algorithm. The results show that it outperforms the start-of-the-art by 40% at most.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139873","federated learning;experience-driven;deep reinforcement learning","Mobile handsets;Training;Machine learning;Data models;Servers;Computational modeling;Bandwidth","data privacy;learning (artificial intelligence);microprocessor chips;mobile computing;optimisation;resource allocation","energy consumption;training time;weighted sum;optimization problem;training group;CPU-cycle frequency;privacy concerns;large-scale machine learning;experience-driven computational resource allocation;federated learning speed;resource-constrained mobile devices;energy efficiency;massive mobile devices;Deep Reinforcement Learning","",50.0,"",52.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"An Active Learning Method for Empirical Modeling in Performance Tuning","J. Zhang; J. Sun; W. Zhou; G. Sun","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","244","253","Tuning performance of scientific applications is a challenging problem since performance can be a complicated nonlinear function with respect to application parameters. Empirical performance modeling is a useful approach to approximate the function and enable efficient heuristic methods to find sub-optimal parameter configurations. However, empirical performance modeling requires a large number of samples from the parameter space, which is resource and time-consuming. To address this issue, existing work based on active learning techniques proposed PBU Sampling method considering performance before uncertainty, which iteratively performs performance biased sampling to model the high-performance subspace instead of the entire space before evaluating the most uncertain samples to reduce redundancy. Compared with uniformly random sampling, this approach can reduce the number of samples, but it still involves redundant sampling that potentially can be improved.We propose a novel active learning based method to exploit the information of evaluated samples and explore possible high-performance parameter configurations. Specifically, we adopt a Performance Weighted Uncertainty (PWU) sampling strategy to identify the configurations with either high performance or high uncertainty and determine which ones are selected for evaluation. To evaluate the effectiveness of our proposed method, we construct random forest to predict the execution time of kernels from SPAPT suite and two typical scientific parallel applications kripke, hypre. Experimental results show that compared with existing methods, our proposed method can reduce the cost of modeling by up to 21x and 3x on average meanwhile hold the same prediction accuracy.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139798","Performance Modeling;Active Learning;Sampling Strategy;Machine Learning","Training;Forestry;Uncertainty;Vegetation;Redundancy;Data models;Learning systems","learning (artificial intelligence);sampling methods","complicated nonlinear function;application parameters;empirical performance modeling;efficient heuristic methods;sub-optimal parameter configurations;parameter space;active learning techniques;high-performance subspace;uncertain samples;uniformly random sampling;redundant sampling;evaluated samples;scientific parallel applications;active learning method;empirical modeling;performance tuning;scientific applications;high-performance parameter configurations;performance weighted uncertainty sampling strategy;PBU sampling method;SPAPT","",2.0,"",22.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"DASSA: Parallel DAS Data Storage and Analysis for Subsurface Event Detection","B. Dong; V. R. Tribaldos; X. Xing; S. Byna; J. Ajo-Franklin; K. Wu","Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","254","263","Recently developed distributed acoustic sensing (DAS) technologies convert fiber-optic cables into large arrays of subsurface sensors, enabling a variety of applications including earthquake detection and environmental characterization. However, DAS systems produce voluminous datasets sampled at high spatial-temporal resolution and consequently, discovering useful geophysical knowledge within these large-scale data becomes a nearly impossible task for geophysicists. It is appealing to use supercomputers for DAS data analysis, as modern supercomputers are capable of performing over a hundred quadrillion FLOPS operations and have access to exabytes of storage space. Unfortunately, the majority of geophysical data processing libraries are not geared towards these supercomputer environments. This paper introduces a parallel DAS Data Storage and Analysis (DASSA) framework to enable easy-to-use and parallel DAS data analysis on modern supercomputers. DASSA uses a hybrid (i.e., MPI and OpenMP) data analysis execution engine that supports a user-defined function (UDF) interface for various operations and automatically parallelizes them for supercomputer execution. DASSA also provides novel data storage and access strategies, such as communication-avoiding parallel I/O, to reduce the cost of retrieving large DAS data for analysis. Compared with existing data analysis pipelines used by the geophysical community, DASSA is 16× faster and can efficiently scale up to 1456 computing nodes with 11648 CPU cores.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139824","","Arrays;Data analysis;Engines;Memory;Sensors;Data models;Supercomputers","acoustic signal processing;data analysis;distributed sensors;geophysical signal processing;mainframes;optical cables;parallel machines;storage management","user-defined function interface;parallel DAS data storage and analysis;distributed acoustic sensing;geophysical data processing libraries;storage space;large-scale data;geophysical knowledge;DAS systems;subsurface sensors;fiber-optic cables;subsurface event detection;data analysis pipelines;supercomputer execution;DASSA","",4.0,"",34.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Scaling of Union of Intersections for Inference of Granger Causal Networks from Observational Data","M. Balasubramanian; T. D. Ruiz; B. Cook; M. Prabhat; S. Bhattacharyya; A. Shrivastava; K. E. Bouchard","Arizona State University, Arizona, USA; Oregon State University, Oregon, USA; Lawrence Berkeley National Laboratory, Calfornia, USA; Lawrence Berkeley National Laboratory, Calfornia, USA; Oregon State University, Oregon, USA; Arizona State University, Arizona, USA; Lawrence Berkeley National Laboratory, Calfornia, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","264","273","The development of advanced recording and measurement devices in scientific fields is producing high-dimensional time series data. Vector autoregressive (VAR) models are well suited for inferring Granger-causal networks from high dimensional time series data sets, but accurate inference at scale remains a central challenge. We have recently introduced a flexible and scalable statistical machine learning framework, Union of Intersections (UoI), which enables low false-positive and low false-negative feature selection along with low bias and low variance estimation, enhancing interpretation and predictive accuracy. In this paper, we scale the UoI framework for VAR models (algorithm UoIV AR) to infer network connectivity from large time series data sets (TBs). To achieve this, we optimize distributed convex optimization and introduce novel strategies for improved data read and data distribution times. We study the strong and weak scaling of the algorithm on a Xeon-phi based supercomputer (100,000 cores). These advances enable us to estimate the largest VAR model as known (1000 nodes, corresponding to 1M parameters) and apply it to large time series data from neurophysiology (192 neurons) and finance (470 companies).","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139820","","Mathematical model;Computational modeling;Time series analysis;Data models;Convex functions;Estimation;Reactive power","autoregressive processes;learning (artificial intelligence);mainframes;parallel machines;time series","VAR models;UoI;network connectivity;data distribution times;strong scaling;weak scaling;Granger causal networks;observational data;measurement devices;high dimensional time series data sets;accurate inference;flexible machine learning framework;scalable statistical machine learning framework;false-negative feature selection;low variance estimation;UoI framework;Granger-causal networks;low bias estimation","",1.0,"",22.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"GPU-Based Static Data-Flow Analysis for Fast and Scalable Android App Vetting","X. Yu; F. Wei; X. Ou; M. Becchi; T. Bicer; D. Yao","Dept. of Computer Science, Virginia Tech, Blacksburg, VA; Dept. of Computer Science and Engineering, University of South Florida, Tampa, FL; Dept. of Computer Science and Engineering, University of South Florida, Tampa, FL; Dept. of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC; Data Science and Learning Division, Argonne National Laboratory, Lemont, IL; Dept. of Computer Science, Virginia Tech, Blacksburg, VA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","274","284","Many popular vetting tools for Android applications use static code analysis techniques. In particular, Interprocedural Data-Flow Graph (IDFG) construction is the computation at the core of Android static data-flow analysis and consumes most of the analysis time. Many analysis tools use a worklist algorithm, an iterative fixed-point approach, to construct the IDFG. In this paper, we observe that a straightforward GPU parallelization of the worklist algorithm leads to significant underutilization of the GPU resources. We identify four performance bottlenecks, namely, frequent dynamic memory allocations, high branch divergence, workload imbalance, and irregular memory access patterns. Accordingly, we propose GDroid, a GPU-based worklist algorithm implementation with multiple fine-grained optimizations tailored to common characteristics of Android applications. The optimizations considered are: matrix-based data structure, memory access-based node grouping, and worklist merging. Our experimental evaluation, performed on 1000 Android applications, shows that the proposed optimizations are beneficial to performance, and GDroid can achieve up to 128X speedups against a plain GPU implementation.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139793","GPU;static program analysis;data-flow analysis;Android security;mobile application vetting;worklist algorithm;application-specific optimization","Graphics processing units;Androids;Humanoid robots;Optimization;Smart phones;Tools;Security","Android (operating system);data flow analysis;data flow graphs;data structures;fixed point arithmetic;graphics processing units;iterative methods;matrix algebra;mobile computing;optimisation;parallel processing;resource allocation;storage management","static code analysis;IDFG;Android static data-flow analysis;iterative fixed-point approach;GPU parallelization;GPU resources;memory access patterns;memory access-based node grouping;worklist merging;Android applications;GPU-based worklist algorithm;dynamic memory allocations;interprocedural data-flow graph construction;vetting tools;GPU-based static data-flow analysis;GDroid;matrix-based data structure","",4.0,"",51.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Robust Server Placement for Edge Computing","D. Lu; Y. Qu; F. Wu; H. Dai; C. Dong; G. Chen","Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Technology, Nanjing University, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","285","294","In this work, we study the problem of Robust Server Placement (RSP) for edge computing, i.e., in the presence of uncertain edge server failures, how to determine a server placement strategy to maximize the expected overall workload that can be served by edge servers. We mathematically formulate the RSP problem in the form of robust max-min optimization, derived from two consequentially equivalent transformations of the problem that does not consider robustness and followed by a robust conversion. RSP is challenging to solve, because the explicit expression of the objective function in RSP is hard to obtain, and RSP is a robust max-min problem with a matroid constraint and a knapsack constraint, which is still an unexplored problem in the literature. To address the above challenges, we first investigate the special properties of the problem, and reveal that the objective function is monotone submodular. We then prove that the involved constraints form a p-independence system constraint, where p is a constant value related to the ratio of the coefficients in the knapsack constraint. Finally, we propose an algorithm that achieves a provable constant approximation ratio in polynomial time. Both synthetic and trace-driven simulation results show that, given any maximum number of server failures, our proposed algorithm outperforms three state-of-the-art algorithms and approaches the optimal solution, which applies exhaustive exponential searches.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139826","","Servers;Robustness;Optimization;Approximation algorithms;Linear programming;Resource management;Delays","combinatorial mathematics;computational complexity;distributed processing;knapsack problems;matrix algebra;minimax techniques;network servers","objective function;robust max-min problem;matroid constraint;knapsack constraint;unexplored problem;involved constraints form;p-independence system constraint;Robust Server Placement;edge computing;uncertain edge server failures;server placement strategy;edge servers;RSP problem;robust max-min optimization;robust conversion","",11.0,"",41.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"EdgeIso: Effective Performance Isolation for Edge Devices","Y. Nam; Y. Choi; B. Yoo; H. Eom; Y. Son","Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea; School of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","295","305","Edges enable cloud services to be provided at low-latency and efficiently reduce the amount of transferred data by placing latency-critical tasks close to users. However, multi-tasking results in resource contention on edge devices, making it challenging to meet the service level objectives (SLOs) of tasks. Compared to the clouds, edges have relatively limited resources, but their tasks are required to meet a higher level of SLOs than clouds. Furthermore, modern edge devices equipped with additional accelerators (e.g., GPU) may worsen the resource contention due to the edge's integrated architecture, sharing the memory bandwidth between CPUs and accelerators. To address these challenges, we present EdgeIso, a light-weight scheduler that dynamically isolates the performance of tasks on edges. EdgeIso periodically monitors the resource contention and mitigates the contention to meet the SLOs of tasks by efficiently enforcing several isolation techniques (e.g., DVFS and core allocation) in an incremental manner. Moreover, it detects the changes of task executions or offered loads for tasks, thus handling high load fluctuations adaptively. We implement EdgeIso as a user-level scheduler on the Linux integrates into an NVIDIA Jetson TX2. Our experimental results show that EdgeIso improves the performance of the low-latency tasks significantly while improving resource efficiency compared with both the offloading and reservation scheme used in clouds.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139806","edge computing;resource management;performance isolation;latency-critical workloads","Task analysis;Image edge detection;Graphics processing units;Cloud computing;Bandwidth;Resource management;Performance evaluation","cloud computing;Linux;power aware computing;processor scheduling;resource allocation","EdgeIso;cloud services;latency-critical tasks;multitasking results;resource contention;service level objectives;edge devices;accelerators;light-weight scheduler;isolation techniques;task executions;user-level scheduler;resource efficiency;memory bandwidth;CPUs;load fluctuations;Linux;NVIDIA Jetson TX2","",1.0,"",36.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Busy-Time Scheduling on Heterogeneous Machines","R. Ren; X. Tang","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","306","315","We study a busy-time scheduling problem on heterogeneous machines (BSHM) which is motivated by server acquisition and task dispatching in cloud computing. The input of BSHM is a set of interval jobs, each specified by a size, an arrival time and a departure time. When a job arrives, it must be placed onto a machine immediately. The execution of a job cannot be interrupted until it departs. At any time, the total size of the jobs running on a machine cannot exceed the machine's capacity. m different types of machines are available and abundant machines are provided for each type. A type-i machine has a capacity gi and is charged at a cost rate ri when busy (running jobs). The target of BSHM is to schedule the given set of jobs onto machines with the minimum accumulated cost. Suppose the machine types are sorted by their capacities so that g1 g2 ≤ ⋯ ≤ gm. We first consider two typical cases of BSHM. In BSHM-DEC, ri/gi ≥ ri+1/gi+1 holds for each i. In BSHM-INC, ri/gi ≤ ri+1/gi+1 holds for each i. For each case, we propose a O(1)-approximation algorithm in the offline setting and a O(μ)-competitive algorithm in the nonclairvoyant online setting. Finally, we discuss how the scheduling strategies developed for these two cases can be combined to deal with the general BSHM problem.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139850","","Approximation algorithms;Cloud computing;Partitioning algorithms;Strips;Schedules;Processor scheduling;Computer science","approximation theory;cloud computing;computational complexity;file servers;scheduling","heterogeneous machines;busy-time scheduling problem;server acquisition;task dispatching;BSHM-DEC;BSHM-INC;cloud computing;O(1)-approximation algorithm;O(μ)-competitive algorithm","",4.0,"",16.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Scheduling Malleable Jobs Under Topological Constraints","E. Bampis; K. Dogeas; A. Kononov; G. Lucarelli; F. Pascual","Sorbonne Université, CNRS, Paris, France; Sorbonne Université, CNRS, Paris, France; Sobolev Institute of Mathematics, Novosibirsk State University, Novosibirsk, Russia; University of Lorraine, LCOMS, Metz, France; Sorbonne Université, CNRS, Paris, France","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","316","325","Bleuse et al. (EuroPar 2018) introduced a general model for interference-aware scheduling in large scale parallel platforms. They considered two different types of communications: the flows induced by data exchanges during computations and the flows related to Input/Output operations. Rather than taking into account these communications explicitly, they restrict the possible allocations of a job by external topological constraints. In their work, jobs are considered to be rigid: a job requires a specific number of machines in order to be executed. Here, we first adopt the same framework for the platform and the aforementioned topological constraints. We show that there is no polynomial time approximation algorithm under the rigid setting with ratio smaller than 3/2, unless P = NP. Then, we focus on the malleable setting. We show that in the proportional-malleable setting, where the work of every job remains constant independently of the number of machines on which it is executed, the scheduling problem remains NPhard even in the uniform case, where the maximum number of machines is the same for all the jobs. Then, we propose a 2-approximation algorithm for this case. Furthermore, we present an approximation algorithm solving the more general case where the maximum number of machines is job-dependent and the work of the jobs is increasing with respect to the number of used machines, due to the communication overhead.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139853","Approximation algorithms;communications;Input/Output;malleable jobs","Approximation algorithms;Computational modeling;Resource management;Topology;Processor scheduling;Complexity theory;Network topology","approximation theory;computational complexity;input-output programs;optimisation;parallel processing;processor scheduling;resource allocation","scheduling malleable jobs;interference-aware scheduling;topological constraints;polynomial time approximation algorithm;proportional-malleable setting;2-approximation algorithm;large scale parallel platforms;input/output operations;NP-hard problem","",1.0,"",17.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"XSP: Across-Stack Profiling and Analysis of Machine Learning Models on GPUs","C. Li; A. Dakkak; J. Xiong; W. Wei; L. Xu; W. -m. Hwu","University of Illinois Urbana-Champaign; University of Illinois Urbana-Champaign; IBM T. J. Watson Research Center; Alibaba Group; Alibaba Group; University of Illinois Urbana-Champaign","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","326","327","There has been a rapid proliferation of machine learning/deep learning (ML) models and wide adoption of them in many application domains. This has made profiling and characterization of ML model performance an increasingly pressing task for both hardware designers and system providers, as they would like to offer the best possible system to serve ML models with the target latency, throughput, cost, and energy requirements while maximizing resource utilization. Such an endeavor is challenging as the characteristics of an ML model depend on the interplay between the model, framework, system libraries, and the hardware (or the HW/SW stack). Existing profiling tools are disjoint, however, and only focus on profiling within a particular level of the stack, which limits the thoroughness and usefulness of the profiling results.This paper proposes XSP - an across-stack profiling design that gives a holistic and hierarchical view of ML model execution. XSP leverages distributed tracing to aggregate and correlate profile data from different sources. XSP introduces a leveled and iterative measurement approach that accurately captures the latencies at all levels of the HW/SW stack in spite of the profiling overhead. We couple the profiling design with an automated analysis pipeline to systematically analyze 65 state-of-the-art ML models. We demonstrate that XSP provides insights which would be difficult to discern otherwise.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139875","","Graphics processing units;Kernel;Predictive models;Hardware;Tools;Pipelines;Analytical models","graphics processing units;hardware-software codesign;learning (artificial intelligence)","machine learning models;application domains;ML model performance;system libraries;across-stack profiling design;aggregate profile data;correlate profile data;profiling overhead;XSP;GPU","",15.0,"",27.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Exploring the Binary Precision Capabilities of Tensor Cores for Epistasis Detection","R. Nobre; A. Ilic; S. Santander-Jiménez; L. Sousa","INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","338","347","Genome-wide association studies are performed to correlate a number of diseases and other physical or even psychological conditions (phenotype) with substitutions of nucleotides at specific positions in the human genome, mainly single-nucleotide polymorphisms (SNPs). Some conditions, possibly because of the complexity of the mechanisms that give rise to them, have been identified to be more statistically correlated with genotype when multiple SNPs are jointly taken into account. However, the discovery of new associations between genotype and phenotype is exponentially slowed down by the increase of computational power required when epistasis, i.e., interactions between SNPs, is considered. This paper proposes a novel graphics processing unit (GPU)-based approach for epistasis detection that combines the use of modern tensor cores with native support for processing binarized inputs with algorithmic and target-focused optimizations. Using only a single mid-range Turing-based GPU, the proposed approach is able to evaluate 64.8×1012 and 25.4×1012 sets of SNPs per second, normalized to the number of patients, when considering 2-way and 3-way epistasis detection, respectively. This proposal is able to surpass the state-of-the-art approach by 6× and 8.2× in terms of the number of pairs and triplets of SNP allelic patient data evaluated per unit of time per GPU.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139823","GWAS;two- and three-way epistasis;performance evaluation","Tensile stress;Graphics processing units;Acceleration;Sociology;Statistics;Diseases;Optimization","diseases;genetics;genomics;graphics processing units;medical computing;optimisation;polymorphism;tensors;Turing machines","graphics processing unit;epistasis detection;tensor cores;single mid-range Turing-based GPU;binary precision capabilities;genome-wide association studies;phenotype;human genome;single-nucleotide polymorphisms;genotype;multiple SNPs;diseases;psychological conditions;nucleotides;optimizations","",7.0,"",28.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Understanding and Improving Persistent Transactions on Optane™ DC Memory","P. Zardoshti; M. Spear; A. Vosoughi; G. Swart","Lehigh University, USA; Lehigh University, USA; Oracle Corp., USA; Oracle Corp., USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","348","357","Storing data structures in high-capacity byte-addressable persistent memory instead of DRAM or a storage device offers the opportunity to (1) reduce cost and power consumption compared with DRAM, (2) decrease the latency and CPU resources needed for an I/O operation compared with storage, and (3) allow for fast recovery as the data structure remains in memory after a machine failure. The first commercial offering in this space is Intel® Optane™ Direct Connect (Optane™ DC) Persistent Memory. Optane™ DC promises access time within a constant factor of DRAM, with larger capacity, lower energy consumption, and persistence. We present an experimental evaluation of persistent transactional memory performance, and explore how Optane™ DC durability domains affect the overall results. Given that neither of the two available durability domains can deliver performance competitive with DRAM, we introduce and emulate a new durability domain, called PDRAM, in which the memory controller tracks enough information (and has enough reserve power) to make DRAM behave like a persistent cache of Optane™ DC memory.In this paper we compare the performance of these durability domains on several configurations of five persistent transactional memory applications. We find a large throughput difference, which emphasizes the importance of choosing the best durability domain for each application and system. At the same time, our results confirm that recently published persistent transactional memory algorithms are able to scale, and that recent optimizations for these algorithms lead to strong performance, with speedups as high as 6× at 16 threads.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139799","Persistent Memory;Non-Volatile Memory;Transactional Memory;Storage;Concurrency;Optane™","Random access memory;Nonvolatile memory;Data structures;Instruction sets;Memory management;Power demand;Throughput","cache storage;computational complexity;data structures;DRAM chips;durability;low-power electronics;storage management;transaction processing","understanding transactions;persistent transactions;data structure;high-capacity byte-addressable persistent memory;DRAM;storage device;power consumption;lower energy consumption;persistent transactional memory performance;durability domain;memory controller;persistent cache;persistent transactional memory applications;recently published persistent transactional memory algorithms;Optane DC Memory;Intel Optane Direct Connect Persistent Memory;durability domains;data structures;power consumption reduction","",7.0,"",46.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"CycLedger: A Scalable and Secure Parallel Protocol for Distributed Ledger via Sharding","M. Zhang; J. Li; Z. Chen; H. Chen; X. Deng","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","358","367","Traditional public distributed ledgers have not been able to scale-out well and work efficiently. Sharding is deemed as a promising way to solve this problem. By partitioning all nodes into small committees and letting them work in parallel, we can significantly lower the amount of communication and computation, reduce the overhead on each node’s storage, as well as enhance the throughput of the distributed ledger. Existing sharding-based protocols still suffer from several serious drawbacks. The first thing is that all non-faulty nodes must connect well with each other, which demands a huge number of communication channels in the network. Moreover, previous protocols have faced great loss in efficiency in the case where the honesty of each committee’s leader is in question. At the same time, no explicit incentive is provided for nodes to actively participate in the protocol.We present CycLedger, a scalable and secure parallel protocol for distributed ledger via sharding. Our protocol selects a leader and a partial set for each committee, who are in charge of maintaining intra-shard consensus and communicating with other committees, to reduce the amortized complexity of communication, computation, and storage on all nodes. We introduce a novel semi-commitment scheme between committees and a recovery procedure to prevent the system from crashing even when leaders of committees are malicious. To add incentive for the network, we use the concept of reputation, which measures each node’s trusty computing power. As nodes with a higher reputation receive more rewards, there is an encouragement for nodes with strong computing ability to work honestly to gain reputation. In this way, we strike out a new path to establish scalability, security, and incentive for the sharding-based distributed ledger.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139886","Distributed Ledger;Sharding;Scalability;Security;Reputation","Protocols;Peer-to-peer computing;Scalability;Bitcoin","computer network security;cryptographic protocols;peer-to-peer computing","intrashard consensus;sharding-based protocols;scalable protocol;communication channels;nonfaulty nodes;public distributed ledgers;CycLedger;sharding-based distributed ledger;secure parallel protocol","",11.0,"",28.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Mitigating Large Response Time Fluctuations through Fast Concurrency Adapting in Clouds","J. Liu; S. Zhang; Q. Wang; J. Wei","Louisiana State University, Baton Rouge; Louisiana State University, Baton Rouge; Louisiana State University, Baton Rouge; University of North Carolina, Charlotte","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","368","377","Dynamically reallocating computing resources to handle bursty workloads is a common practice for web applications (e.g., e-commerce) in clouds. However, our empirical analysis on a standard n-tier benchmark application (RUBBoS) shows that simply scaling an n-tier application by reallocating hardware resources without fast adapting soft resources (e.g., server threads, connections) may lead to large response time fluctuations. This is because soft resources control the workload concurrency of component servers in the system: adding or removing hardware resources such as Virtual Machines (VMs) can implicitly change the workload concurrency of dependent servers, causing either under- or over-utilization of the critical hardware resource in the system. To quickly identify the optimal soft resource allocation of each server in the system and stabilize response time fluctuation, we propose a novel Scatter-Concurrency-Throughput (SCT) model based on the monitoring of each server's real-time concurrency and throughput. We then implement a Concurrency-aware system Scaling (ConScale) framework which integrates the SCT model to fast adapt the soft resource allocations of key servers during the system scaling process. Our experiments using six realistic bursty workload traces show that ConScale can effectively mitigate the response time fluctuations of the target web application compared to the state-of-the-art cloud scaling strategies such as EC2-AutoScaling.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139832","scalability;soft resources;web applications","Servers;Concurrent computing;Hardware;Throughput;Time factors;Resource management;Adaptation models","cloud computing;concurrency control;resource allocation;virtual machines","ConScale framework;SCT model;concurrency-aware system scaling framework;scatter-concurrency-throughput model;state-of-the-art cloud scaling strategies;Web application;realistic bursty workload traces;system scaling process;key servers;soft resource allocations;real-time concurrency;optimal soft resource allocation;critical hardware resource;dependent servers;component servers;workload concurrency;server threads;soft resources;hardware resources;n-tier application;standard n-tier benchmark application;Web applications;bursty workloads;computing resources;fast concurrency;response time fluctuation","","","",30.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"DAG-Aware Joint Task Scheduling and Cache Management in Spark Clusters","Y. Xu; L. Liu; Z. Ding","The Key Lab of Embedded System and Service Computing, Tongji University, Shanghai, China; The Key Lab of Embedded System and Service Computing, Tongji University, Shanghai, China; The Key Lab of Embedded System and Service Computing, Tongji University, Shanghai, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","378","387","Data dependency, often presented as directed acyclic graph (DAG), is a crucial application semantics for the performance of data analytic platforms such as Spark. Spark comes with two built-in schedulers, namely FIFO and Fair scheduler, which do not take advantage of data dependency structures. Recently proposed DAG-aware task scheduling approaches, notably GRAPHENE, have achieved significant performance improvements but paid little attention to cache management. The resulted data access patterns interact poorly with the built-in LRU caching, leading to significant cache misses and performance degradation. On the other hand, DAG-aware caching schemes, such as Most Reference Distance (MRD), are designed for FIFO scheduler instead of DAG-aware task schedulers.In this paper, we propose and develop a middleware Dagon, which leverages the complexity and heterogeneity of DAGs to jointly execute task scheduling and cache management. Dagon relies on three key mechanisms: DAG-aware task assignment that considers dependency structure and heterogeneous resource demands to reduce potential resource fragmentation, sensitivity-aware delay scheduling that prevents executors from long waiting for tasks insensitive to locality, and priority-aware caching that makes the cache eviction and prefetching decisions based on the stage priority determined by DAG-aware task assignment. We have implemented Dagon in Apache Spark. Evaluation on a testbed shows that Dagon improves the job completion time by up to 42% and CPU utilization by up to 46% respectively, compared to GRAPHENE plus MRD.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139887","","Task analysis;Delays;Sparks;Data analysis;Graphene;Complexity theory;Sensitivity","cache storage;cluster computing;data analysis;directed graphs;middleware;parallel processing;resource allocation;scheduling;storage management","DAG-aware joint task scheduling;cache management;Spark clusters;directed acyclic graph;data analytic platforms;performance degradation;DAG-aware caching schemes;FIFO scheduler;DAG-aware task schedulers;DAG-aware task assignment;sensitivity-aware delay scheduling;priority-aware caching;Apache Spark;resource fragmentation;Dagon middleware;Fair scheduler;data dependency","",5.0,"",22.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Solving the Container Explosion Problem for Distributed High Throughput Computing","T. Shaffer; N. Hazekamp; J. Blomer; D. Thain","University of Notre Dame; University of Notre Dame; European Laboratory for Particle Physics (CERN); University of Notre Dame","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","388","398","Container technologies are seeing wider use at advanced computing facilities for managing highly complex applications that must execute at multiple sites. However, in a distributed high throughput computing setting, the unrestricted use of containers can result in the container explosion problem. If a new container image is generated for each variation of a job dispatched to a site, shared storage is soon exceeded. On the other hand, if a single large container image is used to meet multiple needs, the size of that container may become a problem for storage and transport. To address this problem, we observe that many containers have an internal structure generated by a structured package manager, and this information could be used to strategically combine and share container images. We develop Landlord to exploit this property and evaluate its performance through a combination of simulation studies and empirical measurement of high energy physics applications.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139776","","Containers;Software;Explosions;Large Hadron Collider;Throughput;Computational modeling;High energy physics","grid computing;high energy physics instrumentation computing;software packages","Landlord;structured package manager;internal structure;distributed high throughput computing;container explosion problem;high energy physics applications;container image","",1.0,"",27.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Amoeba: QoS-Awareness and Reduced Resource Usage of Microservices with Serverless Computing","Z. Li; Q. Chen; S. Xue; T. Ma; Y. Yang; Z. Song; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Alibaba Cloud, China; Alibaba Cloud, China; Alibaba Cloud, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","399","408","While microservices that have stringent Quality-of-Service constraints are deployed in the Clouds, the long-term rented infrastructures that host the microservices are under-utilized except peak hours due to the diurnal load pattern. It is resource efficient for Cloud vendors and cost efficient for service maintainers to deploy the microservices in the long-term infrastructure at high load and in the serverless computing platform at low load. However, prior work fails to take advantage of the opportunity, because the contention between microservices on the serverless platform seriously affects their response latencies.Our investigation shows that the load of a microservice, the shared resource contentions on the serverless platform, and its sensitivities to the contention together affect the response latency of the microservice on the platform. To this end, we propose Amoeba, a runtime system that dynamically switches the deployment of a microservice. Amoeba is comprised of a contention-aware deployment controller, a hybrid execution engine, and a multi-resource contention monitor. The deployment controller predicts the tail latency of a microservice based on its load and the contention on the serverless platform, and determines the appropriate deployment of the microservice. The hybrid execution engine enables the quick switch of the two deploy modes. The contention monitor periodically quantifies the contention on multiple types of shared resources. Experimental results show that Amoeba is able to significantly reduce up to 72.9% of CPU usage and up to 84.9% of memory usage compared with the traditional pure IaaS-based deployment, while ensuring the required latency target.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139803","Serverless computing;Microservices;QoS","Quality of service;Benchmark testing;Containers;Cloud computing;Servers;Sensitivity;Runtime","cloud computing;quality of service;resource allocation;service-oriented architecture","hybrid execution engine;runtime system;service maintainers;cloud vendors;quality-of-service constraints;reduced resource usage;QoS-awareness;shared resource contentions;serverless computing platform;Amoeba;microservice;multiresource contention monitor;contention-aware deployment controller","",14.0,"",31.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Efficient I/O for Neural Network Training with Compressed Data","Z. Zhang; L. Huang; J. G. Pauloski; I. T. Foster","Texas Advanced Computing Center; Texas Advanced Computing Center; University of Texas at Austin; Argonne National Laboratory, University of Chicago","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","409","418","FanStore is a shared object store that enables efficient and scalable neural network training on supercomputers. By providing a global cache layer on node-local burst buffers using a compressed representation, it significantly enhances the processing capability of deep learning (DL) applications on existing hardware. In addition, FanStore allows POSIX-compliant file access to the compressed data in user space. We investigate the tradeoff between runtime overhead and data compression ratio using real-world datasets and applications, and propose a compressor selection algorithm to maximize storage capacity given performance constraints. We consider both asynchronous (i.e., with prefetching) and synchronous I/O strategies, and propose mechanisms for selecting compressors for both approaches. Using FanStore, the same storage hardware can host 2–13× more data for example applications without significant runtime overhead. Empirically, our experiments show that FanStore scales to 512 compute nodes with near linear performance scalability.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139800","","Training;Compressors;Metadata;Program processors;Hardware;Training data;Neural networks","cache storage;data compression;learning (artificial intelligence);neural nets;parallel machines;storage management;Unix","performance constraints;storage capacity;node-local burst buffers;global cache layer;shared object store;neural network training;linear performance scalability;FanStore scales;runtime overhead;storage hardware;compressor selection algorithm;data compression ratio;user space;compressed data;POSIX-compliant file access;deep learning applications;compressed representation","",3.0,"",45.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Not All Explorations Are Equal: Harnessing Heterogeneous Profiling Cost for Efficient MLaaS Training","J. Yi; C. Zhang; W. Wang; C. Li; F. Yan","University of Nevada, Reno; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; University of Science and Technology of China; University of Nevada, Reno","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","419","428","Machine-Learning-as-a-Service (MLaaS) enables practitioners and AI service providers to train and deploy ML models in the cloud using diverse and scalable compute resources. A common problem for MLaaS users is to choose from a variety of training deployment options, notably scale-up (using more capable instances) and scale-out (using more instances), subject to the budget limits and/or time constraints. State-of-the-art (SOTA) approaches employ analytical modeling for finding the optimal deployment strategy. However, they have limited applicability as they must be tailored to specific ML model architectures, training framework, and hardware. To quickly adapt to the fast evolving design of ML models and hardware infrastructure, we propose a new Bayesian Optimization (BO) based method HeterBO for exploring the optimal deployment of training jobs. Unlike the existing BO approaches for general applications, we consider the heterogeneous exploration cost and machine learning specific prior to significantly improve the search efficiency. This paper culminates in a fully automated MLaaS training Cloud Deployment system (MLCD) driven by the highly efficient HeterBO search method. We have extensively evaluated MLCD in AWS EC2, and the experimental results show that MLCD outperforms two SOTA baselines, conventional BO and CherryPick, by 3.1× and 2.34×, respectively.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139864","","Training;Analytical models;Computational modeling;Optimization;Graphics processing units;Computer architecture;Hardware","Bayes methods;cloud computing;learning (artificial intelligence);optimisation;search problems;software cost estimation","MLaaS training cloud deployment system;ML model architectures;heterogeneous profiling cost;cloud deployment system;HeterBO search method;heterogeneous exploration cost;Bayesian Optimization;AI service providers;Machine-Learning-as-a-Service;MLaaS training","",2.0,"",44.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"ASYNC: A Cloud Engine with Asynchrony and History for Distributed Machine Learning","S. Soori; B. Can; M. Gurbuzbalaban; M. M. Dehnavi","Department of Computer Science, University of Toronto; Department of MSIS, Rutgers University; Department of MSIS, Rutgers University; Department of Computer Science, University of Toronto","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","429","439","ASYNC is a framework that supports the implementation of asynchrony and history for optimization methods on distributed computing platforms. The popularity of asynchronous optimization methods has increased in distributed machine learning. However, their applicability and practical experimentation on distributed systems are limited because current bulk-processing cloud engines do not provide a robust support for asynchrony and history. With introducing three main modules and bookkeeping system-specific and application parameters, ASYNC provides practitioners with a framework to implement asynchronous machine learning methods. To demonstrate ease-of-implementation in ASYNC, the synchronous and asynchronous variants of two well-known optimization methods, stochastic gradient descent and SAGA, are demonstrated in ASYNC.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139871","Machine learning;cloud computing","History;Computational modeling;Task analysis;Optimization methods;Engines;Machine learning;Sparks","gradient methods;grid computing;learning (artificial intelligence);optimisation;stochastic processes","ASYNC;cloud engine;distributed machine learning;distributed computing platforms;asynchronous optimization methods;robust support;bookkeeping system;asynchronous machine learning methods;synchronous variants;asynchronous variants;bulk-processing cloud engines","",2.0,"",38.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Benanza: Automatic μBenchmark Generation to Compute ""Lower-bound"" Latency and Inform Optimizations of Deep Learning Models on GPUs","C. Li; A. Dakkak; J. Xiong; W. -m. Hwu","University of Illinois Urbana-Champaign, Urbana, USA; University of Illinois Urbana-Champaign, Urbana, USA; IBM T. J. Watson Research Center, Yorktown Heights, USA; University of Illinois Urbana-Champaign, Urbana, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","440","450","As Deep Learning (DL) models have been increasingly used in latency-sensitive applications, there has been a growing interest in improving their response time. An important venue for such improvement is to profile the execution of these models and characterize their performance to identify possible optimization opportunities. However, the current profiling tools lack the highly desired abilities to characterize ideal performance, identify sources of inefficiency, and quantify the benefits of potential optimizations. Such deficiencies have led to slow characterization/optimization cycles that cannot keep up with the fast pace at which new DL models are introduced.We propose Benanza, a sustainable and extensible benchmarking and analysis design that speeds up the characterization/optimization cycle of DL models on GPUs. Benanza consists of four major components: a model processor that parses models into an internal representation, a configurable benchmark generator that automatically generates micro-benchmarks given a set of models, a database of benchmark results, and an analyzer that computes the ""lower-bound"" latency of DL models using the benchmark data and informs optimizations of model execution. The ""lower-bound"" latency metric estimates the ideal model execution on a GPU system and serves as the basis for identifying optimization opportunities in frameworks or system libraries. We used Benanza to evaluate 30 ONNX models in MXNet, ONNX Runtime, and PyTorch on 7 GPUs ranging from Kepler to the latest Turing, and identified optimizations in parallel layer execution, cuDNN convolution algorithm selection, framework inefficiency, layer fusion, and using Tensor Cores.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139782","","Computational modeling;Integrated circuits;Benchmark testing;Optimization;Graphics processing units;Tensile stress;Analytical models","convolutional neural nets;graphics processing units;learning (artificial intelligence);optimisation;synchronisation","Kepler;cuDNN convolution algorithm selection;parallel layer execution;framework inefficiency;layer fusion;tensor cores;PyTorch;ONNX runtime;configurable benchmark generator;DL models;profiling tools;latency-sensitive applications;Deep Learning models;automatic μBenchmark generation;GPU;ONNX models;Benanza;lower-bound latency metric estimates","",2.0,"",53.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Adaptive Page Migration for Irregular Data-intensive Applications under GPU Memory Oversubscription","D. Ganguly; Z. Zhang; J. Yang; R. Melhem","Department of Computer Science, University of Pittsburgh; Department of Computer Science, University of Pittsburgh; Department of Computer Science, University of Pittsburgh; Department of Computer Science, University of Pittsburgh","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","451","461","Unified Memory in heterogeneous systems serves a wide range of applications. However, limited capacity of the device memory becomes a first order performance bottleneck for data-intensive general-purpose applications with increasing working sets. The performance overhead under memory oversubscription depends on the memory access pattern of the corresponding workload. While a regular application with sequential, dense memory access suffers from long latency write-backs, performance of a irregular application with sparse, seldom access to large data-sets degrades due to page thrashing. Although smart spatio-temporal prefetching and large page eviction yield good performance in general, remote zero-copy access to host-pinned memory proves to be beneficial for irregular, data-intensive applications. Further, new generation GPUs introduced hardware access counters to delay page migration and reduce memory thrashing. However, the responsibility of deciding what strategy is the best fit for a given application relies heavily on the programmer based on thorough understanding of the memory access pattern through intrusive profiling. In this work, we propose a programmer-agnostic runtime that leverages the hardware access counters to automatically categorize memory allocations based on the access pattern and frequency. The proposed heuristic adaptively navigates between remote zero-copy access to host-pinned memory and first-touch page migration based on the trade-off between low latency remote access and high-bandwidth local access. We show that although designed to address memory oversubscription, our scheme has no impact on performance when working sets fit in the device-local memory. Experimental results show that our scheme provides performance improvement of 22% to 78% for irregular applications under 125% memory oversubscription compared to the state of the art. At the same time, regular applications are not impacted by the framework.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139797","page migration;pinning;memory management;CPU-GPU;Unified Memory","Prefetching;Graphics processing units;Resource management;Runtime;Kernel;Performance evaluation;Memory management","graphics processing units;storage management","adaptive page migration;memory access pattern;remote zero-copy access;host-pinned memory;gpu memory oversubscription;unified memory;heterogeneous systems;data-intensive general-purpose application;sequential memory access;dense memory access;smart spatio-temporal prefetching","",15.0,"",29.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"LOGAN: High-Performance GPU-Based X-Drop Long-Read Alignment","A. Zeni; G. Guidi; M. Ellis; N. Ding; M. D. Santambrogio; S. Hofmeyr; A. Buluç; L. Oliker; K. Yelick","Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA, USA; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milan, Italy; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA, USA; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Department of Electrical Engineering and Computer Science, University of California at Berkeley, Berkeley, CA, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","462","471","Pairwise sequence alignment is one of the most computationally intensive kernels in genomic data analysis, accounting for more than 90% of the runtime for key bioinformatics applications. This method is particularly expensive for third-generation sequences due to the high computational cost of analyzing sequences of length between 1Kb and 1Mb. Given the quadratic overhead of exact pairwise algorithms for long alignments, the community primarily relies on approximate algorithms that search only for high-quality alignments and stop early when one is not found. In this work, we present the first GPU optimization of the popular X-drop alignment algorithm, that we named LOGAN. Results show that our high-performance multi-GPU implementation achieves up to 181.6 GCUPS and speed-ups up to 6.6× and 30.7× using 1 and 6 NVIDIA Tesla V100, respectively, over the state-of-the-art software running on two IBM Power9 processors using 168 CPU threads, with equivalent accuracy. We also demonstrate a 2.3× LOGAN speed-up versus ksw2, a state-of-art vectorized algorithm for sequence alignment implemented in minimap2, a long-read mapping software. To highlight the impact of our work on a real-world application, we couple LOGAN with a many-to-many long-read alignment software called BELLA, and demonstrate that our implementation improves the overall BELLA runtime by up to 10.6×. Finally, we adapt the Roofline model for LOGAN and demonstrate that our implementation is near optimal on the NVIDIA Tesla V100s.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139808","","Graphics processing units;Software algorithms;Computer architecture;Acceleration;Heuristic algorithms;Parallel processing","bioinformatics;circuit optimisation;data analysis;genomics;graphics processing units","third-generation sequences;quadratic overhead;exact pairwise algorithms;high-quality alignments;GPU optimization;high-performance multiGPU implementation;GCUPS;NVIDIA Tesla V100;IBM Power9 processors;CPU threads;vectorized algorithm;long-read mapping software;couple LOGAN;BELLA runtime;pairwise sequence alignment;genomic data analysis;bioinformatics applications;computational cost;X-drop long-read alignment;LOGAN speed-up;many-to-many long-read alignment software;X-drop alignment algorithm;Roofline model","",14.0,"",24.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Coordinated Page Prefetch and Eviction for Memory Oversubscription Management in GPUs","Q. Yu; B. Childers; L. Huang; C. Qian; H. Guo; Z. Wang","School of Computer, National University of Defense Technology; University of Pittsburgh; School of Computer, National University of Defense Technology; School of Computer, National University of Defense Technology; School of Computer, National University of Defense Technology; School of Computer, National University of Defense Technology","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","472","482","The adoption of unified memory and demand paging has simplified programming and eased memory management in discrete GPUs. However, long-latency page faults cause significant performance overhead. While several software-based mechanisms have been proposed to address this issue, they suffer from inefficiency when page prefetching and pre-eviction are combined. For example, a state-of-the-art page replacement policy, hierarchical page eviction (HPE), is inefficient when prefetching is enabled. Furthermore, the prefetcher semantics-aware pre-evicting policy, which pre-evicts continuous pages in bulk the way they were brought in by the prefetcher, may cause thrashing for some irregular applications.In this paper, coordinated page prefetch and eviction (CPPE) is proposed to manage memory oversubscription in GPUs with unified memory. CPPE incorporates a modified page eviction policy, MHPE, and an access pattern-aware prefetcher in a fine-grained manner: MHPE is aware of prefetch semantics and the prefetcher prefetches pages according to access patterns in eviction candidates selected by MHPE. Simulation results show that, when the GPU memory is 75% and 50% oversubscribed, CPPE achieves an average speedup of 1.56x and 1.64x (up to 10.97x) over the state-of-the-art baseline, which combines a sequential-local prefetcher and LRU pre-eviction policy. CPPE also outperforms other approaches, including Random/reserved LRU with the sequential-local prefetcher, and simply disabling prefetching under memory oversubscription.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139830","GPUs;Unified Memory;Eviction Policy;Prefetching;Access Pattern","Prefetching;Graphics processing units;Memory management;Runtime;Semantics","cache storage;graphics processing units;paged storage;storage management","GPUs;unified memory;demand paging;memory management;long-latency page faults;page prefetching;hierarchical page eviction;prefetcher semantics-aware pre-evicting policy;continuous pages;coordinated page prefetch;CPPE;memory oversubscription;modified page eviction policy;access pattern-aware prefetcher;prefetch semantics;prefetcher prefetches pages;eviction candidates;GPU memory;sequential-local prefetcher;LRU pre-eviction policy;page replacement policy","",5.0,"",29.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"A Study of Single and Multi-device Synchronization Methods in Nvidia GPUs","L. Zhang; M. Wahib; H. Zhang; S. Matsuoka","Tokyo Institute of Technology; National Institute of Advanced Industrial Science and Technology; miHoYo Inc; RIKEN Center for Computational Science","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","483","493","GPUs are playing an increasingly important role in general-purpose computing. Many algorithms require synchronizations at different levels of granularity in a single GPU. Additionally, the emergence of dense GPU nodes also calls for multi-GPU synchronization. Nvidia's latest CUDA provides a variety of synchronization methods. Until now, there is no full understanding of the characteristics of those synchronization methods. This work explores important undocumented features and provides an in-depth analysis of the performance considerations and pitfalls of the state-of-art synchronization methods for Nvidia GPUs. The provided analysis would be useful when making design choices for applications, libraries, and frameworks running on single and/or multi-GPU environments. We provide a case study of the commonly used reduction operator to illustrate how the knowledge gained in our analysis can be useful. We also describe our micro-benchmarks and measurement methods.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139854","CUDA Barrier;Synchronization;GPUs","Synchronization;Graphics processing units;Kernel;Programming;Instruction sets;Throughput","graphics processing units;multiprocessing systems;parallel architectures;synchronisation","state-of-art synchronization methods;Nvidia GPU;provided analysis;multiGPU environments;measurement methods;general-purpose computing;single GPU;dense GPU nodes;multiGPU synchronization;important undocumented features;in-depth analysis;CUDA","",5.0,"",25.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"DPF-ECC: Accelerating Elliptic Curve Cryptography with Floating-Point Computing Power of GPUs","L. Gao; F. Zheng; N. Emmart; J. Dong; J. Lin; C. Weems","State Key Laboratory of Information Security, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Chinese Academy of Sciences, Beijing, China; College of Information and Computer Sciences, University of Massachusetts, Amherst, MA, USA; State Key Laboratory of Information Security, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Chinese Academy of Sciences, Beijing, China; College of Information and Computer Sciences, University of Massachusetts, Amherst, MA, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","494","504","Driven by artificial intelligence (AI) and computer vision industries, Graphics Processing Units (GPUs) are now rapidly achieving extraordinary computing power. In particular, the floating-point computing power, which is heavily relied on by graphics rendering and AI computation workload, is developing much faster in GPUs. Meanwhile, in many fields such as ecommerce and online finance, the demand for cryptographic operations for secure communications and authentication is also expanding.In this contribution, targeting the important cryptographic primitives widely used in TLS 1.3, etc., we implement Curve25519 and Edwards25519 with GPUs' floating-point computing power, where various performance optimization methods are customized for the target platform, including novel big-number representations combined with a new floating-point-based computing algorithm, efficient merged reduction strategies, and curve-level acceleration. This paper reports record-setting performance for the elliptic-curve method: on TITAN V, we respectively achieve 7.21 and 77.30 million operations per second of unknown and known point multiplication of Edwards25519, and 13.55 million operations per second of point multiplication of Curve25519. To the best of our knowledge, this contribution is the first to show that floating-point-based ECC implementations can outperform the integer-based ones by a huge margin. The experimental result in Tesla P100 achieves over double performance of the existing fastest integer work on the same platform, and the result in TITAN V sets a record for the throughput which is 4.43 times better than the second.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139772","Elliptic Curve Cryptography;Graphics Processing Unit;Double Precision Floating-point","Elliptic curve cryptography;Graphics processing units;Protocols;Standards;Acceleration","artificial intelligence;computer vision;floating point arithmetic;graphics processing units;power aware computing;public key cryptography;rendering (computer graphics)","elliptic curve cryptography;artificial intelligence;computer vision industries;Graphics Processing Units;AI computation workload;Edwards25519;elliptic-curve method;floating-point-based ECC implementations;GPU floating-point computing power","",10.0,"",38.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Scalability Challenges of an Industrial Implicit Finite Element Code","F. -H. Rouet; C. Ashcraft; J. Dawson; R. Grimes; E. Guleryuz; S. Koric; R. F. Lucas; J. S. Ong; T. A. Simons; T. -T. Zhu","Livermore Software Technology, ANSYS Company, Livermore, CA, USA; Livermore Software Technology, ANSYS Company, Livermore, CA, USA; Cray, Inc., Bloomington, MN, USA; Livermore Software Technology, ANSYS Company, Livermore, CA, USA; National Center for Supercomputing Applications, University of Illinois, Urbana-Champaign, IL, USA; National Center for Supercomputing Applications, University of Illinois, Urbana-Champaign, IL, USA; Livermore Software Technology, ANSYS Company, Livermore, CA, USA; Rolls-Royce, Indianapolis, IN, USA; Rolls-Royce, Indianapolis, IN, USA; Cray, Inc., Bloomington, MN, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","505","514","LS-DYNA is a well-known multiphysics code with both explicit and implicit time stepping capabilities. Implicit simulations rely heavily on sparse matrix computations, in particular direct solvers, and are notoriously much harder to scale than explicit simulations. In this paper, we investigate the scalability challenges of the implicit structural mode of LS- DYNA. In particular, we focus on linear constraint analysis, sparse matrix reordering, symbolic factorization, and numerical factorization. Our problem of choice for this study is a thermomechanical simulation of jet engine models built by Rolls-Royce with up to 200 million degrees of freedom, or equations. The models are used for engine performance analysis and design optimization, in particular optimization of tip clearances in the compressor and turbine sections of the engine. We present results using as many as 131,072 cores on the Blue Waters Cray XE6/XK7 supercomputer at NCSA and the Titan Cray XK7 supercomputer at OLCF. Since the main focus is on general linear algebra problems, this work is of interest for all linear algebra practitioners, not only developers of implicit finite element codes.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139822","Finite Elements;Sparse Matrices;Graph Partitioning;Direct Solvers","Mathematical model;Atmospheric modeling;Engines;Scalability;Finite element analysis;Software;Computational modeling","compressors;finite element analysis;jet engines;linear algebra;mainframes;mechanical engineering computing;parallel machines;sparse matrices;turbines","thermomechanical simulation;jet engine models;Rolls-Royce;engine performance analysis;design optimization;compressor;turbine sections;Titan Cray XK7 supercomputer;general linear algebra problems;linear algebra practitioners;implicit finite element codes;scalability challenges;industrial implicit finite element code;LS-DYNA;multiphysics code;explicit time;implicit time;implicit simulations;sparse matrix computations;direct solvers;explicit simulations;implicit structural mode;LS- DYNA;linear constraint analysis;sparse matrix reordering;symbolic factorization;numerical factorization","","","",32.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"ETH: An Architecture for Exploring the Design Space of In-situ Scientific Visualization","G. Abram; V. Adhinarayanan; W. -c. Feng; D. Rogers; J. Ahrens","Texas Advanced Computing Center, The University of Texas at Austin, Austin, TX; Department of Computer Science, Virginia Tech, Blacksburg, Virginia; Department of Computer Science, Virginia Tech, Blacksburg, Virginia; Computer, Computational, and Statistical Sciences Division, Los Alamos National Laboratory, New Mexico; Computer, Computational, and Statistical Sciences Division, Los Alamos National Laboratory, New Mexico","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","515","526","As high-performance computing (HPC) moves towards the exascale era, large-scale scientific simulations are generating enormous datasets. Many techniques (e.g., in-situ methods, data sampling, and compression) have been proposed to help visualize these large datasets under various constraints such as storage, power, and energy. However, evaluating these techniques and understanding the trade-offs (e.g., performance, efficiency, and quality) remains a challenging task.To enable exploration of the design space across such trade-offs, we propose the Exploration Test Harness (ETH), an architecture for the early-stage exploration of visualization and rendering approaches, job layout, and visualization pipelines. ETH covers a broader parameter space than current large-scale visualization applications such as ParaView and VisIt. It also promotes the study of simulation-visualization coupling strategies through a data-centric approach, rather than requiring coupling with a specific scientific simulation code. Furthermore, with experimentation on an extensively instrumented supercomputer, we study more metrics of interest than was previously possible. Importantly, ETH will help to answer important what-if scenarios and trade-off questions in the early stages of pipeline development, helping scientists to make informed choices about how to best couple a simulation code with visualization at extreme scale.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139783","In-situ Techniques;High-Performance Computing;Design-space Exploration;Raycasting;Energy Efficiency","Data visualization;Data models;Analytical models;Computational modeling;Couplings;Task analysis;Pipelines","computer simulation;data visualisation;parallel processing;rendering (computer graphics)","ETH;rendering;job layout;visualization pipelines;simulation-visualization coupling strategies;data-centric approach;design space;high-performance computing;HPC;exascale era;large-scale scientific simulations;data sampling;scientific simulation code;large-scale visualization applications;exploration test harness;in-situ scientific visualization","",1.0,"",36.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Scaling Betweenness Approximation to Billions of Edges by MPI-based Adaptive Sampling","A. van der Grinten; H. Meyerhenke","Department of Computer Science, Humboldt-Universität zu Berlin, Berlin, Germany; Department of Computer Science, Humboldt-Universität zu Berlin, Berlin, Germany","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","527","535","Betweenness centrality is one of the most popular vertex centrality measures in network analysis. Hence, many (sequential and parallel) algorithms to compute or approximate betweenness have been devised. Recent algorithmic advances have made it possible to approximate betweenness very efficiently on shared-memory architectures. Yet, the best shared-memory algorithms can still take hours of running time for large graphs, especially for graphs with a high diameter or when a small relative error is required.In this work, we present an MPI-based generalization of the state-of-the-art shared-memory algorithm for betweenness approximation. This algorithm is based on adaptive sampling; our parallelization strategy can be applied in the same manner to adaptive sampling algorithms for other problems. In experiments on a 16-node cluster, our MPI-based implementation is by a factor of 16.1x faster than the state-of-the-art shared-memory implementation when considering our parallelization focus - the adaptive sampling phase - only. For the complete algorithm, we obtain an average (geom. mean) speedup factor of 7.4x over the state of the art. For some previously very challenging inputs, this speedup is much higher. As a result, our algorithm is the first to approximate betweenness centrality on graphs with several billion edges in less than ten minutes with high accuracy.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139825","betweenness centrality;approximation;adaptive sampling;MPI-parallel graph algorithm;big graph data analytics","Approximation algorithms;Clustering algorithms;Data structures;Instruction sets;Computer science;Distributed algorithms;Probabilistic logic","application program interfaces;graph theory;memory architecture;message passing;network theory (graphs);parallel algorithms;sampling methods;shared memory systems","state-of-the-art shared-memory algorithm;betweenness approximation;sampling algorithms;MPI-based implementation;state-of-the-art shared-memory implementation;adaptive sampling phase;complete algorithm;approximate betweenness centrality;MPI-based adaptive sampling;popular vertex centrality measures;algorithmic advances;shared-memory architectures;shared-memory algorithms;MPI-based generalization","",4.0,"",24.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Improved Intermediate Data Management for MapReduce Frameworks","H. Wang; H. Shen; C. Reiss; A. Jain; Y. Zhang","Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Facebook, Seattle, WA, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","536","545","MapReduce is a popular distributed framework for big data analysis. However, the current MapReduce framework is insufficiently efficient in handling intermediate data, which may cause bottlenecks in I/O operations, computation, and network bandwidth. Previous work addresses the I/O problem by aggregating map task outputs (i.e. intermediate data) for each single reduce task on one machine. Unfortunately, when there are a large number of reduce tasks, their concurrent requests for intermediate data generate a large amount of I/O operations. In this paper, we present APA (Aggregation, Partition, and Allocation), a new intermediate data management system for the MapReduce framework. APA aggregates the intermediate data from the map tasks in each rack to one file, and the file host pushes the needed intermediate data to each reduce task. Thus, it reduces the number of disk seeks involved in handling intermediate data within one job. Rather than evenly distributing the intermediate data among reduce tasks based on the keys as in current MapReduce, APA partitions the intermediate data to balance the execution latency of different reduce tasks. APA further decides where to allocate each reduce task to minimize the intermediate data transmission time between map tasks and reduce tasks. Through experiments on a real MapReduce Hadoop cluster using the HiBench benchmark suite, we show that APA improves the performance of the current Hadoop by 40%-50%.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139844","","Task analysis;Data communication;Data aggregation;Hard disks;Facebook;Bandwidth;Resource management","Big Data;data analysis;parallel processing","reduce task;intermediate data management system;MapReduce;map tasks;intermediate data transmission time;distributed framework;big data analysis;aggregation partition and allocation;APA;intermediate data handling","",3.0,"",45.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Bandwidth-Aware Page Placement in NUMA","D. Gureya; J. Neto; R. Karimi; J. Barreto; P. Bhatotia; V. Quema; R. Rodrigues; P. Romano; V. Vlassov","INESC-ID, Instituto Superior Técnico, University of Lisbon, Lisbon, Portugal; INESC-ID, Instituto Superior Técnico, University of Lisbon, Lisbon, Portugal; Emory University, Atlanta, GA, USA; INESC-ID, Instituto Superior Técnico, University of Lisbon, Lisbon, Portugal; University of Edinburgh, Edinburgh, United Kingdom; Grenoble INP/ENSIMAG, Grenoble, France; INESC-ID, Instituto Superior Técnico, University of Lisbon, Lisbon, Portugal; INESC-ID, Instituto Superior Técnico, University of Lisbon, Lisbon, Portugal; KTH Royal Institute of Technology, Stockholm, Sweden","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","546","556","Page placement is a critical problem for memory-intensive applications running on a shared-memory multiprocessor with a non-uniform memory access (NUMA) architecture. State-of-the-art page placement mechanisms interleave pages evenly across NUMA nodes. However, this approach fails to maximize memory throughput in modern NUMA systems, characterized by asymmetric bandwidths and latencies, and sensitive to memory contention and interconnect congestion phenomena.We propose BWAP, a novel page placement mechanism based on asymmetric weighted page interleaving. BWAP combines an analytical performance model of the target NUMA system with on-line iterative tuning of page distribution for a given memory-intensive application. Our experimental evaluation with representative memory-intensive workloads shows that BWAP performs up to 66% better than state-of-the-art techniques. These gains are particularly relevant when multiple co-located applications run in disjoint partitions of a large NUMA machine or when applications do not scale up to the total number of cores.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139869","non-uniform memory access;page placement;memory-intensive applications","Instruction sets;Tuners;Throughput;Analytical models;Topology;Linux","iterative methods;memory architecture;paged storage;shared memory systems","analytical performance model;latencies;on-line iterative tuning;NUMA machine;representative memory-intensive workloads;page distribution;asymmetric weighted page interleaving;BWAP;asymmetric bandwidths;NUMA nodes;nonuniform memory access architecture;shared-memory multiprocessor;bandwidth-aware page placement","",2.0,"",43.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"HCompress: Hierarchical Data Compression for Multi-Tiered Storage Environments","H. Devarajan; A. Kougkas; L. Logan; X. -H. Sun","Department of Computer Science, Illinois Institute of Technology, Chicago, IL; Department of Computer Science, Illinois Institute of Technology, Chicago, IL; Department of Computer Science, Illinois Institute of Technology, Chicago, IL; Department of Computer Science, Illinois Institute of Technology, Chicago, IL","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","557","566","Modern scientific applications read and write massive amounts of data through simulations, observations, and analysis. These applications spend the majority of their runtime in performing I/O. HPC storage solutions include fast node-local and shared storage resources to elevate applications from this bottleneck. Moreover, several middleware libraries (e.g., Hermes) are proposed to move data between these tiers transparently. Data reduction is another technique that reduces the amount of data produced and, hence, improve I/O performance. These two technologies, if used together, can benefit from each other. The effectiveness of data compression can be enhanced by selecting different compression algorithms according to the characteristics of the different tiers, and the multi-tiered hierarchy can benefit from extra capacity. In this paper, we design and implement HCompress, a hierarchical data compression library that can improve the application's performance by harmoniously leveraging both multi-tiered storage and data compression. We have developed a novel compression selection algorithm that facilitates the optimal matching of compression libraries to the tiered storage. Our evaluation shows that HCompress can improve scientific application's performance by 7x when compared to other state-of-the-art tiered storage solutions.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139838","hierarchical;multi-tiered;data compression;data-reduction;dynamic choice;workflow priorities;library","Data compression;Libraries;Optimization;Random access memory;Buffer storage;Data models;Middleware","data compression;data reduction;image coding;middleware;parallel processing;storage management","HCompress;multitiered storage environments;middleware libraries;data reduction;multitiered hierarchy;hierarchical data compression library;compression selection algorithm;compression libraries;scientific application;tiered storage solutions;HPC storage solutions;I/O performance","",7.0,"",52.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"FRaZ: A Generic High-Fidelity Fixed-Ratio Lossy Compression Framework for Scientific Floating-point Data","R. Underwood; S. Di; J. C. Calhoun; F. Cappello","School of Computing Clemson University, Clemson, SC; Argonne National Laboratory, IL; Holcolmbe Department of Electrical and Computing Engineering, Clemson University, Clemson, SC; Argonne National Laboratory, IL","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","567","577","With ever-increasing volumes of scientific floating-point data being produced by high-performance computing applications, significantly reducing scientific floating-point data size is critical, and error-controlled lossy compressors have been developed for years. None of the existing scientific floating-point lossy data compressors, however, support effective fixed-ratio lossy compression. Yet fixed-ratio lossy compression for scientific floating-point data not only compresses to the requested ratio but also respects a user-specified error bound with higher fidelity. In this paper, we present FRaZ: a generic fixed-ratio lossy compression framework respecting user-specified error constraints. The contribution is twofold. (1) We develop an efficient iterative approach to accurately determine the appropriate error settings for different lossy compressors based on target compression ratios. (2) We perform a thorough performance and accuracy evaluation for our proposed fixed-ratio compression framework with multiple state-of-the-art error-controlled lossy compressors, using several real-world scientific floating-point datasets from different domains. Experiments show that FRaZ effectively identifies the optimum error setting in the entire error setting space of any given lossy compressor. While fixed-ratio lossy compression is slower than fixed-error compression, it provides an important new lossy compression technique for users of very large scientific floating-point datasets.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139812","","Compressors;Data models;Image coding;Distortion;Bandwidth;Data compression;Quantization (signal)","data compression;iterative methods;parallel processing","real-world scientific floating-point datasets;FRaZ;fixed-error compression;high-performance computing;scientific floating-point data size;user-specified error constraints;lossy compressors;target compression ratios;scientific floating-point lossy data compressors;high-fidelity fixed-ratio lossy compression;fixed-ratio compression;iterative approach;error-controlled lossy compressors","",17.0,"",40.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"DELTA: Distributed Locality-Aware Cache Partitioning for Tile-based Chip Multiprocessors","N. Holtryd; M. Manivannan; P. Stenström; M. Pericàs","Department of Computer Science and Engineering, Chalmers University of Technology, Gotebörg, Sweden; Department of Computer Science and Engineering, Chalmers University of Technology, Gotebörg, Sweden; Department of Computer Science and Engineering, Chalmers University of Technology, Gotebörg, Sweden; Department of Computer Science and Engineering, Chalmers University of Technology, Gotebörg, Sweden","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","578","589","Cache partitioning in tile-based CMP architectures is a challenging problem because of i) the need to determine capacity allocations with low computational overhead and ii) the need to place allocations close to where they are used, in order to reduce access latency. Although, previous solutions have addressed the problem of reducing the computational overhead and incorporating locality-awareness, they suffer from the overheads of centrally determining allocations.In this paper, we propose DELTA, a novel distributed and locality-aware cache partitioning solution which works by exchanging asynchronous challenges among cores. The distributed nature of the algorithm coupled with the low computational complexity allows for frequent reconfigurations at negligible cost and for the scheme to be implemented directly in hardware. The allocation algorithm is supported by an enforcement mechanism which enables locality-aware placement of data. We evaluate DELTA on 16- and 64-core tiled CMPs with multi-programmed workloads. Our evaluation shows that DELTA improves performance by 9% and 16%, respectively, on average, compared to an unpartitioned shared last-level cache.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139842","cache partitioning;multicore architectures;performance isolation","Resource management;Partitioning algorithms;Pain;Proposals;Throughput;Heuristic algorithms;Software algorithms","cache storage;computational complexity;microprocessor chips;multiprocessing systems","DELTA;low computational complexity;64-core tiled CMPs;distributed locality-aware cache partitioning;tile-based chip multiprocessors;tile-based CMP architectures;capacity allocations;enforcement mechanism;16-core tiled CMPs;multiprogrammed workloads;unpartitioned last-level cache","",1.0,"",51.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Coordinated Management of Processor Configuration and Cache Partitioning to Optimize Energy under QoS Constraints","M. Nejat; M. Manivannan; M. Pericàs; P. Stenström","Department of Computer Science and Engineering, Chalmers University of Technology, Göteborg, Sweden; Department of Computer Science and Engineering, Chalmers University of Technology, Göteborg, Sweden; Department of Computer Science and Engineering, Chalmers University of Technology, Göteborg, Sweden; Department of Computer Science and Engineering, Chalmers University of Technology, Göteborg, Sweden","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","590","601","An effective way to improve energy efficiency is to throttle hardware resources to meet a certain QoS target, specified as a performance constraint, associated with all applications running on a multicore system. Prior art has proposed resource management (RM) frameworks in which the share of the last-level cache (LLC) assigned to each processor core and the voltage-frequency (VF) setting for each core is managed in a coordinated fashion to reduce energy. A drawback of such a scheme is that, while one core gives up LLC resources for another core, the performance drop must be compensated by a higher VF setting which leads to a quadratic increase in energy consumption. By allowing each core to be adapted to exploit instruction and memory-level parallelism (ILP/MLP), substantially higher energy savings are enabled.This paper proposes a coordinated RM for LLC partitioning, processor adaptation, and per-core VF scaling. A first contribution is a systematic study of the resource trade-offs enabled when trading between the three classes of resources in a coordinated fashion. A second contribution is a new RM framework that utilizes these trade-offs to save more energy. Finally, a challenge to accurately model the impact of resource throttling on performance is to predict the amount of MLP with high accuracy. To this end, the paper contributes with a mechanism that estimates the effect of MLP over different processor configurations and LLC allocations. Overall, we show that up to 18% of energy, and on average 10%, can be saved using the proposed scheme.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139837","Multicore processor;Resource management;Quality of service;Cache partitioning;Dynamic voltage-frequency scaling;Memory level parallelism;Reconfigurable architectures;Performance and energy modeling","Quality of service;Resource management;Parallel processing;Multicore processing;Hardware;Art;Energy consumption","cache storage;energy conservation;multiprocessing systems;optimisation;power aware computing;quality of service","coordinated management;processor configuration;cache partitioning;optimize energy;QoS constraints;energy efficiency;throttle hardware resources;QoS target;performance constraint;multicore system;resource management frameworks;last-level cache;processor core;voltage-frequency;coordinated fashion;LLC resources;performance drop;VF setting;quadratic increase;energy consumption;memory-level parallelism;energy savings;coordinated RM;LLC partitioning;processor adaptation;per-core VF scaling;resource trade-offs;RM framework;resource throttling;processor configurations","",4.0,"",36.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"StragglerHelper: Alleviating Straggling in Computing Clusters via Sharing Memory Access Patterns","W. Liu; P. Huang; X. He","Department of Computer and Information Sciences, Temple University; Department of Computer and Information Sciences, Temple University; Department of Computer and Information Sciences, Temple University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","602","611","Clusters have been a prevalent and successful computing framework for processing large amount of data due to their distributed and parallelized working paradigm. A task submitted to a cluster is typically divided into a number of subtasks which are designated to different work nodes running the same code but dealing with different equal portion of the dataset to be processed. Due to the existence of heterogeneity, it could easily result in stragglers unfairly slowing down the entire processing, because work nodes finish their subtasks at different rates. In this study, we aim to speed up straggling work nodes to quicken the overall processing by leveraging exhibited performance variation. More specifically, we propose StragglerHelper which conveys the memory access characteristics experienced by the forerunner to the stragglers such that stragglers can be sped up due to the accurately informed memory prefetching. A Progress Monitor is deployed to supervise the respective progresses of the work nodes and inform the memory access patterns of forerunner to straggling nodes. Our evaluation results with the SPEC MPI 2007 and BigDataBench on a cluster of 64 work nodes have shown that StragglerHelper is able to improve the execution time of stragglers by up to 99.5% with an average of 61.4%, contributing to an overall improvement of the entire cohort of the cluster by up to 46.7% with an average of 9.9% compared to the baseline cluster.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139789","Cluster;Memory;straggler mitigation;performance variation","Degradation;Binary codes;Monitoring;Benchmark testing;Task analysis;Data structures;Biomedical monitoring","application program interfaces;cache storage;memory architecture;message passing;parallel processing;pattern clustering;scheduling;shared memory systems;software performance evaluation","accurately informed memory prefetching;memory access patterns;straggling nodes;StragglerHelper;baseline cluster;prevalent computing framework;successful computing framework;subtasks;straggling work nodes;performance variation;memory access characteristics;parallelized working paradigm;distributed working paradigm","",1.0,"",38.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Evaluating the Numerical Stability of Posit Arithmetic","N. Buoncristiani; S. Shah; D. Donofrio; J. Shalf","Lawrence Berkeley National Laboratory and UC Berkeley, Berkeley, California; Lawrence Berkeley National Laboratory and UC Berkeley, Berkeley, California; Lawrence Berkeley National Laboratory and Tactical Computing Laboratories, Berkeley, California; Lawrence Berkeley National Laboratory, Berkeley, California","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","612","621","The Posit number format has been proposed by John Gustafson as an alternative to the IEEE 754 standard floatingpoint format. Posits offer a unique form of tapered precision whereas IEEE floating-point numbers provide the same relative precision across most of their representational range. Posits are argued to have a variety of advantages including better numerical stability and simpler exception handling.The objective of this paper is to evaluate the numerical stability of Posits for solving linear systems where we evaluate Conjugate Gradient Method to demonstrate an iterative solver and Cholesky-Factorization to demonstrate a direct solver. We show that Posits do not consistently improve stability across a wide range of matrices, but we demonstrate that a simple rescaling of the underlying matrix improves convergence rates for Conjugate Gradient Method and reduces backward error for Cholesky Factorization. We also demonstrate that 16-bit Posit outperforms Float16 for mixed precision iterative refinement - especially when used in conjunction with a recently proposed matrix re-scaling strategy proposed by Nicholas Higham.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139786","Posit;floating-point;linear algebra;numerical stability","Numerical stability;Open area test sites;Standards;Linear systems;Gradient methods;Iterative methods;Linear algebra","conjugate gradient methods;floating point arithmetic;iterative methods;matrix decomposition;numerical stability","numerical stability;Posit arithmetic;Posit number format;standard floatingpoint format;tapered precision;IEEE floating-point numbers;relative precision;exception handling;conjugate gradient method;Cholesky factorization;mixed precision iterative refinement","",3.0,"",14.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Varity: Quantifying Floating-Point Variations in HPC Systems Through Randomized Testing","I. Laguna","Lawrence Livermore National Laboratory","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","622","633","Floating-point arithmetic can be confusing and it is sometimes misunderstood by programmers. While numerical reproducibility is desirable in HPC, it is often unachievable due to the different ways compilers treat floating-point arithmetic and generate code around it. This reproducibility problem is exacerbated in heterogeneous HPC systems where code can be executed on different floating-point hardware, e.g., a host and a device architecture, producing in some situations different numerical results. We present VARITY, a tool to quantify floatingpoint variations in heterogeneous HPC systems. Our approach generates random test programs for multiple architectures (host and device) using the compilers that are available in the system. Using differential testing, it compares floating-point results and identifies unexpected variations in the program results. The results can guide programmers in choosing the compilers that produce the most similar results in a system, which is useful when numerical reproducibility is critical. By running 50,000 experiments with Varity on a system with IBM POWER9 CPUs, NVIDIA V100 GPUs, and four compilers (gcc, clang, xl, and nvcc), we identify and document several programs that produce significantly different results for a given input when different compilers or architectures are used, even when a similar optimization level is used everywhere.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139882","floating-point;differential testing;random testing;numerical reproducibility","Program processors;Optimization;Computer architecture;Testing;Hardware;Tools;Grammar","C language;floating point arithmetic;graphics processing units;parallel architectures;parallel processing;program compilers","device architecture;situations different numerical results;VARITY;floatingpoint variations;heterogeneous HPC systems;random test programs;differential testing;floating-point results;unexpected variations;program results;numerical reproducibility;quantifying floating-point variations;randomized testing;floating-point arithmetic;compilers;reproducibility problem;floating-point hardware;IBM POWER9 CPU;NVIDIA V100 GPU;optimization level","",2.0,"",30.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Demystifying Tensor Cores to Optimize Half-Precision Matrix Multiply","D. Yan; W. Wang; X. Chu","Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Hong Kong Baptist University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","634","643","Half-precision matrix multiply has played a key role in the training of deep learning models. The newly designed Nvidia Tensor Cores offer the native instructions for half-precision small matrix multiply, based on which Half-precision General Matrix Multiply (HGEMM) routines are developed and can be accessed through high-level APIs. In this paper, we, for the first time, demystify how Tensor Cores on NVIDIA Turing architecture work in great details, including the instructions used, the registers and data layout required, as well as the throughput and latency of Tensor Core operations. We further benchmark the memory system of Turing GPUs and conduct quantitative analysis of the performance. Our analysis shows that the bandwidth of DRAM, L2 cache and shared memory is the new bottleneck for HGEMM, whose performance is previously believed to be bound by computation. Based on our newly discovered features of Tensor Cores, we apply a series of optimization techniques on the Tensor Core-based HGEMM, including blocking size optimization, data layout redesign, data prefetching, and instruction scheduling. Extensive evaluation results show that our optimized HGEMM routine achieves an average of 1.73× and 1.46× speedup over the native implementation of cuBLAS 10.1 on NVIDIA Turing RTX2070 and T4 GPUs, respectively. The code of our implementation is written in native hardware assembly (SASS).","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139835","GEMM;GPU;Tensor Core;Half-precision","Tensile stress;Hidden Markov models;Graphics processing units;Registers;Benchmark testing;Layout;C++ languages","application program interfaces;cache storage;computer graphic equipment;coprocessors;DRAM chips;learning (artificial intelligence);matrix multiplication;optimisation;shared memory systems;Turing machines","Half-precision General Matrix Multiply routines;NVIDIA Turing architecture work;optimized HGEMM routine;NVIDIA Turing RTX2070;demystifying tensor cores;half-precision matrix multiply optimization;tensor core operations;tensor core-based HGEMM;memory system;Turing GPU;DRAM;L2 cache;Nvidia Tensor Cores;shared memory;optimization techniques;blocking size optimization;data layout redesign;hardware assembly","",20.0,"",19.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Data Collection of IoT Devices Using an Energy-Constrained UAV","Y. Li; W. Liang; W. Xu; X. Jia","The Australian National University, Canberra, Australia; The Australian National University, Canberra, Australia; Sichuan University, Chengdu, P. R. China; City University of Hong Kong, Hong Kong, P. R. China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","644","653","In this paper, we study sensing data collection from IoT devices in a wireless sensor network, using an energy-constrained Unmanned Aerial Vehicle (UAV), where the sensory data is stored in IoT devices while the IoT devices may or may not be within the transmission range of each other. We formulate two novel data collection problems to fully or partially collect data from IoT devices using the UAV, by finding a closed tour for the UAV that includes hovering locations and the sojourn duration at each of the hovering locations such that the accumulative volume of data collected is maximized, subject to the energy capacity on the UAV, where the UAV consumes its energy on both hovering and flying from one hovering location to another hovering location. To this end, we first propose a novel data collection framework that enables the UAV to collect the sensory data from multiple IoT devices simultaneously if the IoT devices are within the hovering coverage range of the UAV. We then formulate two data collection maximization problems, and show that both of the problems are NP-hard. We instead devise efficient approximation and heuristic algorithms for the problems. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results demonstrated that the proposed algorithms are promising.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139851","","Sensors;Data collection;Aggregates;Approximation algorithms;Unmanned aerial vehicles;Monitoring;Heuristic algorithms","aerospace computing;approximation theory;autonomous aerial vehicles;computational complexity;control engineering computing;data acquisition;Internet of Things;optimisation;wireless sensor networks","hovering location;sensory data;IoT devices;data collection maximization problems;energy-constrained UAV;energy-constrained unmanned aerial vehicle;wireless sensor network;energy capacity;NP-hard;heuristic algorithms;approximation algorithms","",14.0,"",16.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Argus: Multi-Level Service Visibility Scoping for Internet-of-Things in Enterprise Environments","Q. Zhou; O. Pandey; F. Ye","Electrical and Computer Engineering, Stony Brook University; Computer Science, Stony Brook University; Electrical and Computer Engineering, Stony Brook University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","654","663","In IoT, what services from which nearby devices are available, must be discovered by a user's device (e.g., smartphone) before she can issue commands to access them. Service visibility scoping in large scale, heterogeneous enterprise environments has multiple unique features, e.g., proximity based interactions, differentiated visibility according to device natures and user attributes, frequent user churns thus revocation. They render existing solutions completely insufficient. We propose Argus, a distributed algorithm offering three-level, fine-grained visibility scoping in parallel: i) Level 1 public visibility where services are identically visible to everyone; ii) Level 2 differentiated visibility where service visibility depends on users' non-sensitive attributes; iii) Level 3 covert visibility where service visibility depends on users' sensitive attributes that are never explicitly disclosed. Extensive analysis and experiments show that: i) Argus is secure; ii) its Level 2 is 10x as scalable and computationally efficient as work using Attribute-based Encryption, Level 3 is 10x as efficient as work using Paring-based Cryptography; iii) it is fast and agile for satisfactory user experience, costing 0.25 s to discover 20 Level 1 devices, and 0.63 s for Level 2 or Level 3 devices.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139877","","Public key;Servers;Sociology;Statistics;Privacy","cryptography;Internet of Things","paring-based cryptography;attribute-based encryption;Internet-of-Things;multilevel service visibility scoping;Argus","","","",22.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"G-PBFT: A Location-based and Scalable Consensus Protocol for IoT-Blockchain Applications","L. Lao; X. Dai; B. Xiao; S. Guo","The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, Hong Kong; The Hong Kong Polytechnic University, Hong Kong; Chongqing University, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","664","673","IoT-blockchain applications have advantages of managing massive IoT devices, achieving advanced data security, and data credibility. However, there are still some challenges when deploying IoT applications on blockchain systems due to limited storage, power, and computing capability of IoT devices. Applying current consensus protocols to IoT applications may be vulnerable to Sybil node attacks or suffer from high-computational cost and low scalability. In this paper, we propose G-PBFT (Geographic-PBFT), a new location-based and scalable consensus protocol designed for IoT-blockchain applications. The principle of G-PBFT is based on the fact that most IoT-blockchain applications rely on fixed IoT devices for data collection and processing. Fixed IoT devices have more computational power than other mobile IoT devices, e.g., mobile phones and sensors, and are less likely to become malicious nodes. G-PBFT exploits geographic information of fixed IoT devices to reach consensus, thus avoiding Sybil attacks. In G-PBFT, we select those fixed, loyal, and capable nodes as endorsers, reducing the overhead for validating and recording transactions. As a result, G-PBFT achieves high consensus efficiency and low traffic intensity. Moreover, G-PBFT uses a new era switch mechanism to handle the dynamics of the IoT network. To evaluate our protocol, we conduct extensive experiments to compare the performance of G-PBFT against existing consensus protocol with over 200 participating nodes in a blockchain system. Experimental results demonstrate that G-PBFT significantly reduces consensus time, network overhead, and is scalable for IoT applications.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139827","IoT;blockchain;consensus protocol;PBFT;geographic location;scalable","Protocols;Internet of Things;Scalability;Mobile handsets;Sensors","computer network security;cryptographic protocols;Internet of Things","Geographic-PBFT;Sybil attacks;fixed IoT devices;deploying IoT applications;massive IoT devices;IoT-blockchain applications;scalable consensus protocol;blockchain system;IoT network;G-PBFT","",36.0,"",36.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Byzantine Generalized Lattice Agreement","G. A. Di Luna; E. Anceaume; L. Querzoni","DIAG, Sapienza University of Rome, Italy; CNRS, Univ Rennes, Inria, IRISA, France; DIAG, Sapienza University of Rome, Italy","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","674","683","The paper investigates the Lattice Agreement (LA) problem in asynchronous systems. In LA each process proposes an element e from a predetermined lattice, and has to decide on an element e' of the lattice such that e ≤ e'. Moreover, decisions of different processes have to be comparable (no two processes can decide two elements e' and e such that (e ≤ e') ∧ (e' ≤ e)).It has been shown that Generalized LA (i.e., a version of LA proposing and deciding on sequences of values) can be used to build a Replicated State Machine (RSM) with commutative update operations. The key advantage of LA and Generalized LA is that they can be solved in asynchronous systems prone to crash-failures (which is not the case with standard Consensus).In this paper we assume Byzantine failures. We propose the Wait Till Safe (WTS) algorithm for LA, and we show that its resilience to f ≤ (n - 1)/3 Byzantine processes is optimal. We then generalize WTS obtaining a Generalized LA algorithm, namely GWTS. We use GWTS to build a RSM with commutative updates. Our RSM works in asynchronous systems and tolerates f ≤ (n - 1)/3 malicious entities. All our algorithms use the minimal assumption of authenticated channels. When the more powerful public signatures are available, we discuss how to improve the message complexity of our results (from quadratic to linear, when f = O(1)). To the best of our knowledge this is the first paper proposing a solution for Byzantine LA that works on any possible lattice, and it is the first work proposing a Byzantine tolerant RSM built on it.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139889","lattice agreement;replicated state machine;Byzantine faults","Lattices;Commutation;Computer crashes;Complexity theory;Reliability;Delays;Upper bound","communication complexity;computational complexity;distributed algorithms;distributed processing;fault tolerant computing;finite state machines;system recovery","Byzantine failures;WTS;GWTS;commutative updates;RSM;asynchronous systems;Byzantine LA;Byzantine tolerant RSM;replicated state machine;commutative update operations;crash-failures;Byzantine generalized lattice agreement;generalized LA algorithm;till safe algorithm","",2.0,"",15.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"A Heterogeneous PIM Hardware-Software Co-Design for Energy-Efficient Graph Processing","Y. Huang; L. Zheng; P. Yao; J. Zhao; X. Liao; H. Jin; J. Xue","National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; UNSW, Sydney, Australia","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","684","695","Processing-In-Memory (PIM) is an emerging technology that addresses the memory bottleneck of graph processing. In general, analog memristor-based PIM promises high parallelism provided that the underlying matrix-structured crossbar can be fully utilized while digital CMOS-based PIM has a faster single-edge execution but its parallelism can be low. In this paper, we observe that there is no absolute winner between these two representative PIM technologies for graph applications, which often exhibit irregular workloads. To reap the best of both worlds, we introduce a new heterogeneous PIM hardware, called Hetraph, to facilitate energy-efficient graph processing. Hetraph incorporates memristor-based analog computation units (for high-parallelism computing) and CMOS-based digital computation cores (for efficient computing) on the same logic layer of a 3D die-stacked memory device. To maximize the hardware utilization, our software design offers a hardware heterogeneity-aware execution model and a workload offloading mechanism. For performance speedups, such a hardware-software co-design outperforms the state-of-the-art by 7.54 ×(CPU), 1.56 ×(GPU), 4.13× (memristor-based PIM) and 3.05× (CMOS-based PIM), on average. For energy savings, Hetraph reduces the energy consumption by 57.58× (CPU), 19.93× (GPU), 14.02 ×(memristor-based PIM) and 10.48 ×(CMOS-based PIM), on average.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139839","accelerator;graph processing;heterogeneous architecture;processing-in-memory","Hardware;Computer architecture;Parallel processing;Three-dimensional displays;Computational modeling;Graphics processing units;Memristors","CMOS memory circuits;energy conservation;graph theory;hardware-software codesign;integrated circuit modelling;integrated memory circuits;low-power electronics;memory architecture;memristor circuits;three-dimensional integrated circuits","CMOS-based PIM;memristor-based PIM;CMOS-based digital computation core;memristor-based analog computation unit;Hetraph;matrix-structured crossbar;processing-in-memory;heterogeneous PIM hardware-software codesign;3D die-stacked memory device;digital CMOS-based PIM;analog memristor-based PIM;energy-efficient graph processing;hardware heterogeneity-aware execution model","",15.0,"",58.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Spara: An Energy-Efficient ReRAM-Based Accelerator for Sparse Graph Analytics Applications","L. Zheng; J. Zhao; Y. Huang; Q. Wang; Z. Zeng; J. Xue; X. Liao; H. Jin","National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; UNSW Sydney, Australia; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","696","707","Resistive random access memory (ReRAM) addresses the high memory bandwidth requirement challenge of graph analytics by integrating the computing logic in the memory. Due to the matrix-structured crossbar architecture, existing ReRAM-based accelerators, when handling real-world graphs that often have the skewed degree distribution, suffer from the severe sparsity problem arising from zero fillings and activation nondeterminism, incurring substantial ineffectual computations.In this paper, we observe that the sparsity sources lie in the consecutive mapping of source and destination vertex index onto the wordline and bitline of a crossbar. Although exhaustive graph reordering improves the sparsity-induced inefficiency, its totally-random (source and destination) vertex mapping leads to expensive overheads. This work exploits the insight in a mid-point vertex mapping with the random wordlines and consecutive bitlines. A cost-effective preprocessing is proposed to exploit the insight by rapidly exploring the crossbar-fit vertex reorderings but ignores the sparsity arising from activation dynamics. We present a novel ReRAM-based graph analytics accelerator, named Spara, which can maximize the workload density of crossbars dynamically by using a tightly-coupled bank parallel architecture further proposed. Results on real-world and synthesized graphs show that Spara outperforms GraphR and GraphSAR by 8.21 × and 5.01 × in terms of performance, and by 8.97 × and 5.68× in terms of energy savings (on average), while incurring a reasonable (<; 9.98%) pre-processing overhead.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139778","ReRAM;graph analytics;energy efficiency","Indexes;Electrodes;Random access memory;Parallel architectures;Resistance;Memory management","graph theory;integrated circuit design;low-power electronics;matrix algebra;memory architecture;resistive RAM","mid-point vertex mapping;energy-efficient ReRAM-based accelerator;sparse graph analytics applications;resistive random access memory;matrix-structured crossbar architecture;memory computing logic;ReRAM-based accelerator;graph reordering;crossbar-fit vertex reordering;ReRAM-based graph analytics accelerator","",17.0,"",44.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Optimal Encoding and Decoding Algorithms for the RAID-6 Liberation Codes","Z. Huang; H. Jiang; Z. Shen; H. Che; N. Xiao; N. Li","Dept. of Computer Science and Engineering, University of Texas at Arlington, Arlington, USA; Dept. of Computer Science and Engineering, University of Texas at Arlington, Arlington, USA; School of Informatics, Xiamen University, Xiamen, China; Dept. of Computer Science and Engineering, University of Texas at Arlington, Arlington, USA; School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China; Dept. of Computer Science and Engineering, University of Texas at Arlington, Arlington, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","708","717","RAID-6 is gradually replacing RAID-5 as the dominant form of disk arrays due to its capability of tolerating concurrent failures of any two disks, as well as the case of encountering an uncorrectable read error during recovery. Implementing a RAID-6 system relies on some erasure coding schemes, and so far the most representative solutions are EVENODD codes [1], RDP codes [2] and Liberation codes [3], none of which has emerged as a clear ""all-around"" winner. In this paper, we are interested in revealing the undiscovered potential of the Liberation codes, since these codes have the following attractive features: (a) they have the best update performance, (b) they have better scalability, and (c) they are open-sourced and publicly available, as well as the following drawbacks: fair encoding performance and, more importantly, relatively poor decoding performance. Specificly, we present novel optimal encoding and decoding algorithms for the Liberation codes by introducing an alternative, geometric presentation of these codes. The proposed algorithms completely eliminate redundant computations during the encoding and decoding procedures by extracting and reusing common expressions between the two types of parity constraints, and do not involve any matrix operations on which the original algorithms are based. Our experiment results show that compared with the original solution, the proposed encoding and decoding algorithms reduce the number of XOR's by up to 16 percent and 15 ~20 percent respectively, and the encoding and decoding throughputs are increased by 22.3 percent and at most 155 percent respectively. Moreover, the encoding complexity reaches the theoretical lower bound, while the decoding complexity is also very close to the theoretical lower bound.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139777","MDS array codes;RAID-6;reliability;erasure codes","Encoding;Decoding;Arrays;Complexity theory;Measurement;Standards;Reed-Solomon codes","decoding;encoding;error correction codes;RAID;storage management","RAID-6 Liberation codes;RAID-5;disk arrays;RAID-6 system;erasure coding schemes;EVENODD codes;RDP codes;fair encoding performance;optimal encoding;decoding algorithms;decoding procedures;decoding throughputs;encoding complexity;decoding complexity","",2.0,"",24.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Sturgeon: Preference-aware Co-location for Improving Utilization of Power Constrained Computers","P. Pang; Q. Chen; D. Zeng; C. Li; J. Leng; W. Zheng; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; School of Computer Science, China University of Geosciences, Wuhan, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","718","727","Large-scale datacenters often host latency-sensitive services that have stringent Quality-of-Service requirement and experience diurnal load pattern. Co-locating best-effort applications that have no QoS requirement with latency-sensitive services has been widely used to improve the resource utilization with careful shared resource management. However, existing co-location techniques tend to result in the power overload problem on power constrained computers due to the ignorance of the power consumption. To this end, we propose Sturgeon, a runtime system proactively manages resources between colocated applications in a power constrained environment, to ensure the QoS of latency-sensitive services while maximizing the resource utilization. Our investigation shows that, at a given load, there are multiple feasible resource configurations to meet both QoS requirement and power budget, while one of them yields the maximum throughput of best-effort applications. To find such a configuration, we establish models to accurately predict the performance and power consumption of the colocated applications. Sturgeon monitors the QoS periodically in order to eliminate the potential QoS violation caused by the unpredictable interference. The experimental results show that Sturgeon improves the throughput of best-effort applications by 24.96% compared to the state-of-the-art technique, while guaranteeing the 95%-ile latency within the QoS target.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139802","QoS;Improved Utilization;Power Constrained Computers","Quality of service;Resource management;Power demand;Throughput;Central Processing Unit;Servers;Computers","computer centres;power aware computing;quality of service;resource allocation","QoS violation;Sturgeon;power constrained computers;latency-sensitive services;QoS requirement;resource utilization;shared resource management;existing co-location techniques;power overload problem;power consumption;colocated applications;power constrained environment;resource configurations;power budget;preference-aware co-location;diurnal load pattern","",5.0,"",33.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"A High-Throughput Solver for Marginalized Graph Kernels on GPU","Y. -H. Tang; O. Selvitopi; D. T. Popovici; A. Buluç","Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory; Computational Research Division, Lawrence Berkeley National Laboratory","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","728","738","We present the design and optimization of a linear solver on General Purpose GPUs for the efficient and high-throughput evaluation of the marginalized graph kernel between pairs of labeled graphs. The solver implements a preconditioned conjugate gradient (PCG) method to compute the solution to a generalized Laplacian equation associated with the tensor product of two graphs. To cope with the gap between the instruction throughput and the memory bandwidth of current generation GPUs, our solver forms the tensor product linear system on-the-fly without storing it in memory when performing matrix-vector dot product operations in PCG. Such on-the-fly computation is accomplished by using threads in a warp to cooperatively stream the adjacency and edge label matrices of individual graphs by small square matrix blocks called tiles, which are then staged in registers and the shared memory for later reuse. Warps across a thread block can further share tiles via the shared memory to increase data reuse. We exploit the sparsity of the graphs hierarchically by storing only non-empty tiles using a coordinate format and nonzero elements within each tile using bitmaps. Besides, we propose a new partition-based reordering algorithm for aggregating nonzero elements of the graphs into fewer but denser tiles to improve the efficiency of the sparse format.We carry out extensive theoretical analyses on the graph tensor product primitives for tiles of various density and evaluate their performance on synthetic and real-world datasets. Our solver delivers three to four orders of magnitude speedup over existing CPU-based solvers such as GraKeL and GraphKernels. The capability of the solver enables kernel-based learning tasks at unprecedented scales.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139866","","Kernel;Symmetric matrices;Linear systems;Mathematical model;Tensile stress;Task analysis;Graphics processing units","conjugate gradient methods;graph theory;graphics processing units;learning (artificial intelligence);matrix algebra;optimisation;parallel processing;sparse matrices;tensors;vectors","tensor product linear system;partition-based reordering algorithm;linear solver design;linear solver optimization;generalized Laplacian equation;PCG;preconditioned conjugate gradient method;labeled graphs;General Purpose GPUs;marginalized graph kernel;high-throughput solver;kernel-based learning tasks;graph tensor product primitives;thread block;shared memory","",7.0,"",25.0,"USGov","14 Jul 2020","","","IEEE","IEEE Conferences"
"Dynamic Graphs on the GPU","M. A. Awad; S. Ashkiani; S. D. Porumbescu; J. D. Owens","Dept. of Elect. & Computer Engineering, UC Davis, Davis, California, USA; Dept. of Elect. & Computer Engineering, UC Davis, Davis, California, USA; Dept. of Elect. & Computer Engineering, UC Davis, Davis, California, USA; Dept. of Elect. & Computer Engineering, UC Davis, Davis, California, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","739","748","We present a fast dynamic graph data structure for the GPU. Our dynamic graph structure uses one hash table per vertex to store adjacency lists and achieves 3.4-14.8x faster insertion rates over the state of the art across a diverse set of large datasets, as well as deletion speedups up to 7.8x. The data structure supports queries and dynamic updates through both edge and vertex insertion and deletion. In addition, we define a comprehensive evaluation strategy based on operations, workloads, and applications that we believe better characterize and evaluate dynamic graph data structures.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139818","dynamic;graph;data structures;GPU","Gold;Graphics processing units;Arrays;Memory management;Sparse matrices;Maintenance engineering","data structures;graph theory;graphics processing units","dynamic graph data structures;dynamic updates;deletion speedups;adjacency lists;hash table;GPU","",9.0,"",8.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Accelerating Parallel Hierarchical Matrix-Vector Products via Data-Driven Sampling","L. Erlandson; D. Cai; Y. Xi; E. Chow","School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, Georgia, United States of America; Department of Mathematics, Emory University, Atlanta, Georgia, United States of America; Department of Mathematics, Emory University, Atlanta, Georgia, United States of America; School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, Georgia, United States of America","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","749","758","Hierarchical matrices are scalable matrix representations particularly suited to the case where the matrix entries are defined by a smooth kernel function evaluated between pairs of points. In this paper, we present a new scheme to alleviate the computational bottlenecks present in many hierarchical matrix methods. For general kernel functions, a popular approach to construct hierarchical matrices is through interpolation, due to its efficiency compared to computationally expensive algebraic techniques. However, interpolation-based methods often lead to larger ranks, and do not scale well to higher dimensions. We propose a new data-driven method to resolve these issues. The new method is able to accomplish the rank reduction by using a surrogate for the global distribution of points. The surrogate is generated using a hierarchical data-driven sampling. As a result of the lower rank, the construction cost, memory requirements, and matrix-vector product costs decrease. Using state-of-theart dimension independent sampling, the new method makes it possible to tackle problems in higher dimensions. We also discuss an on-the-fly variation of hierarchical matrix construction and matrix-vector products that is able to reduce memory usage by an order of magnitude. This is accomplished by postponing the generation of certain intermediate matrices until they are used, generating them just in time. We provide results demonstrating the effectiveness of our improvements, both individually and in conjunction with each other. For a problem involving 320,000 points in 3D, our data-driven approach reduces the memory usage from 58.75 GiB using state-of-the-art methods (762.9 GiB if stored dense) to 18.60 GiB. In combination with our on-thefly approach, we are able to reduce the total memory usage to 543.74 MiB.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139849","","Kernel;Interpolation;Sparse matrices;Complexity theory;Taylor series;Scientific computing","data reduction;matrix algebra;parallel processing;storage management;vectors","parallel hierarchical matrix-vector products;scalable matrix representations;matrix entries;smooth kernel function;hierarchical matrix methods;rank reduction;hierarchical data-driven sampling;memory requirements;hierarchical matrix construction;memory usage;intermediate matrices;dimension independent sampling;matrix-vector product costs","",8.0,"",29.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"NC Algorithms for Popular Matchings in One-Sided Preference Systems and Related Problems","C. Hu; V. K. Garg","Department of Electrical and Computer Engineering, University of Texas, Austin; Department of Electrical and Computer Engineering, University of Texas, Austin","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","759","768","The popular matching problem is of matching a set of applicants to a set of posts, where each applicant has a preference list, ranking a non-empty subset of posts in the order of preference, possibly with ties. A matching M is popular if there is no other matching M' such that more applicants prefer M' to M. We give the first NC algorithm to solve the popular matching problem without ties. We also give an NC algorithm that solves the maximum-cardinality popular matching problem. No NC or RNC algorithms were known for the matching problem in preference systems prior to this work. Moreover, we give an NC algorithm for a weaker version of the stable matching problem, that is, the problem of finding the ""next"" stable matching given a stable matching.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139795","","Impedance matching;Bipartite graph;Computational modeling;Resource management;Pareto optimization;Distributed processing;Economics","computational complexity;graph theory;set theory","maximum-cardinality popular matching problem;NC algorithm;preference list;applicant;one-sided preference systems","","","",28.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Smartly Handling Renewable Energy Instability in Supporting A Cloud Datacenter","J. Gao; H. Wang; H. Shen","Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Department of Computer Science, University of Virginia, Charlottesville, VA, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","769","778","The size and energy consumption of datacenters have been increasing significantly over the past years. As a result, datacenters' increasing electricity monetary cost, energy consumption and energy harmful gas emissions have become a severe problem. Renewable energy supply is widely seen as a promising solution. However, the instability of renewable energy brings about a new challenge since insufficient energy supply may lead to job running interruptions or failures. Though previous works attempt to more accurately predict the amount of produced renewable energy, due to the instability of its influencing factors (e.g., wind, temperature), sufficient renewable energy supply cannot be always guaranteed. To handle this problem, in this paper, we propose allocating jobs with the same service-level-objective (SLO) level to the same physical machine (PM) group, and power each PM group with renewable energy generators that have probability no less than its SLO to produce the amount no less than its energy demand. It ensures that insufficient renewable energy supply will not lead to SLO violations. We use a deep learning technique to predict the probability of producing amount no less than each value of each renewable energy source and predict the energy demands of each PM area. We formulate an optimization problem: how to match renewable energy resources with different instabilities to different PM groups as energy supply in order to minimize the number of SLO violations (due to interruption from insufficient renewable energy supply), total energy monetary cost and total carbon emission. We then use reinforcement learning method and linear programming method to solve the optimization problem. The real trace driven experiments show that our method can achieve much lower SLO violations, total energy monetary cost and total carbon emission compared to other methods.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139872","","Renewable energy sources;Generators;Carbon dioxide;Servers;Energy consumption;Schedules;Clouds","computer centres;energy consumption;learning (artificial intelligence);linear programming;power engineering computing;power generation economics;probability;renewable energy sources;supply and demand","optimization problem;SLO violations;energy harmful gas emissions;service-level-objective level;renewable energy generators;energy demand;renewable energy supply;cloud datacenter;energy consumption;electricity monetary cost;physical machine group;probability;deep learning technique;total carbon emission;reinforcement learning method;linear programming method","",131.0,"",46.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"A Self-Optimized Generic Workload Prediction Framework for Cloud Computing","V. K. Jayakumar; J. Lee; I. K. Kim; W. Wang","Department of Computer Science, The University of Texas at San Antonio; Department of Computer Science, The University of Georgia; Department of Computer Science, The University of Georgia; Department of Computer Science, The University of Texas at San Antonio","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","779","788","The accurate prediction of the future workload, such as the job arrival rate and the user request rate, is critical to the efficiency of resource management and elasticity in the cloud. However, designing a generic workload predictor that works properly for various types of workload is very challenging due to the large variety of workload patterns and the dynamic changes within a workload. Because of these challenges, existing workload predictors are usually hand-tuned for specific (types of) workloads for maximum accuracy. This necessity to individually tune the predictors also makes it very difficult to reproduce the results from prior research, as the predictor designs have a strong dependency on the workloads.In this paper, we present a novel generic workload prediction framework, LoadDynamics, that can provide high accuracy predictions for any workloads. LoadDynamics employs Long-Short-Term-Memory models and can automatically optimize its internal parameters for an individual workload to achieve high prediction accuracy. We evaluated LoadDynamics with a mixture of workload traces representing public cloud applications, scientific applications, data center jobs and web applications. The evaluation results show that LoadDynamics have only 18% prediction error on average, which is at least 6.7% lower than state-of-the-art workload prediction techniques. The error of LoadDynamics was also only 1% higher than the best predictor found by exhaustive search for each workload. When applied in the Google Cloud, LoadDynamics-enabled auto-scaling policy also outperformed the state-of-the-art predictors by reducing the job turnaround time by at least 24.6% and reducing virtual machine over-provisioning by at least 4.8%.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139813","Cloud Computing;Workload Prediction;Long Short-Term Memory;Self-Optimized Framework;Resource Management","Cloud computing;Predictive models;Load modeling;Google;Logic gates;Encyclopedias","cloud computing;recurrent neural nets;resource allocation;virtual machines","workload patterns;workload predictors;LoadDynamics;workload traces;cloud computing;job arrival rate;user request rate;resource management;long-short-term-memory models;public cloud applications;scientific applications;data center jobs;Web applications;self-optimized generic workload prediction;virtual machine","",1.0,"",56.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"SeeSAw: Optimizing Performance of In-Situ Analytics Applications under Power Constraints","I. Marincic; V. Vishwanath; H. Hoffmann","University of Chicago, Chicago, IL, USA; Argonne National Laboratory, Lemont, IL, USA; University of Chicago, Chicago, IL, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","789","798","Future supercomputers will need to operate under a power budget. At the same time, in-situ analysis-where a set of analysis tasks are concurrently executed and periodically communicate with a scientific simulation-is expected to be a primary HPC workload to overcome the increasing gap between the performance of the storage system relative to the computational capabilities of these machines. Ongoing research focuses on efficient coupling of simulation and analysis considering memory or I/O constraints, but power poses a new constraint that has not yet been addressed for these workflows. There are two state-of-the-art HPC power management approaches: 1) a power-aware scheme that measures and reallocates power based on observed usage and 2) a time-aware scheme that measures the relative time between communicating software modules and reallocates power based on timing differences. We find that considering only one feedback metric has two major drawbacks: 1) both approaches miss opportunities to improve performance and 2) they often make incorrect decisions when facing the unique requirements of in-situ analysis. We therefore propose SeeSAw-an application-aware power management approach, which uses both time and power feedback to balance a power budget and maximize performance for in-situ analysis workloads. We evaluate SeeSAw using the molecular dynamics simulation LAMMPS with a set of built-in analyses running on the Theta supercomputer on up to 1024 nodes. We find that the strictly power-aware approach slows down LAMMPS as much as ~25%. The strictly time-aware approach shows improvements of up to ~13% and slowdowns as much as ~60%. In contrast, SeeSAw achieves ~4-30% performance improvements.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139791","HPC;power-constraints;in-situ analysis","Analytical models;Synchronization;Task analysis;Power system management;Power measurement;Resource management;Computational modeling","data analysis;parallel processing;power aware computing;storage management","power-aware scheme;time-aware scheme;software modules;application-aware power management approach;power feedback;power budget;in-situ analysis workloads;SeeSAw;power constraints;supercomputers;HPC workload;storage system;HPC power management approaches","",2.0,"",45.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"What does Power Consumption Behavior of HPC Jobs Reveal? : Demystifying, Quantifying, and Predicting Power Consumption Characteristics","T. Patel; A. Wagenhäuser; C. Eibel; T. Hönig; T. Zeiser; D. Tiwari","Northeastern University; Friedrich-Alexander University Erlangen-Nürnberg (FAU); Friedrich-Alexander University Erlangen-Nürnberg (FAU); Friedrich-Alexander University Erlangen-Nürnberg (FAU); Friedrich-Alexander University Erlangen-Nürnberg (FAU); Northeastern University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","799","809","As we approach exascale computing, large-scale HPC systems are becoming increasingly power-constrained, requiring them to run HPC workloads in an energy-efficient manner. The first step toward achieving this goal is to better understand, analyze, and quantify the power consumption characteristics of HPC jobs. However, there is a lack of understanding of the power consumption characteristics of HPC jobs which run on production HPC systems. Such characterization is required to guide the design of the next generation of power-aware resource management. To the best of our knowledge, we are the first study to open-source the data and analysis of power-consumption characteristics of HPC jobs and users from two medium-scale production HPC clusters.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139801","","Power demand;Production;Open source software;Power measurement;Energy efficiency;Computational fluid dynamics;Fans","parallel processing;power aware computing;power consumption;virtual machines","HPC jobs;large-scale HPC systems;HPC workloads;production HPC systems;power-aware resource management;medium-scale production HPC clusters;power consumption characteristics prediction;power consumption behavior","",12.0,"",64.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Efficient Parallel and Adaptive Partitioning for Load-balancing in Spatial Join","J. Yang; S. Puri","Computer Science Department, Marquette University, Milwaukee, USA; Computer Science Department, Marquette University, Milwaukee, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","810","820","Due to the developments of topographic techniques, clear satellite imagery, and various means for collecting information, geospatial datasets are growing in volume, complexity, and heterogeneity. For efficient execution of spatial computations and analytics on large spatial data sets, parallel processing is required. To exploit fine-grained parallel processing in large scale compute clusters, partitioning in a load-balanced way is necessary for skewed datasets. In this work, we focus on spatial join operation where the inputs are two layers of geospatial data. Our partitioning method for spatial join uses Adaptive Partitioning (ADP) technique, which is based on Quadtree partitioning. Unlike existing partitioning techniques, ADP partitions the spatial join workload instead of partitioning the individual datasets separately to provide better load-balancing. Based on our experimental evaluation, ADP partitions spatial data in a more balanced way than Quadtree partitioning and Uniform grid partitioning. ADP uses an output-sensitive duplication avoidance technique which minimizes duplication of geometries that are not part of spatial join output. In a distributed memory environment, this technique can reduce data communication and storage requirements compared to traditional methods.To improve the performance of ADP, an MPI+Threads based parallelization is presented. With ParADP, a pair of real world datasets, one with 717 million polylines and another with 10 million polygons, is partitioned into 65,536 grid cells within 7 seconds. ParADP performs well with both good weak scaling up to 4,032 CPU cores and good strong scaling up to 4,032 CPU cores.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139867","spatial join;spatial data partitioning;load-balancing;Adaptive Partitioning;Parallel Partitioning;MPI-GIS;HPC","Geometry;Spatial databases;Partitioning algorithms;Heuristic algorithms;Clustering algorithms;Geospatial analysis;Parallel processing","application program interfaces;geographic information systems;message passing;parallel processing;pattern clustering;quadtrees;resource allocation;visual databases","fine-grained parallel processing;scale compute clusters;load-balanced way;skewed datasets;spatial join operation;geospatial data;partitioning method;adaptive partitioning technique;quadtree partitioning;partitioning techniques;spatial join workload;individual datasets;ADP partitions spatial data;output-sensitive duplication avoidance technique;spatial join output;data communication;storage requirements;parallelization;uniform grid partitioning;spatial data sets;spatial computations;geospatial datasets;clear satellite imagery;topographic techniques;load-balancing;time 7.0 s","",4.0,"",26.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Union: An Automatic Workload Manager for Accelerating Network Simulation","X. Wang; M. Mubarak; Y. Kang; R. B. Ross; Z. Lan","Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","821","830","With the rapid growth of the machine learning applications, the workloads of future HPC systems are anticipated to be a mix of scientific simulation, big data analytics, and machine learning applications. Simulation is a great research vehicle to understand the performance implications of co-running scientific applications with big data and machine learning workloads on large-scale systems. In this paper, we present Union, a workload manager that provides an automatic framework to facilitate hybrid workload simulation in CODES. Furthermore, we use Union, along with CODES, to investigate various hybrid workloads composed of traditional simulation applications and emerging learning applications on two dragonfly systems. The experiment results show that both message latency and communication time are important performance metrics to evaluate network interference. Network interference on HPC applications is more reflected by the message latency variation, whereas ML application performance depends more on the communication time.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139831","High-performance computing;interference;heterogeneous workloads","Skeleton;Task analysis;Computational modeling;Generators;Machine learning;Data models;Analytical models","Big Data;learning (artificial intelligence);parallel processing","machine learning applications;performance implications;scientific applications;machine learning workloads;large-scale systems;automatic framework;hybrid workload simulation;dragonfly systems;network interference;HPC applications;ML application performance;automatic workload manager;network simulation;scientific simulation;Big Data analytics;CODES","",1.0,"",31.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Auto-tuning Parameter Choices in HPC Applications using Bayesian Optimization","H. Menon; A. Bhatele; T. Gamblin","Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, Livermore, California, USA; Department of Computer Science, University of Maryland, College Park, Maryland, USA; Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, Livermore, California, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","831","840","High performance computing applications, runtimes, and platforms are becoming more configurable to enable applications to obtain better performance. As a result, users are increasingly presented with a multitude of options to configure application-specific as well as platform-level parameters. The combined effect of different parameter choices on application performance is difficult to predict, and an exhaustive evaluation of this combinatorial parameter space is practically infeasible. One approach to parameter selection is a user-guided exploration of a part of the space. However, such an ad hoc exploration of the parameter space can result in suboptimal choices. Therefore, an automatic approach that can efficiently explore the parameter space is needed. In this paper, we propose HiPerBOt, a Bayesian optimization based configuration selection framework to identify application and platform-level parameters that result in high performing configurations. We demonstrate the effectiveness of HiPerBOt in tuning parameters that include compiler flags, runtime settings, and application-level options for several parallel codes, including, Kripke, Hypre, LULESH, and OpenAtom.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139814","parameter selection;autotuning;transfer learning;performance","Optimization;Bayes methods;Tuning;Computational modeling;Runtime;Linear programming;History","Bayes methods;configuration management;optimisation;parallel processing;program compilers","Bayesian optimization;platform-level parameters;high performing configurations;application-level options;parameter choices;HPC applications;high performance computing;application performance;combinatorial parameter space;parameter selection;user-guided exploration;configuration selection framework;HiPerBOt;compiler flags;parallel codes;Kripke codes;Hypre codes;LULESH code;OpenAtom code","",11.0,"",36.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Inter-Job Scheduling of High-Throughput Material Screening Applications","Z. Du; X. Hui; Y. Wang; J. Jiang; J. Liu; B. Lu; C. Wang","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Beijing Computing Center, Beijing, China; School of Computing and Information Sciences, Florida International University, Miami, Florida, USA; Department of Physics, Tsinghua University, Beijing, China; Department of Physics, Tsinghua University, Beijing, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","841","852","Material screening entails a large number of electronic structure simulations. Traditionally, these simulation runs are treated separately as solving independent Kohn-Sham (KS) equations. In this paper, we formulate material screening as an inter-job scheduling problem for solving a system of KS equations, and in doing so allowing one to explore different scheduling methods that use the results of some equations to expedite the solution of others. We propose the concept of sharing iterative simulation and employ several optimization methods to initialize a simulation run using the distribution of particles from similar jobs as the initial condition. More specifically, we propose two similarity metrics, one qualitative and the other quantitative, to predict the simulation runtime of a material screen job based on its similarity to other jobs. Accordingly, we present two inter-job scheduling algorithms that make use the qualitative and quantitative similarity information. We conducted extensive experiments on the Sunway TaihuLight supercomputer for a practical material screening problem to evaluate the performance of the two scheduling algorithms using the proposed similarity metrics. We show that the total time required to run the large number of material screening jobs can be significantly reduced, and the algorithms are robust even with moderate inaccurate prediction on the simulation runtime. The quantitative algorithm achieves better results than the qualitative algorithm using more accurate prediction and thus achieving more significant runtime reduction.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139773","High Throughput Computation;HPC Job Scheduling;Material Screening Applications;Similarity Analysis","Mathematical model;Computational modeling;Supercomputers;Processor scheduling;Measurement;Predictive models;Runtime","density functional theory;iterative methods;materials science computing;optimisation;scheduling","high-throughput material screening applications;interjob scheduling algorithms;quantitative similarity information;qualitative similarity information;simulation runtime;similarity metrics;similar jobs;simulation run;iterative simulation;KS equations;independent Kohn-Sham equations;electronic structure simulations","","","",29.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Reservation and Checkpointing Strategies for Stochastic Jobs","A. Gainaru; B. Goglin; V. Honoré; G. Pallez Aupy; P. Raghavan; Y. Robert; H. Sun","Department of EECS, Vanderbilt University, Nashville, TN, USA; Inria, LaBRI, Univ. Bordeaux, Talence, France; Inria, LaBRI, Univ. Bordeaux, Talence, France; Inria, LaBRI, Univ. Bordeaux, Talence, France; Department of EECS, Vanderbilt University, Nashville, TN, USA; Laboratoire LIP, ENS Lyon, France & University of Tennessee, Knoxville, USA; Department of EECS, Vanderbilt University, Nashville, TN, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","853","863","In this paper, we are interested in scheduling and checkpointing stochastic jobs on a reservation-based platform, whose cost depends both (i) on the reservation made, and (ii) on the actual execution time of the job. Stochastic jobs are jobs whose execution time cannot be determined easily. They arise from the heterogeneous, dynamic and data-intensive requirements of new emerging fields such as neuroscience. In this study, we assume that jobs can be interrupted at any time to take a checkpoint, and that job execution times follow a known probability distribution. Based on past experience, the user has to determine a sequence of fixed-length reservation requests, and to decide whether the state of the execution should be checkpointed at the end of each request. The objective is to minimize the expected cost of a successful execution of the jobs. We provide an optimal strategy for discrete probability distributions of job execution times, and we design fully polynomial-time approximation strategies for continuous distributions with bounded support. These strategies are then experimentally evaluated and compared to standard approaches such as periodic-length reservations and simple checkpointing strategies (either checkpoint all reservations, or none). The impact of an imprecise knowledge of checkpoint and restart costs is also assessed experimentally.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139860","scheduling;checkpointing;stochastic job;reservation-based platform;reservation sequence;neuroscience application","Probability distribution;Checkpointing;Stochastic processes;Neuroscience;Standards;Computational modeling;Heuristic algorithms","checkpointing;computational complexity;optimisation;polynomial approximation;scheduling;statistical distributions;stochastic processes","scheduling;reservation-based platform;heterogeneous data-intensive requirements;job execution times;fixed-length reservation requests;discrete probability distributions;polynomial-time approximation strategies;checkpointing strategies;stochastic jobs checkpointing;reservation strategies;stochastic jobs scheduling;data-intensive requirements","",1.0,"",32.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"A Scheduling Approach to Incremental Maintenance of Datalog Programs","S. Singh; S. Madaminov; M. A. Bender; M. Ferdman; R. Johnson; B. Moseley; H. Ngo; D. Nguyen; S. Olesen; K. Stirewalt; G. Washburn","Williams College, Williamstown, MA, USA; Stony Brook University, Stony Brook, NY, USA; Stony Brook University, Stony Brook, NY, USA; Stony Brook University, Stony Brook, NY, USA; Amazon, Inc.; Carnegie Mellon University, Pittsburgh, PA, USA; Relational AI; Infor, Inc.; Infor, Inc.; Relational AI; Infor, Inc.","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","864","873","In this paper, we study the problem of incremental maintenance of Datalog programs and model it as a scheduling problem on DAGs. We design provably good time- and memory-efficient scheduling algorithms for (re)executing a Datalog program where some (but not necessarily all) of the inputs have changed. We prove that our schedulers, called LevelBased and LevelBased with lookahead, have asymptotically improved running time and space efficiency when compared with benchmark algorithms used in production at LogicBlox.The main result of the paper is a hybrid scheduler, which combines LevelBased with the production LogicBlox scheduler (or any other heuristic scheduler). The hybrid scheduler achieves strong worst-case guarantees and robustness without losing out on the best-case behavior of the production LogicBlox scheduler. Our experiments show that the hybrid scheduler results in similar or improved total execution times compared to LogicBlox scheduler, while consistently reducing the scheduling overhead-by as much as 50% on some datasets. This hybrid scheme requires little to no overhead but provides predictability and reliability, which are crucial in a commercial application such as LogicBlox.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139794","Datalog programs;incremental maintenance;DAG scheduling;parallel task scheduling;databases;incremental computing;LogicBlox","Task analysis;Processor scheduling;Maintenance engineering;Databases;Schedules;Heuristic algorithms","computational complexity;DATALOG;scheduling;software maintenance","incremental maintenance;Datalog program;memory-efficient scheduling algorithms;LevelBased scheduler;LevelBased with lookahead scheduler;LogicBlox scheduler;application reliability;time-efficient scheduling algorithm","","","",35.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Dynamic Scheduling in Distributed Transactional Memory","C. Busch; M. Herlihy; M. Popovic; G. Sharma","Louisiana State University, Baton Rouge, LA, USA; Brown University, Providence, RI, USA; University of Novi Sad, Novi Sad, Serbia; Kent State University, Kent, OH, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","874","883","We investigate scheduling algorithms for distributed transactional memory systems where transactions residing at nodes of a communication graph operate on shared, mobile objects. A transaction requests the objects it needs, executes once those objects have been assembled, and then sends the objects to other waiting transactions. We study scheduling algorithms with provable performance guarantees. Previously, only the offline batch scheduling setting was considered in the literature where transactions and the objects they access are known a priori. Minimizing execution time, even for the offline batch scheduling, is known to be NP-hard for arbitrary communication graphs. In this paper, we analyze for the very first time scheduling algorithms in the online dynamic scheduling setting where transactions and the objects they access are not known a priori and the transactions may arrive online over time. We provide efficient and near-optimal execution time schedules for dynamic scheduling in many specialized network architectures. The core of our technique is a method to convert offline schedules to online. We first describe a centralized scheduler which we then adapt it to a purely distributed scheduler. To our knowledge, these are the first attempts to obtain provably efficient online execution schedules for distributed transactional memory.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139858","Transactional memory;distributed systems;execution time;data-flow model;dynamic scheduling","Schedules;Dynamic scheduling;Program processors;Scheduling algorithms;Hypercubes;Heuristic algorithms","batch processing (computers);computational complexity;distributed processing;graph theory;optimisation;processor scheduling;transaction processing","communication graph;transaction requests;arbitrary communication graphs;online dynamic scheduling;distributed scheduler;distributed transactional memory systems;online execution schedules;NP-hard;offline batch scheduling settin","",3.0,"",30.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Learning Cost-Effective Sampling Strategies for Empirical Performance Modeling","M. Ritter; A. Calotoiu; S. Rinke; T. Reimann; T. Hoefler; F. Wolf","Department of Computer Science, Technical University of Darmstadt, Germany; Department of Computer Science, Technical University of Darmstadt, Germany; Department of Computer Science, Technical University of Darmstadt, Germany; Department of Computer Science, Technical University of Darmstadt, Germany; Department of Computer Science, ETH Zürich, Switzerland; Department of Computer Science, Technical University of Darmstadt, Germany","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","884","895","Identifying scalability bottlenecks in parallel applications is a vital but also laborious and expensive task. Empirical performance models have proven to be helpful to find such limitations, though they require a set of experiments in order to gain valuable insights. Therefore, the experiment design determines the quality and cost of the models. Extra-P is an empirical modeling tool that uses small-scale experiments to assess the scalability of applications. Its current version requires an exponential number of experiments per model parameter. This makes the creation of empirical performance models very expensive, and in some situations even impractical. In this paper, we propose a novel parameter-value selection heuristic, which functions as a guideline for the experiment design, leveraging sparse performance-modeling, a technique that only needs a polynomial number of experiments per model parameter. Using synthetic analysis and data from three different case studies, we show that our solution reduces the average modeling costs by about 85% while retaining 92% of the model accuracy.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139859","Performance analysis;performance modeling;reinforcement learning;high-performance computing;parallel processing","Computational modeling;Biological system modeling;Analytical models;Data models;Mathematical model;Current measurement","computational complexity;learning (artificial intelligence);parallel processing;performance evaluation;polynomials;sampling methods","small-scale experiments;experiment design;average modeling costs;empirical performance modeling;cost-effective sampling strategies;sparse performance-modeling leveraging;Extra-P","",10.0,"",39.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"The Case of Performance Variability on Dragonfly-based Systems","A. Bhatele; J. J. Thiagarajan; T. Groves; R. Anirudh; S. A. Smith; B. Cook; D. K. Lowenthal","Department of Computer Science, University of Maryland, College Park, Maryland, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, California, USA; NERSC, Lawrence Berkeley National Laboratory, Berkeley, California, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, California, USA; Department of Computer Science, The University of Arizona, Tucson, Arizona, USA; NERSC, Lawrence Berkeley National Laboratory, Berkeley, California, USA; Department of Computer Science, The University of Arizona, Tucson, Arizona, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","896","905","Performance of a parallel code running on a large supercomputer can vary significantly from one run to another even when the executable and its input parameters are left unchanged. Such variability can occur due to perturbation of the computation and/or communication in the code. In this paper, we investigate the case of performance variability arising due to network effects on supercomputers that use a dragonfly topology - specifically, Cray XC systems equipped with the Aries interconnect. We perform post-mortem analysis of network hardware counters, profiling output, job queue logs, and placement information, all gathered from periodic representative application runs. We investigate the causes of performance variability using deviation prediction and recursive feature elimination. Additionally, using time-stepped performance data of individual applications, we train machine learning models that can forecast the execution time of future time steps.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139880","performance variability;dragonfly network;data analytics;machine learning;performance models;forecasting","Network topology;3G mobile communication;Topology;Predictive models;Supercomputers;Data models;Production","learning (artificial intelligence);multiprocessor interconnection networks;parallel processing;performance evaluation;support vector machines","network hardware counters;periodic representative application runs;performance variability;time-stepped performance data;dragonfly-based systems;parallel code;supercomputer;dragonfly topology;Cray XC systems;profiling output;job queue logs;placement information","",13.0,"",21.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Predicting and Comparing the Performance of Array Management Libraries","D. Kang; O. Rübel; S. Byna; S. Blanas","The Ohio State University; Lawrence Berkeley National Laboratory; Lawrence Berkeley National Laboratory; The Ohio State University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","906","915","Many applications are increasingly becoming I/O-bound. To improve scalability, analytical models of parallel I/O performance are often consulted to determine possible I/O optimizations. However, I/O performance modeling has predominantly focused on applications that directly issue I/O requests to a parallel file system or a local storage device. These I/O models are not directly usable by applications that access data through standardized I/O libraries, such as HDF5, FITS, and NetCDF, because a single I/O request to an object can trigger a cascade of I/O operations to different storage blocks. The I/O performance characteristics of applications that rely on these libraries is a complex function of the underlying data storage model, user-configurable parameters and object-level access patterns. As a consequence, I/O optimization is predominantly an ad-hoc process that is performed by application developers, who are often domain scientists with limited desire to delve into nuances of the storage hierarchy of modern computers.This paper presents an analytical cost model to predict the end-to-end execution time of applications that perform I/O through established array management libraries. The paper focuses on the HDF5 and Zarr array libraries, as examples of I/O libraries with radically different storage models: HDF5 stores every object in one file, while Zarr creates multiple files to store different objects. We find that accessing array objects via these I/O libraries introduces new overheads and optimizations. Specifically, in addition to I/O time, it is crucial to model the cost of transforming data to a particular storage layout (memory copy cost), as well as model the benefit of accessing a software cache. We evaluate the model on real applications that process observations (neuroscience) and simulation results (plasma physics). The evaluation on three HPC clusters reveals that I/O accounts for as little as 10% of the execution time in some cases, and hence models that only focus on I/O performance cannot accurately capture the performance of applications that use standard array storage libraries. In parallel experiments, our model correctly predicts the fastest storage library between HDF5 and Zarr 94% of the time, in contrast with 70% of the time for a cutting-edge I/O model.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139845","","Libraries;Arrays;Servers;Predictive models;Data models;Indexes;Computational modeling","cache storage;parallel processing;software libraries;software performance evaluation;storage management","analytical models;optimizations;parallel file system;local storage device;access data;different storage blocks;performance characteristics;complex function;underlying data storage model;user-configurable parameters;object-level access patterns;ad-hoc process;application developers;domain scientists;storage hierarchy;analytical cost model;end-to-end execution time;Zarr array libraries;radically different storage models;HDF5 stores;multiple files;accessing array objects;particular storage layout;memory copy cost;standard array storage libraries;parallel experiments;fastest storage library;Zarr 94;array management libraries;parallel I/O performance;standardized I/O libraries","",3.0,"",18.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Demystifying the Performance of HPC Scientific Applications on NVM-based Memory Systems","I. Peng; K. Wu; J. Ren; D. Li; M. Gokhale","Lawrence Livermore National Laboratory, Livermore, USA; University of California, Merced, Merced, USA; University of California, Merced, Merced, USA; University of California, Merced, Merced, USA; Lawrence Livermore National Laboratory, Livermore, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","916","925","The emergence of high-density byte-addressable non-volatile memory (NVM) is promising to accelerate data-and compute-intensive applications. Current NVM technologies have lower performance than DRAM and, thus, are often paired with DRAM in a heterogeneous main memory. Recently, byte-addressable NVM hardware becomes available. This work provides a timely evaluation of representative HPC applications from the ""Seven Dwarfs"" on NVM-based main memory. Our results quantify the effectiveness of DRAM-cached-NVM for accelerating HPC applications and enabling large problems beyond the DRAM capacity. On uncached-NVM, HPC applications exhibit three tiers of performance sensitivity, i.e., insensitive, scaled, and bottlenecked. We identify write throttling and concurrency control as the priorities in optimizing applications. We highlight that concurrency change may have a diverging effect on read and write accesses in applications. Based on these findings, we explore two optimization approaches. First, we provide a prediction model that uses datasets from a small set of configurations to estimate performance at various concurrency and data sizes to avoid exhaustive search in the configuration space. Second, we demonstrate that write-aware data placement on uncached-NVM could achieve 2x performance improvement with a 60% reduction in DRAM usage.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139816","Non-volatile memory;Optane;heterogeneous memory;persistent memory;byte-addressable NVM;HPC","Nonvolatile memory;Random access memory;Concurrent computing;Sockets;Hardware;Bandwidth;Sensitivity","circuit optimisation;DRAM chips;integrated circuit modelling;memory architecture","byte-addressable NVM hardware;NVM-based main memory;DRAM-cached-NVM;NVM-based memory systems;high-density byte-addressable nonvolatile memory;heterogeneous main memory;high-density byte-addressable NVM;data-intensive application;compute-intensive application;DRAM;read and write access","",8.0,"",34.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Packet-in Request Redirection for Minimizing Control Plane Response Time","R. Xia; H. Dai; J. Zheng; H. Xu; M. Li; G. Chen","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, CHINA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, CHINA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, CHINA; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, CHINA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, CHINA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, CHINA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","926","935","A distributed control plane is more scalable and robust in software defined networking. This paper focuses on controller load balancing using packet-in request redirection, that is, given the instantaneous state of the system, determining whether to redirect packet-in requests for each switch, such that the overall control plane response time (CPRT) is minimized. To address the above problem, we propose a framework based on Lyapunov optimization. First, we use the drift-plus-penalty algorithm to combine CPRT minimization problem with controller capacity constraints, and further derive a non-linear program, whose optimal solution is obtained with brute force using standard linearization techniques. Second, we present a greedy strategy to efficiently obtain a solution with a bounded approximation ratio. Third, we reformulate the program as a problem of maximizing a non-monotone submodular function subject to matroid constraints. We implement a controller proto-type for packet-in request redirection, and conduct trace-driven simulations to validate our theoretical results. The results show that our algorithms can reduce the average CPRT by 81.6% compared to static controller-switch assignment, and achieve a 3× improvement in maximum controller capacity violation ratio.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139878","","Switches;Minimization;Process control;Approximation algorithms;Heuristic algorithms;Optimization","approximation theory;distributed control;greedy algorithms;linearisation techniques;minimisation;nonlinear programming;optimisation;resource allocation;software defined networking;telecommunication control","standard linearization techniques;trace-driven simulations;Lyapunov optimization;CPRT minimization;control plane response time minimization;static controller-switch assignment;nonlinear program;controller capacity constraints;controller load balancing;software defined networking;distributed control plane;packet-in request redirection;maximum controller capacity violation ratio","","","",32.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"PCGCN: Partition-Centric Processing for Accelerating Graph Convolutional Network","C. Tian; L. Ma; Z. Yang; Y. Dai","Peking University, Beijing, China; Peking University, Beijing, China; Peking University, Beijing, China; Peking University, Beijing, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","936","945","Inspired by the successes of convolutional neural networks (CNN) in computer vision, the convolutional operation has been moved beyond low-dimension grids (e.g., images) to high-dimensional graph-structured data (e.g., web graphs, social networks), leading to graph convolutional network (GCN). And GCN has been gaining popularity due to its success in real-world applications such as recommendation, natural language processing, etc. Because neural network and graph propagation have high computation complexity, GPUs have been introduced to both neural network training and graph processing. However, it is notoriously difficult to perform efficient GCN computing on data parallel hardware like GPU due to the sparsity and irregularity in graphs. In this paper, we present PCGCN, a novel and general method to accelerate GCN computing by taking advantage of the locality in graphs. We experimentally demonstrate that real-world graphs usually have the clustering property that can be used to enhance the data locality in GCN computing. Then, PCGCN proposes to partition the whole graph into chunks according to locality and process subgraphs with a dual-mode computing strategy which includes a selective and a full processing methods for sparse and dense subgraphs, respectively. Compared to existing state-of-the-art implementations of GCN on real-world and synthetic datasets, our implementation on top of TensorFlow achieves up to 8.8× speedup over the fastest one of the baselines.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139807","graph convolutional network (GCN);GPU;graph;deep learning","Graphics processing units;Acceleration;Neural networks;Computational modeling;Partitioning algorithms;Training;Machine learning","convolutional neural nets;graph theory;learning (artificial intelligence);pattern clustering","PCGCN;process subgraphs;dual-mode computing strategy;partition-centric processing;graph convolutional network;convolutional neural networks;computer vision;high-dimensional graph-structured data;high computation complexity;neural network training;graph processing;GCN computing;real-world graphs;graph propagation;computation complexity;clustering property;data locality","",9.0,"",45.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"ConMidbox: Consolidated Middleboxes Selection and Routing in SDN/NFV-Enabled Networks","G. Liu; S. Guo; P. Li; L. Liu","School of Electronic and Information Engineering, Southwest University, Chongqing, P. R. China; College of Computer Science, Chongqing University, Chongqing, P. R. China; School of Electronic and Information Engineering, Southwest University, Chongqing, P. R. China; School of Electronic and Information Engineering, Southwest University, Chongqing, P. R. China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","946","955","Software defined networking (SDN) and network function virtualization (NFV) can flexibly manage software middlebox based services, and the consolidated middlebox model is able to simplify traffic routing and reduce the number of routing rules in the SDN-enabled switches. However, different network functions in middleboxes may change the volume of processed traffics, thus high congestion may occur in specific bottleneck links if middlebox selection and traffic routing are not well jointly planned. Besides, in a statically switch-controller configured SDN, traffic dynamics will not only affect the link load in data plane, but also pose a challenge to controller load balancing. Therefore, it’s necessary to achieve better quality-of-service (QoS) performance in both control and data plane. This paper first formulates it as a joint traffic-aware consolidated middleboxes selection and routing (JTMSR) problem and proves its NP-hardness. Then, a two-phase RL_RFRD algorithm is designed to achieve the controller and link load balancing where the first phase is to redirect selected flows by applying wildcard rules and the second phase is to find fine-grained routing path by a rounding-based algorithm with bounded approximation factor. Finally, the extensive simulation results demonstrate that the proposed algorithm has near-optimal controller load balancing and link load balancing performance and reduces response time by about 2x-5x compared with other algorithms.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139840","Software defined networking;Consolidated middlebox;Load balancing;Rounding;Scalability","Middleboxes;Routing;Optical switches;Load management;Approximation algorithms;Time factors","quality of service;resource allocation;software defined networking;telecommunication network routing;telecommunication traffic;virtualisation","consolidated middleboxes selection;network function virtualization;software middlebox based services;consolidated middlebox model;traffic routing;routing rules;SDN-enabled switches;network functions;middlebox selection;statically switch-controller configured SDN;traffic dynamics;data plane;quality-of-service performance;joint traffic-aware;two-phase RL_RFRD algorithm;fine-grained routing path;rounding-based algorithm;near-optimal controller load balancing;ConMidbox;software defined networking;quality-of-service;control plane;link load balancing performance;bounded approximation factor","","","",33.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Scalable and Memory-Efficient Kernel Ridge Regression","G. Chávez; Y. Liu; P. Ghysels; X. S. Li; E. Rebrova","Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, USA; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, USA; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, USA; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, USA; Department of Mathematics, University of California, Los Angeles, Los Angeles, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","956","965","We present a scalable and memory-efficient framework for kernel ridge regression. We exploit the inherent rank deficiency of the kernel ridge regression matrix by constructing an approximation that relies on a hierarchy of low-rank factorizations of tunable accuracy, rather than leverage scores or other subsampling techniques. Without ever decompressing the kernel matrix approximation, we propose factorization and solve methods to compute the weight(s) for a given set of training and test data. We show that our method performs an optimal number of operations $\mathcal{O}\left( {{r^2}n} \right)$ with respect to the number of training samples (n) due to the underlying numerical low-rank (r) structure of the kernel matrix. Furthermore, each algorithm is also presented in the context of a massively parallel computer system, exploiting two levels of concurrency that take into account both shared-memory and distributed-memory inter-node parallelism. In addition, we present a variety of experiments using popular datasets – small, and large – to show that our approach provides sufficient accuracy in comparison with state-of-the-art methods and with the exact (i.e. non-approximated) kernel ridge regression method. For datasets, in the order of 106 data points, we show that our framework strong-scales to 103 cores. Finally, we provide a Python interface to the scikit-learn library so that scikit-learn can leverage our high-performance solver library to achieve much-improved performance and memory footprint.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139856","","Kernel;Approximation algorithms;Matrix decomposition;Libraries;Complexity theory;Symmetric matrices","approximation theory;computational complexity;learning (artificial intelligence);mathematics computing;matrix decomposition;regression analysis","memory-efficient kernel ridge regression;scalable memory-efficient framework;kernel ridge regression matrix;low-rank factorizations;tunable accuracy;subsampling techniques;kernel matrix approximation;optimal number;training samples;numerical low-rank structure;massively parallel computer system;kernel ridge regression method;distributed-memory internode parallelism;shared-memory system","",2.0,"",37.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"SSDKeeper: Self-Adapting Channel Allocation to Improve the Performance of SSD Devices","R. Liu; X. Chen; Y. Tan; R. Zhang; L. Liang; D. Liu","Key Lab. of Dependable Service Computing in Cyber Physical Society (Chongqing Univ.), Ministry of Education, China; Key Lab. of Dependable Service Computing in Cyber Physical Society (Chongqing Univ.), Ministry of Education, China; Key Lab. of Dependable Service Computing in Cyber Physical Society (Chongqing Univ.), Ministry of Education, China; Key Lab. of Dependable Service Computing in Cyber Physical Society (Chongqing Univ.), Ministry of Education, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; Key Lab. of Dependable Service Computing in Cyber Physical Society (Chongqing Univ.), Ministry of Education, China","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","966","975","Solid state drives (SSDs) have been widely deployed in high performance data center environments, where multiple tenants usually share the same hardware. However, traditional SSDs distribute the users' incoming data uniformly across all SSD channels, which leads to numerous access conflicts. Meanwhile, SSDs that statically allocate one or several channels to one tenant sacrifice device parallelism and capacity. When SSDs are shared by tenants with different access patterns, inappropriate channel allocation results in SSDs performance degradation. In this paper, we propose a self-adapting channel allocation mechanism, named SSDKeeper, for multiple tenants to share one SSD. SSDKeeper employs a machine learning assisted algorithm to take full advantage of SSD parallelism while providing performance isolation. By collecting multi-tenant access patterns and training a model, SSDKeeper selects an optimal channel allocation strategy for multiple tenants with the lowest overall response latency. Experimental results show that SSDKeeper improves the overall performance by 24% with negligible overhead.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139770","","Channel allocation;Performance evaluation;Hardware;Parallel processing;Optimization;Training;Neural networks","channel allocation;learning (artificial intelligence);parallel processing;solid state drives;storage management","SSDs performance degradation;self-adapting channel allocation mechanism;SSD parallelism;performance isolation;multitenant access patterns;optimal channel allocation strategy;SSD devices;solid state drives;high performance data center environments;SSD channels;tenant sacrifice device parallelism;SSDKeeper;machine learning","",9.0,"",29.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"FlashKey:A High-Performance Flash Friendly Key-Value Store","M. Ray; K. Kant; P. Li; S. Trika","Temple University, Philadelphia, PA; Temple University, Philadelphia, PA; Intel Corporation, Aloha, OR; Intel Corporation, Aloha, OR","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","976","985","Key-value stores (KVS) provide an efficient storage for increasing amounts of semi-structured or unstructured data generated by many applications. Most KVS in existence have been designed for hard-disk based storage where avoiding random accesses is crucial for good performance. Unfortunately, the resulting storage structures result in high read, write, and space amplifications when used on modern SSDs. In this paper, we introduce a KV store especially designed for SSDs, called FlashKey, and demonstrate that even as an initial implementation, it substantially outperforms the two most popular commercial KVS in existence, namely, Google's LevelDB and Facebook's RocksDB. In particular, we show that FlashKey achieves up to 85% improvement in average access latency, 2x improvement in tail latencies, and 12x improvement in write amplification, at comparable or better space-amplification. Furthermore, FlashKey can easily trade off space and write amplifications, thereby providing a new tuning knob that is difficult to implement in LevelDB and RocksDB.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139846","key-value store;read amplification;tail latency;write amplification;space amplification;YCSB;hashing","Indexes;Performance evaluation;Quality of service;Compaction;Google","flash memories;storage management","high-performance flash friendly key-value store;hard-disk based storage;storage structures;KV store;popular commercial KVS;FlashKey;random accesses;semi-structured data;unstructured data","",1.0,"",37.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Pacon: Improving Scalability and Efficiency of Metadata Service through Partial Consistency","Y. Liu; Y. Lu; Z. Chen; M. Zhao","Sun Yat-sen University; Sun Yat-sen University; Sun Yat-sen University; Arizona State University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","986","996","Traditional distributed file systems (DFS) use centralized service to manage metadata. Many studies based on this centralized architecture enhanced metadata processing capability by scaling the metadata server cluster, which is however still difficult to keep up with the growing number of clients and the increasingly metadata-intensive applications. Some solutions abandoned the centralized metadata service and improved scalability by embedding a private metadata service in an HPC application, but these solutions are suitable for only some specific applications and the absence of global namespace makes data sharing and management difficult. This paper addresses the shortcomings of existing studies by optimizing the consistency model of client- side metadata cache for the HPC scenario using a novel partial consistency model. It provides the application with strong consistency guarantee for only its workspace, thus improving metadata scalability without adding hardware or sacrificing the versatility and manageability of DFSes. In addition, the paper proposes batch permission management to reduce path traversal overhead, thereby improving metadata processing efficiency. The result is a library (Pacon) that allows existing DFSes to achieve partial consistency for scalable and efficient metadata management. The paper also presents a comprehensive evaluation using intensive benchmarks and representative application. For example, in file creation, Pacon improves the performance of BeeGFS by more than 76.4 times, and outperforms the state-of-the-art metadata management solution (IndexFS) by more than 4.6 times.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139884","metadata;scalability;efficiency;distributed file system;consistency","Metadata;Scalability;Servers;Hardware;Computer architecture;Throughput;Libraries","cache storage;cluster computing;data privacy;distributed databases;meta data;parallel processing;storage management","Pacon;file systems;metadata processing capability;metadata server cluster;metadata-intensive applications;centralized metadata service;private metadata service;HPC application;data sharing;partial consistency;metadata scalability;batch permission management;metadata management solution;client-side metadata cache","",3.0,"",23.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"XPlacer: Automatic Analysis of Data Access Patterns on Heterogeneous CPU/GPU Systems","P. Pirkelbauer; P. -H. Lin; T. Vanderbruggen; C. Liao","Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","997","1007","This paper presents XPlacer, a framework to automatically analyze problematic data access patterns in C++ and CUDA code. XPlacer records heap memory operations in both host and device code for later analysis. To this end, XPlacer instruments read and write operations, function calls, and kernel launches. Programmers mark points in the program execution where the recorded data is analyzed and anomalies diagnosed. XPlacer reports data access anti-patterns, including alternating CPU/GPU accesses to the same memory, memory with low access density, and unnecessary data transfers. The diagnostic also produces summative information about the recorded accesses, which aids users in identifying code that could degrade performance. The paper evaluates XPlacer using LULESH, a Lawrence Livermore proxy application, Rodina benchmarks, and an implementation of the Smith-Waterman algorithm. XPlacer diagnosed several performance issues in these codes. The elimination of a performance problem in LULESH resulted in a 3x speedup on a heterogeneous platform combining Intel CPUs and Nvidia GPUs.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139836","GPGPU;heterogeneous systems;high-performance computing;code instrumentation","Graphics processing units;Instruments;Memory management;Runtime;Data transfer;Kernel","graphics processing units;parallel architectures;storage management","automatic analysis;data access patterns;CUDA code;device code;XPlacer instruments;access density;data transfers;recorded accesses;heap memory operations;data access antipatterns;XPlacer rec;heterogeneous CPU/GPU systems;programmer mark points;LULESH;Lawrence Livermore proxy application;Rodina benchmarks","",2.0,"",29.0,"USGov","14 Jul 2020","","","IEEE","IEEE Conferences"
"Improving Transactional Code Generation via Variable Annotation and Barrier Elision","J. P. L. de Carvalho; B. C. Honorio; A. Baldassin; G. Araujo","Institute of Computing - UNICAMP, Campinas, Brazil; Institute of Computing - UNICAMP, Campinas, Brazil; UNESP - Univ Estadual Paulista, Rio Claro, Brazil; Institute of Computing - UNICAMP, Campinas, Brazil","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1008","1017","With chip manufacturers such as Intel, IBM and ARM offering native support for transactional memory in their instruction set architectures, memory transactions are on the verge of being considered a genuine application tool rather than just an interesting research topic. Despite this recent increase in popularity on the hardware side of transactional memory (HTM), software support for transactional memory (STM) is still scarce and the only compiler with transactional support currently available, the GNU Compiler Collection (GCC), does not generate code that achieves desirable performance. This paper presents a detailed analysis of transactional code generated by GCC and by a proposed transactional memory support added to the Clang/LLVM compiler framework. Experimental results support the following contributions: (a) STM's performance overhead is due to an excessive amount of read and write barriers added by the compiler; (b) a new annotation mechanism for the Clang/LLVM compiler framework that aims to overcome the barrier over-instrumentation problem by allowing programmers to specify which variables should be free from transactional instrumentation; (c) a profiling tool that ranks the most accessed memory locations at runtime, working as a guiding tool for programmers to annotate the code. Furthermore, it is revealed that, by correctly using the annotations on just a few lines of code, it is possible to reduce the total number of instrumented barriers by 95% and to achieve speed-ups of up to 7× when compared to the original code generated by GCC and the Clang compiler.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139775","Transactional Memory;Compilers;Debugging","Instruments;Hardware;Program processors;Tools;Runtime;Genomics","instruction sets;program compilers;storage management;transaction processing","barrier over-instrumentation problem;transactional instrumentation;accessed memory locations;instrumented barriers;GCC;Clang compiler;transactional code generation;variable annotation;barrier elision;instruction set architectures;memory transactions;software support;transactional support;GNU compiler collection;transactional memory;annotation mechanism;STM performance overhead;LLVM compiler","",3.0,"",36.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Evaluating Thread Coarsening and Low-cost Synchronization on Intel Xeon Phi","H. Wu; M. Becchi","North Carolina State University; North Carolina State University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1018","1029","Manycore processors such as GPUs and Intel Xeon Phis have become popular due to their massive parallelism and high power-efficiency. To achieve optimal performance, it is necessary to optimize the use of the compute cores and of the memory system available on these devices. Previous work has proposed techniques to improve the use of the GPU resources. While Intel Phi can provide massive parallelism through their x86 cores and vector units, optimization techniques for these platforms have received less consideration.In this work, we study the benefits of thread coarsening and low-cost synchronization on applications running on Intel Xeon Phi processors and encoded in SIMT fashion. Specifically, we explore thread coarsening as a way to remap the work to the available cores and vector lanes. In addition, we propose low- overhead synchronization primitives, such as atomic operations and barriers, which transparently apply to threads mapped to the same or different VPUs and x86 cores. Finally, we consider the combined use of thread coarsening and our proposed synchronization primitives. We evaluate the effect of these techniques on the performance of two kinds of kernels: collaborative and non-collaborative ones, the former using scratchpad memory to explicitly control data sharing among threads. Our evaluation leads to the following results. First, while not always beneficial for non-collaborative kernels, thread coarsening improves the performance of collaborative kernels consistently by reducing the synchronization overhead. Second, our synchronization primitives outperform standard pthread APIs by a factor up to 8x in real-world benchmarks. Last, the combined use of the proposed techniques leads to performance improvements, especially for collaborative kernels.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139883","SIMT;manycore processors;Intel Xeon Phi;thread coarsening;synchronization","Erbium;Distributed processing;Hafnium;Hidden Markov models","graphics processing units;multiprocessing systems;multi-threading;parallel algorithms;parallel architectures;parallel processing;synchronisation","synchronization overhead;collaborative kernels;overhead synchronization primitives;vector lanes;Intel Xeon Phi processors;optimization techniques;vector units;x86 cores;Intel Phi;compute cores;optimal performance;massive parallelism;low-cost synchronization;thread coarsening","",1.0,"",28.0,"USGov","14 Jul 2020","","","IEEE","IEEE Conferences"
"AnySeq: A High Performance Sequence Alignment Library based on Partial Evaluation","A. Müller; B. Schmidt; A. Hildebrandt; R. Membarth; R. Leißa; M. Kruse; S. Hack","Johannes Gutenberg University; Johannes Gutenberg University; Johannes Gutenberg University; Saarland Informatics Campus, DFKI; Saarland Informatics Campus, Saarland University; Saarland Informatics Campus, Saarland University; Saarland Informatics Campus, Saarland University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1030","1040","Sequence alignments are fundamental to bioinformatics which has resulted in a variety of optimized implementations. Unfortunately, the vast majority of them are hand-tuned and specific to certain architectures and execution models. This not only makes them challenging to understand and extend, but also difficult to port to other platforms. We present AnySeq-a novel library for computing different types of pairwise alignments of DNA sequences. Our approach combines high performance with an intuitively understandable implementation, which is achieved through the concept of partial evaluation. Using the AnyDSL compiler framework, AnySeq enables the compilation of algorithmic variants that are highly optimized for specific usage scenarios and hardware targets with a single, uniform codebase. The resulting domain-specific library thus allows the variation of alignment parameters (such as alignment type, scoring scheme, and traceback vs. plain score) by simple function composition rather than metaprogramming techniques which are often hard to understand. Our implementation supports multithreading and SIMD vectorization on CPUs, CUDA-enabled GPUs, and FPGAs. AnySeq is at most 7% slower and in many cases faster (up to 12%) than state-of-the art manually optimized alignment libraries on CPUs (SeqAn) and on GPUs (NVBio).","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139784","","Libraries;Computer architecture;Generators;Bioinformatics;Hardware;Field programmable gate arrays;Two dimensional displays","bioinformatics;field programmable gate arrays;graphics processing units;multi-threading;parallel architectures;parallel processing;program compilers","FPGA;CUDA-enabled GPU;SIMD vectorization;multithreading;CPU;AnySeq;hardware targets;specific usage scenarios;AnyDSL compiler framework;intuitively understandable implementation;DNA sequences;pairwise alignments;execution models;optimized implementations;sequence alignments;partial evaluation;high performance sequence alignment library;state-of-the art manually optimized alignment libraries;alignment parameters;resulting domain-specific library;uniform codebase","",3.0,"",29.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Analysis of a List Scheduling Algorithm for Task Graphs on Two Types of Resources","L. Eyraud-Dubois; S. Kumar","Inria Bordeaux – Sud-Ouest, Université de Bordeaux, France; Inria, Paris, France","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1041","1050","We consider the problem of scheduling task graphs on two types of unrelated resources, which arises in the context of task-based runtime systems on modern platforms containing CPUs and GPUs. In this paper, we focus on an algorithm named HeteroPrio, which was originally introduced as an efficient heuristic for a particular application. HeteroPrio is an adaptation of the well known list scheduling algorithm, in which the tasks are picked by the resources in the order of their acceleration factor. This algorithm is augmented with a spoliation mechanism: a task assigned by the list algorithm can later on be reassigned to a different resource if it allows to finish this task earlier.We propose here the first theoretical analysis of the HeteroPrio algorithm in the presence of dependencies. More specifically, if the platform contains m and n processors of each type, we show that the worst-case approximation ratio of HeteroPrio is between 1 + max (m/n, n/m) and 2 + (m/n, n/m). Our proof structure allows to precisely identify the necessary conditions on the spoliation strategy to obtain such a guarantee. We also present an in-depth experimental analysis, comparing several such spoliation strategies, and comparing HeteroPrio with other algorithms from the literature. Although the worst case analysis shows the possibility of pathological behavior, HeteroPrio is able to produce, in very reasonable time, schedules of significantly better quality.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139885","Unrelated Resources;Scheduling;Task Graphs;Approximation Proofs;Linear Algebra","Task analysis;Approximation algorithms;Schedules;Graphics processing units;Runtime;Scheduling algorithms","approximation theory;computational complexity;graph theory;resource allocation;scheduling;theorem proving","task-based runtime systems;list scheduling algorithm;spoliation mechanism;HeteroPrio algorithm;worst-case approximation ratio;task graph scheduling;proof structure;time complexity","","","",24.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Optimal Convex Hull Formation on a Grid by Asynchronous Robots with Lights","R. Hector; R. Vaidyanathan; G. Sharma; J. L. Trahan","Louisiana State University, Baton Rouge, LA, USA; Louisiana State University, Baton Rouge, LA, USA; Kent State University, Kent, OH, USA; Louisiana State University, Baton Rouge, LA, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1051","1060","We consider the distributed setting of n autonomous mobile robots that operate in Look-Compute-Move (LCM) cycles and communicate with other robots using a constant number of colored lights (the robots with lights model). We assume obstructed visibility where a robot cannot see another robot if a third robot is positioned between them on the straight line connecting them. In addition, we consider a grid-based terrain embedded in the 2-dimensional Euclidean plane that restricts each robot movement to one of the four neighboring grid points from its current position. This grid setting is a natural discretization of the 2-dimensional real plane and extends the robot swarm model in directions of greater applicability. The CONVEX HULL FORMATION problem is to relocate the n robots (starting at arbitrary, but distinct, initial positions) so that each robot is positioned on a vertex of a convex hull. In this paper, we provide two asynchronous algorithms for CONVEX HULL FORMATION, both using a constant number of colors. Key measures of the algorithms' performance include the time taken and the space occupied (measured as the perimeter of the smallest rectangle enclosing the convex hull formed). The first O(max{n2, D})-time and O(n2)-perimeter algorithm serves to introduce key ideas, where D is the diameter of the initial 3 configuration. The second algorithm runs in O(max{n3/2, D}) 3 time with a perimeter of O(n3/2). We also prove lower bounds of Ω(n2/3) for both the time and perimeter for any CONVEX HULL FORMATION algorithm; that is, we establish our second algorithm as optimal in both time and perimeter.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139862","","Robot kinematics;Color;Robot sensing systems;Complexity theory;Time measurement","computational complexity;computational geometry;image colour analysis;mobile robots;robot vision","convex hull formation;autonomous mobile robots;constant number;colored lights;lights model;grid-based terrain;2-dimensional Euclidean plane;O(max{n2, D})-time and O(n2)-perimeter algorithm;Look-Compute-Move cycles;asynchronous algorithms;robot swarm model;robot movement","",2.0,"",24.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"On the Complexity of Conditional DAG Scheduling in Multiprocessor Systems","A. Marchetti-Spaccamela; N. Megow; J. Schlöter; M. Skutella; L. Stougie","Sapienza University of Rome and INRIA-Erable; University of Bremen; University of Bremen; Technical University of Berlin; CWI Amsterdam, Vrije Universiteit Amsterdam and INRIA-Erable","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1061","1070","As parallel processing became ubiquitous in modern computing systems, parallel task models have been proposed to describe the structure of parallel applications. The workflow scheduling problem has been studied extensively over past years, focusing on multiprocessor systems and distributed environments (e.g. grids, clusters). In workflow scheduling, applications are modeled as directed acyclic graphs (DAGs). DAGs have also been introduced in the real-time scheduling community to model the execution of multi-threaded programs on a multi-core architecture. The DAG model assumes, in most cases, a fixed DAG structure capturing only straight-line code. Only recently, more general models have been proposed. In particular, the conditional DAG model allows the presence of control structures such as conditional (if-then-else) constructs. While first algorithmic results have been presented for the conditional DAG model, the complexity of schedulability analysis remains wide open. We perform a thorough analysis on the worst-case makespan (latest completion time) of a conditional DAG task under list scheduling (a.k.a. fixed-priority scheduling). We show several hardness results concerning the complexity of the optimization problem on multiple processors, even if the conditional DAG has a well-nested structure. For general conditional DAG tasks, the problem is intractable even on a single processor. Complementing these negative results, we show that certain practice-relevant DAG structures are very well tractable.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139785","parallel processing;makespan;conditional DAG;complexity","Task analysis;Processor scheduling;Program processors;Complexity theory;Scheduling;Computational modeling","computational complexity;directed graphs;multiprocessing systems;multi-threading;processor scheduling;program control structures","multicore architecture;multithreaded programs;real-time scheduling community;DAGs;workflow scheduling problem;parallel applications;parallel task models;modern computing systems;parallel processing;multiprocessor systems;conditional DAG scheduling;practice-relevant DAG structures;general conditional DAG tasks;fixed-priority scheduling;list scheduling;conditional DAG task;worst-case makespan;schedulability analysis;control structures;conditional DAG model;fixed DAG structure","",6.0,"",21.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Weaver: Efficient Coflow Scheduling in Heterogeneous Parallel Networks","X. S. Huang; Y. Xia; T. S. E. Ng","Rice University; Facebook, Inc.; Facebook, Inc.","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1071","1081","Leveraging application-level requirements expressed in Coflows has been shown to improve application-level communication efficiency. However, most existing works assume all application traffic is serviced by one monolithic network. This over-simplified assumption is no longer sufficient in a modern, evolving data center which operates on multiple generations of network fabrics, an architecture that we define as Heterogeneous Parallel Networks (HPNs). In this paper, we present the first scheduler, called Weaver, that addresses the Coflow management problem in HPNs. To exploit HPNs fully, achieving high communication efficiency for applications is crucial, yet it is also challenging because of the complex traffic patterns in applications and the heterogeneous bandwidth distribution in HPNs. Weaver addresses these challenges at two levels. At the microscopic level, for each application, Weaver leverages an efficient algorithm to exploit the distributed bandwidth in HPNs, which we proved to be within a constant factor of the optimal. At the macroscopic level involving multiple applications, Weaver can adopt a range of application traffic scheduling policies as desired by the system operator. Under realistic traffic, Weaver enables HPNs to service Coflows as efficiently as a monolithic network with equivalent aggregated capacity.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139821","","Weaving;Bandwidth;Data centers;Receivers;Switches;IEEE 802.3 Standard;EPON","bandwidth allocation;computer centres;computer network management;telecommunication scheduling;telecommunication traffic","Weaver;leveraging application-level requirements;coflow scheduling;realistic traffic;application traffic scheduling policies;macroscopic level;microscopic level;heterogeneous bandwidth distribution;complex traffic patterns;high communication efficiency;Coflow management problem;HPN;Heterogeneous Parallel Networks;data center;monolithic network;application-level communication efficiency","",1.0,"",36.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Fault-Tolerant Containers Using NiLiCon","D. Zhou; Y. Tamir","Computer Science Department, UCLA; Computer Science Department, UCLA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1082","1091","Many services deployed in the cloud require high reliability and must thus survive machine failures. Providing such fault tolerance transparently, without requiring application modifications, has motivated extensive research on replicating virtual machines (VMs). Cloud computing typically relies on VMs or containers to provide an isolation and multitenancy layer. Containers have advantages over VMs in smaller size, faster startup, and avoiding the need to manage updates of multiple VMs. This paper reports on the design, implementation, and evaluation of NiLiCon — a transparent container replication mechanism for fault tolerance. To the best of our knowledge, NiLiCon is the first implementation of container replication, demonstrating that it can be used for transparent deployment of critical services in the cloud.NiLiCon is based on high-frequency asynchronous incremental checkpointing to a warm spare, as previously used for VMs. The challenge to accomplishing this is that, compared to VMs, there is much tighter coupling between the container state and the state of the underlying platform. NiLiCon meets this challenge, eliminating the need to deploy services in VMs, with performance overheads that are competitive with those of similar VM replication mechanisms. Specifically, with the seven benchmarks used in the evaluation, the performance overhead of NiLiCon is in the range of 19%-67%. For fail-stop faults, the recovery rate is 100%.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139863","fault tolerance;replication","Containers;Kernel;Checkpointing;Fault tolerance;Fault tolerant systems;Sockets;Linux","checkpointing;cloud computing;fault tolerant computing;software fault tolerance;software reliability;system recovery;virtual machines","multitenancy layer;fault tolerance;cloud.NiLiCon;high-frequency asynchronous incremental checkpointing;VM replication;fault-tolerant containers;machine failures;virtual machines;cloud computing;transparent container replication","","","",37.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Aarohi: Making Real-Time Node Failure Prediction Feasible","A. Das; F. Mueller; B. Rountree","North Carolina State University; North Carolina State University; Lawrence Livermore National Laboratory","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1092","1101","Large-scale production systems are well known to encounter node failures, which affect compute capacity and energy. Both in HPC systems and enterprise data centers, combating failures is becoming challenging with increasing hardware and software complexity. Several data mining solutions of logs have been investigated in the context of anomaly detection in such systems. However, with subsequent proactive failure mitigation, the existing log mining solutions are not sufficiently fast for real-time anomaly detection. Machine learning (ML)-based training can produce high accuracy but the inference scheme needs to be enhanced with rapid parsers to assess anomalies in real-time. This work tackles online anomaly prediction in computing systems by exploiting context free grammar-based rapid event analysis. We present our framework Aarohi1, which describes an effective way to predict failures online. Aarohi is designed to be generic and scalable making it suitable as a real-time predictor. Aarohi obtains more than 3 minutes lead times to node failures with an average of 0.31 msecs prediction time for a chain length of 18. The overall improvement obtained w.r.t. the existing state-of-the-art is over a factor of 27.4×. Our compiler-based approach provides new research directions for lead time optimization with a significant prediction speedup required for the deployment of proactive fault tolerant solutions in practice.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139847","Online Prediction;HPC;Node Failures;Parsing","Training;Real-time systems;Production;Hardware;Software;Xenon;Anomaly detection","context-free grammars;data mining;learning (artificial intelligence);parallel processing;program compilers;software fault tolerance;system recovery","HPC systems;enterprise data centers;software complexity;subsequent proactive failure mitigation;real-time anomaly detection;machine learning-based training;rapid parsers;online anomaly prediction;context free grammar-based rapid event analysis;real-time predictor;node failures;compiler-based approach;lead time optimization;proactive fault tolerant solutions;large-scale production systems;prediction speedup;real-time node failure prediction;Aarohi;time 0.31 ms","",15.0,"",43.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"FP4S: Fragment-based Parallel State Recovery for Stateful Stream Applications","P. Liu; H. Xu; D. Da Silva; Q. Wang; S. T. Ahmed; L. Hu","Florida International University; Florida International University; Texas A&M University; Louisiana State University; Texas A&M University; Florida International University","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1102","1111","Streaming computations are by nature long-running. They run in highly dynamic distributed environments where many stream operators may leave or fail at the same time. Most of them are stateful, in which stream operators need to store and maintain large-sized state in memory, resulting in expensive time and space costs to recover them. The state-of-the-art stream processing systems offer failure recovery mainly through three approaches: replication recovery, checkpointing recovery, and DStream-based lineage recovery, which are either slow, resource-expensive or fail to handle many simultaneous failures.We present FP4S, a novel fragment-based parallel state recovery mechanism that can handle many simultaneous failures for a large number of concurrently running stream applications. The novelty of FP4S is that we organize all the application's operators into a distributed hash table (DHT) based consistent ring to associate each operator with a unique set of neighbors. Then we divide each operator's in-memory state into many fragments and periodically save them in each node's neighbors, ensuring that different sets of available fragments can reconstruct lost state in parallel. This approach makes this failure recovery mechanism extremely scalable, and allows it to tolerate many simultaneous operator failures. We apply FP4S on Apache Storm and evaluate it using large-scale real-world experiments, which demonstrate its scalability, efficiency, and fast failure recovery features. When compared to the state-of-the-art solutions (Apache Storm), FP4S reduces 37.8% latency of state recovery and saves more than half of the hardware costs. It can scale to many simultaneous failures and successfully recover the states when up to 66.6% of states fail or get lost.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139796","","Storms;Checkpointing;Hardware;Scalability;Sparks;Task analysis;Bars","checkpointing;file organisation;parallel processing;system recovery","FP4S;stateful stream applications;dynamic distributed environments;DStream-based lineage recovery;fragment-based parallel state recovery mechanism;failure recovery mechanism;distributed hash table;Apache Storm","","","",58.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Implementation and Evaluation of a Hardware Decentralized Synchronization Lock for MPSoCs","M. France-Pillois; J. Martin; F. Rousseau","CEA, LETI, MINATEC Campus, Univ. Grenoble Alpes, Grenoble, France; CEA, LETI, MINATEC Campus, Univ. Grenoble Alpes, Grenoble, France; TIMA, Univ. Grenoble Alpes, CNRS, Grenoble INP, Grenoble, France","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1112","1121","Each generation of shared memory Multi-Processor System-on-Chips (MPSoCs) tend to embed more and more computing units. The cores of modern MPSoCs are often grouped into clusters communicating with each other through Networks on Chip (NoCs). Having efficient scalable synchronization mechanisms is then mandatory to benefit from the high parallelism they offer.In this work we propose an innovative hardware support for synchronization locks. First of all, a non-intrusive measurement tool-chain allows us to prove a fundamental hypothesis as to optimization of the lock mechanism: although a lock may be used, at runtime, by various cores belonging to different clusters, it is often reused by the last core which has released it. Based on this observation, we provide a hardware decentralized solution to manage dynamic re-homing of locks in a dedicated memory, close to the latest access-granted core. This reduces overall access latency and network traffic in case of reuse of the lock within the same cluster.This paper presents our solution, called Lockality, and its performance evaluation on a characteristic MPSoC running on a hardware emulator. Experiments show large gains at low level (physical lock acquisition) as well as at the application level.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139815","","Hardware;Synchronization;Instruction sets;Message systems;Proposals;Optimization","multiprocessing systems;network-on-chip;shared memory systems;synchronisation","access-granted core;shared memory multiprocessor system-on-chips;physical lock acquisition;hardware emulator;performance evaluation;network traffic;nonintrusive measurement tool-chain;efficient scalable synchronization mechanisms;MPSoC;computing units;hardware decentralized synchronization lock","","","",16.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Communication-Efficient Jaccard similarity for High-Performance Distributed Genome Comparisons","M. Besta; R. Kanakagiri; H. Mustafa; M. Karasikov; G. Rätsch; T. Hoefler; E. Solomonik","Department of Computer Science, ETH Zurich; Department of Computer Science, Indian Institute of Technology Tirupati; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Department of Computer Science, University of Illinois at Urbana-Champaign","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1122","1132","The Jaccard similarity index is an important measure of the overlap of two sets, widely used in machine learning, computational genomics, information retrieval, and many other areas. We design and implement SimilarityAtScale, the first communication-efficient distributed algorithm for computing the Jaccard similarity among pairs of large datasets. Our algorithm provides an efficient encoding of this problem into a multiplication of sparse matrices. Both the encoding and sparse matrix product are performed in a way that minimizes data movement in terms of communication and synchronization costs. We apply our algorithm to obtain similarity among all pairs of a set of large samples of genomes. This task is a key part of modern metagenomics analysis and an evergrowing need due to the increasing availability of high-throughput DNA sequencing data. The resulting scheme is the first to enable accurate Jaccard distance derivations for massive datasets, using large-scale distributed-memory systems. We package our routines in a tool, called GenomeAtScale, that combines the proposed algorithm with tools for processing input sequences. Our evaluation on real data illustrates that one can use GenomeAtScale to effectively employ tens of thousands of processors to reach new frontiers in large-scale genomic and metagenomic analysis. While GenomeAtScale can be used to foster DNA research, the more general underlying SimilarityAtScale algorithm may be used for high-performance distributed similarity computations in other data analytics application domains.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139876","Distributed Jaccard Distance;Distributed Jaccard similarity;Genome Sequence Distance;Metagenome Sequence Distance;High-Performance Genome Processing;k-Mers;Matrix-Matrix Multiplication;Cyclops Tensor Framework","Bioinformatics;Genomics;Sequential analysis;Indexes;Sparse matrices;Tools","bioinformatics;data analysis;distributed algorithms;distributed memory systems;DNA;genetics;genomics;parallel processing;sparse matrices","high-performance distributed genome comparisons;Jaccard similarity index;sparse matrices;data movement minimization;DNA sequencing data;large-scale distributed-memory systems;GenomeAtScale;metagenomic analysis;high-performance distributed similarity computations;data analytics;communication-efficiency Jaccard similarity;SimilarityAtScale algorithm","",18.0,"",92.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Engineering Worst-Case Inputs for Pairwise Merge Sort on GPUs","K. Berney; N. Sitchinava","Department of Information & Computer Sciences, University of Hawaii at Manoa, Honolulu, Hawaii, USA; Department of Information & Computer Sciences, University of Hawaii at Manoa, Honolulu, Hawaii, USA","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1133","1142","Currently, the fastest comparison-based sorting implementation on GPUs is implemented using a parallel pairwise merge sort algorithm (Thrust library). To achieve fast runtimes, the number of threads t to sort the input of N elements is fine-tuned experimentally for each generation of Nvidia GPUs in such a way that the number of elements E = N/t that each thread accesses in each merging round results in a small (empirically measured) number of shared memory contentions, known as bank conflicts, while balancing the number of global memory accesses and latency-hiding through thread oversubscription/occupancy.In this paper, we show that for every choice of E <; w, such that E and w are co-prime, there exists an input permutation on which every warp of w threads of the Thrust merge sort is effectively reduced to using at most ⌈w/E⌉ threads due to sequentialization of shared memory accesses due to bank conflicts. Note that this matches the trivial worst-case bound on the loss of parallelism due to memory contentions for any warp accessing wE contiguous shared memory locations.Our proof is constructive, i.e., we are able to automatically construct such permutation for every value of E. We also show in practice that such constructed inputs result in up to ~50% slowdown, compared to the performance on random inputs, on modern GPU hardware.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139792","GPGPU;sorting;bank conflicts;worst-case","Instruction sets;Graphics processing units;Runtime;Merging;Memory modules;Sorting;Computational modeling","computational complexity;graphics processing units;merging;parallel algorithms;parallel architectures;shared memory systems;sorting","constructed inputs;random inputs;engineering worst-case inputs;pairwise merge sort;fast runtimes;N elements;Nvidia GPU;thread accesses;merging round results;shared memory contentions;bank conflicts;global memory accesses;input permutation;shared memory accesses;thrust library;contiguous shared memory locations","","","",34.0,"USGov","14 Jul 2020","","","IEEE","IEEE Conferences"
"The Impossibility of Fast Transactions","K. Antoniadis; D. Didona; R. Guerraoui; W. Zwaenepoel","EPFL; IBM Research - Zurich; EPFL; University of Sydney","2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","14 Jul 2020",2020,"","","1143","1154","We prove that transactions cannot be fast in an asynchronous fault-tolerant system. Our result holds in any system where we require transactions to ensure monotonic writes, or any stronger consistency model, such as, causal consistency. Thus, our result unveils an important, and so far unknown, limitation of fast transactions: they are impossible if we want to tolerate the failure of even one server.","1530-2075","978-1-7281-6876-0","10.1109/IPDPS47924.2020.00120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139787","","Servers;Fault tolerance;Fault tolerant systems;Data models;Distributed processing;Computational modeling;Surges","checkpointing;concurrency theory;fault tolerant computing;shared memory systems","causal consistency;asynchronous fault-tolerant system;consistency model","",1.0,"",34.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Conferences"
"Editor's Note","M. Parashar","","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,251,252,"Presents the introductory editorial for this issue of the publication.","1558-2183","","10.1109/TPDS.2019.2958395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8956009","","","","","","","",0.0,"IEEE","10 Jan 2020","","","IEEE","IEEE Journals"
"Pache: A Packet Management Scheme of Cache in Data Center Networks","T. Chen; X. Gao; T. Liao; G. Chen","Shanghai Key Laboratory of Scalable and Computing and Systems, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Scalable and Computing and Systems, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Scalable and Computing and Systems, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Scalable and Computing and Systems, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,253,265,"The communication traffic among servers is a principal bottleneck for data center networks (DCNs), in which redundant traffic has a significant impact on the performance of DCNs. In this paper, we propose Pache, a distributed and efficient cache scheme to achieve traffic redundancy elimination in DCNs. In Pache, nodes cache packets to reduce bandwidth consumption and gain performance revenue. Each cache node manages the local cache and improves cache efficiency with counting bloom filter, hash table and Trie. Each cache node broadcasts the local cache information by a bloom filter to other servers, and servers have cache information tables (cache-table) to record them, which achieves a cache sharing mechanism. Moreover, Pache employs false positive information table (fp-table) to reduce the false positive rate caused by bloom filters. A server determines an original packet or the corresponding fingerprint packet to send by querying cache-table and fp-table. Cache node placement mechanism is also designed to select which node to cache a packet. For evaluating the performance of Pache, we execute extensive simulations and conduct a case study on Amazon EC2 platform from different aspects. The results show that Pache can eliminate about 40 percent redundant traffic on average, and only increase 10 percent runtime, and it is also a scalable and feasible cache scheme in DCNs.","1558-2183","","10.1109/TPDS.2019.2931905","National Natural Science Foundation of China(grant numbers:61872238,61672353); Shanghai Science and Technology Fund(grant numbers:17510740200); Huawei Innovation Research Program(grant numbers:HO2018085286); State Key Laboratory of Air Traffic Management System and Technology(grant numbers:SKLATM20180X); CCF-Huawei Database System Innovation Research Plan(grant numbers:CCF-Huawei DBIR2019002A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8781932","Data center network;cache;traffic redundancy elimination;counting bloom filter;hash table;trie","Servers;Redundancy;Peer-to-peer computing;Topology;Data centers;Bandwidth;Internet","cache storage;cloud computing;computer centres;data structures;telecommunication traffic","Pache;data center networks;communication traffic;DCNs;distributed cache scheme;efficient cache scheme;traffic redundancy elimination;cache packets;bandwidth consumption;gain performance revenue;cache efficiency;bloom filter;hash table;local cache information;cache information tables;cache-table;cache sharing mechanism;false positive information table;fp-table;original packet;corresponding fingerprint packet;cache node placement mechanism;40 percent redundant traffic;scalable cache scheme;feasible cache scheme;efficiency 40.0 percent;efficiency 10.0 percent","",4.0,"",40.0,"IEEE","30 Jul 2019","","","IEEE","IEEE Journals"
"APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method","Y. Tan; B. Wang; Z. Yan; W. Srisa-an; X. Chen; D. Liu","College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; HewlettPackard Enterprise, Charlotte, USA; University of Nebraska Lincoln, Lincoln, USA; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,266,278,"Byte-addressable, non-volatile memory (NVRAM) combines the benefits of DRAM and flash memory. However, due to its slower speed than DRAM, it is best to deploy it in combination with typical DRAM. In such Hybrid NVRAM systems, frequently accessed, hotpages can be stored in DRAM while other cold pages can reside in NVRAM, providing the benefits of both high performance (from DRAM) and lower power consumption and cost/performance (from NVRAM). While the idea seems beneficial, realizing an efficient hybrid NVRAM system requires careful page migration and accurate data temperature measurement. Existing solutions, however, often cause invalid migrations due to inaccurate data temperature accounting, because hot and cold pages are separately identified in DRAM and NVRAM regions. Moreover, since a new NVRAM frame is always allocated for each page swapped back NVRAM, a large amount of unnecessary NVRAM writes are generated during each page migration. Based on these observations, we propose APMigrate, an adaptive data migration approach for hybrid NVRAM systems. APMigrate consist of two parts, UIMigrate and LazyWriteback. UIMigrate focuses on eliminating invalid page migrations by considering data temperature in the entire DRAM-NVRAM space, while LazyWriteback focus on rewriting only dirty data back when the page is swapped back to NVRAM. Our experiments using SPEC 2006 show that APMigrate can reduce the number of migrations and improves performance by up to 90 percent compared to existing state-of-the-art approaches. For some workloads, LazyWriteback can reduce unnecessary NVRAM writes for existing page migrations by up to 75 percent.","1558-2183","","10.1109/TPDS.2019.2933521","Fundamental Research Funds for the Central Universities(grant numbers:2019CDJGFJSJ001,2018CDXYJSJ0026); National Natural Science Foundation of China(grant numbers:61402061,61672116,61802038); China Postdoctoral Science Foundation(grant numbers:2017M620412); Chongqing Postdoctoral Special Science Foundation(grant numbers:XmT2018003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789491","Hybrid memory system;page migration;unified temperature identification;finer-grained access status","Random access memory;Nonvolatile memory;Memory management;Performance evaluation;Adaptive systems;Hybrid power systems;Power demand","low-power electronics;random-access storage","APMigration;adaptive page migration method;nonvolatile memory;flash memory;hybrid NVRAM systems;hybrid memory performance;DRAM;page migration;data temperature measurement","",4.0,"",23.0,"IEEE","6 Aug 2019","","","IEEE","IEEE Journals"
"Optimizing Geo-Distributed Data Analytics with Coordinated Task Scheduling and Routing","L. Zhao; Y. Yang; A. Munir; A. X. Liu; Y. Li; W. Qu","Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China; Department of Computer Science, Michigan State University, East Lansing, USA; Department of Computer Science, Michigan State University, East Lansing, USA; Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,279,293,"Recent trends show that cloud computing is growing to span more and more globally distributed datacenters. For geo-distributed datacenters, there is an increasingly need for scheduling algorithms to place tasks across datacenters, by jointly considering WAN traffic and computation. This scheduling must deal with situations such as wide-area distributed data, data sharing, WAN bandwidth costs and datacenter capacity limits, while also minimizing makespan. However, this scheduling problem is NP-hard. We propose a new resource allocation algorithm called HPS+, an extension to Hypergraph Partition-based Scheduling. HPS+ models the combined task-data dependencies and data-datacenter dependencies as an augmented hypergraph, and adopts an improved hypergraph partition technique to minimize WAN traffic. It further uses a coordination mechanism to allocate network resources closely following the guidelines of task requirements, for minimizing the makespan. Evaluation across the real China-Astronomy-Cloud model and Google datacenter model show that HPS+ saves the amount of data transfers by upto 53 percent and reduces the makespan by 39 percent compared to existing algorithms.","1558-2183","","10.1109/TPDS.2019.2938164","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000205); National Natural Science Foundation of China(grant numbers:61402325,61872265,U1836214,61672131); Generation of Artificial Intelligence Science and Technology Major Project of Tianjin(grant numbers:18ZXZNGX00190); Key research and Development Program for Guangdong Province(grant numbers:2019B010136001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818672","Data Analytics;Task Scheduling;Routing;Geo-distributed Cloud","Task analysis;Data transfer;Wide area networks;Bandwidth;Processor scheduling;Scheduling;Cloud computing","cloud computing;computer centres;data analysis;graph theory;resource allocation;scheduling;wide area networks","combined task-data dependencies;data-datacenter dependencies;augmented hypergraph;improved hypergraph partition technique;WAN traffic;coordination mechanism;network resources;task requirements;data transfers;geo-distributed data analytics;cloud computing;globally distributed datacenters;geo-distributed datacenters;scheduling algorithms;wide-area distributed data;data sharing;WAN bandwidth costs;datacenter capacity limits;scheduling problem;NP-hard problem;resource allocation algorithm;HPS+ models;coordinated task scheduling;coordinated task routing;hypergraph partition-based scheduling;efficiency 53.0 percent;efficiency 39.0 percent","",13.0,"",46.0,"IEEE","28 Aug 2019","","","IEEE","IEEE Journals"
"Wiera: Policy-Driven Multi-Tiered Geo-Distributed Cloud Storage System","K. Oh; N. Qin; A. Chandra; J. Weissman","Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, USA; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, USA; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, USA; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, USA","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,294,305,"Multi-tiered geo-distributed cloud storage systems must tame complexity at many levels: uniform APIs for storage access, supporting flexible storage policies that meet a wide array of application metrics, determining an optimal data placement, handling uncertain network dynamics and access dynamism, and operating across many levels of heterogeneity both within and across data-centers (DCs). In this paper, we present an integrated solution called Wiera. Wiera enables the specification of data management policies both within a local DC and across DCs. Such policies enable the user to optimize for cost, performance, reliability, durability, and consistency, and to express their tradeoffs. In addition, Wiera determines an optimal data placement for the user to meet their desired tradeoffs easily in such an environment. A key aspect of Wiera is first-class support for dynamism due to network, workload, and access patterns changes. As far as we know, Wiera is the first geo-distributed cloud storage system which handles dynamism actively at run-time. Wiera allows unmodified applications to reap the benefits of flexible data/storage policies by externalizing the policy specification. We show how Wiera enables a rich specification of dynamic policies using a concise notation and describe the design and implementation of the system. We have implemented a Wiera prototype on multiple cloud environments, AWS and Azure, that illustrates potential benefits from managing dynamics and in using multiple cloud storage tiers both within and across DCs.","1558-2183","","10.1109/TPDS.2019.2935727","NSF(grant numbers:CSR-1162405); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805417","Data locality;multi-DCs;multi-tiered storage;wide area storage;in memory storage","Cloud computing;Data models;Complexity theory;Data centers;Pricing;Fault tolerance;Fault tolerant systems","application program interfaces;cloud computing;storage management","Wiera;policy-driven multitiered geo-distributed cloud storage system;multitiered geo-distributed cloud storage systems;storage access;flexible storage policies;optimal data placement;uncertain network dynamics;access dynamism;data management policies;policy specification;dynamic policies;multiple cloud storage tiers","",8.0,"",38.0,"IEEE","19 Aug 2019","","","IEEE","IEEE Journals"
"Energy and Task-Aware Partitioning on Single-ISA Clustered Heterogeneous Processors","A. Suyyagh; Z. Zilic","Department of Electrical and Computer Engineering, McGill University, Montreal, Canada; Department of Electrical and Computer Engineering, McGill University, Montreal, Canada","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,306,317,"Heterogeneous multi-core processing is increasingly adopted in embedded systems. Heterogeneous platforms can provide energy consumption reduction by employing longstanding techniques like Dynamic Voltage and Frequency Scaling (DVFS) and Dynamic Power Management (DPM). An effective energy-management strategy simultaneously exploits hardware-and software-level energy-reduction techniques. Energy-efficient partitioning is one software-level method where task allocation to heterogeneous clusters directly impacts the total system energy. In this paper, we couple the problem of energy-efficient partitioning on single-ISA heterogeneous platforms with task-aware scheduling. Tasks differ in their instruction mix, cache, memory and I/O access, execution path, and active processing and SoC circuitry. This affects their power demand. We make further use of underlying hardware frequency scaling to reduce the system energy. We propose four variants of our Task and Cluster Heterogeneity Aware Partitioning (TCHAP) targeting ARM big.LITTLE platforms, and show that our algorithms achieve up to 30 percent energy-reduction on average compared to a state-of-the-art scheme.","1558-2183","","10.1109/TPDS.2019.2937029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812914","Energy-aware scheduling;task partitioning;real-time systems;heterogeneous multicores;ARM big.LITTLE","Task analysis;Program processors;Multicore processing;Real-time systems;Clustering algorithms;Energy consumption;Hardware","embedded systems;energy consumption;energy management systems;microprocessor chips;multiprocessing systems;power aware computing;processor scheduling;system-on-chip","single-ISA clustered heterogeneous processors;heterogeneous multicore processing;embedded systems;energy consumption reduction;dynamic power management;energy-management strategy;software-level energy-reduction techniques;energy-efficient partitioning;task allocation;heterogeneous clusters;total system energy;single-ISA heterogeneous platforms;task-aware scheduling;hardware frequency scaling;energy-reduction;task-aware partitioning;instruction mix;I/O access;execution path;active processing;SoC circuitry;power demand;task and cluster heterogeneity aware partitioning","",7.0,"",38.0,"IEEE","26 Aug 2019","","","IEEE","IEEE Journals"
"Performance Modeling of Parallel Loops on Multi-Socket Platforms Using Queueing Systems","Y. Cho; S. Oh; B. Egger","School of Computer Science and Engineering, Seoul National University, Seoul, Korea; SAP Labs Korea, Seoul, Korea; School of Computer Science and Engineering, Seoul National University, Seoul, Korea","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,318,331,"Predicting the performance of parallel loops on modern shared-memory multi-socket multi-core systems in dependence of the allocated resources is an important means to achieve better system utilization. Previous prediction techniques are tied to specific architectures and do not allow for purely online performance predictions without requiring an offline analysis of the parallel program. This paper presents a practical approach based on queueing theory to model the performance of parallel programs in dependence of the number of allocated core resources. Based on the key insight that scalability of scientific parallel loops is limited by memory performance, a hierarchically constructed M/M/1/N/N queue system is used to analytically compute the response time at the different congestion points in the memory system of modern NUMA architectures. After automatically tuning the model to a specific architecture by executing a number of micro-benchmarks, the required parameter values are obtained at runtime from hardware performance counters present in modern commodity AMD and Intel processors. Evaluated with 24 OpenMP parallel loops on a 64-core AMD and a 72-core Intel multi-socket platform, the presented queueing system is able to accurately predict the speedup of parallel loops with a mean absolute percentage error of 8.3 percent on the AMD system and 6.7 percent on the Intel platform.","1558-2183","","10.1109/TPDS.2019.2938172","National Research Foundation of Korea; Korean government(grant numbers:NRF-2015K1A3A1A14021288,2016R1A2B4009193); Dept. of Computer Science and Engineering, SNU(grant numbers:21A20151113068); Seoul National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818668","Performance modeling;parallel loop;queueing system;multi-socket system;OpenMP;NUMA","Computational modeling;Servers;Predictive models;Time factors;Multicore processing;Dynamic scheduling","microprocessor chips;parallel architectures;parallel programming;performance evaluation;queueing theory;shared memory systems","performance modeling;multisocket platforms;queueing systems;shared-memory multisocket multicore systems;allocated resources;system utilization;purely online performance predictions;parallel program;queueing theory;allocated core resources;scientific parallel loops;memory performance;memory system;modern NUMA architectures;hardware performance counters;64-core AMD;72-core Intel multisocket platform;efficiency 6.7 percent;efficiency 8.3 percent","",7.0,"",35.0,"IEEE","28 Aug 2019","","","IEEE","IEEE Journals"
"Improving Overall Performance of TLC SSD by Exploiting Dissimilarity of Flash Pages","W. Zhang; Q. Cao; H. Jiang; J. Yao","Wuhan National Laboratory for Optoeletronics, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoeletronics, Huazhong University of Science and Technology, Wuhan, China; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, USA; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,332,346,"TLC flash has three types of pages to accommodate the three bits in each TLC physical cell exhibiting very different program latencies. This paper proposes PA-SSD to effectively improve the overall performance by exploiting the dissimilarity of TLC pages on program latency throughout the write request handling workflow. The main idea behind PA-SSD is to coordinately allocate the same type of pages for sub-requests of any given user write request, to mitigate the potential program latency imbalance among the sub-requests, and to schedule sub-requests according to their page-types. We achieve the PA-SSD design goal by answering three key research questions: (1) how to properly determine page-type for each user write request? (2) how to actually allocate a physical page for each sub-request with an assigned page-type from (1)? (3) how to effectively schedule the sub-requests in the chips queues when their page-types are judiciously allocated from (2)? To answer the first question, we propose seven page-type specifying schemes to investigate their effects under different workloads. We answer the second question by redesigning the page allocation strategy in TLC SSD to uniformly and sequentially determine physical pages for allocation following the internal programming process of TLC flash. Lastly, a page-type aware scheduling policy is presented to reorder the sub-requests within chips' queues. Our experiments show that PA-SSD can accelerate both the write and read performance. Particularly, our proposed queue-depth based page-type specifying scheme improves write performance by 2.6 times and read performance by 1.5 times over the conventional TLC SSD.","1558-2183","","10.1109/TPDS.2019.2934458","National Natural Science Foundation of China(grant numbers:61872156); Science Fund for Creative Research Groups(grant numbers:61821003); National Basic Research Program of China (973 Program)(grant numbers:2018YFA0701804); Fundamental Research Funds for the Central Universities(grant numbers:2018KFYXKJC037); Alibaba Innovative Research; National Science Foundation(grant numbers:CCF-1704504,CCF-1629625); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798696","TLC SSD;diverse program latencies;TSU scheduling;write performance;page-type aware","Resource management;Time factors;Schedules;Registers;Programming;Proposals","flash memories;processor scheduling;solid state drives","flash pages dissimilarity;write performance;chips queues;internal programming process;program latencies;page-type aware scheduling policy;page allocation strategy;seven page-type specifying schemes;PA-SSD design goal;TLC pages;TLC physical cell;TLC flash;conventional TLC SSD;queue-depth based page-type specifying scheme;read performance","",5.0,"",25.0,"IEEE","14 Aug 2019","","","IEEE","IEEE Journals"
"Coded Load Balancing in Cache Networks","M. J. Siavoshani; F. Parvaresh; A. Pourmiri; S. P. Shariatpanahi","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; School of Mathematics, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran; Department of Computing, Macquarie University, Macquarie Park, Australia; School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,347,358,"We consider load balancing problem in a cache network consisting of storage-enabled servers forming a distributed content delivery scenario. Previously proposed load balancing solutions cannot perfectly balance out requests among servers, which is a critical issue in practical networks. Therefore, in this paper, we investigate a coded cache content placement where coded chunks of original files are stored in servers based on the files popularity distribution. In our scheme, upon each request arrival at the delivery phase, by dispatching enough coded chunks to the request origin from the nearest servers, the requested file can be decoded. Here, we show that if n requests arrive randomly at n servers, the proposed scheme results in the maximum load of O(1) in the network. This result is shown to be valid under various assumptions for the underlying network topology. Our results should be compared to the maximum load of two baseline schemes, namely, nearest replica and power of two choices strategies, which are O(log n) and O(log log n), respectively. This finding shows that using coding, results in a considerable load balancing performance improvement, without compromising communications cost performance. This is confirmed by performing extensive simulation results, in non-asymptotic regimes as well.","1558-2183","","10.1109/TPDS.2019.2933839","IPM(grant numbers:95680425); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792130","Distributed caching servers;content delivery networks;coded caching;request routing;load balancing;communication cost","Servers;Load management;Measurement;Encoding;Network topology;Wireless communication;Content distribution networks","cache storage;computational complexity;computer networks;resource allocation;telecommunication network topology","underlying network topology;baseline schemes;nearest replica;considerable load balancing performance improvement;coded load;cache network;load balancing problem;storage-enabled servers;distributed content delivery scenario;load balancing solutions;coded cache content placement;coded chunks;original files;files popularity distribution;request arrival;delivery phase;request origin;nearest servers;requested file","",1.0,"",35.0,"IEEE","8 Aug 2019","","","IEEE","IEEE Journals"
"Thread Isolation to Improve Symbiotic Scheduling on SMT Multicore Processors","J. Feliu; J. Sahuquillo; S. Petit; L. Eeckhout","Department of Computer Engineering, Universitat Politécnica de Valéncia, Valéncia, Spain; Department of Computer Engineering, Universitat Politécnica de Valéncia, Valéncia, Spain; Department of Computer Engineering, Universitat Politécnica de Valéncia, Valéncia, Spain; Department of Electronics and Information Systems, Ghent University, Gent, Belgium","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,359,373,"Resource sharing is a critical issue in simultaneous multithreading (SMT) processors as threads running simultaneously on an SMT core compete for shared resources. Symbiotic job scheduling, which co-schedules applications with complementary resource demands, is an effective solution to maximize hardware utilization and improve overall system performance. However, symbiotic job scheduling typically distributes threads evenly among cores, i.e., all cores get assigned the same number of threads, which we find to lead to sub-optimal performance. In this paper, we show that asymmetric schedules (i.e., schedules that assign a different number of threads to each SMT core) can significantly improve performance compared to symmetric schedules. To leverage this finding, we propose thread isolation, a technique that turns symmetric schedules into asymmetric ones yielding higher overall system performance. Thread isolation identifies SMT-adverse applications and schedules them in isolation on a dedicated core to mitigate their sharp performance degradation under SMT. Our experimental results on an IBM POWER8 processor show that thread isolation improves system throughput by up to 5.5 percent compared to a state-of-the-art symmetric symbiotic job scheduler.","1558-2183","","10.1109/TPDS.2019.2934955","Generalitat Valenciana(grant numbers:APOSTD/2017/052); Ministerio de Ciencia, Innovación y Universidades and the European ERDF(grant numbers:RTI2018-098156-B-C51); Universitat Politècnica de València(grant numbers:SP20180140); FWO(grant numbers:G.0434.16N,G.0144.17N); H2020 European Research Council(grant numbers:741097); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798706","Simultaneous multithreading (SMT);symbiotic job scheduling;thread isolation","Schedules;Message systems;Symbiosis;Program processors;Degradation;Resource management;Throughput","microprocessor chips;multiprocessing systems;multi-threading;processor scheduling;resource allocation","resource sharing;simultaneous multithreading processors;SMT core;shared resources;symbiotic job scheduling;co-schedules applications;complementary resource demands;SMT-adverse applications;SMT multicore processors;symbiotic scheduling;SMT-adverse applications;thread isolation;symmetric schedules;asymmetric schedules;sub-optimal performance;system performance;efficiency 5.5 percent","",2.0,"",29.0,"IEEE","14 Aug 2019","","","IEEE","IEEE Journals"
"A Highly Reliable Metadata Service for Large-Scale Distributed File Systems","J. Zhou; Y. Chen; W. Wang; S. He; D. Meng","Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Texas Tech University, Lubbock, USA; Chinese Academy of Sciences, Beijing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,374,392,"Many massive data processing applications nowadays often need long, continuous, and uninterrupted data accesses. Distributed file systems are used as the back-end storage to provide the global namespace management and reliability guarantee. Due to increasing hardware failures and software issues with the growing system scale, metadata service reliability has become a critical issue as it has a direct impact on file and directory operations. Existing metadata management mechanisms can provide fault tolerance capability to some level but are inadequate. They often have limitations in system availability, state consistence, and performance overhead and lack an effective mechanism to offer metadata reliability. This paper introduces a novel highly reliable metadata service to address these issues in large-scale file systems. Different from traditional strategies, this proposed reliable metadata service adopts a new active-standby architecture for fault tolerance and uses a holistic approach to improve file system availability. A new shared storage pool (SSP) is designed for transparent metadata synchronization and replication between active and standby servers. Based on the SSP, a new policy called multiple actives multiple standbys (MAMS) is presented to perform metadata service recovery in case of failures. A new global state recovery strategy and a smart client fault tolerance mechanism are achieved to maintain the continuity of metadata service. We have implemented such highly reliable metadata service in a prototype file system CFS (Clover file system) and conducted extensive tests to evaluate it. Experimental results confirm that it can significantly improve file system reliability with fast failover under different failure scenarios while having negligible influence on performance. Compared with typical reliability designs in Hadoop Avatar, Hadoop HA, and Boom-FS file systems, the mean-time-to-recovery (MTTR) with the highly reliable metadata service was reduced by 80.23, 65.46 and 28.13 percent, respectively.","1558-2183","","10.1109/TPDS.2019.2937492","Beijing Municipal Science and Technology Commission(grant numbers:Z191100007119002); National Science Foundation(grant numbers:CNS-1338078,CNS-1362134,CCF-1409946,CCF-1718336,OAC-1835892,CNS-1817094); National Natural Science Foundation of China(grant numbers:61572377); Natural Science Foundation of Hubei Province(grant numbers:2017CFC889); Fundamental Research Funds for the Central Universities(grant numbers:2018QNA5015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812918","Distributed file systems;metadata service;metadata reliability;fault tolerance;shared metadata storage","Metadata;Servers;Protocols;Synchronization;Fault tolerance;Fault tolerant systems","back-up procedures;client-server systems;data handling;distributed databases;fault tolerance;fault tolerant computing;meta data;parallel processing;reliability;software fault tolerance;storage management;system recovery;telecommunication network reliability","hardware failures;software issues;growing system scale;metadata service reliability;directory operations;metadata reliability;large-scale file systems;file system availability;transparent metadata synchronization;metadata service recovery;smart client fault tolerance mechanism;prototype file system CFS;Clover file system;file system reliability;typical reliability designs;Boom-FS file systems;large-scale distributed file systems;long data accesses;uninterrupted data accesses;global namespace management;reliability guarantee;continuous data accesses;highly reliable metadata service;metadata management mechanisms;efficiency 28.13 percent;FS","",6.0,"",61.0,"IEEE","26 Aug 2019","","","IEEE","IEEE Journals"
"Throughput Maximization of NFV-Enabled Multicasting in Mobile Edge Cloud Networks","Y. Ma; W. Liang; J. Wu; Z. Xu","Research School of Computer Science, The Australian National University, Canberra, Australia; Research School of Computer Science, The Australian National University, Canberra, Australia; Department of Computer and Information Sciences, Temple University, Philadelphia, USA; School of Software, Dalian University of Technology, Dalian, China","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,393,407,"Mobile Edge Computing (MEC) reforms the cloud paradigm by bringing unprecedented computing capacity to the vicinity of end users at the mobile network edge. This provides end users with swift and powerful computing and storage capacities, energy efficiency, and mobility- and context-awareness support. Furthermore, Network Function Virtualization (NFV) is another promising technique that implements various network functions for many applications as pieces of software in servers or cloudlets in MEC networks. The provisioning of virtualized network services in MEC can improve user service experiences, simplify network service deployment, and ease network resource management. However, user requests arrive dynamically and different users demand different amounts of resources, while the resources in MEC are dynamically occupied or released by different services. It thus poses a significant challenge to optimize the performance of MEC through efficient computing and communication resource allocations to meet ever-growing resource demands of users. In this paper, we study NFV-enabled multicasting that is a fundamental routing problem in an MEC network, subject to resource capacities on both its cloudlets and links. Specifically, we first devise an approximation algorithm for the cost minimization problem of admitting a single NFV-enabled multicast request. We then develop an efficient algorithm for the throughput maximization problem for the admissions of a given set of NFV-enabled multicast requests. We third devise an online algorithm with a provable competitive ratio for the online throughput maximization problem when NFV-enabled multicast requests arrive one by one without the knowledge of future request arrivals. We finally evaluate the performance of the proposed algorithms through experimental simulations. Simulation results demonstrate that the proposed algorithms are promising.","1558-2183","","10.1109/TPDS.2019.2937524","National Natural Science Foundation of China(grant numbers:61802048,61802047); fundamental research funds for the central universities in China(grant numbers:DUT17RC(3)061,DUT17RC(3)070); Xinghai Scholar Program in Dalian University of Technology, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812911","Mobile edge-cloud networks (MEC);distributed resource allocation and provisioning;NFV-enabled multicast requests;virtualized network function (VNF);VNF instance placement and sharing;service function chains (SFCs);throughput maximization;Steiner tree problems;online algorithms","Cloud computing;Multicast algorithms;Heuristic algorithms;Approximation algorithms;Throughput;Task analysis;Bandwidth","approximation theory;cloud computing;minimisation;mobile computing;multicast communication;resource allocation;telecommunication network routing;virtualisation","NFV-enabled multicasting;mobile Edge cloud networks;Mobile Edge Computing reforms;cloud paradigm;unprecedented computing capacity;end users;mobile network edge;powerful computing;storage capacities;energy efficiency;context-awareness support;Network Function Virtualization;network functions;MEC network;virtualized network services;user service experiences;network service deployment;ease network resource management;user requests;efficient computing;communication resource allocations;single NFV-enabled multicast request;throughput maximization problem;NFV-enabled multicast requests;future request arrivals","",31.0,"",29.0,"IEEE","26 Aug 2019","","","IEEE","IEEE Journals"
"iCELIA: A Full-Stack Framework for STT-MRAM-Based Deep Learning Acceleration","H. Yan; H. R. Cherian; E. C. Ahn; X. Qian; L. Duan","Samsung, Austin, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, USA; Ming Hsieh Department of Electrical Engineering and the Department of Computer Science, University of Southern California, Los Angeles, USA; Computing Technology Lab, Alibaba DAMO Academy, Sunnyvale, USA","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,408,422,"A large variety of applications rely on deep learning to process big data, learn sophisticated features, and perform complicated tasks. Utilizing emerging non-volatile memory (NVM)'s unique characteristics, including the crossbar array structure and gray-scale cell resistances, to perform neural network (NN) computation is a well-studied approach in accelerating deep learning applications. Compared to other NVM technologies, STT-MRAM has its unique advantages in performing NN computation. However, the state-of-the-art research have not utilized STT-MRAM for deep learning acceleration due to its device and architecture-level challenges. Consequently, this paper enables STT-MRAM, for the first time, as an effective and practical deep learning accelerator. In particular, it proposes a full-stack framework iCELIA spanning multiple design levels, including device-level fabrication, circuit-level enhancements, architecture-level synaptic weight quantization, and system-level accelerator design. The primary contributions of iCELIA over our prior work CELIA include a new non-uniform weight quantization scheme and much enhanced accelerator system design. The proposed framework significantly mitigates the model accuracy loss due to reduced data precision in a cohesive manner, constructing a comprehensive STT-MRAM accelerator system for fast NN computation with high energy efficiency and low cost.","1558-2183","","10.1109/TPDS.2019.2937517","National Science Foundation(grant numbers:CCF-1566158); University of Texas System; UTSA Office of Vice President for Research, Economic Development and Knowledge Enterprise; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812913","STT-MRAM;deep learning acceleration;processing-in-memory;device and architecture co-design","Deep learning;Nonvolatile memory;Computer architecture;Acceleration;Artificial neural networks;Resistance;Microprocessors","Big Data;learning (artificial intelligence);MRAM devices;neural nets","crossbar array structure;gray-scale cell resistances;neural network computation;deep learning applications;NVM technologies;NN computation;deep learning acceleration;architecture-level challenges;effective learning accelerator;practical deep learning accelerator;full-stack framework iCELIA;multiple design levels;device-level fabrication;circuit-level enhancements;architecture-level synaptic weight quantization;system-level accelerator design;nonuniform weight quantization scheme;enhanced accelerator system design;comprehensive STT-MRAM accelerator system;fast NN computation;big data;sophisticated features;complicated tasks;nonvolatile memory","",10.0,"",50.0,"IEEE","26 Aug 2019","","","IEEE","IEEE Journals"
"Optimized Block-Based Algorithms to Label Connected Components on GPUs","S. Allegretti; F. Bolelli; C. Grana","Dipartimento di Ingegneria Enzo Ferrari, Università degli Studi di Modena e Reggio Emilia, Modena, Italy; Dipartimento di Ingegneria Enzo Ferrari, Università degli Studi di Modena e Reggio Emilia, Modena, Italy; Dipartimento di Ingegneria Enzo Ferrari, Università degli Studi di Modena e Reggio Emilia, Modena, Italy","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,423,438,"Connected Components Labeling (CCL) is a crucial step of several image processing and computer vision pipelines. Many efficient sequential strategies exist, among which one of the most effective is the use of a block-based mask to drastically cut the number of memory accesses. In the last decade, aided by the fast development of Graphics Processing Units (GPUs), a lot of data parallel CCL algorithms have been proposed along with sequential ones. Applications that entirely run in GPU can benefit from parallel implementations of CCL that allow to avoid expensive memory transfers between host and device. In this paper, two new eight-connectivity CCL algorithms are proposed, namely Block-based Union Find (BUF) and Block-based Komura Equivalence (BKE). These algorithms optimize existing GPU solutions introducing a block-based approach. Extensions for three-dimensional datasets are also discussed. In order to produce a fair comparison with previously proposed alternatives, YACCLAB, a public CCL benchmarking framework, has been extended and made suitable for evaluating also GPU algorithms. Moreover, three-dimensional datasets have been added to its collection. Experimental results on real cases and synthetically generated datasets demonstrate the superiority of the new proposals with respect to state-of-the-art, both on 2D and 3D scenarios.","1558-2183","","10.1109/TPDS.2019.2934683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798895","Parallel processing;connected components labeling;GPU;CUDA","Vegetation;Graphics processing units;Labeling;Indexes;Data structures;Three-dimensional displays;Lattices","computer vision;graphics processing units;parallel algorithms;parallel architectures","label connected components;connected components labeling;image processing;computer vision pipelines;sequential strategies;parallel implementations;expensive memory;eight-connectivity CCL algorithms;Block-based Union;GPU solutions;block-based approach;three-dimensional datasets;public CCL benchmarking framework;GPU algorithms;optimized block-based algorithms","",20.0,"",58.0,"IEEE","14 Aug 2019","","","IEEE","IEEE Journals"
"Achieving Load-Balanced, Redundancy-Free Cluster Caching with Selective Partition","Y. Yu; W. Wang; R. Huang; J. Zhang; K. B. Letaief","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,439,454,"Data-intensive clusters increasingly rely on in-memory storages to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hot spots, which significantly degrade the benefits of in- memory caching. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory overhead due to cache redundancy or incur non-trivial encoding/decoding complexity. In this paper, we propose an effective approach to achieve load balancing without cache redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on the loads they contribute and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for a hot file—too few partitions are incapable of mitigating hot spots, while too many are susceptible to stragglers. We have implemented SP-Cache atop Alluxio, a popular in-memory distributed storage system, and evaluated its performance through EC2 deployment and trace-driven simulations. SP-Cache can quickly react to the changing load by dynamically re-balancing cache servers. Compared to the state-of-the-art solution, SP-Cache reduces the file access latency by up to 40 percent in both the mean and the tail, using 40 percent less memory.","1558-2183","","10.1109/TPDS.2019.2931004","Kong ITF(grant numbers:ITS/391/15FX); RGC GRF(grant numbers:16200214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772169","Cloud computing;cluster caching systems;load balancing;selective partition","Servers;Load management;Redundancy;Production;Encoding;Bandwidth;Computational modeling","cache storage;decoding;encoding;file organisation;graph theory;pattern clustering;resource allocation;storage management","cache redundancy;termed SP-Cache;partitions files;hot file-too;mitigating hot spots;in-memory distributed storage system;changing load;re-balancing cache servers;file access;40 percent less memory;achieving load-balanced;redundancy-free cluster caching;selective partition;data-intensive clusters;in-memory storages;routinely observed file popularity skew;memory caching;tame load imbalance;multiple replicas;hot files;creating parity chunks;storage codes;high memory;load balancing;efficiency 40.0 percent","",5.0,"",58.0,"IEEE","25 Jul 2019","","","IEEE","IEEE Journals"
"Efficient and Portable Workgroup Size Tuning","C. -L. Yu; S. -L. Tsao","Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan; Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,455,469,"The performance of an OpenCL program is strongly influenced by both hardware and software attributes. To achieve superior performance, developers may leverage automatic performance tuning techniques to determine the optimal parameters on the target device. Although existing approaches have shown promising tuning results in their target scenarios, other requirements such as efficiency, portability, and usability should also be considered because of the rapid growth of heterogeneous computing applications and platforms. In this paper, we re-examine the workgroup size tuning problem and propose a novel approach to meet the aforementioned requirements. We abstract the architectural details into a set of hardware parameters so that the proposed approach can be applied without the presence of target devices, which makes it more accessible to developers. The proposed approach is evaluated on 20 OpenCL kernels and six devices, including both CPUs and GPUs. Experimental results demonstrate that, with negligible overhead, our approach filters out 88.6 percent of the possible workgroup sizes on average. Among all the workgroup size candidates, the bestand worst-performing candidates can achieve average performance of 95.5 and 92.1 percent, respectively, compared with the optimal workgroup size.","1558-2183","","10.1109/TPDS.2019.2937295","Ministry of Science and Technology of the People's Republic of China(grant numbers:108-2321-B-009-004-,108-2218-E-009-052-,107-2622-8-009-019-TA,107-2218-E-009-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812929","OpenCL;workgroup size selection;automatic performance tuning;microbenchmarking","Tuning;Performance evaluation;Kernel;Hardware;Indexes;Computational modeling;Graphics processing units","graphics processing units;multiprocessing systems;optimisation;parallel programming;power aware computing","heterogeneous computing applications;workgroup size tuning problem;architectural details;hardware parameters;target device;workgroup size candidates;optimal workgroup size;OpenCL program;software attributes;OpenCL kernels;portability;target scenarios;tuning results;optimal parameters;leverage automatic performance","",1.0,"",36.0,"IEEE","26 Aug 2019","","","IEEE","IEEE Journals"
"PALE: Time Bounded Practical Agile Leader Election","B. Sidik; R. Puzis; P. Zilberman; Y. Elovici","Telekom Innovation Laboratories, Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Telekom Innovation Laboratories, Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Telekom Innovation Laboratories, Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Telekom Innovation Laboratories, Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,470,485,"Many tasks executed in dynamic distributed systems, such as sensor networks or enterprise environments with bring-your-own-device policy, require central coordination by a leader node. In the past it has been proven that distributed leader election in dynamic environments with constant changes and asynchronous communication is not possible. Thus, state-of-the-art leader election algorithms are not applicable in asynchronous environments with constant network changes. Some algorithms converge only after the network stabilizes (an unrealistic requirement in many dynamic environments). Other algorithms reach consensus in the presence of network changes but require a global clock or some level of communication synchrony. Determining the weakest assumptions, under which bounded leader election is possible, remains an unresolved problem. In this study we present a leader election algorithm that operates in the presence of changes and under weak (realistic) assumptions regarding message delays and regarding the clock drifts of the distributed nodes. The proposed algorithm is self-sufficient, easy to implement and can be extended to support multiple regions, self-stabilization, and mobile ad-hoc networks. We prove the algorithm's correctness and provide a complexity analysis of the time, space, and number of messages required to elect a leader.","1558-2183","","10.1109/TPDS.2019.2933620","Lockheed Martin and RSA EMC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789472","Leader election;dynamic;partial synchrony;timed;distributed algorithms","Heuristic algorithms;Voting;Delays;Clocks;Task analysis;Stability criteria","computational complexity;distributed algorithms","practical agile leader election;dynamic distributed systems;sensor networks;enterprise environments;bring-your-own-device policy;central coordination;leader node;distributed leader election;dynamic environments;constant changes;asynchronous communication;asynchronous environments;constant network changes;global clock;communication synchrony;weakest assumptions;leader election algorithm;weak assumptions;distributed nodes","",1.0,"",50.0,"IEEE","6 Aug 2019","","","IEEE","IEEE Journals"
"The Network-Integrated Storage System","I. Kettaneh; A. Alquraan; H. Takruri; S. Yang; A. C. Arpaci-Dusseau; R. H. Arpaci-Dusseau; S. Al-Kiswany","Cheriton School of Computer Science, University of Waterloo,, Waterloo, Canada; Cheriton School of Computer Science, University of Waterloo,, Waterloo, Canada; Cheriton School of Computer Science, University of Waterloo,, Waterloo, Canada; Computer Sciences Department, University of Wisconsin-Madison, Madison, USA; Computer Sciences Department, University of Wisconsin-Madison, Madison, USA; Computer Sciences Department, University of Wisconsin-Madison, Madison, USA; Cheriton School of Computer Science, University of Waterloo,, Waterloo, Canada","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,2.0,486,500,"We present NICE, a key-value storage system design that leverages new software-defined network capabilities to build cluster-based network-efficient storage system. NICE presents novel techniques to co-design network routing and multicast with storage replication, consistency, and load balancing to achieve higher efficiency, performance, and scalability. We implement the NICEKV prototype. NICEKV follows the NICE approach in designing four essential network-centric storage mechanisms: request routing, replication, consistency, and load balancing. Our evaluation shows that the proposed approach brings significant performance gains compared with the current systems design: up to 7× put/get performance improvement, up to 2× reduction in network load, 3× to 9× load reduction on the storage nodes, and the elimination of scalability bottlenecks present in current designs.","1558-2183","","10.1109/TPDS.2019.2938158","NSERC Discovery; NSERC Engage; Canada Foundation for Innovation; NSF(grant numbers:CNS-1419199,CNS-1421033,CNS-1319405,CNS-1218405); NetApp VTC, Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818663","Key-value storage;software-defined networks;network-integrated design;network-system co-design;distributed storage","Peer-to-peer computing;Routing;Control systems;Standards;Protocols;System analysis and design;Load management","resource allocation;storage management;telecommunication network routing;wireless sensor networks","network-integrated storage system;key-value storage system design;software-defined network capabilities;cluster-based network-efficient storage system;co-design network routing;multicast;storage replication;load balancing;NICEKV prototype;NICE approach;essential network-centric storage mechanisms;request routing;current systems design;network load;9× load reduction;storage nodes;current designs","",3.0,"",63.0,"IEEE","28 Aug 2019","","","IEEE","IEEE Journals"
"Low-Cost Datacenter Load Balancing With Multipath Transport and Top-of-Rack Switches","E. Dong; X. Fu; M. Xu; Y. Yang","Institute of Computer Science, University of Goettingen, Goettingen, Germany; Institute of Computer Science, University of Goettingen, Goettingen, Germany; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","8 May 2020",2020,31.0,10.0,2232,2247,"Load balancing in datacenter networks (DCNs) is an important and challenging task for datacenter managers. A number of sophisticated technologies have been proposed to improve load balancing performance in a complicated circumstance, i.e., with various traffic characteristics. Many approaches need a high cost to implement, such as changing switch hardware. The efficiency problem has not been well addressed. MPTCP was proposed as a low-cost approach to improve data transmission in DCNs, which uses subflows to balance workloads across multiple paths. However, current MPTCP is not satisfying, especially when there are rack-local flows or many-to-one short flows. In this article, we propose DCMPTCP to improve the efficacy of MPTCP. We gradually develop three mechanisms. First, DCMPTCP identifies rack-local traffic and eliminates unnecessary subflows to reduce the overhead. Second, DCMPTCP estimates flow length and establishes subflows in a smarter way. Third, DCMPTCP strengthens explicit congestion notification to improve the congestion control performance on inter-rack many-to-one short flows. We have implemented DCMPTCP in both the Linux kernel and ns-3 simulator. Our comprehensive testbed experiments and simulations show that DCMPTCP outperforms MPTCP in both 1 Gbps testbed, and 10 Gbps large-scale simulation network.","1558-2183","","10.1109/TPDS.2020.2989441","National Natural Science Foundation of China(grant numbers:61625203,61832013); National Key R&D Program of China(grant numbers:2017YFB0801701); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076326","Datacenter networks;multipath TCP;load balancing;network protocols;network communications","Load management;Throughput;Computer science;Hardware;Data communication;Servers;Bandwidth","cloud computing;computer centres;Linux;resource allocation;telecommunication congestion control;telecommunication traffic;transport protocols","inter-rack many-to-one;congestion control performance;flow length;eliminates unnecessary subflows;rack-local traffic;DCMPTCP;rack-local flows;efficiency problem;switch hardware;traffic characteristics;complicated circumstance;load balancing performance;sophisticated technologies;datacenter managers;important task;DCN;datacenter networks;top-of-rack switches;low-cost datacenter load balancing","",5.0,"",55.0,"IEEE","22 Apr 2020","","","IEEE","IEEE Journals"
"Deterministic Data Distribution for Efficient Recovery in Erasure-Coded Storage Systems","L. Xu; M. Lyu; Z. Li; Y. Li; Y. Xu","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China","IEEE Transactions on Parallel and Distributed Systems","8 May 2020",2020,31.0,10.0,2248,2262,"Due to individual unreliable commodity components, failures are common in large-scale distributed storage systems. Erasure codes are widely deployed in practical storage systems to provide fault tolerance with low storage overhead. However, random data distribution (RDD), commonly used in erasure-coded storage systems, induces heavy cross-rack traffic, load imbalance, and random access, which adversely affects failure recovery. In this article, with orthogonal arrays, we define a Deterministic Data Distribution (D3) to uniformly distribute data/parity blocks among nodes, and propose an efficient failure recovery approach based on D3, which minimizes the cross-rack repair traffic against a single node failure. Thanks to the uniformity of D3, the proposed recovery approach balances the repair traffic not only among nodes within a rack but also among racks. We implement D3 over Reed-Solomon codes and Locally Repairable Codes in Hadoop Distributed File System (HDFS) with a cluster of 28 machines. Compared with RDD, our experiments show that D3 significantly speeds up the failure recovery up to 2.49 times for RS codes and 1.38 times for LRCs. Moreover, D3 supports front-end applications better than RDD in both of normal and recovery states.","1558-2183","","10.1109/TPDS.2020.2987837","National Key R&D Program of China(grant numbers:2018YFB1003204); National Natural Science Foundation of China(grant numbers:61832011,61772486); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069296","Distributed storage system;erasure coding;traffic;orthogonal array;load balance","Arrays;Bandwidth;Maintenance engineering;Layout;Fault tolerance;Fault tolerant systems","distributed databases;fault tolerant computing;file organisation;network operating systems;parallel processing;Reed-Solomon codes;storage management;system recovery;telecommunication network reliability;telecommunication traffic","failure recovery approach;Hadoop distributed file system;locally repairable codes;Reed-Solomon codes;cross-rack repair traffic;deterministic data distribution;cross-rack traffic;erasure-coded storage systems;RDD;random data distribution;storage overhead;erasure codes;large-scale distributed storage systems","",11.0,"",44.0,"IEEE","16 Apr 2020","","","IEEE","IEEE Journals"
"SF-Sketch: A Two-Stage Sketch for Data Streams","L. Liu; Y. Shen; Y. Yan; T. Yang; M. Shahzad; B. Cui; G. Xie","School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; Department of Computer and Science, Peking University, Beijing, China; Department of Computer and Science, Peking University, Beijing, China; Department of Computer Science, North Carolina State University, Raleigh, USA; Department of Computer and Science, Peking University, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","8 May 2020",2020,31.0,10.0,2263,2276,"Sketches are probabilistic data structures designed for recording frequencies of items in a multi-set. They are widely used in various fields, especially for gathering Internet statistics from distributed data streams in network measurements. In a distributed streaming application with high data rates, a sketch in each monitoring node “fills up” very quickly and then its content is transferred to a remote collector responsible for answering queries. Thus, the size of the contents transferred must be kept as small as possible while meeting the desired accuracy requirement. To obtain significantly higher accuracy while keeping the same update and query speed as the best prior sketches, in this article, we propose a new sketch - the Slim-Fat (SF) sketch. The key idea behind the SF-sketch is to maintain two separate sketches: a larger sketch, the Fat-subsketch, and a smaller sketch, the Slim-subsketch. The Fat-subsketch is used for updating and periodically producing the Slim-subsketch, which is then transferred to the remote collector for answering queries quickly and accurately. We also present the error bound as well as an accurate model of the correct rate of the SF-sketch, and verify their correctness through experiments. We implemented and extensively evaluated the SF-sketch along with several prior sketches. Our results show that when the size of our Slim-subsketch and of the widely used Count-Min (CM) sketch are kept the same, our SF-sketch outperforms the CM-sketch by up to 33.1 times in terms of accuracy (when the ratio of the sizes of the Fat-subsketch and the Slim-subsketch is 16:1). We have made all source codes publicly available at Github [“Source code of SF sketches,” [Online]. Available: https://github.com/paper2017/SF-sketch].","1558-2183","","10.1109/TPDS.2020.2987609","National Basic Research Program of China (973 Program)(grant numbers:2018YFE0207600,2018YFB2100403); National Natural Science Foundation of China(grant numbers:61672061,U1736216); National Science Foundation(grant numbers:CNS-1616317,CNS-1616273); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068427","Network measurements;sketch;distributed monitoring;multiset;frequent items","Distributed databases;Monitoring;Bars;Frequency measurement;Registers;Fats;Hash functions","data handling;data structures;Internet;query processing","Internet statistics;probabilistic data structures;query answering;SF sketches;Slim-subsketch;Fat-subsketch;Slim-Fat sketch;distributed data streams;two-stage sketch","",5.0,"",37.0,"IEEE","15 Apr 2020","","","IEEE","IEEE Journals"
"Integrating Task Duplication in Optimal Task Scheduling With Communication Delays","M. Orr; O. Sinnen","Department of Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand; Department of Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand","IEEE Transactions on Parallel and Distributed Systems","8 May 2020",2020,31.0,10.0,2277,2288,"Task scheduling with communication delays is an NP-hard problem. Some previous attempts at finding optimal solutions to this problem have used branch-and-bound state-space search, with promising results. Duplication is an extension to the task scheduling model which allows tasks to be executed multiple times within a schedule, providing benefits to schedule length where this allows a reduction in communication costs. This article proposes the first approach to state-space search for optimal task scheduling with task duplication. Also presented are new definitions for important standard bounding metrics in the context of duplication. An extensive empirical evaluation shows that the use of duplication significantly increases the difficulty of optimal scheduling, but the proposed approach also gives certainty that a large proportion of task graphs can be scheduled more effectively when duplication is allowed, and permits to quantify the exact advantage.","1558-2183","","10.1109/TPDS.2020.2989767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076867","Scheduling;parallel systems;graph and tree search strategies;optimization","Task analysis;Program processors;Schedules;Optimal scheduling;Processor scheduling;Delays;Search problems","computational complexity;graph theory;optimisation;processor scheduling;tree searching","state-space search;branch-and-bound state-space search;task graphs;task duplication;optimal task scheduling;communication costs;schedule length;optimal solutions;NP-hard problem;communication delays","",4.0,"",25.0,"IEEE","23 Apr 2020","","","IEEE","IEEE Journals"
"Automated Fine-Grained CPU Cap Control in Serverless Computing Platform","Y. K. Kim; M. R. HoseinyFarahabady; Y. C. Lee; A. Y. Zomaya","University of Sydney, Camperdown, Australia; University of Sydney, Camperdown, Australia; Macquarie University, Macquarie Park, Australia; University of Sydney, Camperdown, Australia","IEEE Transactions on Parallel and Distributed Systems","8 May 2020",2020,31.0,10.0,2289,2301,"Serverless computing has emerged as a new cloud computing execution model that liberates users and application developers from explicitly managing `physical' resources, leaving such a resource management burden to service providers. In this article, we study the problem of resource allocation for multi-tenant serverless computing platforms explicitly taking into account workload fluctuations including sudden surges. In particular, we investigate different root causes of performance degradation in these platforms where tenants (their applications) have different workload characteristics. To this end, we develop a fine-grained CPU cap control solution as a resource manager that dynamically adjusts CPU usage limit (or CPU cap) concerning applications with same/similar performance requirements, i.e., application groups. The adjustment of CPU caps applies primarily to co-located worker processes of serverless computing platforms to minimize resource contention, which is the major source of performance degradation. The actual adjustment decisions are made based on performance metrics (e.g., throttled time and queue length) using a group-aware scheduling algorithm. The extensive experimental results performed in our local cluster confirm that the proposed resource manager can effectively eliminate the burden of explicit reservation of computing capacity, even when fluctuations and sudden surges in the incoming workload exist. We measure the robustness of the proposed resource manager by comparing it with several heuristics which extensively used in practice, including the enhanced version of round robin and the least length queue scheduling policies, under various workload intensities driven by real-world scenarios. Notably, our resource manager outperforms other heuristics by decreasing skewness and average response time up to 44 and 94 percent, respectively, while it does not over-use the CPU resources.","1558-2183","","10.1109/TPDS.2020.2989771","Australian Research Council(grant numbers:DP190103710); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076860","Serverless computing;virtualized cloud platforms;operating system process management;dynamic CPU scheduling;performance modeling","Interference;Round robin;Time factors;Resource management;Measurement;Computer architecture","cloud computing;queueing theory;resource allocation;scheduling","least length queue scheduling policies;group-aware scheduling algorithm;multitenant serverless computing platform;cloud computing execution model;automated fine-grained CPU cap control;workload characteristics;workload fluctuations;physical resource management;resource allocation;CPU resources;performance metrics;performance degradation;resource contention","",27.0,"",29.0,"IEEE","23 Apr 2020","","","IEEE","IEEE Journals"
"Improving Restore Performance for In-Line Backup System Combining Deduplication and Delta Compression","Y. Zhang; Y. Yuan; D. Feng; C. Wang; X. Wu; L. Yan; D. Pan; S. Wang","School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, and Division of Data Storage System, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","13 May 2020",2020,31.0,10.0,2302,2314,"Data deduplication, though being efficient in removing duplicate chunks, introduces chunk fragmentation which decreases restore performance. Rewriting algorithms are proposed to reduce the chunk fragmentation. Delta compression is often used as a complement for data deduplication to further improve storage efficiency. We observe that delta compression introduces a new type of chunk fragmentation stemming from improper delta compression for chunks of which the base chunks are fragmented. The new type of chunk fragmentation severely decreases restore performance and cannot be addressed by existing rewriting algorithms. To address this problem, we propose SDC, a scheme performing post-deduplication delta compression only for the chunks of which the bases can be directly found in the restore cache to eliminate additional disk reads for base chunks, thus avoiding the new type of chunk fragmentation. In addition, self-referenced chunks can be fragmented, which decrease restore performance, and these fragmented chunks can serve as bases to decrease the restore performance repeatedly. We propose a hybrid rewriting scheme for SDC to rewrite such fragmented chunks. Experimental results show that SDC improves the restore performance of the approach that directly performs delta compression after data deduplication by 2.9-16.9x, and achieves more than 95 percent of its compression gains.","1558-2183","","10.1109/TPDS.2020.2991030","National Natural Science Foundation of China(grant numbers:61821003,61832007,61772222,U1705261,61772180,61902116); technological innovation project of Hubei Province(grant numbers:2019(2019 AAA047)); scientific research fund of Hubei Provincial Department of Education(grant numbers:B2017042); Hubei University of Technology(grant numbers:BSQD2019025,BSQD2019022,BSQD2019020,BSQD2019026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080096","Data deduplication;delta compression;storage system;chunk fragmentation;restore performance","Containers;Acceleration;Computer science;Indexes;Measurement;Data storage systems;Redundancy","data compression;data handling;rewriting systems;storage management","SDC;post-deduplication delta compression;in-line backup system;self-referenced chunks;base chunks;improper delta compression;rewriting algorithms;chunk fragmentation;duplicate chunks;data deduplication;fragmented chunks;restore performance;efficiency 95.0 percent","",7.0,"",34.0,"IEEE","28 Apr 2020","","","IEEE","IEEE Journals"
"Cross-Rack-Aware Updates in Erasure-Coded Data Centers: Design and Evaluation","Z. Shen; P. P. C. Lee","State Key Laboratory of Integrated Services Networks, Xidian University, Xi'an, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","13 May 2020",2020,31.0,10.0,2315,2328,"The update performance in erasure-coded data centers is often bottlenecked by the constrained cross-rack bandwidth. We propose CAU, a cross-rack-aware update mechanism that aims to mitigate the cross-rack update traffic in erasure-coded data centers. CAU builds on three design elements: (i) selective parity updates, which select the appropriate parity update approach based on the update pattern and the data layout to reduce the cross-rack update traffic; (ii) data grouping, which relocates and groups updated data chunks in the same rack to further reduce the cross-rack update traffic; and (iii) interim replication, which stores a specified number of temporary replicas for each newly updated data chunk. We evaluate CAU via trace-driven analysis, local cluster experiments, and Amazon EC2 experiments. We show that CAU enhances state-of-the-arts by mitigating the cross-rack update traffic as well as maintaining high update performance in both local cluster and geo-distributed environments.","1558-2183","","10.1109/TPDS.2020.2991021","Research Grants Council of the Hong Kong Special Administrative Region, China(grant numbers:GRF 14216316 and AoE/P-404/18); National Natural Science Foundation of China(grant numbers:61602120); Xidian University(grant numbers:ISN21-19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080119","Erasure coding;data centers;cross-rack-aware updates","Encoding;Bandwidth;Fault tolerance;Fault tolerant systems;Data centers;Distributed databases","computer centres;data handling;storage management","storage services;trace-driven analysis;CAU;data chunk;selective parity updates;cross-rack update traffic;cross-rack-aware update mechanism;erasure-coded data centers","",4.0,"",40.0,"IEEE","28 Apr 2020","","","IEEE","IEEE Journals"
"aeSpTV: An Adaptive and Efficient Framework for Sparse Tensor-Vector Product Kernel on a High-Performance Computing Platform","Y. Chen; G. Xiao; M. T. Özsu; C. Liu; A. Y. Zomaya; T. Li","David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada; National Supercomputing Center in Changsha, Changsha, China; School of Information Technologies, University of Sydney, Sidney, Australia; Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA","IEEE Transactions on Parallel and Distributed Systems","13 May 2020",2020,31.0,10.0,2329,2345,"Multi-dimensional, large-scale, and sparse data, which can be neatly represented by sparse tensors, are increasingly used in various applications such as data analysis and machine learning. A high-performance sparse tensor-vector product (SpTV), one of the most fundamental operations of processing sparse tensors, is necessary for improving efficiency of related applications. In this article, we propose aeSpTV, an adaptive and efficient SpTV framework on Sunway TaihuLight supercomputer, to solve several challenges of optimizing SpTVon high-performance computing platforms. First, to map SpTV to Sunway architecture and tame expensive memory access latency and parallel writing conflict due to the intrinsic irregularity of SpTV, we introduce an adaptive SpTV parallelization. Second, to co-execute with the parallelization design while still ensuring high efficiency, we design a sparse tensor data structure named CSSoCR. Third, based on the adaptive SpTV parallelization with the novel tensor data structure, we present an autotuner that chooses the most befitting tensor partitioning method for aeSpTV using the variance analysis theory of mathematical statistics to achieve load balance. Fourth, to further leverage the computing power of Sunway, we propose customized optimizations for aeSpTV. Experimental results show that aeSpTV yields good sacalability on both thread-level and process-level parallelism of Sunway. It achieves a maximum GFLOPS of 195.69 on 128 processes. Additionally, it is proved that optimization effects of the partitioning autotuner and optimization techniques are remarkable.","1558-2183","","10.1109/TPDS.2020.2990429","National Key R&D Program of China(grant numbers:2018 YFB0203800); National Natural Science Foundation of China(grant numbers:61625202,61661146006,61751204,61772182,61860206011,61806077); Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9078890","Parallel;partition;sparse tensor data structure;sparse tensor-vector product;Sunway architecture","Tensors;Sparse matrices;Data structures;Optimization;Kernel;Parallel processing","data structures;optimisation;parallel processing;software architecture;tensors;vectors","tensor partitioning method;SpTV parallelization;SpTV framework;tensor-vector product kernel;aeSpTV;tensor data structure;parallelization design;Sunway architecture;Sunway TaihuLight supercomputer;high-performance computing platform","",12.0,"",40.0,"IEEE","27 Apr 2020","","","IEEE","IEEE Journals"
"A Ubiquitous Machine Learning Accelerator With Automatic Parallelization on FPGA","C. Wang; L. Gong; X. Li; X. Zhou","School of Computer Science, University of Science and Technology of China, Hefei, China; School of Computer Science, University of Science and Technology of China, Hefei, China; School of Computer Science, University of Science and Technology of China, Hefei, China; School of Computer Science, University of Science and Technology of China, Hefei, China","IEEE Transactions on Parallel and Distributed Systems","13 May 2020",2020,31.0,10.0,2346,2359,"Machine learning has been widely applied in various emerging data-intensive applications, and has to be optimized and accelerated by powerful engines to process very large scale data. Recently, the instruction set based accelerators on Field Progarmmable Gate Arrays (FPGAs) have been a promising topic for machine learning applications. The customized instructions can be further scheduled to achieve higher instruction-level parallelism. In this article, we design a ubiquitous accelerator with out-of-order automatic parallelization for large-scale data-intensive applications. The accelerator accommodates four representative applications, including clustering algorithms, deep neural networks, genome sequencing, and collaborative filtering. In order to improve the coarse-grained instruction-level parallelism, the accelerator employs an out-of-order scheduling method to enable parallel dataflow computation. We use Colored Petri Net (CPN) tools to analyze the dependences in the applications, and build a hardware prototype on the real FPGA platform. For cluster applications, the accelerator can support four different algorithms, including K-Means, SLINK, PAM, and DBSCAN. For collaborative filtering applications, it accommodates Tanimoto, euclidean, Cosine, and Pearson Correlation as Similarity metrics. For deep learning applications, we implement hardware accelerators for both training process and inference process. Finally, for genome sequencing, we design a hardware accelerator for the BWA-SW algorithm. Experimental results show that the accelerator architecture can reach up to 25X speedup against Intel processors with affordable hardware cost, insignificant power consumption, and high flexibility.","1558-2183","","10.1109/TPDS.2020.2990924","National Basic Research Program of China (973 Program)(grant numbers:2017YFA0700900); National Natural Science Foundation of China(grant numbers:61976200); Natural Science Foundation of Jiangsu Province(grant numbers:BK20181193); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2017497); Fundamental Research Funds for the Central Universities(grant numbers:WK2150110003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079640","Machine learning;accelerator;FPGA;out-of-order execution;automatic parallelization","Clustering algorithms;Hardware;Machine learning algorithms;Machine learning;Out of order;Partitioning algorithms;Parallel processing","collaborative filtering;data flow computing;field programmable gate arrays;instruction sets;learning (artificial intelligence);parallel processing;pattern clustering;Petri nets;scheduling","DBSCAN algorithm;PAM algorithm;SLINK algorithm;K-means algorithm;CPN tools;colored Petri net;field progarmmable gate arrays;ubiquitous machine learning accelerator;accelerator architecture;hardware accelerator;deep learning applications;collaborative filtering applications;cluster applications;parallel dataflow computation;out-of-order scheduling method;coarse-grained instruction-level parallelism;genome sequencing;representative applications;large-scale data-intensive applications;out-of-order automatic parallelization;ubiquitous accelerator;higher instruction-level parallelism;customized instructions;machine learning applications;emerging data-intensive applications","",20.0,"",53.0,"IEEE","27 Apr 2020","","","IEEE","IEEE Journals"
"Fast and Accurate Traffic Measurement With Hierarchical Filtering","H. Wang; H. Xu; L. Huang; Y. Zhai","Department of Computer and Information Science and Technology, University of Florida, Gainesville, USA; Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, China; Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, China; Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, China","IEEE Transactions on Parallel and Distributed Systems","13 May 2020",2020,31.0,10.0,2360,2374,"Sketches have been widely used to record traffic statistics using sub-linear space data structure. Most sketches focus on the traffic estimation of elephant flows (i.e., heavy hitters) due to their importance to many network optimization tasks, e.g., traffic engineering and load balancing. In fact, the information of aggregate mice flows (e.g., all the mice flows with the same source IP) is also crucial to many security-associated tasks, e.g., DDoS detection and network scan detection. However, the previous solutions, e.g., measuring each individual flow or using multiple sketches for independent measurement tasks, will result in worse estimation error or higher computational overhead. To conquer the above disadvantages, we propose an accurate traffic measurement framework with multiple filters, called Sketchtree, to efficiently measure both elephant flows and aggregate mice flows. These filters in Sketchtree are organized in a hierarchical manner, and help to alleviate the hash collision and improve the measurement accuracy, as the number of flows through hierarchical filters in turn will be decreased gradually. We also design some mechanisms to improve the resource utilization efficiency. To validate our proposal, we have implemented Sketchtree and conducted experimental evaluation using real campus traffic traces. The experimental results show that Sketchtree can increase the processing speed by 100 percent, and reduce the measurement error by over 30 percent compared with state-of-the-art sketches.","1558-2183","","10.1109/TPDS.2020.2991007","National Natural Science Foundation of China(grant numbers:61822210,61936015,U1709217); Anhui Initiative in Quantum Information Technologies(grant numbers:AHY150300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080095","Network measurement;heavy hitters;hierarchical filtering;sketch;attribute","Mice;Aggregates;Task analysis;Computer crime;Size measurement;Data structures;Denial-of-service attack","computer network management;data structures;IP networks;resource allocation;telecommunication traffic","measurement error;state-of-the-art sketches;campus traffic traces;resource utilization efficiency;hierarchical filters;measurement accuracy;hierarchical manner;called Sketchtree;multiple filters;accurate traffic measurement framework;higher computational overhead;worse estimation error;independent measurement tasks;individual flow;network scan detection;source IP;aggregate mice;load balancing;traffic engineering;network optimization tasks;heavy hitters;elephant flows;traffic estimation;sub-linear space data structure;traffic statistics;hierarchical filtering;efficiency 100.0 percent;efficiency 30.0 percent","",3.0,"",56.0,"IEEE","28 Apr 2020","","","IEEE","IEEE Journals"
"An Integrated Indexing and Search Service for Distributed File Systems","H. Sim; A. Khan; S. S. Vazhkudai; S. -H. Lim; A. R. Butt; Y. Kim","Oak Ridge National Laboratory, Oak Ridge, USA; Sogang University, Seoul, South Korea; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Virginia Tech, Blacksburg, USA; Sogang University, Seoul, South Korea","IEEE Transactions on Parallel and Distributed Systems","15 May 2020",2020,31.0,10.0,2375,2391,"Data services such as search, discovery, and management in scalable distributed environments have traditionally been decoupled from the underlying file systems, and are often deployed using external databases and indexing services. However, modern data production rates, looming data movement costs, and the lack of metadata, entail revisiting the decoupled file system-data services design philosophy. In this article, we present TagIt, a scalable data management service framework aimed at scientific datasets, which can be integrated into prevalent distributed file system architectures. A key feature of TagIt is a scalable, distributed metadata indexing framework, which facilitates a flexible tagging capability to support data discovery. Furthermore, the tags can also be associated with an active operator, for pre-processing, filtering, or automatic metadata extraction, which we seamlessly offload to file servers in a load-aware fashion. We have integrated TagIt into two popular distributed file systems, i.e., GlusterFS and CephFS. Our evaluation demonstrates that TagIt can expedite data search operation by up to 10× over the extant decoupled approach.","1558-2183","","10.1109/TPDS.2020.2990656","U.S. DOE's Scientific data management program; National Science Foundation(grant numbers:CNS-1615411,CNS-1405697,CNS-1565314); National Research Foundation of Korea(grant numbers:2018R1A1A1A05079398); Oak Ridge Leadership Computing Facility(grant numbers:DE-AC05-00OR22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079563","Distributed systems;storage management;scientific data management","Metadata;Servers;Indexing;Production;Tagging","data handling;database indexing;distributed databases;meta data","TagIt;scalable data management service framework;prevalent distributed file system architectures;scalable distributed metadata indexing framework;data discovery;automatic metadata extraction;popular distributed file systems;data search operation;integrated indexing;search service;scalable distributed environments;decoupled file system-data services design philosophy;data movement costs;modern data production rates;indexing services;external databases;underlying file systems","",10.0,"",61.0,"IEEE","27 Apr 2020","","","IEEE","IEEE Journals"
"RMWPaxos: Fault-Tolerant In-Place Consensus Sequences","J. Skrzypczak; F. Schintke; T. Schütt","Department of Distributed Algorithms, Zuse Institute Berlin, Berlin, Germany; Department of Distributed Algorithms, Zuse Institute Berlin, Berlin, Germany; Department of Distributed Algorithms, Zuse Institute Berlin, Berlin, Germany","IEEE Transactions on Parallel and Distributed Systems","14 May 2020",2020,31.0,10.0,2392,2405,"Building consensus sequences based on distributed, fault-tolerant consensus, as used for replicated state machines, typically requires a separate distributed state for every new consensus instance. Allocating and maintaining this state causes significant overhead. In particular, freeing the distributed, outdated states in a fault-tolerant way is not trivial and adds further complexity and cost to the system. In this article, we propose an extension to the single-decree Paxos protocol that can learn a sequence of consensus decisions `in-place', i.e., with a single set of distributed states. Our protocol does not require dynamic log structures and hence has no need for distributed log pruning, snapshotting, compaction, or dynamic resource allocation. The protocol builds a fault-tolerant atomic register that supports arbitrary read-modify-write operations. We use the concept of consistent quorums to detect whether the previous consensus still needs to be consolidated or is already finished so that the next consensus value can be safely proposed. Reading a consolidated consensus is done without state modifications and is thereby free of concurrency control and demand for serialisation. A proposer that is not interrupted reaches agreement on consecutive consensus decisions within a single message round-trip per decision by preparing the acceptors eagerly with the previous request.","1558-2183","","10.1109/TPDS.2020.2981891","Deutsche Forschungsgemeinschaft(grant numbers:RE 1389); DFG priority program(grant numbers:SPP 2037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050852","Consensus;Paxos;atomic register;consistent quorum;fault-tolerance;data management","Registers;Fault tolerance;Fault tolerant systems;Protocols;Safety;Proposals;Computer crashes","concurrency control;distributed processing;fault tolerant computing;finite state machines;protocols;resource allocation","consecutive consensus decisions;state modifications;consolidated consensus;consensus value;fault-tolerant atomic register;dynamic resource allocation;distributed log pruning;dynamic log structures;single-decree Paxos protocol;outdated states;distributed states;consensus instance;replicated state machines;distributed fault-tolerant consensus;building consensus sequences;In-Place Consensus Sequences","",6.0,"",51.0,"CCBY","30 Mar 2020","","","IEEE","IEEE Journals"
"An Optimal Locality-Aware Task Scheduling Algorithm Based on Bipartite Graph Modelling for Spark Applications","Z. Fu; Z. Tang; L. Yang; C. Liu","College of Information Science and Engineering, and National Supercomputing Center in Changsha, Hunan University, China; Science and Technology on Parallel and Distributed Processing Laboratory, National University of Defense Technology, Changsha, China; College of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China; College of Information Science and Engineering, and National Supercomputing Center in Changsha, Hunan University, China","IEEE Transactions on Parallel and Distributed Systems","14 May 2020",2020,31.0,10.0,2406,2420,"In the distributed computing framework of Spark, cross-node/rack data transfer produced by map tasks and reduce tasks are common problems resulting in performance degradation, such as prolonging of entire execution time and network congestion. To address these problems, this article utilizes the bipartite graph modelling to propose an optimal locality-aware task scheduling algorithm. By considering global optimality, the algorithm can generate the optimal scheduling solution for both the map tasks and the reduce tasks for data locality. Because of the different communication modes, this article uses a unified graph to model the map task scheduling and the reduce task scheduling respectively. Then, by calculating the communication cost matrix of tasks, we formulate an optimal task scheduling scheme to minimize overall communication cost and transform the problem as the well-known graph problem: minimum weighted bipartite matching (MWBM), which can be resolved by Kuhn-Munkres algorithm. In addition, this article proposes a locality-aware executor allocation strategy to improve the data locality further. We implement our algorithm and strategy in Spark-2.4.1 and evaluate its performance using several representative micro-benchmarks, macro-benchmarks, and HiBench benchmark suite. The experimental results verify that by reducing the network traffic and access latency, the proposed algorithm can improve the job performance substantially compared to some other task scheduling algorithms.","1558-2183","","10.1109/TPDS.2020.2992073","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1701401,2018YFB0203804,2017YFB0202201); National Natural Science Foundation of China(grant numbers:61873090,L1824034,L1924056); Ministry of Education-China Mobile Research Fund Project(grant numbers:MCM20170506); China Knowledge Centre for Engineering Sciences and Technology(grant numbers:CKCEST-2018-1-13,CKCEST-2019-2-13); Science and Technology on Parallel and Distributed Processing Laboratory(grant numbers:WDZC20195500110); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9085890","Communication cost;data locality;task scheduling;weighted bipartite graph;Spark","Task analysis;Optimal scheduling;Sparks;Scheduling;Scheduling algorithms;Data transfer","data handling;graph theory;optimisation;parallel processing;scheduling","optimal locality-aware task scheduling algorithm;data locality;map task scheduling;optimal task scheduling scheme;graph problem;locality-aware executor allocation strategy;bipartite graph modelling","",16.0,"",40.0,"IEEE","4 May 2020","","","IEEE","IEEE Journals"
"A Dynamic Multi–Objective Approach for Dynamic Load Balancing in Heterogeneous Systems","A. Cabrera; A. Acosta; F. Almeida; V. Blanco","HPC Group of Universidad de La Laguna, Escuela Superior de Ingeniería y Tecnología, San Cristóbal de La Laguna, Spain; HPC Group of Universidad de La Laguna, Escuela Superior de Ingeniería y Tecnología, San Cristóbal de La Laguna, Spain; HPC Group of Universidad de La Laguna, Escuela Superior de Ingeniería y Tecnología, San Cristóbal de La Laguna, Spain; HPC Group of Universidad de La Laguna, Escuela Superior de Ingeniería y Tecnología, San Cristóbal de La Laguna, Spain","IEEE Transactions on Parallel and Distributed Systems","15 May 2020",2020,31.0,10.0,2421,2434,"Modern standards in High Performance Computing (HPC) have started to consider energy consumption and power draw as a limiting factor. New and more complex architectures have been introduced in HPC systems to afford these new restrictions, and include coprocessors such as GPGPUs for intensive computational tasks. As systems increase in heterogeneity, workload distribution becomes a more core problem to achieve the maximum efficiency in every computational component. We present a Multi-Objective Dynamic Load Balancing (DLB) approach where several objectives can be applied to tune an application. These objectives can be dynamically exchanged during the execution of an algorithm to better adapt to the resources available in a system. We have implemented the Multi-Objective DLB together with a generic heuristic engine, designed to perform multiple strategies for DLB in iterative problems. We also present Ull Multiobjective Framework (UllMF), an open-source tool that implements the Multi-Objective generic approach. UllMF separates metric gathering, objective functions to be optimized and load balancing algorithms, and improves code portability using a simple interface to reduce the costs of new implementations. We illustrate how performance and energy consumption are improved for the implemented techniques, and analyze their quality using different DLB techniques from the literature.","1558-2183","","10.1109/TPDS.2020.2989869","Spanish Ministry of Science, Innovation and Universities(grant numbers:TIN2016-78919-R); Government of the Canary Islands(grant numbers:ProID2017010130,TESIS2017010134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076818","Dynamic load balancing;energy efficiency;iterative algorithms;heterogeneous computing","Load management;Heuristic algorithms;Measurement;Linear programming;Energy consumption;Task analysis;Optimization","coprocessors;genetic algorithms;parallel processing;resource allocation","multiobjective DLB;multiobjective generic approach;multiobjective dynamic load balancing approach;dynamic multiobjective approach;load balancing algorithms;objective functions;computational component;core problem;systems increase;intensive computational tasks;HPC systems;more complex architectures;limiting factor;power draw;energy consumption;high performance computing;modern standards;heterogeneous systems","",4.0,"",35.0,"IEEE","23 Apr 2020","","","IEEE","IEEE Journals"
"Data-Driven Derivation of an Analytic Model for Parallel Servers With Job Replication","N. Bajunaid; D. A. Menascé","King Abdulaziz University, Saudi Arabia; Department of Computer Science, George Mason University, Fairfax, USA","IEEE Transactions on Parallel and Distributed Systems","20 May 2020",2020,31.0,10.0,2435,2452,"The job replication problem has been studied recently as a mechanism to improve performance and availability of systems with n parallel servers, each with its own queue. A dispatcher using some policy sends d (1 ≤ d ≤ n) copies of a job to d of the servers. Copies are eliminated from the system as soon as the first copy completes from any of the d servers. This article introduces a datadriven method to derive closed-form expressions for the average response time and other metrics of jobs as a function of the degree of replication d. This method consists of developing a simulator for the system in order to generate a very large number of datasets for a wide range of input parameters. A statistical and visualization analysis of the data provides the analytical models. It is important to emphasize the difference between using simulation methods to obtain the value of metrics (e.g., average response time) of a computer system given values of input parameters and using our data-driven method to obtain closed-form expressions that relate output metrics to input parameters. The latter is the focus of our approach. The analysis presented here covers results for homogeneous and heterogeneous servers with exponentially distributed service times and for homogeneous servers with hypo-exponentially and hyper-exponentially distributed service times. This article also presents a closed-form equation for the optimal replication degree for the case of homogeneous servers with hypo-exponentially distributed service times.","1558-2183","","10.1109/TPDS.2020.2992571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086162","Parallel servers;job replication;data-driven analytic model derivation;simulation","Servers;Analytical models;Computational modeling;Time factors;Measurement;Closed-form solutions;Mathematical model","data analysis;data visualisation;exponential distribution;file servers;parallel processing;statistical analysis","simulation methods;hypo-exponentially distributed service times;optimal replication degree;closed-form equation;hyper-exponentially distributed service times;heterogeneous servers;homogeneous servers;output metrics;data-driven method;computer system;analytical models;data visualization analysis;statistical analysis;input parameters;average response time;closed-form expressions;job replication problem;parallel servers;data-driven derivation","",1.0,"",20.0,"IEEE","4 May 2020","","","IEEE","IEEE Journals"
"Reconciling Time Slice Conflicts of Virtual Machines With Dual Time Slice for Clouds","T. Kim; C. H. Park; J. Huh; J. Ahn","Department of Software and Computer Engineering, Ajou University, Suwon, South Korea; Department of Information Technology, Uppsala University, Uppsala, Sweden; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Department of Software and Computer Engineering, Ajou University, Suwon, South Korea","IEEE Transactions on Parallel and Distributed Systems","20 May 2020",2020,31.0,10.0,2453,2465,"The proliferation of system virtualization poses a new challenge for the coarse-grained time sharing techniques for consolidation, since operating systems are running on virtual CPUs. The current system stack was designed under the assumption that operating systems can seize CPU resources at any moment. However, for the guest operating system on a virtual machine (VM), such assumption cannot be guaranteed, since virtual CPUs of VMs share a limited number of physical cores. Due to the time-sharing of physical cores, the execution of a virtual CPU is not contiguous, with a gap between the virtual and real time spaces. Such a virtual time discontinuity problem leads to significant inefficiency for lock and interrupt handling, which rely on the immediate availability of CPUs whenever the operating system requires computation. To reduce scheduling latencies of virtual CPUs, shortening time slices can be a straightforward strategy, but it may lead to the increased overhead of context switching costs across virtual machines for some workloads. It is challenging to determine a single time slice to satisfy all the VMs. In this article, we propose to have dual time slice to resolve the time slice conflict problem occurred in different types of virtual machines.","1558-2183","","10.1109/TPDS.2020.2993252","National Research Foundation of Korea(grant numbers:NRF-2013R1A2A2A01015514,NRF-2019R1C1C1005166); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090316","Virtualization;virtual time discontinuity;dual time slice","Kernel;Virtual machine monitors;Virtual machining;Throughput;Real-time systems;Context","cloud computing;interrupts;operating systems (computers);scheduling;virtual machines;virtualisation","context switching costs;scheduling latency reduction;lock handling;interrupt handling;CPU resources;cloud computing;coarse-grained time sharing technique;physical cores;guest operating system;system stack;system virtualization;time slice conflict problem;dual time slice;single time slice;virtual machine;virtual CPU;virtual time discontinuity problem;real time spaces;virtual time spaces","",1.0,"",41.0,"IEEE","8 May 2020","","","IEEE","IEEE Journals"
"Endpoint-Flexible Coflow Scheduling Across Geo-Distributed Datacenters","W. Li; X. Yuan; K. Li; H. Qi; X. Zhou; R. Xu","Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China","IEEE Transactions on Parallel and Distributed Systems","20 May 2020",2020,31.0,10.0,2466,2481,"Over the last decade, we have witnessed growing data volumes generated and stored across geographically distributed datacenters. Processing such geo-distributed datasets may suffer from significant slowdown as the underlying network flows have to go through the inter-datacenter networks with relatively low and highly heterogeneous available link bandwidth. Thus, optimizing the transmissions of inter-datacenter flows, especially coflows that capture application-level semantics, is important for improving the communication performance of such geo-distributed applications. However, prior solutions on coflow scheduling have significant limitations: they schedule coflows with already-fixed endpoints of flows, making them insufficient to optimize the coflow completion time (CCT). In this article, we focus on the problem of jointly considering endpoint placement and coflow scheduling to minimize the average CCT of coflows across geo-distributed datacenters. To solve this problem without any prior knowledge of coflow arrivals, we present a coflow-aware optimization framework called SmartCoflow. In SmartCoflow, we first apply an approximate algorithm to obtain the endpoint placement and scheduling decisions for a single coflow. Based on the single-coflow solution, we then develop an efficient online algorithm to handle the dynamically arrived coflows. Through rigorous theoretical analysis, we prove that SmartCoflow has a non-trivial competitive ratio. We also extend SmartCoflow to incorporate various design choices or requirements of applications and operators, such as enforcing an inter-datacenter bandwidth usage budget and considering coflow deadline. Through experimental results from testbed implementation and trace-driven simulations, we demonstrate that SmartCoflow can reduce the average CCT, lower bandwidth usage, and improve coflow deadline meet rate, when compared to the state-of-the-art scheduling-only method.","1558-2183","","10.1109/TPDS.2020.2992615","NSFC General Technology Basic Research Joint Funds(grant numbers:U1836214); National Natural Science Foundation of China(grant numbers:61832013); Artificial Intelligence Science and Technology Major Project of Tianjin(grant numbers:18ZXZNGX00190); National Key R&D Program of China(grant numbers:2019QY1302); National Natural Science Foundation of China(grant numbers:61672379); National Key R&D Program of China(grant numbers:2019YFB2102404); NSFC-Guangdong Joint Funds(grant numbers:U1701263); Natural Science Foundation of Tianjin City(grant numbers:18ZXZNGX00040); National Key R&D Program of China(grant numbers:2018YFB1004700); National Natural Science Foundation of China(grant numbers:61872265,61672131); Key research and Development Program for Guangdong Province(grant numbers:2019B010136001); National Natural Science Foundation of China(grant numbers:61772112); Science Innovation Foundation of Dalian(grant numbers:2019J12GX037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086773","Inter-datacenter;coflow scheduling;CCT;deadline;endpoint flexibility","Task analysis;Bandwidth;Scheduling;Heuristic algorithms;Distributed databases;Approximation algorithms;Data models","computer centres;optimisation;processor scheduling","low link bandwidth;heterogeneous available link bandwidth;coflow deadline;scheduling-only method;inter-datacenter bandwidth usage budget;SmartCoflow;single-coflow solution;coflow-aware optimization framework;coflow arrivals;CCT;endpoint placement;coflow completion time;geo-distributed applications;application-level semantics;inter-datacenter networks;network flows;geo-distributed datasets;geographically distributed datacenters;geo-distributed datacenters;endpoint-flexible coflow scheduling","",7.0,"",67.0,"IEEE","5 May 2020","","","IEEE","IEEE Journals"
"Power-Aware Allocation of Graph Jobs in Geo-Distributed Cloud Networks","S. Hosseinalipour; A. Nayak; H. Dai","Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, USA; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, USA; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, USA","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,749,765,"In the era of big-data, the jobs submitted to the clouds exhibit complicated structures represented by graphs, where the nodes denote the sub-tasks each of which can be accommodated at a slot in a server, while the edges indicate the communication constraints among the sub-tasks. We develop a framework for efficient allocation of graph jobs in geo-distributed cloud networks (GDCNs), explicitly considering the power consumption of the datacenters (DCs). We address the following two challenges arising in graph job allocation: i) the allocation problem belongs to NP-hard nonlinear integer programming; ii) the allocation requires solving the NP-complete sub-graph isomorphism problem, which is particularly cumbersome in large-scale GDCNs. We develop a suite of efficient solutions for GDCNs of various scales. For small-scale GDCNs, we propose an analytical approach based on convex programming. For medium-scale GDCNs, we develop a distributed allocation algorithm exploiting the processing power of DCs in parallel. Afterward, we provide a novel low-complexity (decentralized) sub-graph extraction method, based on which we introduce cloud crawlers aiming to extract allocations of good potentials for large-scale GDCNs. Given these suggested strategies, we further investigate strategy selection under both fixed and adaptive DC pricing schemes, and propose an online learning algorithm for each.","1558-2183","","10.1109/TPDS.2019.2943457","National Science Foundation(grant numbers:ECCS-1444009,CNS-1824518); Army Research Office(grant numbers:W911NF-17-1-0087); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8847383","Big-data;graph jobs;geo-distributed cloud networks;datacenter power consumption;job allocation;integer programming;convex optimization;online learning","Resource management;Cloud computing;Power demand;Servers;Task analysis;Twitter;Distributed algorithms","Big Data;cloud computing;computational complexity;computer centres;convex programming;graph theory;integer programming;nonlinear programming;power aware computing;resource allocation","online learning algorithm;adaptive DC pricing schemes;decentralized sub-graph extraction method;low-complexity sub-graph extraction method;convex programming;analytical approach;data centers;communication constraints;Big Data;power-aware allocation;cloud crawlers;distributed allocation algorithm;medium-scale GDCNs;small-scale GDCNs;large-scale GDCNs;NP-complete sub-graph isomorphism problem;NP-hard nonlinear integer programming;graph job allocation;DCs;power consumption;geo-distributed cloud networks","",10.0,"",46.0,"IEEE","24 Sep 2019","","","IEEE","IEEE Journals"
"cCUDA: Effective Co-Scheduling of Concurrent Kernels on GPUs","S. . -K. Shekofteh; H. Noori; M. Naghibzadeh; H. Fröning; H. S. Yazdi","Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran; Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran; Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran; Institute of Computer Engineering, Ruprecht-Karls University of Heidelberg, Heidelberg, Germany; Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,766,778,"While GPUs are meantime omnipresent for many scientific and technical computations, they still continue to evolve as processors. An important recent feature is the ability to execute multiple kernels concurrently via queue streams. However, experiments show that different parameters including the behavior of kernels, the order of kernel launches and other execution configurations, e.g., the number of concurrent thread blocks, may result in different execution time for concurrent kernel execution. Since kernels may have different resource requirements, they can be classified into different classes, which are traditionally assumed as either memory-bound or compute-bound. However, a kernel may belong to the different classes on different hardware according to the hardware resources. In this paper, the definition of kernel mix intensity is introduced. Based on this, a scheduling framework called concurrent CUDA (cCUDA) is proposed to co-schedule the concurrent kernels more efficiently. It first profiles and ranks kernels with different execution behaviors and then takes the kernel resource requirements into account to partition thread blocks of different kernels and overlap them to better utilize the GPU resources. Experimental results on real hardware demonstrate performance improvement in terms of execution time of up to 1.86x, and an average speedup of 1.28x for a wide range of kernels. cCUDA is available at https://github.com/kshekofteh/cCUDA.","1558-2183","","10.1109/TPDS.2019.2944602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8853389","Kernel;scheduling;concurrent kernel execution;stream;resource management","Kernel;Graphics processing units;Benchmark testing;Hardware;Scheduling;Analytical models","graphics processing units;operating system kernels;parallel architectures;processor scheduling","execution behaviors;concurrent CUDA;kernel mix intensity;resource requirements;concurrent kernel execution;execution time;concurrent thread blocks;kernel launches;multiple kernels;GPUs;concurrent kernels;cCUDA;kernel resource requirements","",13.0,"",41.0,"IEEE","30 Sep 2019","","","IEEE","IEEE Journals"
"Hotspot-Aware Hybrid Memory Management for In-Memory Key-Value Stores","H. Jin; Z. Li; H. Liu; X. Liao; Y. Zhang","Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,779,792,"Emerging Non-Volatile Memory (NVM) technologies promise much higher memory density and energy efficiency than DRAM, at the expense of higher read/write latency and limited write endurance. Hybrid memory systems composed of DRAM and NVM have the potential to provide very large capacity of main memory for in-memory key-value (K-V) stores. However, there remains challenges to directly deploy DRAM-based K-V stores in hybrid memory systems. The performance and energy efficiency of K-V stores on hybrid memory systems have not been fully explored yet. In this paper, we propose HMCached, an in-memory K-V store built on a hybrid DRAM/NVM system. HMCached utilizes an application-level data access counting mechanism to identify frequently-accessed (hotspot) objects (i.e., K-V pairs) in NVM, and migrates them to fast DRAM to reduce the costly NVM accesses. We also propose an NVM-friendly index structure to store the frequently-updated portion of object metadata in DRAM, and thus further mitigate the NVM accesses. Moreover, we propose a benefit-aware memory reassignment policy to address the slab calcification problem in slab-based K-V store systems, and significantly improve the benefit gain from the DRAM. We implement the proposed schemes with Memcached and evaluate it with Zipfian-like workloads. Experiment results show that HMCached significantly reduces NVM accesses by 70 percent compared to the vanilla Memcached running on a DRAM/NVM hybrid memory system without any optimizations, and improves application performance by up to 50 percent. Moreover, compared to a DRAM-only system, HMCached achieves 90 percent of performance and 46 percent reduction of energy consumption for realistic (read-intensive) workloads while significantly reducing the DRAM usage by 75 percent.","1558-2183","","10.1109/TPDS.2019.2945315","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1001603); National Natural Science Foundation of China(grant numbers:61672251,61732010,61825202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859283","In-memory key-value store;non-volatile memory;hybrid memory system","Random access memory;Nonvolatile memory;Slabs;Memory management;Metadata;Resource management;Indexes","cache storage;DRAM chips;meta data;storage management","slab-based K-V store systems;slab calcification problem;write endurance;read/write latency;Memcached;object metadata;HMCached;hotspot-aware hybrid memory management;benefit-aware memory reassignment policy;NVM-friendly index structure;NVM accesses;application-level data access counting mechanism;DRAM-based K-V;hybrid memory systems;energy efficiency;memory density;nonvolatile memory technologies;in-memory key-value stores","",11.0,"",41.0,"CCBY","4 Oct 2019","","","IEEE","IEEE Journals"
"Resource-Constrained Replication Strategies for Hierarchical and Heterogeneous Tasks","W. C. Ao; K. Psounis","Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, USA; Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, USA","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,793,804,"In large-scale cloud computing systems, a task is often divided into multiple subtasks which can be executed in parallel in different machines. As a result, the task completion time is constrained by the completion time of the slowest subtask. To reduce the task completion time, the strategy of replicating the straggling subtasks has been employed in cloud computing frameworks such as MapReduce and Hadoop. Analyzing mathematically the performance of such replication strategies has recently received great attention. However, most of the analytical work focuses on the case where the completion times of the subtasks are identically distributed. This assumption may not hold in practice due to the modularization and encapsulation of the computation of a task, resulting in different service requirements for different subtasks. In this paper, we consider the case where the completion times of the subtasks of a task are drawn from heterogeneous/empirical distributions. Furthermore, we consider the case where jobs consist of hierarchical tasks that are required to be executed in a specific order described by a task precedence graph. We propose a novel framework to investigate how to allocate replication resources among the subtasks such that the overall task completion time is minimized. Specifically, we devise a Lagrange multiplier-based method and a water-filling-like algorithm for integer programs. We show via analysis and simulations the optimality and efficiency of our proposed algorithms, and explore the tradeoff between cost and latency from introducing replications in a task graph.","1558-2183","","10.1109/TPDS.2019.2945294","NSF(grant numbers:ECCS-1444060); Cisco Research Center; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859329","Task replication;latency;resource allocation;task graph;parallel computing","Task analysis;Resource management;Optimization;Cloud computing;Power system reliability;Probability;Time factors","cloud computing;data handling;graph theory;integer programming;parallel processing;resource allocation","cloud computing frameworks;straggling subtasks;slowest subtask;multiple subtasks;heterogeneous tasks;resource-constrained replication strategies;task graph;task completion time;replication resources;task precedence graph;hierarchical tasks;different subtasks","",8.0,"",25.0,"IEEE","4 Oct 2019","","","IEEE","IEEE Journals"
"Quantum Supremacy Circuit Simulation on Sunway TaihuLight","R. Li; B. Wu; M. Ying; X. Sun; G. Yang","Department of Computer Science & Technology, Tsinghua University, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, and Department of Computer Science and Technology, Tsinghua University, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Computer Science & Technology, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,805,816,"With the rapid progress made by industry and academia, quantum computers with dozens of qubits or even larger size are being realized. However, the fidelity of existing quantum computers often sharply decreases as the circuit depth increases. Thus, an ideal quantum circuit simulator on classical computers, especially on high-performance computers, is needed for benchmarking and validation. We design a large-scale simulator of universal random quantum circuits, often called “quantum supremacy circuits”, and implement it on Sunway TaihuLight. The simulator can be used to accomplish the following two tasks: 1) Computing a complete output state-vector; 2) Calculating one or a few amplitudes. We target the simulation of 49-qubit circuits. For task 1), we successfully simulate such a circuit of depth 39, and for task 2) we reach the 55-depth level. To the best of our knowledge, both of the simulation results reach the largest depth for 49-qubit quantum supremacy circuits.","1558-2183","","10.1109/TPDS.2019.2947511","National Key R&D Program of China(grant numbers:2018YFA0306701); National Supercomputing Center in Wuxi; National Natural Science Foundation of China(grant numbers:61832015); Chinese Academy of Sciences(grant numbers:XDB28000000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869942","Quantum computing;quantum circuit simulation;Sunway TaihuLight","Qubit;Task analysis;Logic gates;Supercomputers;Circuit simulation","quantum entanglement;quantum gates","49-qubit quantum supremacy circuits;universal random quantum circuits;classical computers;ideal quantum circuit simulator;circuit depth;quantum computers;Sunway TaihuLight;quantum supremacy circuit simulation","",16.0,"",30.0,"IEEE","16 Oct 2019","","","IEEE","IEEE Journals"
"On Fault-Tolerant Bin Packing for Online Resource Allocation","C. Li; X. Tang","MOE Key Laboratory of Computer Network and Information Integration, School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,817,829,"We study an online fault-tolerant bin packing problem that models reliable resource allocation. In this problem, each item is replicated and has f + 1 replicas including one primary and f standbys. The packing of items is required to tolerate up to f faulty bins, i.e., to guarantee that at least one correct replica of each item is available regardless of which f bins turn to be faulty. Any feasible packing algorithm must satisfy an exclusion constraint and a space constraint. The exclusion constraint is generalized from the fault tolerance requirement and the space constraint comes from the capacity planning. The target of bin packing is to minimize the number of bins used. We first derive a lower bound on the number of bins needed by any feasible packing algorithm. We then study three heuristic algorithms named mirroring, shifting and mixing under a particular setting where all items have the same size. The mirroring algorithm has a low utilization of the bin capacity. Compared with the mirroring algorithm, the shifting algorithm requires fewer bins. However, in online packing, the process of opening bins by the shifting algorithm is not smooth. It turns out that even for packing a few items, the shifting algorithm needs to quickly open a large number of bins. The mixing algorithm adopts a dual average strategy to gradually open new bins for incoming items. We prove that the mixing algorithm is feasible and show that it balances the number of bins used and the process of opening bins. Finally, to pack items with different sizes, we extend the mirroring algorithm by adopting the First-Fit strategy and extend both the shifting and mixing algorithms by involving the harmonic strategy. The asymptotic competitive ratios of the three extended algorithms are analyzed respectively.","1558-2183","","10.1109/TPDS.2019.2948327","Singapore MOE Academic Research Fund(grant numbers:2013-T1-002-123,2018-T1-002-063); National Natural Science Foundation of China(grant numbers:61902063); Natural Science Foundation of Jiangsu Province(grant numbers:BK20190342); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877781","Fault-tolerance;bin packing;heuristic;online","Fault tolerance;Fault tolerant systems;Servers;Resource management;Heuristic algorithms;Switches","bin packing;fault tolerant computing;harmonic analysis;resource allocation","asymptotic competitive ratios;harmonic strategy;First-Fit strategy;capacity planning;space constraint;exclusion constraint;feasible packing algorithm;f faulty bins;reliable resource allocation;online fault-tolerant bin packing problem;online resource allocation;shifting mixing algorithms;online packing;mirroring algorithm;bin capacity","",6.0,"",21.0,"IEEE","21 Oct 2019","","","IEEE","IEEE Journals"
"A Holistic Heterogeneity-Aware Data Placement Scheme for Hybrid Parallel I/O Systems","S. He; Z. Li; J. Zhou; Y. Yin; X. Xu; Y. Chen; X. -H. Sun","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Computer Science Program, School of Business, Stockton University, Galloway, USA; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Zhejiang Lab, Intelligent Computing System Research Center, Institute of Artificial Intelligence, Hangzhou, China; Department of Computer Science, Kennesaw State University, Kennesaw, USA; Department of Computer Science, Texas Tech University, Lubbock, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, USA","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,830,842,"We present H2DP, a holistic heterogeneity-aware data placement scheme for hybrid parallel I/O systems, which consist of HDD servers and SSD servers. Most of the existing approaches focus on server performance or application I/O pattern heterogeneity in data placement. H2DP considers three axes of heterogeneity: server performance, server space, and application I/O pattern. More specifically, H2DP determines the optimized stripe sizes on servers based on server performance, keeps only critical data on all hybrid servers and the rest data on HDD servers, and dynamically migrates data among different types of servers at run-time. This holistic heterogeneity-awareness enables H2DP to achieve high performance by alleviating server load imbalance, efficiently utilizing SSD space, and accommodating application pattern variation. We have implemented a prototype of H2DP under MPICH2 atop OrangeFS. Extensive experimental results demonstrate that H2DP significantly improve I/O system performance compared to existing data placement schemes.","1558-2183","","10.1109/TPDS.2019.2948901","National Natural Science Foundation of China(grant numbers:61572377); Natural Science Foundation of Hubei Province(grant numbers:2017CFC889); Fundamental Research Funds for the Central Universities(grant numbers:2018QNA5015); Zhejiang Lab Research Project(grant numbers:2019KC0AC01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880508","Parallel I/O system;parallel file system;hybrid parallel file system;data placement;solid state drive","Servers;System performance;Bandwidth;Computer science;Distributed databases;Sun;File systems","computer centres;disc drives;file organisation;file servers;input-output programs;message passing;parallel processing;storage management","hybrid parallel I/O systems;pattern heterogeneity;SSD servers;holistic heterogeneity-aware data placement scheme;server load imbalance;HDD servers;rest data;hybrid servers;critical data;server performance;H2DP;server space","",4.0,"",50.0,"IEEE","24 Oct 2019","","","IEEE","IEEE Journals"
"gQoS: A QoS-Oriented GPU Virtualization with Adaptive Capacity Sharing","Q. Lu; J. Yao; H. Guan; P. Gao","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Tencent Corporation, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,843,855,"Currently, the virtualization technologies for cloud computing infrastructures supporting extra devices, such as GPU, require additional development and refinement. This requirement is particularly evident in the area of resource sharing and allocation under some performance constraints, like the quality of service (QoS) guarantee, in light of the closed GPU platform. This deficiency significantly limits the applicability range of the cloud platform, which aims to support the efficient and fluent execution of business and academic workloads. This paper introduces gQoS, an adaptive virtualized GPU resource capacity sharing system under the QoS target, which can share and allocate the virtualized GPU resource among workloads adaptively, guaranteeing the QoS level with stability and accuracy. We evaluate the workloads and compare our gQoS strategy with other allocation strategies. The experiments show that our strategy guarantees much better accuracy and stability in QoS control and that the total GPU resource utilization under gQoS can be rewarded with at most a 25.85 percent reduction compared with other strategies.","1558-2183","","10.1109/TPDS.2019.2948753","National Key Research & Development Program of China(grant numbers:2018YFB1003603); National Natural Science Foundation of China(grant numbers:61772339,61572322,61525204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880518","GPU virtualization;QoS control;resource scheduling;cloud computing","Graphics processing units;Quality of service;Virtualization;Resource management;Cloud computing;Hardware;Virtual machining","cloud computing;graphics processing units;quality of service;resource allocation;virtualisation","adaptive virtualized GPU resource capacity sharing system;quality of service;resource allocation;cloud computing infrastructures;GPU resource utilization;QoS control;allocation strategies;gQoS strategy;QoS level;QoS target;academic workloads;cloud platform;closed GPU platform;resource sharing;virtualization technologies;adaptive capacity sharing;QoS-oriented GPU virtualization","",6.0,"",30.0,"IEEE","24 Oct 2019","","","IEEE","IEEE Journals"
"A Novel Multi-Stage Forest-Based Key-Value Store for Holistic Performance Improvement","Z. Lu; Q. Cao; F. Mei; H. Jiang; J. Li","Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education, Wuhan National Laboratory for Optoelectronics, and the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education, Wuhan National Laboratory for Optoelectronics, Wuhan, China; Huawei Technologies, Hangzhou; University of Texas at Arlington, Arlington, USA; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,856,870,"Key-value (KV) stores based on multi-stage structures are widely deployed to organize massive amounts of easily searchable user data. However, current KV storage systems inevitably sacrifice at least one of the performance objectives, such as write, read, space efficiency etc., for the optimization of others. To understand the root cause of and ultimately remove such performance disparities among the representative existing KV stores, we analyze their enabling mechanisms and classify them into two fundamental models of data structures facilitating KV operations, namely, the multi-stage tree (MS-tree), and the multi-stage forest (MS-forest). We build SifrDB, a KV store on a novel split forest structure, that achieves the lowest write amplification across all workload patterns and minimizes space reservation for the compaction. To mitigate the read amplification inherent in MS-forest, we introduce a bloom filer mechanism based on Sorted String Tables (SSTs). Furthermore, we also present a highly efficient parallel search approach that fully exploits the access parallelism of modern flash-based storage devices to substantially boost the read performance. Evaluation results show that under both micro and YCSB benchmarks, SifrDB outperforms its closest competitors, i.e., the popular MS-forest implementations, making it a highly desirable choice for the modern KV stores.","1558-2183","","10.1109/TPDS.2019.2950248","National Natural Science Foundation of China(grant numbers:61872156); National Natural Science Foundation of China(grant numbers:61821003); National Basic Research Program of China (973 Program)(grant numbers:2018YFA0701804); Fundamental Research Funds for the Central Universities(grant numbers:2018KFYXKJC037); National Science Foundation(grant numbers:CCF-1704504,CCF-1629625); Alibaba Group through Alibaba Innovative Research (AIR) Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8886435","Key-value;multi-stage;LSM-tree;parallel search","Vegetation;Compaction;Forestry;Indexes;Optimization;Data models;Silicon","data structures;flash memories;parallel processing;sorting;storage management;trees (mathematics)","multistage structures;data structures;multistage tree;MS-tree;KV store;flash-based storage devices;holistic performance improvement;multistage forest;key-value store;MS-forest;bloom filer;split forest structure;parallel search;sorted string tables","","","",47.0,"IEEE","30 Oct 2019","","","IEEE","IEEE Journals"
"Reliability Aware Energy Optimized Scheduling of Non-Preemptive Periodic Real-Time Tasks on Heterogeneous Multiprocessor System","N. Kumar; J. Mayank; A. Mondal","Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, India; Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, India","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,871,885,"Higher reliability and lower energy consumption are conflicting, yet among the most important design objectives for the real-time systems. Moreover, in the domain of real-time systems, non-preemptive scheduling is relatively unexplored with objectives such as reliability and energy. Thus we propose an active replication based framework to schedule a set of periodic real-time tasks in the non-preemptive heterogeneous environment such that the given reliability and timing constraints are satisfied whereas the energy consumption is minimized. First, we formulate the problem as a constraint optimization problem that provides an optimal solution; however, it does not scale well. Thus, we also propose heuristics which apply reservation of processors and reallocation of jobs, to compute suboptimal solution efficiently in terms of energy consumption as well as schedulability. Heuristics make use of the interplay of task-level reliability target, reliability of replicas, number of replicas, reliability of tasks, and energy consumption. We perform an experimental study on the test cases generated by extending UUnisort algorithm [1] and observe the effect of various simulation parameters on energy consumption and schedulability.","1558-2183","","10.1109/TPDS.2019.2950251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8886607","Real-time systems;scheduling;non-preemptive;energy consumption;reliability","Reliability;Task analysis;Energy consumption;Real-time systems;Program processors;Processor scheduling;Timing","fault tolerant computing;multiprocessing systems;optimisation;parallel processing;power aware computing;processor scheduling;real-time systems;resource allocation","transient faults;active replication based framework;non-preemptive periodic real-time tasks;task-level reliability;heterogeneous multiprocessor system;reliability aware energy optimized scheduling;schedulability;constraint optimization problem;nonpreemptive heterogeneous environment;nonpreemptive scheduling;energy consumption minimization","",16.0,"",41.0,"IEEE","30 Oct 2019","","","IEEE","IEEE Journals"
"Combining Size-Based Load Balancing with Round-Robin for Scalable Low Latency","J. Anselmi","CNRS, INRIA Rhone Alpes (Team: POLARIS), Grenoble INP, LIG, University Grenoble Alpes, Grenoble, France","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,886,896,"When dispatching jobs to parallel servers, or queues, the highly scalable round-robin (RR) scheme reduces the variance of interarrival times at all queues to a great extent but has no impact on the variances of service processes. Contrariwise, size-interval task assignment (SITA) routing has little impact on the variances of interarrival times but makes the service processes as deterministic as possible. In this paper, we unify both `static' approaches to design a scalable load balancing framework able to control the variances of the arrival and service processes jointly. It turns out that the resulting combination significantly improves performance and is able to drive the mean job delay to zero in the large-system limit; it is known that this property is not achieved when both approaches are considered separately. Within realistic parameters, we show that the optimal number of size intervals that partition the support of the job size distribution is small with respect to the system size. This enhances the applicability of the proposed load balancing scheme at a large scale. In fact, we find that adding a little bit of information about job sizes to a dispatcher operating under RR improves performance a lot. Under the optimal scaling of size intervals and assuming highly variable job sizes, numerical simulations indicate that the proposed algorithm is competitive with the (less scalable) join-the-shortest-workload algorithm even when the system size grows large.","1558-2183","","10.1109/TPDS.2019.2950621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8889411","Dispatching policies;size-based routing;performance;asymptotic optimality","Dispatching;Servers;Load management;Routing;Heuristic algorithms;Queueing analysis;Task analysis","queueing theory;resource allocation;telecommunication network routing","join-the-shortest-workload;numerical simulations;SITA routing;size-interval task assignment routing;parallel servers;variable job;job size distribution;large-system limit;mean job delay;service processes;interarrival times;round-robin scheme;dispatching jobs;combining size-based load balancing","",5.0,"",30.0,"IEEE","31 Oct 2019","","","IEEE","IEEE Journals"
"HRHS: A High-Performance Real-Time Hardware Scheduler","D. Derafshi; A. Norollah; M. Khosroanjam; H. Beitollahi","Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran; Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran; Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran; Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,897,908,"This article represents an on-line time-predictable distributed hardware scheduler solution, suitable for many-core systems. We have partitioned the Main scheduler into uniform Partial schedulers to achieve a significant gain in term of performance and scalability, while software scheduling solutions impose excessive delays (in order of thousands of clock cycles) to a system. Although we have considered the implementation of the Earliest Deadline First (EDF) algorithm for each Partial scheduler, one can use customized scheduling policies, as needed. Designers can also modify different parts of our proposed architecture to obtain more suitable hardware for their design. HRHS outperforms conventional schedulers, in terms of resource utilization (LUT, register), delay and energy consumption by 36.83, 22.93, 46.36 and 59.26 percent on average, respectively. It also overpowers clustering solutions by circumventing their intrinsic off-line characteristics. The presented designs are also implemented in ASIC with 45-nanometer technology, in which the HRHS design excels in power, area and critical path length by 49.33, 50.67, and 53.33 percent on average, respectively, over other designs implemented in this article.","1558-2183","","10.1109/TPDS.2019.2952136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894365","FPGA;hardware accelerator;hardware scheduler;hard real-time scheduling;many-core;multi-core;real-time system","Task analysis;Hardware;Scheduling;Real-time systems;Computer architecture;Software;Processor scheduling","application specific integrated circuits;integrated circuit design;multiprocessing systems;processor scheduling","critical path length;ASIC;off-line characteristics;clustering solutions;energy consumption;resource utilization;EDF algorithm;uniform partial schedulers;on-line time-predictable distributed hardware scheduler solution;high-performance real-time hardware scheduler;HRHS design;customized scheduling policies;earliest deadline first algorithm;excessive delays;software scheduling solutions;many-core systems;size 45 nm","",14.0,"",38.0,"IEEE","8 Nov 2019","","","IEEE","IEEE Journals"
"Energy-Aware Application Placement in Mobile Edge Computing: A Stochastic Optimization Approach","H. Badri; T. Bahreini; D. Grosu; K. Yang","Department of Industrial & Systems Engineering, Wayne State University, Detroit, USA; Department of Computer Science, Wayne State University, Detroit, USA; Department of Computer Science, Wayne State University, Detroit, USA; Department of Industrial & Systems Engineering, Wayne State University, Detroit, USA","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,909,922,"The Quality of Service (QoS) in Mobile Edge Computing (MEC) systems is significantly dependent on the application offloading and placement decisions. Due to the movement of users in MEC networks, an optimal application placement might turn into the least efficient placement in few minutes. Thus, it is crucial to take the dynamics of the system into account when designing application placement mechanisms. On the other hand, energy consumption of servers is a significant component of the cost of services in MEC systems and must also be considered in the design of the mechanisms. In this article, we model the problem of energy-aware application placement in edge computing systems as a multi-stage stochastic program. The objective is to maximize the QoS of the system while taking into account the limited energy budget of the edge servers. To solve the problem, we design a novel parallel Sample Average Approximation (SAA) algorithm. We conduct an extensive experimental analysis to evaluate the performance of the proposed algorithm using real-world trace data.","1558-2183","","10.1109/TPDS.2019.2950937","National Science Foundation(grant numbers:IIS-1724227); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897679","Mobile edge computing;energy-aware application placement;quality of service;multi-stage stochastic programming;sample average approximation;parallel algorithms","Servers;Quality of service;Stochastic processes;Energy consumption;Cloud computing;Optimization;Computational modeling","cloud computing;mobile computing;optimisation;power aware computing;quality of service;stochastic processes;stochastic programming","MEC networks;MEC systems;energy-aware application placement;QoS;edge servers;stochastic optimization;Mobile Edge Computing systems;parallel sample average approximation;multistage stochastic program","",51.0,"",55.0,"IEEE","13 Nov 2019","","","IEEE","IEEE Journals"
"Towards Accurate Prediction for High-Dimensional and Highly-Variable Cloud Workloads with Deep Learning","Z. Chen; J. Hu; G. Min; A. Y. Zomaya; T. El-Ghazawi","Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter; School of Computer Science, The University of Sydney, Camperdown, Australia; Department of Electrical and Computer Engineering, The George Washington University, Washington, USA","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,923,934,"Resource provisioning for cloud computing necessitates the adaptive and accurate prediction of cloud workloads. However, the existing methods cannot effectively predict the high-dimensional and highly-variable cloud workloads. This results in resource wasting and inability to satisfy service level agreements (SLAs). Since recurrent neural network (RNN) is naturally suitable for sequential data analysis, it has been recently used to tackle the problem of workload prediction. However, RNN often performs poorly on learning long-term memory dependencies, and thus cannot make the accurate prediction of workloads. To address these important challenges, we propose a deep Learning based Prediction Algorithm for cloud Workloads (L-PAW). First, a top-sparse auto-encoder (TSA) is designed to effectively extract the essential representations of workloads from the original high-dimensional workload data. Next, we integrate TSA and gated recurrent unit (GRU) block into RNN to achieve the adaptive and accurate prediction for highly-variable workloads. Using real-world workload traces from Google and Alibaba cloud data centers and the DUX-based cluster, extensive experiments are conducted to demonstrate the effectiveness and adaptability of the L-PAW for different types of workloads with various prediction lengths. Moreover, the performance results show that the L-PAW achieves superior prediction accuracy compared to the classic RNN-based and other workload prediction methods for high-dimensional and highly-variable real-world cloud workloads.","1558-2183","","10.1109/TPDS.2019.2953745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902077","Cloud computing;workload prediction;resource provisioning;sequential data analysis;deep learning","Cloud computing;Feature extraction;Recurrent neural networks;Predictive models;Data mining;Deep learning;Data centers","cloud computing;computer centres;learning (artificial intelligence);recurrent neural nets;resource allocation","DUX-based cluster;Alibaba cloud data centers;Google cloud data centers;gated recurrent unit;top-sparse auto-encoder;deep learning based prediction algorithm;cloud computing;resource provisioning;real-world cloud workloads;workload prediction methods;real-world workload;highly-variable workloads;high-dimensional workload data;sequential data analysis;recurrent neural network;resource wasting;highly-variable cloud workloads;high-dimensional cloud workloads","",63.0,"",36.0,"IEEE","15 Nov 2019","","","IEEE","IEEE Journals"
"Efficient Method for Parallel Computation of Geodesic Transformation on CPU","D. Žlaus; D. Mongus","Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia; Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,935,947,"This article introduces a fast Central Processing Unit (CPU) implementation of geodesic morphological operations using stream processing. In contrast to the current state-of-the-art, that focuses on achieving insensitivity to the filter sizes with efficient data structures, the proposed approach achieves efficient computation of long chains of elementary 3 x 3 filters using multicore and Single Instruction Multiple Data (SIMD) processing. In comparison to the related methods, up to 100 times faster computation of common geodesic operators is achieved in this way, allowing for real-time processing (with over 30 FPS) of up to 1500 filters long chains, applied on 1024 x 1024 images. In addition, the proposed approach outperformed GPGPU, and proved to be more efficient than the comparable streaming method for the computation of morphological erosions and dilations with window sizes up to 183 x 183 in the case of using char and 27 x 27 when using double data types.","1558-2183","","10.1109/TPDS.2019.2953057","Javna Agencija za Raziskovalno Dejavnost RS(grant numbers:P2-0041,J2-8176); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8907491","Geodesic operators;mathematical morphology;SIMD;parallel processing;stream processing","Field programmable gate arrays;Streaming media;Parallel processing;Pipelines;Central Processing Unit;Multicore processing;Kernel","differential geometry;mathematical morphology;microprocessor chips;multiprocessing systems;parallel processing","multicore processing;double data types;morphological erosions;streaming method;real-time processing;geodesic operators;single instruction multiple data processing;data structures;stream processing;geodesic morphological operations;central processing unit;CPU;geodesic transformation;parallel computation","",1.0,"",44.0,"IEEE","20 Nov 2019","","","IEEE","IEEE Journals"
"Online Placement and Scaling of Geo-Distributed Machine Learning Jobs via Volume-Discounting Brokerage","X. Li; R. Zhou; L. Jiao; C. Wu; Y. Deng; Z. Li","School of Computer Science, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Department of Computer and Information Science, University of Oregon, Eugene, USA; University of Hong Kong, Kowloon, Hong Kong; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,4.0,948,966,"Geo-distributed machine learning (ML) often uses large geo-dispersed data collections produced over time to train global models, without consolidating the data to a central site. In the parameter server architecture, “workers” and “parameter servers” for a geo-distributed ML job should be strategically deployed and adjusted on the fly, to allow easy access to the datasets and fast exchange of the model parameters at anytime. Despite many cloud platforms now provide volume discounts to encourage the usage of their ML resources, different geo-distributed ML jobs that run in the clouds often rent cloud resources separately and respectively, thus rarely enjoying the benefit of discounts. We study an ML broker service that aggregates geo-distributed ML jobs into cloud data centers for volume discounts via dynamic online placement and scaling of workers and parameter servers in individual jobs for long-term cost minimization. To decide the number and the placement of workers and parameter servers, we propose an efficient online algorithm which first decomposes the online problem into a series of one-shot optimization problems solvable at each individual time slot by the technique of regularization, and afterwards round the fractional decisions to the integer ones via a carefully-designed dependent rounding method. We prove a parameterized-constant competitive ratio for our online algorithm as the theoretical performance analysis, and also conduct extensive simulation studies to exhibit its close-to-offline-optimum practical performance in realistic settings.","1558-2183","","10.1109/TPDS.2019.2955935","National Natural Science Foundation of China(grant numbers:61502504); Technological Innovation Major Projects of Hubei Province(grant numbers:2017AAA125); Science and Technology Program of Wuhan City(grant numbers:2018010401011288); WHU-Xiaomi AI Lab, Hong Kong RGC GRF HKU(grant numbers:17204715,17225516,17204619,C7036-15G (CRF),C5026-18G (CRF)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913598","Geo-distributed machine learning;online placement;volume discount brokerage","Data centers;Data models;Servers;Cloud computing;Machine learning;Optimization;Minimization","cloud computing;computer centres;learning (artificial intelligence);minimisation","geo-distributed ML job;cloud platforms;ML resources;cloud resources;ML broker service;cloud data centers;dynamic online placement;online algorithm;volume-discounting brokerage;geo-distributed machine learning;geo-dispersed data collections;parameter server architecture;geo-distributed ML jobs","",2.0,"",52.0,"IEEE","26 Nov 2019","","","IEEE","IEEE Journals"
"COPA: Highly Cost-Effective Power Back-Up for Green Datacenters","Y. Yin; J. Wu; X. Zhou; L. Eeckhout; A. Qouneh; T. Li; Z. Yu","Department of Computer Science, University of Science and Technology of China, Hefei, China; Department of Computer Science, University of Science and Technology of China, Hefei, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Science, Sangfor Technologies Inc., Shenzhen, China; Ghent University, Gent, Belgium; New Western England University, Springfield, USA; Chinese Academy of Science, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Chinese Academy of Science, Shenzhen Institutes of Advanced Technology, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","16 Jan 2020",2020,31.0,4.0,967,980,"Traditional datacenters employ costly diesel generators (DG) and uninterrupted power supplies (UPS) to back up power. However, some or even all racks of a green datacenter can still be powered by renewable energy during grid power outages. This makes the utilization of the DGs and UPSs in green datacenters significantly lower than in traditional datacenters. In this paper, we propose a highly cost-effective power back-up (COPA) approach for green datacenters by leveraging the availability characteristics of renewable energy as well as grid power outages. COPA contributes three new techniques. The first technique, called least UPS capacity planning, determines the least rated power capability and runtime of the UPSs to guarantee the normal operations of a green datacenter during grid power outages. The second technique, named cooperative UPS/renewable power supply, employs UPS and renewable energy at the same time to supply power to each rack when grid power fails. The last one, dubbed renewable-energy-aware dynamic power management, controls the power consumption dynamically based on the available capacity of renewable energy and UPS. We build an experimental cluster consisting of 10 servers, and use four representative benchmarks as well as verified data about the availability characteristics of solar and wind energy to evaluate COPA. The results show that COPA reduces 47 percent and 70 percent of the power back-up cost for a solar energy powered datacenter and a wind energy powered datacenter, respectively. Moreover, COPA guarantees the application's Service Level Agreement (SLA) for at least 20 minutes (over 79 percent outages) and 56 minutes on average while enabling the back-up power to last for at least 2 hours and for 3 hours on average, which cannot be achieved by other under-provisioning power back-up approaches.","1558-2183","","10.1109/TPDS.2019.2948336","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000204); National Natural Science Foundation of China(grant numbers:61672511,61702495,61802384); China Postdoctoral Science Foundation(grant numbers:2017M622830); NSF of Guangdong province(grant numbers:2017A030310350); Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877859","Datacenter power backup;renewable energy;cooperative power distribution;dynamic power management","Uninterruptible power systems;Renewable energy sources;Power system reliability;Green products;Power supplies;Power system management;Switches","computer centres;contracts;green computing;power aware computing;power consumption;power grids;renewable energy sources;solar power;uninterruptible power supplies;wind power","application service level agreement;solar energy powered datacenter;wind energy;cooperative UPS-renewable power supply;least rated power capability;UPS capacity planning;highly cost-effective power back-up approach;diesel generators;wind energy powered datacenter;power consumption;dubbed renewable-energy-aware dynamic power management;rated power capability;grid power outages;renewable energy;uninterrupted power supplies;green datacenter;COPA;time 3.0 hour;time 56.0 min","",5.0,"",61.0,"IEEE","21 Oct 2019","","","IEEE","IEEE Journals"
"Towards Power Efficient High Performance Packet I/O","X. Li; W. Cheng; T. Zhang; F. Ren; B. Yang","Xi'an Research Institute of Hi-Tech, Xi'an, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Xi'an Research Institute of Hi-Tech, Xi'an, China","IEEE Transactions on Parallel and Distributed Systems","10 Jan 2020",2020,31.0,4.0,981,996,"Recently, high performance packet I/O frameworks continue to flourish for their ability to process packets from high-speed links. To achieve high throughput and low latency, high performance packet I/O frameworks usually employ busy polling. As busy polling will burn all CPU cycles even if there's no packet to process, these frameworks are quite power inefficient. However, exploiting power management techniques such as DVFS and LPI in the frameworks is challenging, because neither the OS nor the frameworks can provide information (e.g., actual CPU utilization, available idle period, or the target frequency) required by these techniques. In this article, we establish a model that can formulate the packet processing flow of high performance packet I/O to help and address the above challenges. From the model, we can deduce the information needed for power management techniques, and gain the insights to balance the power and latency. After suggesting to use pause instruction to reduce CPU power within short idle period, we propose two approaches to conduct power conservation for high performance packet I/O: one with the aid of traffic information and the other without. Experiments with Intel DPDK show that both approaches can achieve significant power reduction with little latency increase.","1558-2183","","10.1109/TPDS.2019.2957746","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1700103); National Natural Science Foundation of China(grant numbers:61872208); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924615","High performance packet I/O;power consumption;latency;busy polling;DVFS;pause instruction","Power system management;Power demand;Kernel;Batch production systems;Analytical models;Data centers;Load modeling","low-power electronics;multiprocessing systems;power aware computing","high performance packet;high-speed links;power management techniques;packet processing flow","",4.0,"",35.0,"IEEE","5 Dec 2019","","","IEEE","IEEE Journals"
"Toward Designing Cost-Optimal Policies to Utilize IaaS Clouds with Online Learning","X. Wu; P. Loiseau; E. Hyytiä","Fondazione Bruno Kessler, Povo, Italy; MPI-SWS, Saarbrücken, Germany; University of Iceland, Reykjavik, Iceland","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,501,514,"Many businesses possess a small infrastructure that they can use for their computing tasks, but also often buy extra computing resources from clouds. Cloud vendors such as Amazon EC2 offer two types of purchase options: on-demand and spot instances. As tenants have limited budgets to satisfy their computing needs, it is crucial for them to determine how to purchase different options and utilize them (in addition to possible self-owned instances) in a cost-effective manner while respecting their response-time targets. In this paper, we propose a framework to design policies to allocate self-owned, on-demand and spot instances to arriving jobs. In particular, we propose a near-optimal policy to determine the number of self-owned instances and an optimal policy to determine the number of on-demand instances to buy and the number of spot instances to bid for at each time unit. Our policies rely on a small number of parameters and we use an online learning technique to infer their optimal values. Through numerical simulations, we show the effectiveness of our proposed policies, in particular that they achieve a cost reduction of up to 64.51 percent when spot and on-demand instances are considered and of up to 43.74 percent when self-owned instances are considered, compared to previously proposed or intuitive policies.","1558-2183","","10.1109/TPDS.2019.2935199","French National Research Agency; American Friends of the Alexander von Humboldt Foundation; European Union's Horizon 2020 research and innovation programme(grant numbers:754514); Academy of Finland(grant numbers:296206); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821399","On-demand instances;spot instances;cost efficiency;online learning","Task analysis;Cloud computing;Resource management;Pricing;Parallel processing;Rendering (computer graphics);Cleaning","cloud computing;cost reduction;learning (artificial intelligence);retail data processing","cost-optimal policies;cloud vendors;IaaS clouds;intuitive policies;cost reduction;online learning technique;on-demand instances;near-optimal policy;response-time targets;self-owned instances;spot instances;purchase options;Amazon EC2 offer two types;efficiency 43.74 percent;efficiency 64.51 percent","",5.0,"",26.0,"IEEE","30 Aug 2019","","","IEEE","IEEE Journals"
"A Game-Theoretical Approach for User Allocation in Edge Computing Environment","Q. He; G. Cui; X. Zhang; F. Chen; S. Deng; H. Jin; Y. Li; Y. Yang","School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; University of Auckland, Auckland, New Zealand; School of Information Technology, Deakin University, Geelong, Australia; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technolgoy, HuaZhong University of Science and Technology, Wuhan Shi, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing Shi, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,515,529,"Edge Computing provides mobile and Internet-of-Things (IoT) app vendors with a new distributed computing paradigm which allows an app vendor to deploy its app at hired edge servers distributed near app users at the edge of the cloud. This way, app users can be allocated to hired edge servers nearby to minimize network latency and energy consumption. A cost-effective edge user allocation (EUA) requires maximum app users to be served with minimum overall system cost. Finding a centralized optimal solution to this EUA problem is NP-hard. Thus, we propose EUAGame, a game-theoretic approach that formulates the EUA problem as a potential game. We analyze the game and show that it admits a Nash equilibrium. Then, we design a novel decentralized algorithm for finding a Nash equilibrium in the game as a solution to the EUA problem. The performance of this algorithm is theoretically analyzed and experimentally evaluated. The results show that the EUA problem can be solved effectively and efficiently.","1558-2183","","10.1109/TPDS.2019.2938944","Australian Research Council(grant numbers:DP170101932,DP180100212); National Natural Science Foundation of China(grant numbers:61772461); Natural Science Foundation of Zhejiang Province(grant numbers:LR18F020003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823046","Edge user allocation;edge server;cost-effectiveness;pay-as-you-go;game theory;Nash equilibrium;multi-tenancy;edge computing","Servers;Games;Edge computing;Resource management;Nash equilibrium;Cloud computing;Bandwidth","cloud computing;game theory;Internet of Things;mobile computing;optimisation","Internet-of-Things app vendors;distributed computing paradigm;app vendor;hired edge servers;cost-effective edge user allocation;maximum app users;minimum overall system cost;centralized optimal solution;EUA problem;game-theoretic approach;potential game;Nash equilibrium;game-theoretical approach;edge Computing environment","",162.0,"",43.0,"IEEE","3 Sep 2019","","","IEEE","IEEE Journals"
"cuPC: CUDA-Based Parallel PC Algorithm for Causal Structure Learning on GPU","B. Zarebavani; F. Jafarinejad; M. Hashemi; S. Salehkaleybar","Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,530,542,"The main goal in many fields in the empirical sciences is to discover causal relationships among a set of variables from observational data. PC algorithm is one of the promising solutions to learn underlying causal structure by performing a number of conditional independence tests. In this paper, we propose a novel GPU-based parallel algorithm, called cuPC, to execute an order-independent version of PC. The proposed solution has two variants, cuPC-E and cuPC-S, which parallelize PC in two different ways for multivariate normal distribution. Experimental results show the scalability of the proposed algorithms with respect to the number of variables, the number of samples, and different graph densities. For instance, in one of the most challenging datasets, the runtime is reduced from more than 11 hours to about 4 seconds. On average, cuPC-E and cuPC-S achieve 500X and 1300X speedup, respectively, compared to serial implementation on CPU.","1558-2183","","10.1109/TPDS.2019.2939126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823064","Bayesian networks;causal discovery;CUDA;GPU;machine learning;parallel processing;PC algorithm","Markov processes;Graphics processing units;Bayes methods;Graphical models;Parallel algorithms;Scalability","belief networks;coprocessors;graph theory;graphics processing units;learning (artificial intelligence);normal distribution;parallel architectures","CUDA-based parallel PC algorithm;causal structure learning;empirical sciences;causal relationships;observational data;causal structure;conditional independence tests;GPU-based parallel algorithm;called cuPC;order-independent version;cuPC-E;multivariate normal distribution;graph densities","",13.0,"",43.0,"IEEE","3 Sep 2019","","","IEEE","IEEE Journals"
"A Comment on Privacy-Preserving Scalar Product Protocols as Proposed in “SPOC”","T. Schneider; A. Treiber","Cryptography and Privacy Engineering Group (ENCRYPTO), TU Darmstadt, Darmstadt, Germany; Cryptography and Privacy Engineering Group (ENCRYPTO), TU Darmstadt, Darmstadt, Germany","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,543,546,"Privacy-preserving scalar product (PPSP) protocols are an important building block for secure computation tasks in various applications. Lu et al. (TPDS'13) introduced a PPSP protocol that does not rely on cryptographic assumptions and that is used in a wide range of publications to date. In this comment paper, we show that Lu et al.'s protocol is insecure and should not be used. We describe specific attacks against it and, using impossibility results of Impagliazzo and Rudich (STOC'89), show that it is inherently insecure and cannot be fixed without relying on at least some cryptographic assumptions.","1558-2183","","10.1109/TPDS.2019.2939313","DFG; BMBF; HMWK; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823041","Privacy-preserving scalar product protocols;secure computation;oblivious transfer","Protocols;Privacy;Public key cryptography;Task analysis;Encryption","computational complexity;cryptographic protocols;data privacy","privacy-preserving scalar product protocols;secure computation tasks;PPSP protocol;cryptographic assumptions","",7.0,"",27.0,"IEEE","3 Sep 2019","","","IEEE","IEEE Journals"
"Fault-Tolerant Routing Mechanism in 3D Optical Network-on-Chip Based on Node Reuse","P. Guo; W. Hou; L. Guo; W. Sun; C. Liu; H. Bao; L. H. K. Duong; W. Liu","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,547,564,"The three-dimensional Network-on-Chips (3D NoCs) has become a mature multi-core interconnection architecture in recent years. However, the traditional electrical lines have very limited bandwidth and high energy consumption, making the photonic interconnection promising for future 3D Optical NoCs (ONoCs). Since existing solutions cannot well guarantee the fault-tolerant ability of 3D ONoCs, in this paper, we propose a reliable optical router (OR) structure which sacrifices less redundancy to obtain more restore paths. Moreover, by using our fault-tolerant routing algorithm, the restore path can be found inside the disabled OR under the deadlock-free condition, i.e., fault-node reuse. Experimental results show that the proposed approach outperforms the previous related works by maximum 81.1 percent and 33.0 percent on average for throughput performance under different synthetic and real traffic patterns. It can improve the system average optical signal to noise ratio (OSNR) performance by maximum 26.92 percent and 12.57 percent on average, and it can improve the average energy consumption performance by 0.3 percent to 15.2 percent under different topology types/sizes, failure rates, OR structures, and payload packet sizes.","1558-2183","","10.1109/TPDS.2019.2939240","National Natural Science Foundation of China(grant numbers:61501104,61775033,61771120,61801105); Fundamental Research Funds for the Central Universities(grant numbers:N161604004,N161608001,N171602002); China Postdoctoral Science Foundation(grant numbers:2018T110210); Nanjing University(grant numbers:KFKT2018B04); Nanyang Technological University(grant numbers:NAP M4082282,SUG M4082087); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823063","3D optical network-on-chip;fault-tolerant routing mechanism;node reuse","Three-dimensional displays;Fault tolerance;Fault tolerant systems;Optical interconnections;Routing;Silicon;Photonics","fault tolerant computing;integrated circuit reliability;integrated optoelectronics;multiprocessor interconnection networks;network routing;network-on-chip;optical interconnections;performance evaluation;power aware computing;three-dimensional integrated circuits","optical network-on-chip;three-dimensional network-on-chips;photonic interconnection;ONoC;reliable optical router;restore path;fault-tolerant routing algorithm;deadlock-free condition;fault-node reuse;throughput performance;average energy consumption performance improvement;average optical signal to noise ratio performance improvement;3D optical NoC;multicore interconnection architecture","",31.0,"",60.0,"IEEE","3 Sep 2019","","","IEEE","IEEE Journals"
"The Impact of Event Processing Flow on Asynchronous Server Efficiency","S. Zhang; Q. Wang; Y. Kanemasa; H. Shan; L. Hu","Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, USA; Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, USA; Software Laboratory, FUJITSU LABORATORIES LTD, Kawasaki, Japan; JD.com American Technologies Corporation, Mountain View, USA; Computing and Information Sciences, Florida International University, Miami, USA","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,565,579,"Asynchronous event-driven server architecture has been considered as a superior alternative to the thread-based counterpart due to reduced multithreading overhead. In this paper, we conduct empirical research on the efficiency of asynchronous Internet servers, showing that an asynchronous server may perform significantly worse than a thread-based one due to two design deficiencies. The first one is the widely adopted one-event-one-handler event processing model in current asynchronous Internet servers, which could generate frequent unnecessary context switches between event handlers, leading to significant CPU overhead of the server. The second one is a write-spin problem (i.e., repeatedly making unnecessary I/O system calls) in asynchronous servers due to some specific runtime workload and network conditions (e.g., large response size and non-trivial network latency). To address these two design deficiencies, we present a hybrid solution by exploiting the merits of different asynchronous architectures so that the server is able to adapt to dynamic runtime workload and network conditions in the cloud. Concretely, our hybrid solution applies a lightweight runtime request checking and seeks for the most efficient path to process each request from clients. Our results show that the hybrid solution can achieve from 10 to 90 percent higher throughput than all the other types of servers under the various realistic workload and network conditions in the cloud.","1558-2183","","10.1109/TPDS.2019.2938500","National Science Foundation(grant numbers:1566443); Louisiana Board of Regents(grant numbers:LEQSF(2015-18)-RD-A-11); Fujitsu; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825817","Asynchronous;event-driven;thread-based;internet servers;efficiency","Servers;Instruction sets;Connectors;Runtime;Internet;Context;Message systems","file servers;input-output programs;Internet;microprocessor chips","realistic workload;network conditions;event processing flow;asynchronous server efficiency;asynchronous event-driven server architecture;thread-based counterpart;reduced multithreading overhead;design deficiencies;one-event-one-handler event processing model;current asynchronous Internet servers;event handlers;CPU overhead;specific runtime workload;dynamic runtime workload;efficiency 90.0 percent","","","",61.0,"IEEE","5 Sep 2019","","","IEEE","IEEE Journals"
"FeatherCNN: Fast Inference Computation with TensorGEMM on ARM Architectures","H. Lan; J. Meng; C. Hundt; B. Schmidt; M. Deng; X. Wang; W. Liu; Y. Qiao; S. Feng","Tencent AI Lab, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Parallel and Distributed Architectures Group, Institute of Computer Science, Johannes Gutenberg University Mainz, Mainz, Germany; Parallel and Distributed Architectures Group, Institute of Computer Science, Johannes Gutenberg University Mainz, Mainz, Germany; Tencent AI Lab, Shenzhen, China; Tencent AI Lab, Shenzhen, China; Shandong University, Jinan, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,580,594,"Deep Learning is ubiquitous in a wide field of applications ranging from research to industry. In comparison to timeconsuming iterative training of convolutional neural networks (CNNs), inference is a relatively lightweight operation making it amenable to execution on mobile devices. Nevertheless, lower latency and higher computation efficiency are crucial to allow for complex models and prolonged battery life. Addressing the aforementioned challenges, we propose FeatherCNN- a fast inference library for ARM CPUs - targeting the performance ceiling of mobile devices. FeatherCNN employs three key techniques: 1) A highly efficient TensorGEMM (generalized matrix multiplication) routine is applied to accelerate Winograd convolution on ARM CPUs, 2) General layer optimization based on custom high performance kernels improves both the computational efficiency and locality of memory access patterns for non-Winograd layers. 3) The framework design emphasizes joint layer-wise optimization using layer fusion to remove redundant calculations and memory movements. Performance evaluation reveals that FeatherCNN significantly outperforms state-ofthe-art libraries. A forward propagation pass of VGG-16 on a 64-core ARM server is 48, 14, and 12 times faster than Caffe using OpenBLAS, Caffe2 using Eigen, and NNPACK, respectively. In addition, FeatherCNN is 3.19 times faster than the recently released TensorFlow Lite library on an iPhone 7 plus. In terms of GEMM performance, FeatherCNN achieves 14.8 and 39.0 percent higher performance than Apple's Accelerate framework on an iPhone 7 plus and Eigen on a Samsung Galaxy S8, respectively. The source code of FeatherCNN library is publicly available at https://github.com/tencent/feathercnn.","1558-2183","","10.1109/TPDS.2019.2939785","National Natural Science Foundation of China(grant numbers:61702494,U1813203); National High Technology Research and Development Program of China(grant numbers:2015AA020109,2016YFB0201305); Shenzhen Fundamental Research Fund(grant numbers:JCYJ20160331190123578,GGFW2017073114031767); Shenzhen Discipline Construction Project for Urban Computing and Data Intelligence; Youth Innovation Promotion Association of the Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8826372","Convolutional neural networks;ARM architecture;inference computation;tensorGEMM","Convolution;Performance evaluation;Optimization;Computer architecture;Acceleration;Mobile handsets;Libraries","convolutional neural nets;iterative methods;learning (artificial intelligence);matrix multiplication;microprocessor chips","fast inference computation;ARM architectures;Deep Learning;timeconsuming iterative training;convolutional neural networks;relatively lightweight operation;mobile devices;higher computation efficiency;prolonged battery life;fast inference library;ARM CPUs;performance ceiling;Winograd convolution;General layer optimization;custom high performance kernels;computational efficiency;nonWinograd layers;joint layer-wise optimization;layer fusion;performance evaluation;state-ofthe-art libraries;64-core ARM server;GEMM performance;FeatherCNN library;TensorFlow Lite library;efficiency 39.0 percent","",13.0,"",45.0,"IEEE","6 Sep 2019","","","IEEE","IEEE Journals"
"cuTensor-Tubal: Efficient Primitives for Tubal-Rank Tensor Learning Operations on GPUs","T. Zhang; X. -Y. Liu; X. Wang; A. Walid","School of Computer Engineering and Science, Shanghai University, Shanghai, China; Department of Electrical Engineering, Columbia University, New York, USA; Department of Electrical Engineering, Columbia University, New York, USA; Nokia-Bell Labs, Murray Hill, USA","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,595,610,"Tensors are the cornerstone data structures in high-performance computing, big data analysis and machine learning. However, tensor computations are compute-intensive and the running time increases rapidly with the tensor size. Therefore, designing high-performance primitives on parallel architectures such as GPUs is critical for the efficiency of ever growing data processing demands. Existing GPU basic linear algebra subroutines (BLAS) libraries (e.g., NVIDIA cuBLAS) do not provide tensor primitives. Researchers have to implement and optimize their own tensor algorithms in a case-by-case manner, which is inefficient and error-prone. In this paper, we develop the cuTensor-tubal library of seven key primitives for the tubal-rank tensor model on GPUs: t-FFT, inverse t-FFT, t-product, t-SVD, t-QR, t-inverse, and t-normalization. cuTensor-tubal adopts a frequency domain computation scheme to expose the separability in the frequency domain, then maps the tube-wise and slice-wise parallelisms onto the single instruction multiple thread (SIMT) GPU architecture. To achieve good performance, we optimize the data transfer, memory accesses, and design the batched and streamed parallelization schemes for tensor operations with data-independent and data-dependent computation patterns, respectively. In the evaluations oft-product, t-SVD, t-QR, t-inverse and t-normalization, cuTensor-tubal achieves maximum 16.91x, 27.03x, 38.97x, 22.36x,15.43x speedups respectively over the CPU implementations running on dual 10-core Xeon CPUs. Two applications, namely, t-SVD-based video compression and low-tubal-rank tensor completion, are tested using our library and achieve maximum 9.80x and 269.26x speedups over multi-core CPU implementations.","1558-2183","","10.1109/TPDS.2019.2940192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827946","Low-tubal-rank tensor decomposition;GPU;cuTensor-tubal library;t-SVD;tensor completion","Graphics processing units;Libraries;Frequency-domain analysis;Matrix decomposition;Computational modeling;Computer architecture","coprocessors;data analysis;data compression;data structures;fast Fourier transforms;graphics processing units;learning (artificial intelligence);mathematics computing;matrix algebra;multiprocessing systems;optimisation;parallel algorithms;parallel architectures;singular value decomposition;tensors;video coding","t-SVD;t-QR;cuTensor-tubal achieves;low-tubal-rank tensor completion;tubal-rank tensor learning operations;cornerstone data structures;high-performance computing;big data analysis;machine learning;tensor computations;running time;tensor size;high-performance primitives;parallel architectures;data processing demands;GPU basic linear algebra subroutines libraries;tensor algorithms;cuTensor-tubal library;tubal-rank tensor model;inverse t-FFT;frequency domain computation scheme;slice-wise parallelisms;single instruction multiple thread GPU architecture;data transfer;parallelization schemes;tensor operations;data-dependent computation patterns;data-independent computation patterns","",11.0,"",51.0,"IEEE","10 Sep 2019","","","IEEE","IEEE Journals"
"Achieving Flexible Global Reconfiguration in NoCs Using Reconfigurable Rings","L. Wang; L. Liu; J. Han; X. Wang; S. Yin; S. Wei","Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada; School of Software Engineering, South China University of Technology, Guangzhou, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,611,622,"The communication behaviors in NoCs of chip-multiprocessors exhibit great spatial and temporal variations, which introduce significant challenges for the reconfiguration in NoCs. Existing reconfigurable NoCs are still far from ideal reconfiguration scenarios, in which globally reconfigurable interconnects can be immediately reconfigured to provide bandwidths on demand for varying traffic flows. In this paper, we propose a hybrid NoC architecture that globally reconfigures the ring-based interconnect to adapt to the varying traffic flows with a high flexibility. The ring-based interconnect has the following advantages. First, it includes horizontal rings and vertical rings, which can be dynamically combined or split to provide low-latency channels for heavy traffic flows. Second, each combined ring connects a number of nodes, thereby improving both the utilization of each ring and the probability to reuse previous reconfigurable interconnects. Finally, the reconfiguration algorithm has a linear-time complexity and can be implemented using a low-overhead hardware design, making it possible to achieve a fast reconfiguration in NoCs. The experimental results show that compared to recent reconfigurable NoCs, the proposed NoC architecture can greatly improve the saturation throughput for synthetic traffic patterns, and reduce the packet latency over 40 percent for realistic benchmarks without incurring significant area and power overhead.","1558-2183","","10.1109/TPDS.2019.2940190","Ministry of Science and Technology of the People's Republic of China(grant numbers:2018ZX01028201); National Natural Science Foundation of China(grant numbers:61672317,61834002); Natural Science Foundation of Guangdong Province(grant numbers:2018A030313166); Research Grant of Guangdong Province(grant numbers:2017A050501003); Pearl River S and T Nova Program of Guangzhou(grant numbers:201806010038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827943","Reconfiguration;NoCs;rings;traffic flows","Mesh networks;Bandwidth;Complexity theory;Machine learning algorithms;Integrated circuit interconnections;Scalability;Heuristic algorithms","integrated circuit design;integrated circuit interconnections;microprocessor chips;network-on-chip;reconfigurable architectures","horizontal rings;vertical rings;reconfigurable interconnects;reconfigurable NoC;synthetic traffic patterns;flexible global reconfiguration;reconfigurable rings;spatial variations;hybrid NoC architecture;ring-based interconnect;chip multiprocessors;linear-time complexity;low-overhead hardware design;packet latency","",7.0,"",29.0,"IEEE","10 Sep 2019","","","IEEE","IEEE Journals"
"Simultaneous Management of Peak-Power and Reliability in Heterogeneous Multicore Embedded Systems","M. Ansari; J. Saber-Latibari; M. Pasandideh; A. Ejlali","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,623,633,"Analysis of reliability, power, and performance at hardware and software levels due to heterogeneity is a crucial requirement for heterogeneous multicore embedded systems. Escalating power densities have led to thermal issues for heterogeneous multicore embedded systems. This paper proposes a peak-power-aware reliability management scheme to meet power constraints through distributing power density on the whole chip such that reliability targets are satisfied. In this paper, we consider peak power consumption as a system-level power constraint to prevent system failure. To balance the power consumption, we also employ a Dynamic Frequency Scaling (DFS) method to further reduce peak power consumption and satisfy thermal constraints on the chip. We illustrate the benefits of our scheme by comparing it with state-of-the-art schemes, resulting in average in 26.5 percent less peak power consumption (up to 54.3 percent).","1558-2183","","10.1109/TPDS.2019.2940631","Sharif University of Technology(grant numbers:G930827); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827950","Power consumption;reliability;embedded systems;dynamic frequency scaling;thermal safe power;thermal design power","Multicore processing;Reliability;Task analysis;Power demand;Embedded systems;Real-time systems;Timing","embedded systems;integrated circuit reliability;microprocessor chips;multiprocessing systems;power aware computing;power consumption","power constraints;power density;reliability targets;peak power consumption;system-level power constraint;system failure;heterogeneous multicore embedded systems;peak-power-aware reliability management scheme;efficiency 26.5 percent;efficiency 54.3 percent","",13.0,"",44.0,"IEEE","10 Sep 2019","","","IEEE","IEEE Journals"
"Enabling Encrypted Boolean Queries in Geographically Distributed Databases","X. Yuan; X. Yuan; Y. Zhang; B. Li; C. Wang","School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA; Faculty of Information Technology, Monash University, Clayton, Australia; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Computer Science, City University of Hong Kong, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,634,646,"The persistent growth of big data applications has being raising new challenges in managing large volumes of datasets with high scalability, confidentiality protection, and flexible types of search queries. In this paper, we propose a secure design to disassemble the private dataset with the aim to store them across geographically distributed servers while supporting secure multi-client Boolean queries. In this design, the data owner encrypts the private database with the searchable index attributes. The encrypted dataset will be disassembled and distributed evenly across multiple servers by leveraging the property of a distributed index framework. By constructing an encryption structure, generating search tokens, and enabling parallel query, we show how the proposed design performs the secure while efficient Boolean search. These queries are not only limited to those initiated by the data owner but also can be extended to support multiple authorized clients, where each client is allowed to access a necessary part of the private database. In this stage, we advocate a non-interactive authorization scheme where data owner is not required to stay online to process the query request. Moreover, the query operation can be executed in parallel, which significantly improves the search efficiency. We formally characterize the leakage profile, which allow us to follow the existing security analysis method to demonstrate that our system can guarantee data confidentiality and query privacy. To validate our protocol, we implement a system prototype and evaluate the efficiency of our construction. Through experimental results, we demonstrate the effectiveness of our protocol in terms of data outsourcing time and Boolean query time.","1558-2183","","10.1109/TPDS.2019.2940945","Louisiana Board of Regents(grant numbers:LEQSF(2018-21)-RD-A-24); Data61-Monash Collaborative Research Project; Research Grants Council of Hong Kong(grant numbers:CityU 11212717,CityU C1008-16G); National Natural Science Foundation of China(grant numbers:61572412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834804","Searchable symmetric encryption;multi-client data access;key-value stores;Boolean query","Servers;Distributed databases;Cloud computing;Indexes;Encryption;Data privacy","authorisation;cloud computing;cryptography;data privacy;distributed databases;outsourcing;query processing","geographically distributed databases;confidentiality protection;flexible types;search queries;secure design;private dataset;geographically distributed servers;secure multiclient Boolean queries;data owner;private database;searchable index attributes;encrypted dataset;multiple servers;distributed index framework;encryption structure;search tokens;parallel query;Boolean search;multiple authorized clients;noninteractive authorization scheme;query request;query operation;security analysis method;data confidentiality;query privacy;data outsourcing time;Boolean query time;Big Data applications","",6.0,"",33.0,"IEEE","12 Sep 2019","","","IEEE","IEEE Journals"
"A Novel Low Cost Interconnection Architecture Based on the Generalized Hypercube","G. Wang; C. -K. Lin; J. Fan; B. Cheng; X. Jia","School of Computer Science and Technology, Soochow University, Suzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; Department of Computer Science, City University of Hong Kong, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,647,662,"The generalized hypercube (GH) is one key interconnection network with excellent topological properties. It contains many other interconnection topologies, such as the hypercube network, the complete graph, the mesh network, and the k-ary n-cube network. It can also be used to construct some data center networks, such as HyperX, BCube, FBFLY, and SWCube. However, the construction cost of GH is high since it contains too many links. In this paper, we propose a novel low cost interconnection architecture called the exchanged generalized hypercube (EGH). We study the properties of EGH, such as the number of edges, the degree of vertices, connectivity, diameter, and diagnosability. Then, we give a routing algorithm to find the shortest path between any two distinct vertices of EGH. Furthermore, we design an algorithm to give disjoint paths between any two distinct vertices of EGH. In addition, we propose two local diagnosis algorithms: LDTEGH and LDWBEGH in EGH under PMC model and MM model, respectively. Simulation results demonstrate that even if the proportion of faulty vertices in EGH is up to 25 percent, the probability that these two diagnosis algorithms can successfully determine the status of vertices is more than 90 percent. As far as the number of edges is concerned, the analysis shows that the construction cost of EGH is much less than that of GH. We could regard this work as the basis for proposing future new high performance topologies.","1558-2183","","10.1109/TPDS.2019.2941207","National Natural Science Foundation of China(grant numbers:61572337,61872257); Natural Science Research of Jiangsu Higher Education Institutions of China(grant numbers:18KJA520009); Application Foundation Research of Suzhou of China(grant numbers:SYG201653); Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8836112","Exchanged generalized hypercube;low cost;local diagnosis;disjoint path;routing","Hypercubes;Routing;Network topology;Topology;Program processors;Fault tolerance","computer centres;fault diagnosis;graph theory;hypercube networks","novel low cost interconnection architecture;GH;key interconnection network;interconnection topologies;hypercube network;mesh network;k-ary n-cube network;data center networks;construction cost;exchanged generalized hypercube;EGH;routing algorithm;distinct vertices;local diagnosis algorithms;faulty vertices;future new high performance topologies;efficiency 25.0 percent;efficiency 90.0 percent","",15.0,"",43.0,"IEEE","13 Sep 2019","","","IEEE","IEEE Journals"
"Random Priority-Based Thrashing Control for Distributed Shared Memory","Y. -W. Ci; M. R. Lyu; Z. Zhang; D. -C. Zuo; X. -Z. Yang","Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Sciences and Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,663,674,"Shared memory is widely used for inter-process communication. The shared memory abstraction allows computation to be decoupled from communication, which offers benefits, including portability and ease of programming. To enable shared memory access by processes that are on different machines, distributed shared memory (DSM) can be employed. However, DSM systems can suffer from thrashing: while different processes update certain hot data items, the largest amount of effort is spent on data synchronization, and little progress is made by each process. To avoid interference between processes during data updating while providing shared memory at page granularity, more time is reserved for a writer to hold a page in a traditional manner. In this paper, we report on complex thrashing, which can explain why extending the time of holding a page might not be sufficient to control thrashing. To increase the throughput, we propose a thrashing control mechanism that allows each process to update a set of pages during a period of time, where the pages compose a logical area. Because of the isolation of areas, updates on different areas can be performed concurrently. To allow the areas to be fairly well used, each process is assigned with a random priority for thrashing control. The thrashing control mechanism is implemented on a Linux-based DSM system. Performance results show that the execution time of the applications that are apt to cause system thrashing can be significantly reduced by our approach.","1558-2183","","10.1109/TPDS.2019.2942302","Society of Hong Kong Scholars; Hong Kong Special Administrative Region, China(grant numbers:CUHK 14210717); Microsoft Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844812","Distributed shared memory;inter process communication;thrashing control","Synchronization;Process control;Memory management;Programming;Distributed databases;Frequency synchronization;Message passing","distributed shared memory systems;Linux","random priority-based thrashing control;distributed shared memory;interprocess communication;shared memory abstraction;portability;shared memory access;DSM systems;hot data items;data synchronization;page granularity;complex thrashing;thrashing control mechanism;Linux-based DSM system;system thrashing","","","",33.0,"IEEE","19 Sep 2019","","","IEEE","IEEE Journals"
"HPPT-NoC: A Dark-Silicon Inspired Hierarchical TDM NoC with Efficient Power-Performance Trading","S. Hesham; D. Goehringer; M. A. Abd El Ghany","Ruhr-University Bochum, Bochum, Germany; Technische Universität Dresden, Dresden, Germany; Technische Universität Darmstadt, Darmstadt, Germany","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,675,694,"Networks-on-chip (NoCs) acquired substantial advancements as the typical solution for a modular, flexible and high performance communication infrastructure coping with the scalable Multi-/Manycores technology. However, the increasing chip complexity heading towards thousand cores, together with the approaching dark-silicon era, puts energy efficiency as an integral design key for future NoC-based multicores, where NoCs are significantly contributing to the total chip power. In this paper, we propose HPPT-NoC, a dark-silicon inspired energy-efficient hierarchical TDM NoC with online distributed setup-scheme. The proposed network makes use of the dim silicon parts of the chip to hierarchically connect quad-routers units. Normal routers operate at full-chip-frequency at high supply level, and hierarchical routers operate at half-chip-frequency and lower supply voltage with adequate synchronization. Routers follow a proposed TDM architecture that separates the datapath from the control-setup planes. This allows separate clocking and operating supplies between data and control and to keep the control-setup as a single-slot-cycle design independent of the datapath slot size. The proposed NoC architecture is evaluated versus a base NoC from the state-of-the-art in terms of performance and hardware results using Synopsys VCS and Synopsys Design Compiler for SAED90nm and SAED32nm technologies. The obtained results highlight the power-frequency-trading feature supported by the proposed hierarchical NoC through the configurable data-control clock relation and maintained over the different technology nodes. With the same power budget of the base NoC, the proposed architecture provides up to 74% setup latency enhancement, 32% increased NoC saturation load, and 21% higher success rates, offering up to 78% improved power delay product. On the other hand, with 38% power savings, the proposed NoC provides up to 37% enhanced latency and 15% higher success rates, with 72% enhanced power delay product. The proposed design consumes almost double the area of the base NoC, however with an average of 56% under-clocked (dim) silicon area operating at half to quarter the maximum chip frequency. This results in reduced power density as a main concern in the dark-silicon era down to 24% of the base NoC.","1558-2183","","10.1109/TPDS.2019.2942589","Deutsche Forschungsgemeinschaft(grant numbers:SFB/TRR 196); Alexander von Humboldt Foundation-Research Group Linkage Programme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845671","Dark-Silicon;multi-cores;networks-on-chip;power;synthesis;time-division-multiplexing","Time division multiplexing;Multicore processing;Energy efficiency;Resource management;Silicon;Routing","integrated circuit design;low-power electronics;multiprocessing systems;network routing;network-on-chip;synchronisation","networks-on-chip;energy efficiency;dark-silicon inspired energy-efficient;online distributed setup-scheme;quad-routers units;full-chip-frequency;NoC architecture;power delay product;under-clocked silicon area;maximum chip frequency;HPPT-NoC;dark-silicon inspired hierarchical TDM NoC;data-control clock;NoC-based multicores","",5.0,"",44.0,"IEEE","20 Sep 2019","","","IEEE","IEEE Journals"
"Exploring New Opportunities to Defeat Low-Rate DDoS Attack in Container-Based Cloud Environment","Z. Li; H. Jin; D. Zou; B. Yuan","Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Big Data Security Engineering Research Center, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Big Data Security Engineering Research Center, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Big Data Security Engineering Research Center, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Big Data Security Engineering Research Center, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,695,706,"DDoS attacks are rampant in cloud environments and continually evolve into more sophisticated and intelligent modalities, such as low-rate DDoS attacks. But meanwhile, the cloud environment is also developing in constant. Now container technology and microservice architecture are widely applied in cloud environment and compose container-based cloud environment. Comparing with traditional cloud environments, the container-based cloud environment is more lightweight in virtualization and more flexible in scaling service. Naturally, a question that arises is whether these new features of container-based cloud environment will bring new possibilities to defeat DDoS attacks. In this paper, we establish a mathematical model based on queueing theory to analyze the strengths and weaknesses of the container-based cloud environment in defeating low-rate DDoS attack. Based on this, we propose a dynamic DDoS mitigation strategy, which can dynamically regulate the number of container instances serving for different users and coordinate the resource allocation for these instances to maximize the quality of service. And extensive simulations and testbed-based experiments demonstrate our strategy can make the limited system resources be utilized sufficiently to maintain the quality of service acceptable and defeat DDoS attack effectively in the container-based cloud environment.","1558-2183","","10.1109/TPDS.2019.2942591","National Key Research & Development (R&D) Plan of China(grant numbers:2017YFB0802205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845616","Container;microservice;DDoS attack;mitigation;cloud computing","Computer crime;Cloud computing;Containers;Mathematical model;Computer architecture;Resource management","cloud computing;computer network security;queueing theory;resource allocation","low-rate DDoS attack;container-based cloud environment;virtualization;scaling service;queueing theory;dynamic DDoS mitigation strategy;resource allocation;quality of service;testbed-based experiments","",22.0,"",60.0,"CCBY","20 Sep 2019","","","IEEE","IEEE Journals"
"A High Throughput B+tree for SIMD Architectures","W. Zhang; Z. Yan; Y. Lin; C. Zhao; L. Peng","Shanghai Key Laboratory of Data Science, Institute for Big Data, and Shanghai Institute of Intelligent Electrontics & System, Software School, Fudan University, Shanghai, China; Shanghai Key Laboratory of Data Science, Institute for Big Data, and Shanghai Institute of Intelligent Electrontics & System, Software School, Fudan University, Shanghai, China; Shanghai Key Laboratory of Data Science, Institute for Big Data, and Shanghai Institute of Intelligent Electrontics & System, Software School, Fudan University, Shanghai, China; Shanghai Key Laboratory of Data Science, Institute for Big Data, and Shanghai Institute of Intelligent Electrontics & System, Software School, Fudan University, Shanghai, China; Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, USA","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,707,720,"B+tree is one of the most important data structures and has been widely used in different fields. With the increase of concurrent queries and data-scale in storage, designing an efficient B+tree structure has become critical. Due to abundant computation resources, SIMD architectures provide potential opportunities to achieve high query throughput for B+tree. However, prior methods cannot achieve satisfactory performance results due to low resource utilization and poor memory performance. In this paper, we first identify the gaps between B+tree and SIMD architectures. Concurrent B+tree queries involve many global memory accesses and different divergences, which mismatch with SIMD architecture features. Based on this observation, we propose Harmonia, a novel B+tree structure to bridge the gaps. In Harmonia, a B+tree structure is divided into a key region and a child region. The key region stores the nodeswith its keys in a breadth-first order. The child region is organized as a prefix-sum array, which only stores each node's first child index in the key region. Since the prefix-sum child region is small and the children's index can be retrieved through index computations, most of it can be stored in on-chip caches, which can achieve good cache locality. To make it more efficient, Harmonia also includes two optimizations: partially-sorted aggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization. Evaluations on a 28-core INTEL CPU show that Harmonia can achieve up to 207 million queries per second, which is about 1.7X faster than that of CPU-based HB+Tree, a recent state-of-the-art solution. And on a Volta TITAN V GPU, it can achieve up to 3.6 billion queries per second, which is about 3.4X faster than that of GPU-based HB+Tree.","1558-2183","","10.1109/TPDS.2019.2942918","National Natural Science Foundation of China(grant numbers:61672160); Shanghai Municipal Science and Technology Major Project(grant numbers:2018SHZDZX01); Shanghai Technology Development and Entrepreneurship Platform for Neuromorphic and AI SoC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846100","SIMD;B+tree;high-throughput","Indexes;Throughput;Resource management;Vegetation;Memory management;Data structures","cache storage;graphics processing units;parallel architectures;parallel processing;query processing;tree data structures","high throughput B+tree;SIMD architectures;data structures;concurrent queries;data-scale;B+tree structure;concurrent B+tree queries;global memory accesses;SIMD architecture features;Harmonia;prefix-sum child region;GPU-based HB+Tree;28-core INTEL CPU;CPU-based HB+Tree;Volta TITAN V GPU;on-chip caches;cache locality","",2.0,"",52.0,"IEEE","23 Sep 2019","","","IEEE","IEEE Journals"
"Online Scheduling of Task Graphs on Heterogeneous Platforms","L. -C. Canon; L. Marchal; B. Simon; F. Vivien","FEMTO-ST Institute – Université de Bourgogne Franche-Comté, Besançon, France; FEMTO-ST Institute – Université de Bourgogne Franche-Comté, Besançon, France; University of Bremen, Bibliothekstr, Germany; FEMTO-ST Institute – Université de Bourgogne Franche-Comté, Besançon, France","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,721,732,"Modern computing platforms commonly include accelerators. We target the problem of scheduling applications modeled as task graphs on hybrid platforms made of two types of resources, such as CPUs and GPUs. We consider that task graphs are uncovered dynamically, and that the scheduler has information only on the available tasks, i.e., tasks whose predecessors have all been completed. Each task can be processed by either a CPU or a GPU, and the corresponding processing times are known. Our study extends a previous 4√m/k-competitive online algorithm by Amaris et al. [1], where mis the number of CPUs and k the number of GPUs (m≥k). We prove that no online algorithm can have a competitive ratio smaller than √m/k . We also study how adding flexibility on task processing, such as task migration or spoliation, or increasing the knowledge of the scheduler by providing it with information on the task graph, influences the lower bound. We provide a (2√m/k+1)-competitive algorithm as well as a tunable combination of a system-oriented heuristic and a competitive algorithm; this combination performs well in practice and has a competitive ratio in Θ(√m/k). We also adapt all our results to the case of multiple types of processors. Finally, simulations on different sets of task graphs illustrate how the instance properties impact the performance of the studied algorithms and show that our proposed tunable algorithm performs the best among the online algorithms in almost all cases and has even performance close to an offline algorithm.","1558-2183","","10.1109/TPDS.2019.2942909","LABEX MILYON(grant numbers:ANR-10-LABX-0070); Université de Lyon(grant numbers:ANR-11-IDEX- 0007); Agence Nationale de la Recherche; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846007","Scheduling;heterogeneous computing;task graphs;online algorithms","Task analysis;Graphics processing units;Runtime;Heuristic algorithms;Schedules;Processor scheduling","competitive algorithms;computational complexity;graph theory;processor scheduling","task processing;task migration;task graph;competitive ratio;online scheduling;heterogeneous platforms;(2√m/k+1)-competitive algorithm;system-oriented heuristic","",8.0,"",24.0,"IEEE","23 Sep 2019","","","IEEE","IEEE Journals"
"An Attribute-Based Availability Model for Large Scale IaaS Clouds with CARMA","H. Lv; J. Hillston; P. Piho; H. Wang","Department of Computer Science and Technology, Harbin Engineering University, Harbin, China; University of Edinburgh, Edinburgh, United Kingdom; University of Edinburgh, Edinburgh, United Kingdom; Department of Computer Science and Technology, Harbin Engineering University, Harbin, China","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2020",2020,31.0,3.0,733,748,"High availability is one of the core properties of Infrastructure as a Service (IaaS) and ensures that users have anytime access to on-demand cloud services. However, significant variations of workflow and the presence of super-tasks, mean that heterogeneous workload can severely impact the availability of IaaS clouds. Although previous work has investigated global queues, VM deployment, and failure of PMs, two aspects are yet to be fully explored: one is the impact of task size and the other is the differing features across PMs such as the variable execution rate and capacity. To address these challenges we propose an attribute-based availability model of large scale IaaS developed in the formal modeling language CARMA. The size of tasks in our model can be a fixed integer value or follow the normal, uniform or log-normal distribution. Additionally, our model also provides an easy approach to investigating how to arrange the slack and normal resources in order to achieve availability levels. The two goals of our work are providing an analysis of the availability of IaaS and showing that the use of CARMA allows us to easily model complex phenomena that were not readily captured by other existing approaches.","1558-2183","","10.1109/TPDS.2019.2943339","National Natural Science Foundation of China(grant numbers:61402127); China Scholarship Council(grant numbers:201706685020); National Science and Technology Major Project of China(grant numbers:2016ZX03001023-005); Tianjin Key Laboratory of Advanced Networking (TANK); Tianjin University; Engineering and Physical Sciences Research Council(grant numbers:EP/L01503X/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8847381","Availability;cloud computing;formal model;CARMA;PM provisioning;super-tasks;heterogeneous workload","Task analysis;Cloud computing;Analytical models;Computational modeling;Markov processes;Load modeling;Containers","cloud computing;formal languages;log normal distribution;system monitoring;virtual machines","variable execution rate;attribute-based availability model;formal modeling language CARMA;normal log-normal distribution;uniform log-normal distribution;availability levels;model complex phenomena;scale IaaS clouds;on-demand cloud services;super-tasks;task size","",5.0,"",42.0,"IEEE","24 Sep 2019","","","IEEE","IEEE Journals"
"Memory-Efficient and Skew-Tolerant MapReduce Over MPI for Supercomputing Systems","T. Gao; Y. Guo; B. Zhang; P. Cicotti; Y. Lu; P. Balaji; M. Taufer","National University of Defense Technology, Changsha, China; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; NVIDIA, San Diego, USA; Sun Yat-sen University, Guangzhou, China; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA","IEEE Transactions on Parallel and Distributed Systems","26 Jun 2020",2020,31.0,12.0,2734,2748,"Data analytics has become an integral part of large-scale scientific computing. Among various data analytics frameworks, MapReduce has gained the most traction. Although some efforts have been made to enable efficient MapReduce for supercomputing systems, they are often limited to fairly homogeneous workloads where equal partitioning of input data across tasks results in essentially equal output or temporary data generated on each task. For workloads that are more skewed, however, current implementations can result in imbalance in memory usage and, consequently, can cause a slowdown in execution time and a loss in data scalability. To tackle this problem, we enhance a previously published memory-conscious MapReduce over MPI framework called Mimir. Our enhancements to Mimir include combiner and dynamic repartition optimizations to minimize and balance memory usage and to achieve close to optimal balance of the memory usage across processes and to reduce the execution time by up to 12 times. Experimental results show that Mimir can scale to at least 3072 processes on the Tianhe-2 supercomputer on skewed datasets.","1558-2183","","10.1109/TPDS.2019.2932066","U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:#1318445,#1318417,#1841758); National Key R&D Project in China(grant numbers:2016YFB1000302); National Natural Science Foundation of China(grant numbers:U1611261,NSFC61402503); Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); China Scholarship Council; National Science Foundation(grant numbers:ACI-1053575); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103270","Skew mitigation;load balancing;high-performance computing;data analytics;MapReduce;memory efficiency;performance and scalability","Supercomputers;Data models;Optimization;Programming;Operating systems;Aggregates;Data analysis","data analysis;mainframes;message passing;multiprocessing systems;optimisation;parallel machines","skewed datasets;Tianhe-2 supercomputer;dynamic repartition optimizations;Mimir;MPI framework;published memory-conscious MapReduce;data scalability;memory usage;temporary data;fairly homogeneous workloads;data analytics frameworks;large-scale scientific computing;supercomputing systems;skew-tolerant MapReduce;memory-efficient","",3.0,"",38.0,"CCBY","28 May 2020","","","IEEE","IEEE Journals"
"Preemptive and Low Latency Datacenter Scheduling via Lightweight Containers","W. Chen; X. Zhou; J. Rao","Department of Computer Science, University of Colorado Colorado Springs, USA; Department of Computer Science, University of Colorado Colorado Springs, USA; Department of Computer Science, University of Texas, Arlington, USA","IEEE Transactions on Parallel and Distributed Systems","25 Jun 2020",2020,31.0,12.0,2749,2762,"Datacenters are evolving to host heterogeneous workloads on shared clusters to reduce the operational cost and achieve higher resource utilization. However, it is challenging to schedule heterogeneous workloads with diverse resource requirements and QoS constraints. On one hand, latency-critical jobs need to be scheduled as soon as they are submitted to avoid any queuing delays. On the other hand, best-effort long jobs should be allowed to occupy the cluster when there are idle resources to improve cluster utilization. The challenge lies in how to minimize the queuing delays of short jobs while maximizing cluster utilization. In this article, we propose and develop BIG-C, a container-based resource management framework for data-intensive cluster computing. The key design is to leverage lightweight virtualization, a.k.a, containers, to make tasks preemptable in cluster scheduling. We devise two types of preemption strategies: immediate and graceful preemptions and show their effectiveness and tradeoffs with loosely-coupled MapReduce workloads as well as iterative, in-memory Spark workloads. Based on the mechanisms for task preemption, we further develop job-level and task-level preemptive policies as well as a preemptive fair share cluster scheduler. Our implementation on Yarn and evaluation with synthetic and production workloads show that low job latency and high resource utilization can be both attained when scheduling heterogeneous workloads on a contended cluster.","1558-2183","","10.1109/TPDS.2019.2957754","National Science Foundation(grant numbers:SHF-1816850,CNS-1422119); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924761","Datacenter scheduling;OS lightweight virtualization;preemption;multi-tenancy;heterogeneous workloads","Task analysis;Sparks;Yarn;Containers;Delays;Resource management;Processor scheduling","computer centres;data handling;parallel processing;quality of service;resource allocation;scheduling","job-level preemptive policies;task preemption;in-memory Spark workloads;MapReduce workloads;cluster scheduling;lightweight virtualization;data-intensive cluster computing;container-based resource management framework;cluster utilization;best-effort long jobs;latency-critical jobs;QoS constraints;datacenter scheduling;lightweight containers;resource utilization;preemptive fair share cluster scheduler;task-level preemptive policies","",8.0,"",40.0,"IEEE","5 Dec 2019","","","IEEE","IEEE Journals"
"Cartesian Partitioning Models for 2D and 3D Parallel SpGEMM Algorithms","G. V. Demirci; C. Aykanat","Department of Computer Engineering, Bilkent University, Ankara, Turkey; Department of Computer Engineering, Bilkent University, Ankara, Turkey","IEEE Transactions on Parallel and Distributed Systems","30 Jun 2020",2020,31.0,12.0,2763,2775,"The focus is distributed-memory parallelization of sparse-general-matrix-multiplication (SpGEMM). Parallel SpGEMM algorithms are classified under one-dimensional (1D), 2D, and 3D categories denoting the number of dimensions by which the 3D sparse workcube representing the iteration space of SpGEMM is partitioned. Recently proposed successful 2D- and 3D-parallel SpGEMM algorithms benefit from upper bounds on communication overheads enforced by 2D and 3D cartesian partitioning of the workcube on 2D and 3D virtual processor grids, respectively. However, these methods are based on random cartesian partitioning and do not utilize sparsity patterns of SpGEMM instances for reducing the communication overheads. We propose hypergraph models for 2D and 3D cartesian partitioning of the workcube for further reducing the communication overheads of these 2D- and 3D- parallel SpGEMM algorithms. The proposed models utilize two- and three-phase partitioning that exploit multi-constraint hypergraph partitioning formulations. Extensive experimentation performed on 20 SpGEMM instances by using upto 900 processors demonstrate that proposed partitioning models significantly improve the scalability of 2D and 3D algorithms. For example, in 2D-parallel SpGEMM algorithm on 900 processors, the proposed partitioning model respectively achieves 85 and 42 percent decrease in total volume and total number of messages, leading to 1.63 times higher speedup compared to random partitioning, on average.","1558-2183","","10.1109/TPDS.2020.3000708","Türkiye Bilimsel ve Teknolojik Araştirma Kurumu(grant numbers:EEEAG-115E512); National Center for High Performance Computing of Turkey(grant numbers:4005992019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9110752","Sparse matrix-matrix multiplication;SpGEMM;sparse SUMMA SpGEMM;split-3D-SpGEMM;hypergraph partitioning;communication cost;bandwidth;latency","Partitioning algorithms;Two dimensional displays;Three-dimensional displays;Solid modeling;Sparse matrices;Computational modeling;Upper bound","distributed memory systems;graph theory;iterative methods;mathematics computing;matrix multiplication;parallel algorithms;sparse matrices","SpGEMM instances;random Cartesian partitioning;3D Cartesian partitioning;2D-parallel SpGEMM algorithms;Cartesian partitioning models;2D-parallel SpGEMM algorithm;multiconstraint hypergraph partitioning formulations;three-phase partitioning;3D virtual processor grids;3D-parallel SpGEMM algorithms;3D sparse workcube;sparse-general-matrix-multiplication;distributed-memory parallelization","",3.0,"",40.0,"IEEE","8 Jun 2020","","","IEEE","IEEE Journals"
"Interval Job Scheduling With Machine Launch Cost","R. Ren; Y. Zhu; C. Li; X. Tang","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Parallel and Distributed Systems","30 Jun 2020",2020,31.0,12.0,2776,2788,"We study an interval job scheduling problem in distributed systems. We are given a set of interval jobs, with each job specified by a size, an arrival time and a processing length. Once a job arrives, it must be placed on a machine immediately and run for a period of its processing length without interruption. The homogeneous machines to run jobs have the same capacity limits such that at anytime, the total size of the jobs running on any machine cannot exceed its capacity. Launching each machine incurs a fixed cost. After launch, a machine is charged a constant cost per time unit until it is terminated. The problem targets to minimize the total cost incurred by the machines for processing the given set of interval jobs. We focus on the algorithmic aspects of the problem in this article. For the special case where all the jobs have a unit size equal to the machine capacity, we propose an optimal offline algorithm and an optimal 2-competitive online algorithm. For the general case where jobs can have arbitrary sizes, we establish a non-trivial lower bound on the optimal solution. Based on this lower bound, we propose a 5-approximation algorithm in the offline setting. In the non-clairvoyant online setting, we design a O(μ)-competitive Modified First-Fit algorithm which is near optimal (μ is the max/min job processing length ratio). In the clairvoyant online setting, we propose an asymptotically optimal O(√log μ)-competitive algorithm based on our Modified First-Fit strategy.","1558-2183","","10.1109/TPDS.2020.3002786","Singapore Ministry of Education Academic Research Fund Tier 1(grant numbers:2019-T1-002-042); National Natural Science Foundation of China(grant numbers:61902063); Natural Science Foundation of Jiangsu Province(grant numbers:BK20190342); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9119143","Job scheduling;online algorithm;approximation algorithm","Servers;Virtual machining;Computational modeling;Cloud computing;Approximation algorithms;Job shop scheduling","approximation theory;competitive algorithms;computational complexity;processor scheduling","competitive modified first-fit algorithm;nonclairvoyant online setting;nontrivial lower bound;optimal 2-competitive online algorithm;competitive algorithm;5-approximation algorithm;distributed systems;homogeneous machines;interval job scheduling problem;machine launch cost","",6.0,"",32.0,"IEEE","16 Jun 2020","","","IEEE","IEEE Journals"
"Scalable, Multi-Constraint, Complex-Objective Graph Partitioning","G. M. Slota; C. Root; K. Devine; K. Madduri; S. Rajamanickam","Computer Science Department, Rensselaer Polytechnic Institute, Troy, USA; Computer Science Department, Rensselaer Polytechnic Institute, Troy, USA; Scalable Algorithms Department, Sandia National Laboratories, Albuquerque, USA; Electrical Engineering and Computer Science Department, The Pennsylvania State University, University Park, USA; Scalable Algorithms Department, Sandia National Laboratories, Albuquerque, USA","IEEE Transactions on Parallel and Distributed Systems","30 Jun 2020",2020,31.0,12.0,2789,2801,"We introduce XtraPuLP, a distributed-memory graph partitioner designed to process irregular trillion-edge graphs. XtraPuLP is based on the scalable label propagation community detection technique, which has been demonstrated in various prior works as a viable means to produce high quality partitions of skewed and small-world graphs with minimal computation time. Our XtraPuLP implementation can also be generalized to compute partitions with an arbitrary number of constraints, and it can compute partitions with balanced communication load across all parts. On a collection of large sparse graphs, we show that XtraPuLP partitioning is considerably faster than state-of-the-art partitioning methods, while also demonstrating that XtraPuLP can produce partitions of real-world graphs with billion+ vertices and over a hundred billion edges in minutes. Additionally, we demonstrate XtraPuLP on a variety of applications, including large-scale graph analytics and sparse matrix-vector multiplication.","1558-2183","","10.1109/TPDS.2020.3002150","National Science Foundation(grant numbers:OCI-0725070,ACI-1238993,ACI-1444747); U.S. Department of Energy; Rensselaer Polytechnic Institute and Sandia National Laboratories(grant numbers:DE-AC02-05CH11231); National Science Foundation(grant numbers:ACI-1253881,CCF-1439057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115834","Graph partitioning;load balancing;label propagation;graph analysis","Partitioning algorithms;Scalability;Sparse matrices;Computational modeling;Rats;Indexes;Social networking (online)","application program interfaces;computational complexity;graph theory;mathematics computing;matrix multiplication;message passing;network theory (graphs);sparse matrices","multiconstraint graph partitioning;large-scale graph analytics;real-world graphs;XtraPuLP partitioning;sparse graphs;balanced communication load;XtraPuLP implementation;minimal computation time;small-world graphs;scalable label propagation community detection technique;trillion-edge graphs;distributed-memory graph partitioner;complex-objective graph partitioning","",12.0,"",49.0,"IEEE","12 Jun 2020","","","IEEE","IEEE Journals"
"Distributed Training of Deep Learning Models: A Taxonomic Perspective","M. Langer; Z. He; W. Rahayu; Y. Xue","BOSS ZhiPin Career Science Lab (CSL), Beijing, China; La Trobe University, Bundoora, Australia; La Trobe University, Bundoora, Australia; BOSS ZhiPin Career Science Lab (CSL), Beijing, China","IEEE Transactions on Parallel and Distributed Systems","1 Jul 2020",2020,31.0,12.0,2802,2818,"Distributed deep learning systems (DDLS) train deep neural network models by utilizing the distributed resources of a cluster. Developers of DDLS are required to make many decisions to process their particular workloads in their chosen environment efficiently. The advent of GPU-based deep learning, the ever-increasing size of datasets, and deep neural network models, in combination with the bandwidth constraints that exist in cluster environments require developers of DDLS to be innovative in order to train high-quality models quickly. Comparing DDLS side-by-side is difficult due to their extensive feature lists and architectural deviations. We aim to shine some light on the fundamental principles that are at work when training deep neural networks in a cluster of independent machines by analyzing the general properties associated with training deep learning models and how such workloads can be distributed in a cluster to achieve collaborative model training. Thereby we provide an overview of the different techniques that are used by contemporary DDLS and discuss their influence and implications on the training process. To conceptualize and compare DDLS, we group different techniques into categories, thus establishing a taxonomy of distributed deep learning systems.","1558-2183","","10.1109/TPDS.2020.3003307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9120226","Survey;machine learning;deep learning;distributed systems;stochastic gradient descent;big data","Deep learning;Training;Computational modeling;Parallel processing;Taxonomy;Data models;Biological system modeling","distributed processing;learning (artificial intelligence);neural nets","deep neural networks;collaborative model training;DDLS;distributed deep learning systems;distributed training;GPU-based deep learning","",27.0,"",51.0,"IEEE","18 Jun 2020","","","IEEE","IEEE Journals"
"Scheduling Periodical Multi-Stage Jobs With Fuzziness to Elastic Cloud Resources","J. Zhu; X. Li; R. Ruiz; W. Li; H. Huang; A. Y. Zomaya","Jiangsu Key Laboratory of Big Data Security & Intelligent Processing, Nanjing University of Posts and Telecommunications, Nanjing, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, Nanjing, China; Grupo de Sistemas de Optimización Aplicada, Universitat Politècnica de València, València, Spain; Centre of Distributed and High Performance Computing, School of Computer Science, The University of Sydney, Camperdown, Australia; School of Computer Science & Technology, Nanjing University of Posts & Telecommunications, Nanjing, China; Centre of Distributed and High Performance Computing, School of Computer Science, The University of Sydney, Camperdown, Australia","IEEE Transactions on Parallel and Distributed Systems","1 Jul 2020",2020,31.0,12.0,2819,2833,"We investigate a workflow scheduling problem with stochastic task arrival times and fuzzy task processing times and due dates. The problem is common in many real-time and workflow-based applications, where tasks with fixed stage number and linearly dependency are executed on scalable cloud resources with multiple price options. The challenges lie in proposing effective, stable, and robust algorithms under stochastic and fuzzy tasks. A triangle fuzzy number-based model is formulated. Two metrics are explored: the cost and the degree of satisfaction. An iterated heuristic framework is proposed to periodically schedule tasks, which consists of a task collection and a fuzzy task scheduling phases. Two task collection strategies are presented and two task prioritization strategies are employed. In order to achieve a high satisfaction degree, deadline constraints are defined at both job and task levels. By designing delicate experiments and applying sophisticated statistical techniques, experimental results show that the proposed algorithm is more effective and robust than the two existing methods.","1558-2183","","10.1109/TPDS.2020.3004134","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1400800); National Natural Science Foundation of China(grant numbers:61672297,61872077,61832004); Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:18KJB520039); National Science Foundation for Post-doctoral Scientists of China(grant numbers:2018M640510); Spanish Ministry of Science, Innovation, and Universities(grant numbers:RTI2018-094940-B-I00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9122581","Job scheduling;fuzzy processing times;fuzzy deadlines;cloud computing","Task analysis;Dynamic scheduling;Job shop scheduling;Cloud computing;Schedules;Heuristic algorithms","cloud computing;fuzzy set theory;iterative methods;pricing;scheduling;stochastic processes","statistical techniques;task prioritization strategies;fuzzy task scheduling phases;iterated heuristic framework;triangle fuzzy number-based model;stochastic tasks;robust algorithms;multiple price options;fuzzy task processing times;workflow scheduling problem;elastic cloud resources;scheduling periodical multistage jobs;satisfaction degree","",8.0,"",31.0,"IEEE","22 Jun 2020","","","IEEE","IEEE Journals"
"Resource Management for Power-Constrained HEVC Transcoding Using Reinforcement Learning","L. Costero; A. Iranfar; M. Zapater; F. D. Igual; K. Olcoz; D. Atienza","Departamento de Arquitectura de Computadores y Automática, Universidad Complutense de Madrid, Madrid, Spain; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Lausanne, Switzerland; School of Engineering and Management Vaud (HEIG-VD), University of Applied Sciences Western, Delémont, Switzerland; Departamento de Arquitectura de Computadores y Automática, Universidad Complutense de Madrid, Madrid, Spain; Departamento de Arquitectura de Computadores y Automática, Universidad Complutense de Madrid, Madrid, Spain; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Lausanne, Switzerland","IEEE Transactions on Parallel and Distributed Systems","7 Jul 2020",2020,31.0,12.0,2834,2850,"The advent of online video streaming applications and services along with the users' demand for high-quality contents require High Efficiency Video Coding (HEVC), which provides higher video quality and more compression at the cost of increased complexity. On one hand, HEVC exposes a set of dynamically tunable parameters to provide trade-offs among Quality-of-Service (QoS), performance, and power consumption of multi-core servers on the video providers' data center. On the other hand, resource management of modern multi-core servers is in charge of adapting system-level parameters, such as operating frequency and multithreading, to deal with concurrent applications and their requirements. Therefore, efficient multi-user HEVC streaming necessitates joint adaptation of application-and system-level parameters. Nonetheless, dealing with such a large and dynamic design space is challenging and difficult to address through conventional resource management strategies. Thus, in this work, we develop a multi-agent Reinforcement Learning framework to jointly adjust application-and system-level parameters at runtime to satisfy the QoS of multi-user HEVC streaming in power-constrained servers. In particular, the design space, composed of all design parameters, is split into smaller independent sub-spaces. Each design sub-space is assigned to a particular agent so that it can explore it faster, yet accurately. The benefits of our approach are revealed in terms of adaptability and quality (with up to to 4× improvements in terms of QoS when compared to a static resource management scheme), and learning time (6× fasterthan an equivalent mono-agent implementation). Finally, we show that the power-capping techniques formulated outperform the hardware-based power capping with respect to quality.","1558-2183","","10.1109/TPDS.2020.3004735","EU (FEDER) and Spanish MINECO(grant numbers:RTI2018-093684-B-I00,MECD (FPU15/02050),CM(S2018/TCS-4423),UCM (PR65/19-22445); ERC Consolidator(grant numbers:725657); H2020 RECIPE(grant numbers:801137); H2020 DeepHealth project(grant numbers:825111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9124674","Resource management;DVFS;power capping;reinforcement learning;Q-learning;HEVC;self-adaptation","Quality of service;Resource management;Transcoding;Servers;Streaming media;Throughput;Power demand","computer centres;learning (artificial intelligence);multi-agent systems;multiprocessing systems;power aware computing;quality of service;transcoding;video coding;video streaming","joint adaptation;hardware-based power capping;equivalent mono-agent implementation;application-and system-level parameters;multithreading;video provider data center;quality-of-service;multiuser HEVC streaming;power-capping techniques;static resource management scheme;power-constrained servers;multiagent reinforcement learning framework;dynamic design space;concurrent applications;multicore servers;power consumption;QoS;high efficiency video coding;high-quality contents;online video streaming applications;power-constrained HEVC transcoding","",5.0,"",41.0,"IEEE","24 Jun 2020","","","IEEE","IEEE Journals"
"Dynamic Undervolting to Improve Energy Efficiency on Multicore X86 CPUs","P. Koutsovasilis; K. Parasyris; C. D. Antonopoulos; N. Bellas; S. Lalis","Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece; Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, USA; Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece; Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece; Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece","IEEE Transactions on Parallel and Distributed Systems","7 Jul 2020",2020,31.0,12.0,2851,2864,"Chip manufacturers introduce redundancy at various levels of CPU design to guarantee correct operation, even for worst-case combinations of non-idealities in process variation and system operating conditions. This redundancy is implemented partly in the form of voltage margins. However, for a wide range of real-world execution scenarios these margins are excessive and merely translate to increased power consumption, hindering the effort towards higher-energy efficiency in both HPC and general purpose computing. Our study on the x86-64 Haswell and Skylake multicore microarchitectures reveals-wide voltage margins, which vary across different microarchitectures, different chip parts of the same microarchitecture, and across different workloads. We find that it is necessary to quantify-voltage margins using multi-threaded and multi-instance workloads, as characterization with single-threaded and single-instance workloads that do not stress the CPU to its full capacity typically identifies overly optimistic margins that lead to errors when applied in realistic program execution scenarios. In addition, we introduce, deploy and evaluate a run-time governor that dynamically reduces the supply voltage of modern multicore x86-64 CPUs. Our governor employs a model that takes as input a set of performance metrics which are directly measurable via performance monitoring counters and have high predictive value for the minimum tolerable supply voltage (Vmin), to predict and apply the appropriate reduction for the workload at hand. Compared with the conventional DVFS governor, our approach achieves up to 42 percent energy savings for the Skylake family and 34 percent for the Haswell family for complex, real-world applications.","1558-2183","","10.1109/TPDS.2020.3004383","European Commission(grant numbers:688540); U.S. Department of Energy(grant numbers:DEAC52-07NA2734 (LLNL-JRNL-809714)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9123555","Energy efficiency;undervolting;low-energy and low-power technologies measurement;modeling","Benchmark testing;Central Processing Unit;Multicore processing;Monitoring;Stress;Computer crashes;Workstations","energy conservation;microprocessor chips;multiprocessing systems;multi-threading;power aware computing;power consumption","system operating conditions;process variation;nonidealities;worst-case combinations;CPU design;chip manufacturers;improve energy efficiency;energy savings;minimum tolerable supply voltage;modern multicore x86-64 CPUs;realistic program execution scenarios;overly optimistic margins;single-instance workloads;multiinstance workloads;quantify-voltage margins;microarchitecture;redundancy;power consumption;higher-energy efficiency;general purpose computing;microarchitectures;chip parts","",4.0,"",54.0,"IEEE","23 Jun 2020","","","IEEE","IEEE Journals"
"GPGPU Performance Estimation With Core and Memory Frequency Scaling","Q. Wang; X. Chu","Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","7 Jul 2020",2020,31.0,12.0,2865,2881,"Contemporary graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, accurate and straightforward performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is essential to determine the best frequency configuration for energy saving. In this article, we reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Compared to the cycle-level simulators, which are too slow to apply on real hardware, our model only needs simple and one-off micro-benchmarks to extract a set of hardware parameters and kernel performance counters without any source code analysis. Our experimental results show that the proposed performance model can capture the kernel performance scaling behaviors under different frequency settings and achieve decent accuracy (average errors of 3.85, 8.6, 8.82, and 8.83 percent on a set of 20 GPU kernels with four modern Nvidia GPUs).","1558-2183","","10.1109/TPDS.2020.3004623","Hong Kong RGC GRF(grant numbers:HKBU 12200418); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9124659","Graphics processing units;dynamic voltage and frequency scaling;GPU performance modeling","Graphics processing units;Kernel;Hardware;Frequency estimation;Time-frequency analysis;Estimation;Analytical models","computer graphic equipment;graphics processing units;performance evaluation;power aware computing","fine-grained analytical model;memory frequency;kernel performance counters;performance model;GPU kernels;frequency configuration;frequency settings;GPU kernel;straightforward performance estimation;accurate performance estimation;energy consumption;computational performance;frequency scaling;GPGPU performance estimation","",19.0,"",55.0,"IEEE","24 Jun 2020","","","IEEE","IEEE Journals"
"Congestion-Balanced and Welfare-Maximized Charging Strategies for Electric Vehicles","Q. Tang; K. Wang; K. Yang; Y. -s. Luo","Hunan Provincial Key Laboratory of Intelligent Processing of Big Data on Transportation, School of Computer Science and Communication Engineering, Changsha University of Science and Technology, Changsha, China; Department of Computer and Information Sciences, Northumbria University, Newcastle upon Tyne, United Kingdon; School of Computer Science and Electronic Engineering, University of Essex, Essex, United Kingdon; Hunan Provincial Key Laboratory of Intelligent Processing of Big Data on Transportation, School of Computer Science and Communication Engineering, Changsha University of Science and Technology, Changsha, China","IEEE Transactions on Parallel and Distributed Systems","9 Jul 2020",2020,31.0,12.0,2882,2895,"With the increase of the number of electric vehicles (EVs), it is of vital importance to develop the efficient and effective charging scheduling schemes for all the EVs. In this article, we aim to maximize the social welfare of all the EVs, charging stations (CSs) and power plant (PP), by taking into account the changing demand of each EV, the changing price, the capacity and the congestion balance between different CSs. To this end, two efficient scheduling algorithms, i.e., Centralized Charging Strategy (CCS) and Distributed Charging Strategy (DCS) are proposed. CCS has a slightly better performance than the DCS, as it takes all the information and make the decision in the central control unit. On the other hand, DCS dose not require the private information from EVs and can make decentralized decision. Extensive simulation are conducted to verify the effectiveness of the proposed algorithms, in terms of the performance, congestion balance, and computing complexity.","1558-2183","","10.1109/TPDS.2020.3003270","National Natural Science Foundation of China(grant numbers:61620106011,61772087); Zhongshan City Team Project(grant numbers:180809162197874); Outstanding Youth Project of Hunan Province Education Department(grant numbers:18B162); Changsha University of Science and Technology(grant numbers:2018IC23); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9120172","Social welfare maximization;congestion balance;charging strategy;electric vehicle","Cascading style sheets;Optimization;Charging stations;Power grids;Pricing;Electric vehicles;Stochastic processes","battery powered vehicles;optimisation;pricing;scheduling","welfare-maximized charging strategies;electric vehicles;efficient charging scheduling schemes;effective charging scheduling schemes;social welfare;power plant;changing demand;EV;changing price;congestion balance;efficient scheduling algorithms;centralized charging strategy;CCS;distributed charging strategy;DCS;congestion-balanced charging strategies","",25.0,"",43.0,"IEEE","18 Jun 2020","","","IEEE","IEEE Journals"
"GPU-Accelerated Real-Time Stereo Estimation With Binary Neural Network","G. Chen; H. Meng; Y. Liang; K. Huang","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Peng Cheng Laboratory, Shenzhen, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","9 Jul 2020",2020,31.0,12.0,2896,2907,"Depth estimation from stereo images is essential to many applications such as robotics and autonomous vehicles, most of which ask for the real-time response, high energy and storage efficiency. Recent work has shown deep neural networks (DNN) perform extremely well for stereo estimation. However, these state-of-the-art DNN based algorithms are challenging to be deployed into real-world applications due to the high computational complexities of DNNs. Most of them are too slow for real-time inference and require several seconds of GPU computation to process image frames. In this article, we address the problem of fast stereo estimation and propose an efficient and light-weighted stereo matching system, called StereoBit, to produce a disparity map in a real-time manner while achieving close to state-of-the-art accuracy. To achieve this goal, we propose a binary neural network to generate weighted Hamming distance for an efficient similarity join in stereo estimation. In addition, we propose a novel approximation approach to derive StereoBit network directly from the well-trained network with the cosine similarity. Our approximation strategies enable a significant speedup while maintaining almost the same accuracy compared to the network with the cosine similarity. Furthermore, we present an optimization framework for fully exploiting the computing power of StereoBit. The framework provides a significant speedup of stereo estimation routines, and at the same time, reduces the memory usage for storing parameters. The effectiveness of StereoBit is evaluated by comprehensive experiments. StereoBit can achieve 60 frames per second on an NVIDIA TITAN Xp GPU on KITTI 2012 benchmark while achieving 3-pixel non-occluded stereo error 3.56 percent.","1558-2183","","10.1109/TPDS.2020.3006238","National Natural Science Foundation of China(grant numbers:61702085); Shenzhen Basic Research Grants(grant numbers:JCYJ20180507182508857); Science and Technology Planning Project of Guangzhou city of China(grant numbers:202007050004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9130887","GPU acceleration;stereo estimation;binary neural network","Estimation;Neural networks;Real-time systems;Graphics processing units;Computational modeling;Optimization;Convolution","graphics processing units;image matching;neural nets;stereo image processing","GPU-accelerated real-time stereo estimation;binary neural network;depth estimation;stereo images;autonomous vehicles;deep neural networks;GPU computation;fast stereo estimation;light-weighted stereo matching system;weighted Hamming distance;StereoBit network;stereo estimation routines;NVIDIA TITAN Xp GPU;stereo error 3;DNN based algorithm","",18.0,"",30.0,"IEEE","1 Jul 2020","","","IEEE","IEEE Journals"
"Spatially Bursty I/O on Supercomputers: Causes, Impacts and Solutions","J. Yu; W. Yang; F. Wang; D. Dong; J. Feng; Y. Li","Computational Aerodynamics Institute, China Aerodynamics Research and Development Center, Mianyang, China; Computational Aerodynamics Institute, China Aerodynamics Research and Development Center, Mianyang, China; Computational Aerodynamics Institute, China Aerodynamics Research and Development Center, Mianyang, China; College of Computer, National University of Defense Technology, Changsha, China; National Supercomputer Centre in Tianjin, Tianjin, China; National Supercomputer Centre in Tianjin, Tianjin, China","IEEE Transactions on Parallel and Distributed Systems","9 Jul 2020",2020,31.0,12.0,2908,2922,"Understanding the I/O characteristics of supercomputers is crucial for grasping accurate I/O workloads and uncovering potential I/O inefficiency. We collect and analyze I/O traces from two production supercomputers, and find that the I/O traffic peaks in the system not only occur in short periods of time but also originate from a minority of adjacent compute nodes, which we call spatially bursty I/O. Since modern supercomputers widely adopt I/O forwarding architecture, in which an I/O node performs I/O on behalf of a subset of compute nodes in the vicinity, spatially bursty I/O will cause significant load imbalance and underutilization on the I/O nodes. To address such problems, we quantitatively analyze the two causes of spatially bursty I/O, including uneven I/O distribution on job's processes and uneven job nodes distribution on the system. Two different solutions are proposed to mobilize more I/O nodes to participate in job's I/O activity. (1) We change the I/O node mapping, making adjacent compute nodes use different I/O nodes instead of a same one. (2) According to the job's I/O characteristics extracted from history I/O traces, we distribute the compute nodes of data-intensive jobs more sparsely to utilize more I/O nodes. Extensive evaluations of both solutions show that they can further exploit the potential of I/O forwarding layer. We have deployed the proposed I/O node mapping on a production supercomputer for 11 months. Our experience finds that it can effectively promote I/O performance, balance loads, and alleviate I/O interference.","1558-2183","","10.1109/TPDS.2020.3005572","National Numerical Windtunnel Project of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9127806","Parallel I/O;bursty I/O;I/O forwarding;I/O node mapping;resource allocation","Supercomputers;Aerodynamics;Computer architecture;Probability distribution;Entropy;Production;History","parallel machines;resource allocation","production supercomputer;data-intensive jobs;load imbalance","",6.0,"",51.0,"IEEE","29 Jun 2020","","","IEEE","IEEE Journals"
"Towards Unaligned Writes Optimization in Cloud Storage With High-Performance SSDs","J. Shu; F. Li; S. Li; Y. Lu","Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","20 Jul 2020",2020,31.0,12.0,2923,2937,"NVMe SSDs provide extremely high performance and have been widely deployed in distributed object storage systems in data centers. However, we observe that there are still severe performance degradation and write amplification under the unaligned writes scenario with high-performance SSDs. In this article, we identify that the RMW sequence which is used to handle the unaligned writes incurs severe overhead in the data path. Besides, unaligned writes incur additional metadata management overhead in the block map table. To address these problems, we propose an object-based device system named NVStore to optimize the unaligned writes in cloud storage with NVMe SSDs. NVStore provides a Flexible Cache Management to reduce the RMW operations while supporting lazy page sync and ensuring data consistency. To optimize the metadata management, NVStore proposes a KV Affinity Metadata Management which co-designs the block map and key-value store to provides a flattened and decoupled metadata management. Evaluations show that NVStore provides at most 6.11× bandwidth of BlueStore in the cluster. Besides, NVStore can reduce at most 94.7 percent of the write traffic from metadata under unaligned writes compared to BlueStore and achieves smaller data write traffic which is about 50 percent of BlueStore and 65.7 percent of FileStore.","1558-2183","","10.1109/TPDS.2020.3006655","National Key Research & Development Program of China(grant numbers:2018YFB1003301); National Natural Science Foundation of China(grant numbers:61772300,61832011); Research and Development Plan in Key Field of Guangdong Province(grant numbers:2018B010109002); SenseTime Research Fund for Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9132670","Distributed system;object storage;unaligned writes;solid state drives","Metadata;Bandwidth;Indexes;Performance evaluation;Cloud computing;Nonvolatile memory;Optimization","cache storage;cloud computing;meta data;random-access storage;solid state drives","FileStore;BlueStore;write amplification;flexible cache management;KV affinity metadata management;unaligned writes optimization;key-value store;data consistency;lazy page sync;NVMe SSDs;cloud storage;NVStore;object-based device system;block map table;data path;RMW sequence;high-performance SSDs;data centers;distributed object storage systems","",1.0,"",74.0,"IEEE","2 Jul 2020","","","IEEE","IEEE Journals"
"Traffic-Aware Erasure-Coded Archival Schemes for In-Memory Stores","B. Xu; J. Huang; X. Qin; Q. Cao","Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Software Engineering, Shelby Center for Engineering Technology, Samuel Ginn College of Engineering, Auburn University, Auburn, USA; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","27 Jul 2020",2020,31.0,12.0,2938,2953,"Redundancy schemes are introduced to in-memory stores to provide fault tolerance. To achieve good trade-off between access performance and memory efficiency, it is appropriate to adopt replication and erasure coding to keep popular and unpopular data, respectively. Within such a hybrid-redundancy in-memory store, an issue of redundancy transition from replication to erasure coding (a.k.a., erasure-coded archival) should be addressed for unpopular in-memory datasets, since caching workloads exhibit long-tail distributions and most in-memory data are unpopular. If data replicas are distributed across nodes in randomly-selected racks, then subsequent data-block-replica retrieval for erasure-coded archival will create cross-rack traffic, and final parity-block relocation will cause extra cross-rack communications. In this article, we propose an encoding-oriented replica placement policy - ERP - by incorporating an interleaved declustering mechanism. We design two traffic-aware erasure-coded archival schemes -TEA-TL and TEA-SL - for ERP-powered in-memory stores by taking into account temporal locality and spatial locality, respectively. With ERP in place, both TEA-TL and TEA-SL schemes embrace the following three salient features: (i) they alleviate cross-rack traffic raised by retrieving required data-block replicas; (ii) they improve rack-level load balancing by distributing replicas via load-aware primary-rack-selection approach; and (iii) they mitigate block-relocation operations launched to sustain rack-level and node-level fault-tolerance. We conduct quantitative performance evaluations using the YCSB benchmark. The empirical results show that both TEA-TL and TEA-SL schemes not only bring forth lower cross-rack traffic than the four candidate encoding schemes, but also exhibit superb archival-throughput and rack-level-balancing performance. In particular, within a group of comparative tests using the baseline configurations, TEA-TL and TEA-SL accelerate archival throughput by at least 36.3 and 70.8 percent, respectively; both TEA-TL and TEA-SL schemes improve rack-level load-balancing by a factor of more than 1.45x relative to the four candidate encoding schemes.","1558-2183","","10.1109/TPDS.2020.3009092","National Natural Science Foundation of China(grant numbers:61821003); National Natural Science Foundation of China(grant numbers:61702004,61762075,61872413); NSF of Qinghai Province(grant numbers:2020-ZJ-926); National Science Foundation(grant numbers:CCF-0845257,CNS-0917137,CCF-0742187); Higher Education Discipline Innovation Project(grant numbers:B07038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9140411","In-memory store;erasure encoding;replication;encoding-oriented placement","Encoding;Fault tolerant systems;Redundancy;Ear;Distributed databases;Facebook","cache storage;fault tolerant computing;file organisation;performance evaluation;redundancy;resource allocation;storage management;telecommunication traffic","cross-rack traffic;candidate encoding schemes;superb archival-throughput;rack-level-balancing performance;TEA-SL schemes;rack-level load-balancing;in-memory store;good trade-off between access performance;memory efficiency;erasure coding;redundancy transition;in-memory datasets;long-tail distributions;in-memory data;data replicas;randomly-selected racks;subsequent data-block-replica retrieval;final parity-block relocation;extra cross-rack communications;ERP-powered in-memory stores;rack-level load balancing;load-aware primary-rack-selection approach;block-relocation operations;node-level fault-tolerance;data-block replicas;encoding-oriented replica placement policy;traffic-aware erasure-coded archival schemes","",2.0,"",38.0,"IEEE","14 Jul 2020","","","IEEE","IEEE Journals"
"Millimeter-Scale and Billion-Atom Reactive Force Field Simulation on Sunway Taihulight","P. Gao; X. Duan; T. Zhang; M. Zhang; B. Schmidt; X. Zhang; H. Sun; W. Zhang; L. Gan; W. Xue; H. Fu; W. Liu; G. Yang","National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; Institute for Computer Science, Johannes Gutenberg University, Mainz, Germany; Materials Genome Institute, Shanghai University, Shanghai, China; Materials Genome Institute, Shanghai University, Shanghai, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China","IEEE Transactions on Parallel and Distributed Systems","24 Jul 2020",2020,31.0,12.0,2954,2967,"Large-scale molecular dynamics (MD) simulations on supercomputers play an increasingly important role in many research areas. With the capability of simulating charge equilibration (QEq), bonds and so on, Reactive force field (ReaxFF) enables the precise simulation of chemical reactions. Compared to the first principle molecular dynamics (FPMD), ReaxFF has far lower requirements on computational resources so that it can achieve higher efficiencies for large-scale simulations. In this article, we present our efforts on scaling ReaxFF on the Sunway TaihuLight Supercomputer (TaihuLight). We have carefully redesigned the force analysis and neighbor list building steps. By applying fine-grained optimizations we gain better single process performance. For the many-body interactions, we propose an isolated computation and update strategy and implement inverse trigonometric functions. For QEq, we implement a pipelined conjugate gradient (CG) approach to achieving better scalability. Furthermore, we reorganize the data layout and implement the update operation based on data locality in ReaxFF. Our experiments show that this approach can simulate chemical reactions with 1,358,954,496 atoms using 4,259,840 cores with a performance of 0.015 ns/day. To our best knowledge, this is the first realization of chemical reaction simulation with a millimeter-scale force field.","1558-2183","","10.1109/TPDS.2020.3008499","National Natural Science Foundation of China(grant numbers:61972231,U1806205,51761135015,U1839206); Key Project of Joint Fund of Shandong Province(grant numbers:ZR2019LZH007); Shenzhen Basic Research Fund(grant numbers:JCYJ20180507182818013); Center for High Performance Computing and System Simulation, Pilot National Laboratory for Marine Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138781","High performance computing;molecular dynamics;computational science;Sunway TaihuLight Supercomputer (TaihuLight)","Computational modeling;Force;Supercomputers;Chemicals;Dynamics;Mathematical model;Multicore processing","conjugate gradient methods;molecular dynamics method;parallel machines;parallel processing;physics computing","inverse trigonometric functions;charge equilibration;charge equilibration;billion-atom reactive force field simulation;millimeter-scale force field;chemical reaction simulation;pipelined conjugate gradient approach;fine-grained optimizations;force analysis;Sunway TaihuLight Supercomputer;first principle molecular dynamics;ReaxFF;large-scale molecular dynamics simulations","",9.0,"",33.0,"IEEE","10 Jul 2020","","","IEEE","IEEE Journals"
"HeteroYARN: A Heterogeneous FPGA-Accelerated Architecture Based on YARN","R. Li; Q. Yang; Y. Li; X. Gu; W. Xiao; K. Li","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Electrical and Computer Engineering, Virginia Commonwealth University, Richmond, USA; Department of Computer Science, State University of New York, New Paltz, USA","IEEE Transactions on Parallel and Distributed Systems","12 Aug 2020",2020,31.0,12.0,2968,2980,"In recent years, the heterogeneous distributed platform integrating with FPGAs to accelerate computation tasks has been widely studied to deal with the deluge of data. However, most of current works suffer from poor universality and low resource utilization that run specific algorithms with the highly customized structure. Moreover, there are still many challenges, such as data curation, task scheduling, and resource management, which further limit the scalability of a CPU-FPGA distributed platform. In this paper, we present HeteroYARN, an FPGA-accelerated heterogeneous architecture based on YARN platform, which provides resource management and programming support for computing-intensive applications using FPGAs. In particular, the HeteroYARN abstracts FPGA accelerators as general resources and provides programming APIs to utilize those accelerators easily. Our HeteroYARN simplifies the request and usage of FPGA resources to enhance the efficiency of the heterogeneous framework while maintaining previous workflow unchanged. Experimental results using two representative algorithms, K-means and Naive Bayes classifier, which are accelerated by FPGAs, demonstrate the usability of the HeteroYARN framework and show performance speedup improvement by 7.5x (K-means) and 2.3x (Naive Bayes) respectively compared to conventional CPU-only applications provided by Mahout.","1558-2183","","10.1109/TPDS.2019.2905201","National Key Research and Development Program of China(grant numbers:2016YFB0800402,2016QY01W0202); National Natural Science Foundation of China(grant numbers:U1836204,61572221,61433006,U1401258,61572222,61502185); National Social Science Foundation(grant numbers:16ZDA092); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667646","Heterogeneous system;heterogeneous FPGA architecture;FPGA-accelerated computing;data-intensive computing;YARN","Field programmable gate arrays;Yarn;Resource management;Task analysis;Scheduling;Computer architecture;Processor scheduling","","","",1.0,"",40.0,"IEEE","15 Mar 2019","","","IEEE","IEEE Journals"
"Pattern-Based Dynamic Compilation System for CGRAs With Online Configuration Transformation","L. Liu; X. Man; J. Zhu; S. Yin; S. Wei","Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","24 Jul 2020",2020,31.0,12.0,2981,2994,"Prevailing data-intensive applications, such as artificial intelligence and internet of things, demand considerable compute capability. Coarse-grained reconfigurable architectures (CGRAs) can meet this demand via providing abundant compute resources. However, compilation has become an essential problem because the increasing resources need to be orchestrated efficiently. Static compilation is insufficient due to conservative resource allocation and exponentially increasing time cost while state-of-the-art dynamic compilation still performs poorly in both generality and efficiency. This article proposes a dynamic compilation system for CGRAs through online pattern-based configuration transformation, which enables virtualization to improve resource utilization and flexibility. It utilizes statically-generated patterns to straightforwardly determine dynamic placement of registers and operations so that the transformation algorithm has a low complexity. Domain-specific features are extracted by a k-means clustering algorithm to help improve the quality of patterns. The experimental results show that statically compiled applications can be transformed onto arbitrary resources at runtime, reserving 73.5 (22.8-163.3 percent) of the original performance/resource on average, 9.1 (0-52.9 percent) better than the state-of-theart non-general methods.","1558-2183","","10.1109/TPDS.2020.3007492","National Natural Science Foundation of China(grant numbers:61672317,61834002); National Science Technology Major Project of China(grant numbers:2018ZX01028-201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9134964","CGRA;dynamic compilation;configuration transformation","Dynamic compiler;Heuristic algorithms;Registers;Dynamic scheduling;Kernel;Resource management","pattern clustering;program compilers;reconfigurable architectures;resource allocation","domain-specific features;k-means clustering algorithm;nongeneral methods;arbitrary resources;statically compiled applications;transformation algorithm;dynamic placement;flexibility;resource utilization;online pattern-based configuration transformation;conservative resource allocation;static compilation;abundant compute resources;coarse-grained reconfigurable architectures;demand considerable compute capability;internet of things;artificial intelligence;data-intensive applications;online configuration transformation;CGRAs;pattern-based dynamic compilation system","",2.0,"",26.0,"IEEE","7 Jul 2020","","","IEEE","IEEE Journals"
"Crocus: Enabling Computing Resource Orchestration for Inline Cluster-Wide Deduplication on Scalable Storage Systems","P. Hamandawana; A. Khan; C. -G. Lee; S. Park; Y. Kim","Department of Computer Engineering, Ajou University, Suwon, Republic of Korea; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea","IEEE Transactions on Parallel and Distributed Systems","16 Mar 2020",2020,31.0,8.0,1740,1753,"Inline deduplication dramatically improves storage space utilization. However, it degrades I/O throughput due to computeintensive deduplication operations such as chunking, fingerprinting or hashing of chunk content, and redundant lookup I/Os over the network in the I/O path. In particular, the fingerprint or hash generation of content contributes largely to the degraded I/O throughput and is computationally expensive. In this article, we propose CROCUS, a framework that enables compute resource orchestration to enhance cluster-wide deduplication performance. In particular, CROCUS takes into account all compute resources such as local and remote {CPU, GPU} by managing decentralized compute pools. An opportunistic Load-Aware Fingerprint Scheduler (LAFS), distributes and offloads compute-intensive deduplication operations in a load-aware fashion to compute pools. CROCUS is highly generic and can be adopted in both inline and offline deduplication with different storage tier configurations. We implemented CROCUS in Ceph scale-out storage system. Our extensive evaluation shows that CROCUS reduces the fingerprinting overhead by 86 percent with 4KB chunk size compared to Ceph with baseline deduplication while maintaining high disk-space savings. Our proposed LAFS scheduler, when tested in different internal and external contention scenarios also showed 54 percent improvement over a fixed or static scheduling approach.","1558-2183","","10.1109/TPDS.2020.2972882","National Research Foundation of Korea(grant numbers:NRF-2018R1A1A1A05079398); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8993857","Distributed file systems;scheduling;storage management","Servers;Degradation;Graphics processing units;Computer architecture;Message systems;Metadata;Throughput","cloud computing;data handling;resource allocation;scheduling;storage management","opportunistic load-aware fingerprint scheduler;storage tier configurations;internal contention scenarios;cluster-wide deduplication performance;compute resource orchestration;content contributes;chunk content;computeintensive deduplication operations;storage space utilization;inline deduplication;scalable storage systems;inline cluster-wide deduplication;enabling computing resource orchestration;external contention scenarios;baseline deduplication;4KB chunk size;fingerprinting overhead;Ceph scale-out storage system;offline deduplication;load-aware fashion;compute-intensive deduplication operations;compute pools;CROCUS","",6.0,"",47.0,"IEEE","11 Feb 2020","","","IEEE","IEEE Journals"
"Accelerating Federated Learning via Momentum Gradient Descent","W. Liu; L. Chen; Y. Chen; W. Zhang","Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; School of Engineering, University of Warwick, Coventry, United Kingdom; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China","IEEE Transactions on Parallel and Distributed Systems","19 Mar 2020",2020,31.0,8.0,1754,1766,"Federated learning (FL) provides a communication-efficient approach to solve machine learning problems concerning distributed data, without sending raw data to a central server. However, existing works on FL only utilize first-order gradient descent (GD) and do not consider the preceding iterations to gradient update which can potentially accelerate convergence. In this article, we consider momentum term which relates to the last iteration. The proposed momentum federated learning (MFL) uses momentum gradient descent (MGD) in the local update step of FL system. We establish global convergence properties of MFL and derive an upper bound on MFL convergence rate. Comparing the upper bounds on MFL and FL convergence rates, we provide conditions in which MFL accelerates the convergence. For different machine learning models, the convergence performance of MFL is evaluated based on experiments with MNIST and CIFAR-10 datasets. Simulation results confirm that MFL is globally convergent and further reveal significant convergence improvement over FL.","1558-2183","","10.1109/TPDS.2020.2975189","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFA0701603); National Natural Science Foundation of China(grant numbers:61722114); USTC Research Funds(grant numbers:YD3500002001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003425","Accelerating convergence;distributed machine learning;federated learning;momentum gradient descent","Convergence;Machine learning;Servers;Distributed databases;Data models;Acceleration;Computational modeling","convergence of numerical methods;gradient methods;learning (artificial intelligence)","convergence improvement;convergence performance;FL convergence rates;MFL convergence rate;global convergence properties;FL system;local update step;momentum federated learning;momentum term;gradient update;preceding iterations;first-order gradient descent;central server;distributed data;communication-efficient approach;momentum gradient descent","",119.0,"",34.0,"IEEE","19 Feb 2020","","","IEEE","IEEE Journals"
"A Hybrid Update Strategy for I/O-Efficient Out-of-Core Graph Processing","X. Xu; F. Wang; H. Jiang; Y. Cheng; D. Feng; Y. Zhang","Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, USA; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","20 Mar 2020",2020,31.0,8.0,1767,1782,"In recent years, a number of out-of-core graph processing systems have been proposed to process graphs with billions of edges on just one commodity computer, due to their high cost efficiency. To obtain a better performance, these systems adopt a full I/O model that scans all edges during the computation to avoid the inefficiency of random I/Os. Although this model ensures good I/O access locality, it leads to a large number of useless edges to be loaded when running graph algorithms that only access a small portion of edges in each iteration. An intuitive method to solve this I/O inefficiency problem is the on-demand I/O model that only accesses the active edges. However, this method only works well for the graph algorithms with very few active edges, since the I/O cost will grow rapidly as the number of active edges increases due to the increasing amount of random I/Os. In this article, we present HUS-Graph, an efficient out-of-core graph processing system to address the above I/O issues and achieve a good balance between I/O traffic and I/O access locality. HUS-Graph adopts a hybrid update strategy including two update models, Row-oriented Push (ROP) and Column-oriented Pull (COP). It supports switching between ROP and COP adaptively, for the graph algorithms that have different computation and I/O features. For traversal-based algorithms, HUS-Graph also provides an immediate propagation-based vertex update scheme to accelerate the vertex state propagation and convergence speed. Furthermore, HUS-Graph adopts a locality-optimized dual-block representation to organize graph data and an I/O-based performance prediction method to enable the system to dynamically select the optimal update model between ROP and COP. To save the disk space and further reduce I/O traffic, HUS-Graph implements a space-efficient storage format by combining several graph compression methods. Extensive experimental results show that HUS-Graph outperforms two existing out-of-core systems GraphChi and GridGraph by 1.2x-52.8x.","1558-2183","","10.1109/TPDS.2020.2973143","National Defense Preliminary Research Project(grant numbers:31511010202); National Natural Science Foundation of China(grant numbers:61832020,61772216,61821003,U1705261); Wuhan application basic research(grant numbers:2017010201010103); Hubei province technical innovation special Project(grant numbers:2017AAA129); Fundamental Research Funds for the Central Universities; Wuhan National Laboratory for Optoelectronics(grant numbers:2018WNLOKF006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8994089","Graph processing;out-of-core;I/O;hybrid update strategy","Adaptation models;Load modeling;Data models;Computational modeling;Switches;Computer science;Loading","graph theory;optimisation","hybrid update strategy;out-of-core graph processing system;process graphs;scans all edges;access locality;useless edges;running graph algorithms;active edges;HUS-Graph;graph data;optimal update model;graph compression methods","",6.0,"",42.0,"IEEE","11 Feb 2020","","","IEEE","IEEE Journals"
"Power Guarantee for Electric Systems Using Real-Time Scheduling","E. Kim; Y. Lee; L. He; K. G. Shin; J. Lee","Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA; Department of Robotics, Hanyang University, Ansan-si, Korea; Department of Computer Science and Engineering, University of Colorado Denver, Denver, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA; Department of Computer Science and Engineering, Sungkyunkwan University (SKKU), Suwon-si, Republic of Korea","IEEE Transactions on Parallel and Distributed Systems","20 Mar 2020",2020,31.0,8.0,1783,1798,"Modern electric systems, such as electric vehicles, mobile robots, nano satellites, and drones, require to support various power-demand operations for user applications and system maintenance. This, in turn, calls for advanced power management that jointly considers power demand by the operations and power supply from various sources, such as batteries, solar panels, and supercapacitors. In this article, we develop a power scheduling framework for a reliable energy storage system with multiple power-supply sources and multiple power-demand operations. Specifically, we develop offline power-supply guarantee analysis and online power management. The former provides an offline power-supply guarantee such that every power-demand operation completes its execution in time while the sum of power required by individual operations does not exceed the total power supplied by the entire energy storage system at any time; to this end, we develop a plain power-supply analysis as well as its improved version using real-time scheduling techniques. On the other hand, the latter efficiently utilizes the surplus power available at runtime for improving system performance; we propose two approaches, depending on whether future scheduling information of power-demanding tasks is available or not. For evaluation, we perform simulations to evaluate both the plain and improved analyses for offline power guarantee under various synthetic power-demand operations. In addition, we have built a simulation model and demonstrated that the proposed framework with the offline analysis and online management not only guarantees the required power-supply, but also enhances system performance by up to 56.49 percent.","1558-2183","","10.1109/TPDS.2020.2977041","National Science Foundation(grant numbers:CNS-1446117,CNS-1739577); Office of Naval Research(grant numbers:N00014-18-1-2141); LG Chem Ltd.; National Research Foundation of Korea(grant numbers:2019R1A2B5B02001794,2017H1D8A2031628); Grand Information Technology Research Center support program(grant numbers:IITP-2020-2015-0-00742); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018173","Offline power-supply guarantee;online power management;real-time scheduling;electric systems","Batteries;Power system management;Real-time systems;Power demand;Analytical models","energy storage;power supplies to apparatus;real-time systems;scheduling","offline power-supply guarantee analysis;online power management;energy storage system;real-time scheduling techniques;system performance;offline power guarantee;synthetic power-demand operations;electric systems;electric vehicles;system maintenance;advanced power management;power scheduling framework;reliable energy storage system;multiple power-supply sources;multiple power-demand operations;power-supply analysis","",3.0,"",37.0,"IEEE","28 Feb 2020","","","IEEE","IEEE Journals"
"Local-Density Subspace Distributed Clustering for High-Dimensional Data","Y. -a. Geng; Q. Li; M. Liang; C. -Y. Chi; J. Tan; H. Huang","Beijing Key Lab of Transportation Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; Beijing Key Lab of Transportation Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; WeiXin Group, Tencent Company Limited, Beijing, China; Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Business Administration, Beijing Technology and Business University, Beijing, China; JD Finance America Corporation","IEEE Transactions on Parallel and Distributed Systems","20 Mar 2020",2020,31.0,8.0,1799,1814,"Distributed clustering is emerging along with the advent of the era of big data. However, most existing established distributed clustering methods focus on problems caused by a large amount of data rather than caused by the large dimension of data. Consequently, they suffer the “curse” of dimensionality (e.g., poor performance and heavy network overhead) when high-dimensional (HD) data are clustered. In this article, we propose a distributed algorithm, referred to as Local Density Subspace Distributed Clustering (LDSDC) algorithm, to cluster large-scale HD data, motivated by the idea that a local dense region of a HD dataset is usually distributed in a low-dimensional (LD) subspace. LDSDC follows a local-global-local processing structure, including grouping of local dense regions (atom clusters) followed by subspace Gaussian model (SGM) fitting (flexible and scalable to data dimension) at each sub-site, merging of atom clusters at every sub-site according to the merging result broadcast from the global site. Moreover, we propose a fast method to estimate the parameters of SGM for HD data, together with its convergence proof. We evaluate LDSDC on both synthetic and real datasets and compare it with four state-of-the-art methods. The experimental results demonstrate that the proposed LDSDC yields best overall performance.","1558-2183","","10.1109/TPDS.2020.2975550","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFC1501503); Natural Science Foundation of Beijing Municipality(grant numbers:L191016); National Social Science Foundation of China(grant numbers:18CSH019); Beijing Municipal Education Commission Research Program(grant numbers:SM20191001107,PXM2019_014213_000007); China Scholarship Council(grant numbers:201907090062); Ministry of Science and Technology of the People's Republic of China(grant numbers:109-2221-E-007-024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007481","High-dimensional clustering;distributed clustering;density-base clustering;subspace Gaussian model","Clustering algorithms;Distributed databases;Principal component analysis;Data models;Clustering methods;Big Data;Kernel","Big Data;Gaussian processes;pattern clustering","high-dimensional data;big data;distributed clustering methods;network overhead;Local Density Subspace Distributed Clustering algorithm;LDSDC;local dense region;HD dataset;low-dimensional subspace;local-global-local processing structure;atom clusters;subspace Gaussian model fitting;data dimension","",3.0,"",62.0,"IEEE","24 Feb 2020","","","IEEE","IEEE Journals"
"Probabilistic Consistency Guarantee in Partial Quorum-Based Data Store","X. Yao; C. -L. Wang","Department of Computer Science, University of Hong Kong, Hong Kong; Department of Computer Science, University of Hong Kong, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","26 Mar 2020",2020,31.0,8.0,1815,1827,"Many NoSQL databases support quorum-based protocols, which require a subset of replicas (called a quorum) to respond to each write/read operation. These systems configure the quorum size to tune the operation latency and adopt multiple consistency levels. Some recent works illustrate that using probability models to quantify the chance of reading the last update is important because it could avoid returning stale values under eventual consistency. There are two challenging issues: (1) from inconsistent replicas, how to determine the minimum quorum size (i.e., the lowest access latency) to read the newest data at a specified probability; (2) node failure frequently happens in large-scale systems, how to guarantee the probability-based consistent reads. This article presents Probabilistic Consistency Guarantee (PCG), which is the first dynamic quorum decision and failure-aware quantification model. PCG model respectively quantifies the server-side consistency after the latest write, which reflects the object's time-varying update progress, and the possibility of reading this update when responding to the end-users. Our theoretical analysis derives several formulas to determine the quorum size of a read quorum and the consensus result selected from this quorum is the data updated by the last write at the user-specified probability. When some replicas are unavailable, our model knows how to rescale the quorum and read values from surviving replicas could reduce the stale reads caused by node failures. The experimental results in Cassandra demonstrate that the PCG model can achieve up to 77.7 percent more accurate predictions and reduce up to 48.9 percent read latency than those of the previous model.","1558-2183","","10.1109/TPDS.2020.2973619","Hong Kong RGC Collaborative Research Fund(grant numbers:E-RB29); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8998160","Quorum-replicated database;eventual consistency;probabilistic consistency model","Probability;Probabilistic logic;Distributed databases;Data models;Predictive models;Cloud computing;Synchronization","data integrity;Internet;NoSQL databases;probability;protocols;replicated databases;storage management","read quorum;user-specified probability;surviving replicas;node failures;PCG model;partial quorum-based data store;multiple consistency levels;probability models;stale values;eventual consistency;inconsistent replicas;minimum quorum size;large-scale systems;dynamic quorum decision;failure-aware quantification model;server-side consistency;probabilistic consistency guarantee;NoSQL databases;quorum-based protocols","",4.0,"",43.0,"IEEE","13 Feb 2020","","","IEEE","IEEE Journals"
"FULL-KV: Flexible and Ultra-Low-Latency In-Memory Key-Value Store System Design on CPU-FPGA","Y. Qiu; J. Xie; H. Lv; W. Yin; W. -S. Luk; L. Wang; B. Yu; H. Chen; X. Ge; Z. Liao; X. Shi","State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; Huawei Chengdu Research Institute, Chengdu, China; Huawei Chengdu Research Institute, Chengdu, China; Huawei Chengdu Research Institute, Chengdu, China; Huawei Chengdu Research Institute, Chengdu, China; Huawei Chengdu Research Institute, Chengdu, China","IEEE Transactions on Parallel and Distributed Systems","26 Mar 2020",2020,31.0,8.0,1828,1444,"In-memory key-value store (IMKVS) has gained great popularity in data centers. However, big data brings great challenges in performance and power consumption because of the general-purpose Von Neumann computer architecture. Remote direct memory access (RDMA) technology supporting zero-copy networking could partly alleviate the problem but is still not efficient for KVS. To overcome this problem, we present a flexible and ultra-low-latency IMKVS system named FULL-KV, based on a CPU-FPGA heterogeneous architecture. The FPGA serves as a KVS accelerator that can bypass the CPU and implement both the network stacks and the KVS processing with a highly parallel hardware architecture. The system latency of FULL-KV can achieve as low as 1.5μs/2.2μs for the PUT/GET operation, which is 3.0x/1.5x faster than current state-of-the-art hardware-based KVS systems. Besides, FULL-KV can support 4x larger values (up to 4M bytes). Given a total Ethernet bandwidth of 20Gbps, the peak throughput of the single-node FULL-KV can reach 26.0 million key-value operations per second (Mops). In the two-node test system with a commercial Ethernet switch, the peak throughput can reach 52Mops, manifesting the system scalability and practicability.","1558-2183","","10.1109/TPDS.2020.2973965","National Natural Science Foundation of China(grant numbers:61971143); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999599","CPU-FPGA heterogeneous architecture;Hardware accelerator;In-memory key-value store;ultra-low-latency performance","Random access memory;Throughput;Computer architecture;Field programmable gate arrays;Data centers;Power demand;Flash memories","Big Data;computer centres;field programmable gate arrays;local area networks;memory architecture;telecommunication switching","direct memory access;zero-copy networking;flexible IMKVS system;ultra-low-latency IMKVS system;CPU-FPGA heterogeneous architecture;KVS accelerator;network stacks;KVS processing;highly parallel hardware architecture;key-value operations;two-node test system;system scalability;In-memory key-value store;great popularity;data centers;big data;power consumption;general-purpose Von Neumann computer architecture;state-of-the-art hardware-based KVS systems","",8.0,"",39.0,"IEEE","14 Feb 2020","","","IEEE","IEEE Journals"
"Evaluation of Stream Processing Frameworks","G. van Dongen; D. Van den Poel","Ghent University, Ghent, Belgium; Ghent University, Ghent, Belgium","IEEE Transactions on Parallel and Distributed Systems","27 Mar 2020",2020,31.0,8.0,1845,1858,"The increasing need for real-time insights in data sparked the development of multiple stream processing frameworks. Several benchmarking studies were conducted in an effort to form guidelines for identifying the most appropriate framework for a use case. In this article, we extend this research and present the results gathered. In addition to Spark Streaming and Flink, we also include the emerging frameworks Structured Streaming and Kafka Streams. We define four workloads with custom parameter tuning. Each of these is optimized for a certain metric or for measuring performance under specific scenarios such as bursty workloads. We analyze the relationship between latency, throughput, and resource consumption and we measure the performance impact of adding different common operations to the pipeline. To ensure correct latency measurements, we use a single Kafka broker. Our results show that the latency disadvantages of using a micro-batch system are most apparent for stateless operations. With more complex pipelines, customized implementations can give event-driven frameworks a large latency advantage. Due to its micro-batch architecture, Structured Streaming can handle very high throughput at the cost of high latency. Under tight latency SLAs, Flink sustains the highest throughput. Additionally, Flink shows the least performance degradation when confronted with periodic bursts of data. When a burst of data needs to be processed right after startup, however, micro-batch systems catch up faster while event-driven systems output the first events sooner.","1558-2183","","10.1109/TPDS.2020.2978480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025240","Apache spark;structured streaming;apache flink;apache kafka;kafka streams;distributed computing;stream processing frameworks;benchmarking;big data","Benchmark testing;Sparks;Pipelines;Throughput;Storms;Microsoft Windows;Measurement","cluster computing;data handling;parallel processing;software architecture","performance degradation;event-driven systems output;tight latency SLA;highest throughput;high latency;microbatch architecture;latency advantage;event-driven frameworks;customized implementations;complex pipelines;stateless operations;microbatch system;latency disadvantages;single Kafka broker;latency measurements;performance impact;latency throughput;bursty workloads;custom parameter tuning;Kafka Streams;emerging frameworks Structured Streaming;Flink;Spark Streaming;appropriate framework;benchmarking studies;multiple stream processing frameworks;real-time insights","",32.0,"",22.0,"IEEE","5 Mar 2020","","","IEEE","IEEE Journals"
"Analyzing the Performance Trade-Off in Implementing User-Level Threads","S. Iwasaki; A. Amer; K. Taura; P. Balaji","Department of Information and Communication Engineering, University of Tokyo, Tokyo, Japan; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, USA; Department of Information and Communication Engineering, University of Tokyo, Tokyo, Japan; Mathematical and Computer Science, Argonne National Laboratory, Lemont, USA","IEEE Transactions on Parallel and Distributed Systems","27 Mar 2020",2020,31.0,8.0,1859,1877,"User-level threads have been widely adopted as a means of achieving lightweight concurrent execution without the costs of OS-level threads. Nevertheless, the costs of managing user-level threads represent a performance barrier that dictates how fine grained the concurrency exposed by an application can be without incurring significant overheads; this in turn may translate into insufficient parallelism to exploit highly parallel systems. This article is a deep dive into the fundamental costs in implementing user-level threads. We first identify that one of the highest sources of fork-join overheads stems from deviations, events that incur context switching during the execution of a thread and disrupt a run-to-completion execution. We then conduct an in-depth investigation of a wide spectrum of methods with respect to how they handle deviations while covering both parent- and child-first scheduling policies. Our methodology involves a comprehensive instruction- and cache-level analysis of all methods on several modern CPU architectures. The primary finding of our evaluation is that dynamic promotion methods that assume the absence of deviation and dynamically provide context-switching support offer the best trade-off between performance and capability when the likelihood of deviation is low.","1558-2183","","10.1109/TPDS.2020.2976057","Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018074","Multithreading;multitasking;scheduling;user-level threads;context switch;task parallelism","Context;Instruction sets;Switches;Libraries;Runtime;Hardware;Computer architecture","cache storage;microprocessor chips;multi-threading;parallel architectures;scheduling","performance trade-off;implementing user-level threads;lightweight concurrent execution;OS-level threads;fundamental costs;cache-level analysis;modern CPU architectures;dynamic promotion methods;context-switching support;run-to-completion execution","",4.0,"",67.0,"IEEE","28 Feb 2020","","","IEEE","IEEE Journals"
"Optimizing Streaming Parallelism on Heterogeneous Many-Core Architectures","P. Zhang; J. Fang; C. Yang; C. Huang; T. Tang; Z. Wang","National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; University of Leeds, Leeds, United Kingdom","IEEE Transactions on Parallel and Distributed Systems","1 Apr 2020",2020,31.0,8.0,1878,1896,"As many-core accelerators keep integrating more processing units, it becomes increasingly more difficult for a parallel application to make effective use of all available resources. An effective way of improving hardware utilization is to exploit spatial and temporal sharing of the heterogeneous processing units by multiplexing computation and communication tasks - a strategy known as heterogeneous streaming. Achieving effective heterogeneous streaming requires carefully partitioning hardware among tasks, and matching the granularity of task parallelism to the resource partition. However, finding the right resource partitioning and task granularity is extremely challenging, because there is a large number of possible solutions and the optimal solution varies across programs and datasets. This article presents an automatic approach to quickly derive a good solution for hardware resource partition and task granularity for task-based parallel applications on heterogeneous many-core architectures. Our approach employs a performance model to estimate the resulting performance of the target application under a given resource partition and task granularity configuration. The model is used as a utility to quickly search for a good configuration at runtime. Instead of hand-crafting an analytical model that requires expert insights into low-level hardware details, we employ machine learning techniques to automatically learn it. We achieve this by first learning a predictive model offline using training programs. The learned model can then be used to predict the performance of any unseen program at runtime. We apply our approach to 39 representative parallel applications and evaluate it on two representative heterogeneous many-core platforms: a CPU-XeonPhi platform and a CPU-GPU platform. Compared to the single-stream version, our approach achieves, on average, a 1.6x and 1.1x speedup on the XeonPhi and the GPU platform, respectively. These results translate to over 93 percent of the performance delivered by a theoretically perfect predictor.","1558-2183","","10.1109/TPDS.2020.2978045","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB0204301); National Natural Science Foundation of China(grant numbers:61972408,61602501,61872294); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022909","Heterogeneous computing;parallelism;performance tuning;machine learning","Task analysis;Graphics processing units;Hardware;Parallel processing;Runtime;Machine learning;Predictive models","graphics processing units;learning (artificial intelligence);microprocessor chips;multiprocessing systems;parallel architectures;program compilers","single-stream version;many-core platforms;representative parallel applications;predictive model;low-level hardware details;analytical model;task granularity configuration;resource partition;performance model;task-based parallel applications;hardware resource partition;resource partitioning;task parallelism;effective heterogeneous streaming;communication tasks;heterogeneous processing units;temporal sharing;spatial sharing;hardware utilization;parallel application;many-core accelerators;heterogeneous many-core architectures;streaming parallelism;efficiency 93.0 percent","",7.0,"",69.0,"IEEE","3 Mar 2020","","","IEEE","IEEE Journals"
"Accelerating Stochastic Gradient Descent Based Matrix Factorization on FPGA","S. Zhou; R. Kannan; V. K. Prasanna","Microsoft Corporation, Redmond, USA; US Army Research Lab, Los Angeles, USA; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, USA","IEEE Transactions on Parallel and Distributed Systems","3 Apr 2020",2020,31.0,8.0,1897,1911,"Matrix Factorization (MF) based on Stochastic Gradient Descent (SGD) is a powerful machine learning technique to derive hidden features of objects from observations. In this article, we design a highly parallel architecture based on Field-Programmable Gate Array (FPGA) to accelerate the training process of the SGD-based MF algorithm. We identify the challenges for the acceleration and propose novel algorithmic optimizations to overcome them. By transforming the SGD-based MF algorithm into a bipartite graph processing problem, we propose a 3-level hierarchical partitioning scheme that enables conflict-minimizing scheduling and processing of edges to achieve significant speedup. First, we develop a fast heuristic graph partitioning approach to partition the bipartite graph into induced subgraphs; this enables to efficiently use the on-chip memory resources of FPGA for data reuse and completely hide the data communication between FPGA and external memory. Second, we partition all the edges of each subgraph into non-overlapping matchings to extract the maximum parallelism. Third, we propose a batching algorithm to schedule the execution of the edges inside each matching to reduce the memory access conflicts to the on-chip RAMs of FPGA. Compared with non-optimized FPGA-based baseline designs, the proposed optimizations result in up to 60× data dependency reduction, 4.2× bank conflict reduction, and 15.4× speedup. We evaluate the performance of our design using a state-of-the-art FPGA device. Experimental results show that our FPGA accelerator sustains a high computing throughput of up to 217 billion floating-point operations per second (GFLOPS) for training very large real-life sparse matrices. Compared with highly-optimized GPU-based accelerators, our FPGA accelerator achieves up to 12.7× speedup. Based on our optimization methodology, we also implement a software-based design on a multi-core platform, which demonstrates 1.3× speedup compared with the state-of-the-art multi-core implementation.","1558-2183","","10.1109/TPDS.2020.2974744","Intel Strategic Research Alliance funding; National Science Foundation(grant numbers:ACI-1339756,CNS-1643351,OAC-1911229); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9001229","Machine learning;sparse matrix factorization;training acceleration;bipartite graph representation;FPGA accelerator","Field programmable gate arrays;Acceleration;Training;System-on-chip;Optimization;Partitioning algorithms;Bipartite graph","computer graphic equipment;field programmable gate arrays;gradient methods;graph theory;graphics processing units;learning (artificial intelligence);matrix decomposition;parallel architectures;sparse matrices","bank conflict reduction;software-based design;highly-optimized GPU-based accelerators;FPGA accelerator;state-of-the-art FPGA;data dependency reduction;nonoptimized FPGA-based baseline designs;memory access conflicts;batching algorithm;on-chip memory resources;fast heuristic graph partitioning approach;conflict-minimizing scheduling;3-level hierarchical partitioning scheme;bipartite graph processing problem;algorithmic optimizations;SGD-based MF algorithm;training process;field-programmable gate array;highly parallel architecture;powerful machine learning technique;matrix factorization;stochastic gradient descent","",5.0,"",48.0,"IEEE","18 Feb 2020","","","IEEE","IEEE Journals"
"T-BASIR: Finding Shutdown Bugs for Cloud-Based Applications in Cloud Spot Markets","A. Alourani; A. D. Kshemkalyani; M. Grechanik","Majmaah University, Al Majma'ah, Saudi Arabia; Department of Computer Science, University of Illinois at Chicago, Chicago, USA; Department of Computer Science, University of Illinois at Chicago, Chicago, USA","IEEE Transactions on Parallel and Distributed Systems","3 Apr 2020",2020,31.0,8.0,1912,1924,"One of the major advantages of cloud spot instances in cloud computing is to allow stakeholders to economically deploy their applications at much lower costs than that of other types of cloud instances. In exchange, spot instances are often exposed to revocations (i.e., terminations) by cloud providers. With spot instances becoming pervasive, terminations have become a part of the normal behavior of cloud-based applications; thus, these applications may be left in an incorrect state leading to certain bugs. Unfortunately, these applications are not designed or tested to deal with this behavior in the cloud environment, and as a result, the advantages of cloud spot instances could be significantly minimized or even entirely negated. We propose a novel solution to automatically find these bugs and locate their causes in the source code. We evaluate our solution using 10 popular open-source applications. The results show that our solution not only finds more instances and different types of these bugs compared to the random approach, but it also locates the causes of these bugs to help developers improve the design of the shutdown process and is more efficient in finding instances of these bugs since it interposes at the system call layer.","1558-2183","","10.1109/TPDS.2020.2980265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035645","Cloud computing;cloud spot markets;shutdown bugs of cloud-based applications;kernel modules;irregular terminations of cloud-based applications;spot instance revocations","Computer bugs;Cloud computing;Testing;Radio access technologies;Open source software;Kernel;Hardware","cloud computing;program debugging","T-BASIR;source code;system call layer;shutdown bugs;open-source applications;cloud environment;cloud providers;cloud computing;cloud spot markets","",2.0,"",47.0,"IEEE","13 Mar 2020","","","IEEE","IEEE Journals"
"Automatic Generation of High-Performance FFT Kernels on Arm and X86 CPUs","Z. Li; H. Jia; Y. Zhang; T. Chen; L. Yuan; R. Vuduc","Georgia Institute of Technology, Atlanta, USA; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA","IEEE Transactions on Parallel and Distributed Systems","3 Apr 2020",2020,31.0,8.0,1925,1941,"This article presents AutoFFT, a template-based code generation framework that can automatically generate high-performance FFT kernels for all natural-number radices. AutoFFT is based on the Cooley-Tukey FFT algorithm, which exploits the symmetric and periodic properties of the DFT matrix, as the outer parallelization framework. Because butterflies are the core operations of the Cooley-Tukey algorithm, we explore additional symmetric and periodic properties of the DFT matrix and formulate multiple optimized calculation templates to further reduce the number of floating-point operations for butterflies of arbitrary natural numbers. To fully exploit hardware resources, we encapsulate a series of optimizations in an assembly template optimizer. Given any DFT problem, AutoFFT automatically generates C FFT kernels using these calculation templates and converts them into efficient assembly kernels using the template optimizer. Through a series of experiments on Arm, Intel, and AMD processors, we show that AutoFFT-generated kernels can outperform those in Fastest Fourier Transform in the West (FFTW), the Arm Performance Libraries (ARMPL), and the Intel Math Kernel Library (MKL).","1558-2183","","10.1109/TPDS.2020.2977629","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2107YFB0202105,2016YFB0200803,2017YFB0202302); National Natural Science Foundation of China(grant numbers:61602443,61432018,61521092,61502450); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035643","AutoFFT;FFT;code generation;template;DFT","Kernel;Libraries;Discrete Fourier transforms;Computer architecture;Optimization;Symmetric matrices;Hardware","discrete Fourier transforms;floating point arithmetic;matrix algebra;microprocessor chips;optimisation;program compilers","Arm CPU;ARMPL;Arm performance libraries;fastest Fourier transform in the West;high-performance FFT kernels automatic generation;Cooley-Tukey FFT algorithm;natural-number radices;template-based code generation framework;X86 CPUs;Intel Math Kernel Library;AutoFFT-generated kernels;assembly kernels;C FFT kernels;assembly template optimizer;arbitrary natural numbers;floating-point operations;Cooley-Tukey algorithm;outer parallelization framework;DFT matrix;periodic properties;symmetric properties","",3.0,"",50.0,"CCBY","13 Mar 2020","","","IEEE","IEEE Journals"
"Fireplug: Efficient and Robust Geo-Replication of Graph Databases","R. Neiheiser; L. Rech; M. Bravo; L. Rodrigues; M. Correia","Departamento de Engenharia de Automação e Sistemas, Universidade Federal de Santa Catarina, Florianópolis, Brazil; Departamento de Informática e Estatística, Universidade Federal de Santa Catarina, Florianópolis, Brazil; IMDEA Software Institute, Madrid, Spain; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal","IEEE Transactions on Parallel and Distributed Systems","9 Apr 2020",2020,31.0,8.0,1942,1953,"Although graph-databases have been assuming an increasing relevance in applications that exhibit strong dependability requirements, including tolerance to malicious faults, few works have addressed Byzantine fault tolerance in this particular context, and previous attempts suffer from lack of flexibility and poor performance. This article describes and evaluates Fireplug, a flexible architecture to build robust geo-replicated graph databases. Fireplug can be configured to tolerate from crash to Byzantine faults, both within and across different datacenters. Furthermore, Fireplug is robust to bugs in existing graph database implementations, as it allows to combine multiple graph database instances in a cohesive manner. Thus, Fireplug can support many different deployments, according to the performance/robustness trade-offs imposed by the target application. Our evaluation shows that Fireplug is able implement Byzantine fault tolerance without penalty when compared to the built-in replication mechanism of Neo4j, which only supports crash faults. Additionally, performance optimizations introduced by Fireplug improve the overall performance by up to 900 percent in geo-replicated scenarios.","1558-2183","","10.1109/TPDS.2020.2981019","FCT(grant numbers:PTDC/ EEI-SCR/ 1741/ 2014 (Abyss),UIDB/ 50021/ 2020); CNPq/Brasil(grant numbers:401364/2014-3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040450","Graph databases;geo-replication;N-version programming;byzantine faults","Databases;Computer crashes;Fault tolerance;Fault tolerant systems;Software;Protocols;Programming","computer centres;database management systems;distributed processing;fault tolerance;fault tolerant computing;graph theory;software fault tolerance","robust geo-replication;graph-databases;dependability requirements;malicious faults;Byzantine fault tolerance;Fireplug;robust geo-replicated graph databases;Byzantine faults;graph database implementations;multiple graph database instances;crash faults;geo-replicated scenarios;Neo4j","",2.0,"",44.0,"IEEE","18 Mar 2020","","","IEEE","IEEE Journals"
"Location-Aware and Budget-Constrained Service Deployment for Composite Applications in Multi-Cloud Environment","T. Shi; H. Ma; G. Chen; S. Hartmann","School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; Department of Informatics, Clausthal University of Technology, Clausthal-Zellerfeld, Germany","IEEE Transactions on Parallel and Distributed Systems","9 Apr 2020",2020,31.0,8.0,1954,1969,"Enterprise application providers are increasingly moving their workloads to the cloud for technical and economic benefits. Multi-cloud environment makes it possible to orchestrate multiple cloud resources. With the increasing number of available cloud resources provided by multiple cloud providers at different locations with different prices, application providers face the challenge to select proper cloud resources to deploy their applications in the form of a workflow of component service units. Existing studies usually consider minimizing execution time or/and deployment cost. From the perspective of application providers, however, they also pay huge attention to application response time, including particularly network latency between deployed services and users. Meanwhile, application deployment is often subject to stringent budgetary control to ensure financial viability. This article studies a new type of composite application deployment problem that jointly considers both the performance optimization and budget control in multi-cloud at the global scale. To find solutions with minimal response time without running into the risk of over-spending, we propose a hybrid GA-based approach, featuring new design of domain-tailored service clustering, repair algorithm, solution representation, population initialization, and genetic operators. Extensive experiments using the real-world dataset demonstrate that our proposed hybrid GA approach outperforms some recently proposed approaches.","1558-2183","","10.1109/TPDS.2020.2981306","New Zealand Marsden Fund(grant numbers:VUW1510); Royal Society of New Zealand; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9039753","Location;budget;service deployment;multi-cloud;composite applications;genetic algorithm;clustering;repair algorithm","Cloud computing;Maintenance engineering;Optimization;Genetic algorithms;Time factors;Clustering algorithms;Business","cloud computing;genetic algorithms;pattern clustering","genetic operators;population initialization;solution representation;repair algorithm;hybrid GA-based approach;budgetary control;location-aware service deployment;component service units;multiple cloud providers;multiple cloud resources;economic benefits;technical benefits;enterprise application providers;multicloud environment;budget-constrained service deployment;domain-tailored service clustering;performance optimization;composite application deployment problem;application response time","",20.0,"",56.0,"IEEE","17 Mar 2020","","","IEEE","IEEE Journals"
"Bandwidth-Aware Dynamic Prefetch Configuration for IBM POWER8","C. Navarro; J. Feliu; S. Petit; M. E. Gómez; J. Sahuquillo","Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain; Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain; Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain; Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain; Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain","IEEE Transactions on Parallel and Distributed Systems","14 Apr 2020",2020,31.0,8.0,1970,1982,"Advanced hardware prefetch engines are being integrated in current high-performance processors. Prefetching can boost the performance of most applications, however, the induced bandwidth consumption can lead the system to a high contention for main memory bandwidth, which is a scarce resource in current multicores. In such a case, the system performance can be severely damaged. This article characterizes the applications’ behavior in an IBM POWER8 machine, which presents many prefetch settings, varying the bandwidth contention. The study reveals that the best prefetch setting for each application depends on the main memory bandwidth availability, that is, it depends on the co-running applications. Based on this study, we propose Bandwidth-Aware Prefetch Configuration (BAPC) a scalable adaptive prefetching algorithm that improves the performance of multi-program workloads. BAPC increases the performance of the applications in a 12, 15, and 16 percent of 6-, 8-, and 10-application workloads over the IBM POWER8 default configuration. In addition, BAPC reduces bandwidth consumption in 39, 42, and 45 percent, respectively.","1558-2183","","10.1109/TPDS.2020.2982392","Ministerio de Ciencia, Innovación y Universidades and the European(grant numbers:RTI2018-098156-B-C51); Generalitat Valenciana(grant numbers:AICO/2019/317); Universitat Politècnica de València(grant numbers:SP20180140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9044389","Prefetch engine;prefetch settings;performance measures","Prefetching;Bandwidth;Interference;Hardware;Engines;Memory management","cache storage;microprocessor chips;multiprocessing systems;performance evaluation","IBM POWER8 machine;prefetch settings;bandwidth contention;BAPC;scalable adaptive prefetching algorithm;IBM POWER8 default configuration;memory bandwidth availability;hardware prefetch engines;bandwidth-aware dynamic prefetch configuration;multiprogram workload;efficiency 45.0 percent;efficiency 16.0 percent","",3.0,"",29.0,"IEEE","23 Mar 2020","","","IEEE","IEEE Journals"
"Abstraction Layer For Standardizing APIs of Task-Based Engines","R. Alomairy; H. Ltaief; M. Abduljabbar; D. Keyes","Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia","IEEE Transactions on Parallel and Distributed Systems","22 May 2020",2020,31.0,11.0,2482,2495,"We introduce AL4SAN, a lightweight library for abstracting the APIs of task-based runtime engines. AL4SAN unifies the expression of tasks and their data dependencies. It supports various dynamic runtime systems relying on compiler technology and user-defined APIs. It enables a single application to employ different runtimes and their respective scheduling components, while providing user-obliviousness to the underlying hardware configurations. AL4SAN exposes common front-end APIs and connects to different back-end runtimes. Experiments on performance and overhead assessments are reported on various shared- and distributed-memory systems, possibly equipped with hardware accelerators. A range of workloads, from compute-bound to memory-bound regimes, are employed as proxies for current scientific applications. The low overhead (less than 10 percent) achieved using a variety of workloads enables AL4SAN to be deployed for fast development of task-based numerical algorithms. More interestingly, AL4SAN enables runtime interoperability by switching runtimes at runtime. Blending runtime systems permits to achieve a twofold speedup on a task-based generalized symmetric eigenvalue solver, relative to state-of-the-art implementations. The ultimate goal of AL4SAN is not to create a new runtime, but to strengthen co-design of existing runtimes/applications, while facilitating user productivity and code portability. The code of AL4SAN is freely available at https://github.com/ecrc/al4san, with extensions in progress.","1558-2183","","10.1109/TPDS.2020.2992923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9089317","Task-based programming model;dynamic runtime systems;abstraction layer;API standardization;user productivity;LLVM compiler infrastructure;runtime interoperability","Runtime;Task analysis;Hardware;Engines;Libraries;Programming;Productivity","application program interfaces;distributed memory systems;eigenvalues and eigenfunctions;natural sciences computing;open systems;program compilers;scheduling","task-based engines;task-based runtime engines;dynamic runtime systems;compiler technology;task-based numerical algorithms;runtime interoperability;blending runtime systems;task-based generalized symmetric eigenvalue solver;back-end runtimes;AL4SAN;front-end API","",1.0,"",53.0,"IEEE","7 May 2020","","","IEEE","IEEE Journals"
"Efficient SSD Cache for Cloud Block Storage via Leveraging Block Reuse Distances","K. Zhou; Y. Zhang; P. Huang; H. Wang; Y. Ji; B. Cheng; Y. Liu","Wuhan National Laboratory for Optoelectronics and School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics and School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics and School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics and School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Tencent Corporation, Shenzhen, China; Tencent Corporation, Shenzhen, China; Tencent Corporation, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","22 May 2020",2020,31.0,11.0,2496,2509,"Solid State Drives (SSDs) are popularly used for caching in large scale cloud storage systems nowadays. Traditionally, most cache algorithms make replacement upon each miss when cache space is full. However, we observe that in a typical Cloud Block Storage (CBS) system, there is a great percentage of blocks with large reuse distances, which would result in large number of blocks being evicted out of the cache before they ever have a chance to be referenced while they are cached, significantly jeopardizing the cache efficiency. In this article, we propose LEA, Lazy Eviction cache Algorithm, for cloud block storage to efficiently remedy the cache inefficiencies caused by cache blocks with large reuse distances. LEA mainly employs two lists, Lazy Eviction List (LEL) and Block Identity List (BIL), which keep track of two types of victim blocks respectively based on their cache duration when replacements occur, to improve cache efficiency. When a cache miss happens, if the victim block has not resided in cache for longer than its reuse distance, LEA inserts the missed block identity into BIL. Otherwise, it inserts the missed block entry into LEL. We have evaluated LEA by using IO traces collected from Tencent, one of the largest network service providers in the world, and several open source traces. Experimental results show that LEA not only outperforms most of the state-of-the-art cache algorithms in hit ratio, but also greatly reduces the number of SSD writes.","1558-2183","","10.1109/TPDS.2020.2994075","National Natural Science Foundation of China(grant numbers:61821003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091319","Cloud block storage;cache algorithm;SSD;reuse distance","Cloud computing;Servers;Routing;Distributed databases;Indexes;Virtual machine monitors;Reliability","cache storage;cloud computing","missed block entry;LEA;cache algorithms;efficient SSD cache;solid state drives;cache space;cache efficiency;lazy eviction cache algorithm;cache inefficiencies;cache blocks;victim block;cache duration;cache miss;missed block identity;typical cloud block storage system;large scale cloud storage systems;block reuse distances leveraging;IO;SSDs;CBS","",3.0,"",53.0,"IEEE","11 May 2020","","","IEEE","IEEE Journals"
"High Performance Simulation of Spiking Neural Network on GPGPUs","P. Qu; Y. Zhang; X. Fei; W. Zheng","Beijing National Research Center for Information Science and Technology, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 May 2020",2020,31.0,11.0,2510,2523,"Spiking neural network (SNN) is the most commonly used computational model for neuroscience and neuromorphic computing communities. It provides more biological reality and possesses the potential to achieve high computational power and energy efficiency. Because existing SNN simulation frameworks on general-purpose graphics processing units (GPGPUs) do not fully consider the biological oriented properties of SNNs, like spike-driven, activity sparsity, etc., they suffer from insufficient parallelism exploration, irregular memory access, and load imbalance. In this article, we propose specific optimization methods to speed up the SNN simulation on GPGPU. First, we propose a fine-grained network representation as a flexible and compact intermediate representation (IR) for SNNs. Second, we propose the cross-population/-projection parallelism exploration to make full use of GPGPU resources. Third, sparsity aware load balance is proposed to deal with the activity sparsity. Finally, we further provide dedicated optimization to support multiple GPGPUs. Accordingly, BSim, a code generation framework for high-performance simulation of SNN on GPGPUs is also proposed. Tests show that, compared to a state-of-the-art GPU-based SNN simulator GeNN, BSim achieves 1.41× - 9.33× speedup for SNNs with different configurations; it outperforms other simulators much more.","1558-2183","","10.1109/TPDS.2020.2994123","Beijing Academy of Artificial Intelligence(grant numbers:BAAI2019ZD0503); Beijing National Research Center for Information Science and Technology; Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091320","Spiking neural network;SNN simulation;GPGPU;load balance;computational neuroscience","Neurons;Computational modeling;Synapses;Biological system modeling;Parallel processing;Biological neural networks;Mathematical model","computer graphic equipment;coprocessors;graphics processing units;multiprocessing systems;neural nets;optimisation;parallel architectures;resource allocation","high performance simulation;spiking neural network;neuromorphic computing communities;biological reality;high computational power;energy efficiency;SNN simulation frameworks;general-purpose graphics;biological oriented properties;spike-driven;activity sparsity;insufficient parallelism exploration;irregular memory access;load imbalance;specific optimization methods;fine-grained network representation;flexible representation;compact intermediate representation;sparsity aware load balance;multiple GPGPUs;high-performance simulation;state-of-the-art GPU-based SNN simulator GeNN;computational model","",9.0,"",51.0,"IEEE","11 May 2020","","","IEEE","IEEE Journals"
"Towards Fair and Privacy-Preserving Federated Deep Models","L. Lyu; J. Yu; K. Nandakumar; Y. Li; X. Ma; J. Jin; H. Yu; K. S. Ng","Department of Computer Science, National University of Singapore, Singapore; Faculty of Information Technology, Monash University, Clayton, Australia; IBM Singapore Lab, Singapore; School of Computing and Information Systems, The University of Melbourne, Melbourne, Australia; School of Computing and Information Systems, The University of Melbourne, Melbourne, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Software Innovation Institute, Australian National University, Canberra, Australia","IEEE Transactions on Parallel and Distributed Systems","29 May 2020",2020,31.0,11.0,2524,2541,"The current standalone deep learning framework tends to result in overfitting and low utility. This problem can be addressed by either a centralized framework that deploys a central server to train a global model on the joint data from all parties, or a distributed framework that leverages a parameter server to aggregate local model updates. Server-based solutions are prone to the problem of a single-point-of-failure. In this respect, collaborative learning frameworks, such as federated learning (FL), are more robust. Existing federated learning frameworks overlook an important aspect of participation: fairness. All parties are given the same final model without regard to their contributions. To address these issues, we propose a decentralized Fair and Privacy-Preserving Deep Learning (FPPDL) framework to incorporate fairness into federated deep learning models. In particular, we design a local credibility mutual evaluation mechanism to guarantee fairness, and a three-layer onion-style encryption scheme to guarantee both accuracy and privacy. Different from existing FL paradigm, under FPPDL, each participant receives a different version of the FL model with performance commensurate with his contributions. Experiments on benchmark datasets demonstrate that FPPDL balances fairness, privacy and accuracy. It enables federated learning ecosystems to detect and isolate low-contribution parties, thereby promoting responsible participation.","1558-2183","","10.1109/TPDS.2020.2996273","IBM PhD Fellowship; ANU Translational Fellowship; Nanyang Assistant Professorship; NTU-WeBank JRI(grant numbers:NWJ-2019-007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098045","Federated learning;privacy-preserving;deep learning;fairness;encryption","Machine learning;Biological system modeling;Data models;Collaboration;Servers;Privacy;Computational modeling","data privacy;groupware;learning (artificial intelligence)","central server;global model;joint data;distributed framework;parameter server;local model updates;server-based solutions;single-point-of-failure;collaborative learning frameworks;federated learning frameworks;federated deep learning models;local credibility mutual evaluation mechanism;three-layer onion-style encryption scheme;FL paradigm;FL model;federated learning ecosystems;low-contribution parties;deep learning framework;overfitting utility;low utility;centralized framework;privacy-preserving federated deep models;FPPDL","",70.0,"",44.0,"IEEE","21 May 2020","","","IEEE","IEEE Journals"
"Efficient Parallelism of Post-Quantum Signature Scheme SPHINCS","S. Sun; R. Zhang; H. Ma","School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","29 May 2020",2020,31.0,11.0,2542,2555,"SPHINCS was recently proposed as a stateless, quantum-resilient hash-based signature scheme. However, one possible limitation of SPHINCS is its signing speed, namely, the best known implementation merely produces a few hundred of signatures per second, which is not good enough, e.g., for a social website with a huge amount of users. Aiming at improving the singing throughput, we present highly parallel and optimized implementations of SPHINCS, which can be deployed on various multi-core platforms. As a first step, we give an elementary implementation on ×86/64 processors, which proves the effectiveness and correctness of our implementations. To obtain a significantly higherthroughput, we implement SPHINCS on Graphics Processing Units (GPUs). Furthermore, we develop a few general and hardware-specific techniques to take full advantage of the computing power of targeted platforms. We instantiate the underlying hash functions with three primitives. Our comprehensive benchmark shows that our work outperforms all the state-of-the-art implementations of SPHINCS regarding throughput with reasonable latency, and has scalability on multiple cores and multiple GPU cards. For instance, forthe key generation algorithm instantiated with ChaCha running on a GeForce GTX 1080, we obtain 5152 signatures per second which is 7.88x speedup fasterthan a recent FPGA implementation. When upgrade to TITAN Xp, 6,651 signatures are generated in one second. With four TITAN Xp GPUs, the obtained throughput satisfies vast majority scenarios.","1558-2183","","10.1109/TPDS.2020.2995562","National Natural Science Foundation of China(grant numbers:61772520,61802392,61972094,61472416,61632020); Key Research and Development Project of Zhejiang Province(grant numbers:2017C01062,2020C01078); Beijing Municipal Science and Technology Commission(grant numbers:Z191100007119007,Z191100007119002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9095410","Post-quantum cryptography;parallel computation;stateless hash-based signature schemes;SPHINCS;multi-core platforms","Throughput;Graphics processing units;Multicore processing;Field programmable gate arrays;Cryptography","cryptography;digital signatures;graphics processing units","recent FPGA implementation;state-of-the-art implementations;multicore platforms;known implementation;quantum-resilient hash-based signature scheme;post-quantum signature scheme SPHINCS;efficient parallelism","",10.0,"",55.0,"IEEE","18 May 2020","","","IEEE","IEEE Journals"
"Phase-Aware Cache Partitioning to Target Both Turnaround Time and System Performance","L. Pons; J. Sahuquillo; V. Selfa; S. Petit; J. Pons","Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain; Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain; Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain; Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain; Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain","IEEE Transactions on Parallel and Distributed Systems","29 May 2020",2020,31.0,11.0,2556,2568,"The Last Level Cache (LLC) plays a key role in the system performance of current multi-cores by reducing the number of long latency main memory accesses. The inter-application interference at this shared resource, however, can lead the system to undesired situations regarding performance and fairness. Recent approaches have successfully addressed fairness and turnaround time (TT) in commercial processors. Nevertheless, these approaches must face sustaining system performance, which is challenging. This work makes two main contributions. LLC behaviors regarding cache performance, data reuse and cache occupancy, that adversely impact on the final performance are identified. Second, based on these behaviors, we propose the Critical-Phase Aware Partitioning Approach (CPA), which reduces TT while sustaining (and even improving) IPC by making an effective use of the LLC space. Experimental results show that CPA outperforms CA, Dunn and KPart state-of-the-art approaches, and improves TT (over 40 percent in some workloads) over Linux default behavior while sustaining or even improving IPC by more than 3 percent in several mixes.","1558-2183","","10.1109/TPDS.2020.2996031","Ministerio de Ciencia, Innovación y Universidades; European ERDF(grant numbers:RTI2018-098156-B-C51); Generalitat Valenciana(grant numbers:AICO/2019/317); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097450","Cache memories;multi-core multiprocessors;memory structures;memory hierarchy;performance","System performance;Interference;Throughput;Measurement;Heuristic algorithms;Program processors;Multicore processing","cache storage;Linux;microprocessor chips;multiprocessing systems","IPC;Linux default behavior;LLC space;CPA;critical-phase aware partitioning approach;commercial processors;interapplication interference;phase-aware cache partitioning;cache occupancy;data reuse;cache performance;sustaining system performance;TT;shared resource;long latency main memory accesses;current multicores;last level cache;turnaround time","",8.0,"",27.0,"IEEE","20 May 2020","","","IEEE","IEEE Journals"
"Errata to “On-Edge Multi-Task Transfer Learning: Model and Practice With Data-Driven Task Allocation”","Q. Chen; Z. Zheng; C. Hu; D. Wang; F. Liu","Key Laboratory of Services Computing Technology and System, Ministry of Education, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Edge Cloud Innovation Lab, Technical Innovation Department, Cloud BU, Huawei Technologies Company, Ltd., Shenzhen, China; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong; Key Laboratory of Services Computing Technology and System, Ministry of Education, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","2 Jun 2020",2020,31.0,11.0,2569,2569,"Presents corrections to author information for the above named paper.","1558-2183","","10.1109/TPDS.2020.2997321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106908","","Task analysis;Resource management;Technological innovation;Computational modeling;Big Data;Service computing;Computer science","","","","","",1.0,"IEEE","2 Jun 2020","","","IEEE","IEEE Journals"
"MEMPHA: Model of Exascale Message-Passing Programs on Heterogeneous Architectures","S. Z. Koohi; N. A. W. A. Hamid; M. Othman; G. Ibragimov","Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Universiti Putra Malaysia, Seri Kembangan, Malaysia; Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Institute for Mathematical Research, Universiti Putra Malaysia, Seri Kembangan, Malaysia; Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Institute for Mathematical Research, Universiti Putra Malaysia, Seri Kembangan, Malaysia; Department of Mathematics, Faculty of Science, Universiti Putra Malaysia, Seri Kembangan, Malaysia","IEEE Transactions on Parallel and Distributed Systems","2 Jun 2020",2020,31.0,11.0,2570,2581,"Delivering optimum performance on a parallel computer is highly dependant on the efficiency of the scheduling and mapping procedure. If the composition of the parallel application is known a prior, the mapping can be accomplished statically on the compilation time. The mapping algorithm uses the model of the parallel application and maps its tasks to processors in a way to minimize the total execution time. In this article, current modeling approaches have discussed. Later, a new modeling schema named Model of Exascale Message-Passing Programs on Heterogeneous Architectures (MEMPHA) has proposed. A comparative study has been performed between MEMPHA and existing models. To exhibit the efficiency of the MEMPHA, experiments have performed on a set of data-set hypergraphs. The results obtained from the experiments show that deploying the MEMPHA helps to optimize metrics, including the congestion, total communication volume and maximum volume of data being sent or received. These improvements vary from 76 to 1 percent, depending on the metric and benchmark model. Moreover, MEMPHA supports the modeling of applications with multiple producers for a single data transmission, where the rest of the approaches fail.","1558-2183","","10.1109/TPDS.2020.2995867","Universiti Putra Malaysia(grant numbers:GP/2017/9569600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096589","Parallel models;scheduling and task partitioning;heterogeneous (hybrid) systems;modelling and prediction","Task analysis;Computational modeling;Mathematical model;Computer architecture;Program processors;Parallel processing;Measurement","graph theory;message passing;optimisation;parallel architectures;scheduling","modeling schema;MEMPHA;data-set hypergraphs;parallel computer;parallel application;compilation time;mapping algorithm;total execution time minimization;model of exascale message-passing programs on heterogeneous architectures;single data transmission;metrics optimization","",1.0,"",41.0,"IEEE","19 May 2020","","","IEEE","IEEE Journals"
"Correlation of Performance Optimizations and Energy Consumption for Stencil-Based Application on Intel Xeon Scalable Processors","L. Szustak; R. Wyrzykowski; T. Olas; V. Mele","Department of Computer and Information Science, Czestochowa University of Technology, Częstochowa, Poland; Department of Computer and Information Science, Czestochowa University of Technology, Częstochowa, Poland; Department of Computer and Information Science, Czestochowa University of Technology, Częstochowa, Poland; Department of Mathematics and Applications R. Caccioppoli, University of Naples Federico II, Napoli, Italy","IEEE Transactions on Parallel and Distributed Systems","10 Jun 2020",2020,31.0,11.0,2582,2593,"This article provides a comprehensive study of the impact of performance optimizations on the energy efficiency of a real-world CFD application called MPDATA, as well as an insightful analysis of performance-energy interaction of these optimizations with the underlying hardware that represents the first generation of Intel Xeon Scalable processors. Considering the MPDATA iterative application as a use case, we explore the fundamentals of energy and performance analysis for a memory-bound application when exposed to a set of optimization steps that increase the application performance, by improving the operational intensity of code and utilizing resources more efficiently. It is shown that for memory-bound applications, optimizing toward high performance could be a powerful strategy for improving the energy efficiency as well. In fact, for the considered performance optimizations, the energy gain is correlated with the performance gain but with varying degrees. As a result, these optimizations allow improving both performance and energy consumption radically, up to about 10.9 and 8.8 times, respectively. The impact of the Intel AVX-512 SIMD extension on the energy consumption and performance is demonstrated. Also, we discover limitations on the usability of CPU frequency scaling as a tool for balancing energy savings with admissible performance losses.","1558-2183","","10.1109/TPDS.2020.2996314","National Science Centre, Poland(grant numbers:UMO-2017/26/D/ST6/00687,UMO-2015/17/D/ST6/04059); Polish Minister of Science and Higher Education(grant numbers:020/RID/2018/19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103117","CFD;MPDATA;intel xeon scalable;performance-energy trade-off;yokogawa WT310;RAPL","Optimization;Energy consumption;Hardware;Program processors;Platinum;Energy measurement","computational fluid dynamics;energy conservation;energy consumption;iterative methods;multiprocessing systems;optimisation;parallel processing;power aware computing","CPU frequency scaling;application performance;optimization steps;performance analysis;performance-energy interaction;insightful analysis;MPDATA;real-world CFD application;Intel Xeon scalable processors;stencil-based application;admissible performance losses;energy savings;Intel AVX-512 SIMD extension;energy consumption;performance gain;energy gain;performance optimizations;energy efficiency;memory-bound application;utilizing resources","",9.0,"",43.0,"CCBY","28 May 2020","","","IEEE","IEEE Journals"
"Generalized Cost-Based Job Scheduling in Very Large Heterogeneous Cluster Systems","W. R. KhudaBukhsh; S. Kar; B. Alt; A. Rizk; H. Koeppl","Mathematical Biosciences Institute, The Ohio State University, Columbus, USA; Multimedia Communications Lab (KOM), Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany; Bioinspired Communication Systems Lab (BCS), Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany; Institute of Measurement, Control and Microtechnology, Universität Ulm, Ulm, Germany; Bioinspired Communication Systems Lab (BCS), Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany","IEEE Transactions on Parallel and Distributed Systems","10 Jun 2020",2020,31.0,11.0,2594,2604,"We study job assignment in large, heterogeneous resource-sharing clusters of servers with finite buffers. This load balancing problem arises naturally in today's communication and big data systems, such as Amazon Web Services, Network Service Function Chains, and Stream Processing. Arriving jobs are dispatched to a server, following a load balancing policy that optimizes a performance criterion such as job completion time. Our contribution is a randomized Cost-Based Scheduling (CBS) policy in which the job assignment is driven by general cost functions of the server queue lengths. Beyond existing schemes, such as the Join the Shortest Queue (JSQ), the power of d or the SQ(d) and the capacity-weighted JSQ, the notion of CBS yields new application-specific policies such as hybrid locally uniform JSQ. As today's data center clusters have thousands of servers, exact analysis of CBS policies is tedious. In this article, we derive a scaling limit when the number of servers grows large, facilitating a comparison of various CBS policies with respect to their transient as well as steady state behavior. A byproduct of our derivations is the relationship between the queue filling proportions and the server buffer sizes, which cannot be obtained from infinite buffer models. Finally, we provide extensive numerical evaluations and discuss several applications including multi-stage systems.","1558-2183","","10.1109/TPDS.2020.2997771","Deutsche Forschungsgemeinschaft; Ohio State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9099971","Job scheduling;performance evaluation;mean-field limit","Scheduling;Load management;Load modeling;Cost function;Data centers;Performance evaluation","computer centres;Internet;network servers;queueing theory;resource allocation;telecommunication scheduling;Web services","join the shortest queue;heterogeneous resource-sharing clusters;very large heterogeneous cluster systems;multistage systems;infinite buffer models;server buffer sizes;queue filling proportions;CBS policies;data center clusters;hybrid locally uniform JSQ;application-specific policies;capacity-weighted JSQ;server queue lengths;general cost functions;randomized Cost-Based Scheduling policy;job completion time;load balancing policy;Stream Processing;Network Service Function Chains;Amazon Web Services;Big Data systems;load balancing problem;job assignment;generalized Cost-Based job Scheduling","",4.0,"",43.0,"IEEE","26 May 2020","","","IEEE","IEEE Journals"
"Towards Usable Cloud Storage Auditing","F. Chen; F. Meng; T. Xiang; H. Dai; J. Li; J. Qin","Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer Science and Engineering, Shenzhen University, Shenzhen, China; College of Computer Science, Chongqing University, Chongqing, China; School of Computer Science and Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer Science and Engineering, Shenzhen University, Shenzhen, China; Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","11 Jun 2020",2020,31.0,11.0,2605,2617,"Cloud storage security has gained considerable research efforts with the wide adoption of cloud computing. As a security mechanism, researchers have been investigating cloud storage auditing schemes that enable a user to verify whether the cloud keeps the user's outsourced data undamaged. However, existing schemes have usability issues in compatibility with existing real world cloud storage applications, error-tolerance, and efficiency. To mitigate this usability gap, this article proposes a new general cloud storage auditing scheme that is more usable. The proposed scheme uses the idea of integrating linear error correcting codes and linear homomorphic authentication schemes together. This integration uses only one additional block to achieve error tolerance and authentication simultaneously. To demonstrate the power of the general construction, we also propose one detailed scheme based on the proposed general construction using the Reed Solomon code and the universal hash based MAC authentication scheme, both of which are implemented over the computation-efficient Galois field $\mathrm {GF}{(2^8)}$ GF (28). We also show that the proposed scheme is secure under the standard definition. Moreover, we implemented and open-sourced the proposed scheme. Experimental results show that the proposed scheme is orders of magnitude more efficient than the state-of-the-art scheme.","1558-2183","","10.1109/TPDS.2020.2998462","National Natural Science Foundation of China(grant numbers:61872243,61872197,61672118,61672358,U1713212); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2020A151501489); Science and Technology Plan Projects of Shenzhen(grant numbers:JCYJ20180305124126741); Jiangsu Key Laboratory of Big Data Security & Intelligent Processing NJUPT; Hong Kong Polytechnic University(grant numbers:YBZE); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103614","Cloud storage;integrity checking;usability;homomorphic authentication;error correction code","Cloud computing;Authentication;Error correction codes;Usability;Indexes;Encoding","cloud computing;cryptography;data integrity;error correction codes;Galois fields;Reed-Solomon codes;storage management","usable cloud storage auditing;computation-efficient Galois field GF;MAC authentication scheme;error tolerance;linear homomorphic authentication schemes;linear error correcting codes;general cloud storage;usability gap;error-tolerance;world cloud storage applications;usability issues;cloud storage auditing schemes;security mechanism;cloud computing;research efforts;cloud storage security","",13.0,"",45.0,"IEEE","29 May 2020","","","IEEE","IEEE Journals"
"Comment on “A Tag Encoding Scheme Against Pollution Attack to Linear Network Coding”","J. Chang; B. Shao; Y. Ji; G. Bian","School of Information and Control Engineering, Xi'An University of Architecture and Technology, Xi'An, P.R. China; School of Management, Xi'An University of Architecture and Technology, Xi'An, P.R. China; School of Management, Xi'An University of Architecture and Technology, Xi'An, P.R. China; School of Information and Control Engineering, Xi'An University of Architecture and Technology, Xi'An, P.R. China","IEEE Transactions on Parallel and Distributed Systems","11 Jun 2020",2020,31.0,11.0,2618,2619,"In 2014, Wu et al. proposed a tag encoding scheme, named KEPTE, to protect network coding against pollution attack. They also carefully analyzed the security of KEPTE based on the transmission of a data file through their key-pre-distributed network. In this article, we point out that their security analysis only holds for single data file transmitted in this network. If multiple files are multicasted though it, then any adversary may completely recover source node's signing key. A concrete example says that, after pre-distributing 90 keys to all the nodes in the network, it only allows to securely transmit (at most) 3 data files. More importantly, this scheme is completely insecure in standard security model for network model since the adversary is allowed to make polynomial times queries on any data files of its choice before outputting its final forgery. Finally, we also propose a twisted KEPTE scheme that is secure against any eavesdropping adversary no matter how many data files it has queried.","1558-2183","","10.1109/TPDS.2020.2999523","National Natural Science Foundation of China(grant numbers:61672416,61672059,61872284); Natural Science Research in Shaanxi(grant numbers:2018JM6105,2019JM118); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106826","Network coding;tag encoding;pollution attack;KEPTE-scheme","Security;Network coding;Encoding;Eavesdropping;Pollution;Mathematical model;Data models","network coding;polynomials;public key cryptography;telecommunication security","tag encoding scheme;pollution attack;polynomial times;data file transmission;twisted KEPTE scheme;standard security model;source node;security analysis;key-pre-distributed network;linear network coding","",6.0,"",5.0,"IEEE","2 Jun 2020","","","IEEE","IEEE Journals"
"Time-Optimal Leader Election in Population Protocols","Y. Sudo; F. Ooshita; T. Izumi; H. Kakugawa; T. Masuzawa","Osaka University, Osaka, Japan; Nara Institute of Science and Technology, Ikoma, Japan; Nagoya Institute of Technology, Nagoya, Japan; Ryukoku University, Shiga, Japan; Osaka University, Osaka, Japan","IEEE Transactions on Parallel and Distributed Systems","11 Jun 2020",2020,31.0,11.0,2620,2632,"In this article, we present the first leader election protocol in the population protocol model that stabilizes within O(logn) parallel time in expectation with O(logn) states per agent, where n is the number of agents. Given a rough knowledge m of lg n such that m ≥ lg n and m = O(logn), the proposed protocol guarantees that exactly one leader is elected and the unique leader is kept forever thereafter. This protocol is time-optimal because it was recently proven that any leader election protocol requires Ω(logn) parallel time.","1558-2183","","10.1109/TPDS.2020.2991771","JSPS KAKENHI(grant numbers:17K19977,18K11167,18K18000,19H04085,19K11826,20H04140); JST SICORP(grant numbers:JPMJSC1606); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086071","Population protocols;leader election;stabilization time","Protocols;Voting;Sociology;Statistics;Phase locked loops;Schedules;Epidemics","computational complexity;knowledge engineering;protocols","time-optimal protocol;rough knowledge;first leader election protocol;population protocols;time-optimal leader election","",8.0,"",19.0,"CCBY","4 May 2020","","","IEEE","IEEE Journals"
"QWEB: High-Performance Event-Driven Web Architecture With QAT Acceleration","J. Li; X. Hu; D. Qian; C. Wei; G. McFadden; B. Will; P. Yu; W. Li; H. Guan","Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","16 Jun 2020",2020,31.0,11.0,2633,2649,"Hardware accelerators have been a promising solution to reduce the cost of cloud datacenters. This article investigates the acceleration of an important datacenter workload: the web server (or proxy) that faces high computational consumption originated from SSL/TLS processing and HTTP compression. Our study reveals that for the widely-deployed event-driven web architecture, the straight offloading of SSL/TLS or compression tasks suffers from frequent blockings in the offload I/O, leading to the underutilization of both CPU and accelerator resources. To achieve efficient acceleration, we propose QWEB, a comprehensive offload solution based on Intel QuickAssist Technology (QAT). QWEB introduces an asynchronous offload mode for SSL/TLS processing and a pipelining offload mode for HTTP compression, both allowing concurrent offload tasks from a single application process/thread. With these two novel offload modes, the blocking penalty is amortized or even eliminated, and the utilization rate of the parallel computation engines inside the QAT accelerator is greatly increased. The evaluation shows that QWEB provides up to 9x handshake performance with TLS-RSA (2048-bit) over the software baseline. Additionally, the secure data transfer throughput is enhanced by 2x for the SSL/TLS offloading only, 3.5x for the compression offloading only and 5x for the combined offloading.","1558-2183","","10.1109/TPDS.2020.2999353","National Natural Science Foundation of China(grant numbers:61972245); National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000502); National Science Fund for Distinguished Young Scholars(grant numbers:61525204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106846","Accelerator;event-driven web architecture;SSL/TLS;HTTP compression;Offload I/O;concurrency","Computer architecture;Service-oriented architecture;Servers;Elliptic curve cryptography;Pipeline processing;Acceleration","cloud computing;data compression;Internet;security of data;Web services","data transfer security;TLS-RSA;handshake performance;concurrent offload tasks;Intel QuickAssist technology;computational consumption;Web server;datacenter workload;cloud datacenters;hardware accelerators;high-performance event-driven web architecture;QAT accelerator;parallel computation engines;HTTP compression;asynchronous offload mode;QWEB;frequent blockings;compression tasks","",2.0,"",77.0,"IEEE","2 Jun 2020","","","IEEE","IEEE Journals"
"Cooperative Memory Expansion via OS Kernel Support for Networked Computing Systems","P. Srinuan; X. Yuan; N. -F. Tzeng","School of Computing and Informatics, University of Louisiana, Lafayette, USA; School of Computing and Informatics, University of Louisiana, Lafayette, USA; School of Computing and Informatics, University of Louisiana, Lafayette, USA","IEEE Transactions on Parallel and Distributed Systems","17 Jun 2020",2020,31.0,11.0,2650,2667,"The growing popularity of in-memory computing for bigdata analytics often causes performance bottlenecks to memory subsystem resided in operating systems (OS). This article purposes cooperative memory expansion (COMEX), an OS kernel extension. COMEX establishes a stable pool of memory collectively across nodes in a cluster and enhances OS's memory subsystem for memory aggregation from connected machines by allowing process's page table to track remote memory page frames without programmer effort or modifications to application codes. COMEX employs Remote Direct Memory Access (RDMA) for low-latency data transfer with destination kernel bypassed and does not rely on an old design of the I/O block subsystem usually adopted by all known remote paging. COMEX fits soundly in the emerging system design approach of resource disaggregation which breaks hard walls between server-centric machines into a new design paradigm of separated resource pools. The new architecture facilitates both system scaling-up and scaling-out, also eliminates imbalance resources existing in datacenters. We have implemented COMEX based on Linux kernel 3.10.87 and deployed on our 32 networked servers. Performance evaluation results under ten applications from two benchmark suites reveal the speedup of up to 170 times when application execution footprints are 10 times larger than available system memory.","1558-2183","","10.1109/TPDS.2020.2999507","National Science Foundation(grant numbers:CCF-1423302,CNS-1527051,III-1763620); Louisiana Board of Regents(grant numbers:LEQSF(2018-21)-RD-A-24); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9107499","I/O block devices;memory management;networked computer systems;operating systems (OS);page tables;remote direct memory access (RDMA)","Memory management;Kernel;Servers;Random access memory;Slabs;Performance evaluation","Big Data;computer centres;file organisation;Linux;network servers;operating system kernels;paged storage;performance evaluation;storage management","cooperative memory expansion;RDMA;OS memory subsystem;system memory;networked servers;Linux kernel;server-centric machines;remote paging;Remote Direct Memory Access;remote memory page frames;memory aggregation;OS kernel extension;COMEX;Big Data analytics;in-memory computing;networked computing systems;OS kernel support","",3.0,"",70.0,"CCBY","3 Jun 2020","","","IEEE","IEEE Journals"
"Fully Homomorphic based Privacy-Preserving Distributed Expectation Maximization on Cloud","A. Alabdulatif; I. Khalil; A. Y. Zomaya; Z. Tari; X. Yi","Department of Computer Science, Qassim University, Buraydah, Saudi Arabia; Department of Distributed Systems and Networking, Royal Melbourne Institute of Technology (RMIT) University, Melbourne, Australia; School of Information Technologies, University of Sydney, Camperdown, Australia; School of Computer Science and IT, RMIT University, Melbourne, Australia; School of Computer Science and IT, RMIT University, Melbourne, Australia","IEEE Transactions on Parallel and Distributed Systems","19 Jun 2020",2020,31.0,11.0,2668,2681,"Expectation maximization (EM) is a clustering-based machine learning algorithm that is widely used in many areas of science (e.g., bioinformatics and computer vision) to find maximum likelihood and maximum a posteriori estimates for models with latent variables. To deploy such an algorithm in cloud environments, security and privacy issues need be considered to avoid data breaches or abuses by external malicious parties or even by cloud service providers. However, the processing performance of the EM algorithm poses a challenge in terms of building a secure environment. This article describes an innovative and practical privacy-preserving EM algorithm for cloud systems that addresses this challenge, and estimates the EM parameters in an accurate and secure manner. Fully homomorphic encryption (FHE) is used to ensure the privacy of both the EM algorithm computations and the users' sensitive data in the cloud. A distributed-based approach is also proposed to overcome the overheads of FHE computations and ensure a fast convergence of the EM algorithm. The conducted experiments demonstrate a significant improvement in the convergence time of the distributed EM algorithm, while achieving a high level of accuracy and reducing the associated computational FHE overheads.","1558-2183","","10.1109/TPDS.2020.2999407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106834","Expectation maximization;distributed analytics;data privacy;fully homomorphic encryption;cloud computing","Cloud computing;Data models;Computational modeling;Data privacy;Analytical models;Signal processing algorithms;Clustering algorithms","cloud computing;cryptography;data privacy;expectation-maximisation algorithm;learning (artificial intelligence);maximum likelihood estimation;pattern clustering","privacy issues;computational FHE overheads;clustering-based machine learning;fully homomorphic encryption;cloud service providers;data breaches;maximum likelihood estimation;fully homomorphic based privacy-preserving distributed expectation maximization","",5.0,"",37.0,"IEEE","2 Jun 2020","","","IEEE","IEEE Journals"
"Improving MPI Collective I/O for High Volume Non-Contiguous Requests With Intra-Node Aggregation","Q. Kang; S. Lee; K. Hou; R. Ross; A. Agrawal; A. Choudhary; W. -k. Liao","Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA","IEEE Transactions on Parallel and Distributed Systems","19 Jun 2020",2020,31.0,11.0,2682,2695,"Two-phase I/O is a well-known strategy for implementing collective MPI-IO functions. It redistributes I/O requests among the calling processes into a form that minimizes the file access costs. As modern parallel computers continue to grow into the exascale era, the communication cost of such request redistribution can quickly overwhelm collective I/O performance. This effect has been observed from parallel jobs that run on multiple compute nodes with a high count of MPI processes on each node. To reduce the communication cost, we present a new design for collective I/O by adding an extra communication layer that performs request aggregation among processes within the same compute nodes. This approach can significantly reduce inter-node communication contention when redistributing the I/O requests. We evaluate the performance and compare it with the original two-phase I/O on Cray XC40 parallel computers (Theta and Cori) with Intel KNL and Haswell processors. Using I/O patterns from two large-scale production applications and an I/O benchmark, we show our proposed method effectively reduces the communication cost and hence maintains the scalability for a large number of processes.","1558-2183","","10.1109/TPDS.2020.3000458","Exascale Computing Project(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; DOE Office of Advanced Scientific Computing Research; DOE(grant numbers:DE-SC0014330,DE-SC0019358); DOE Office of Science User Facility(grant numbers:DE-AC02-06CH11357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9109678","Parallel I/O;MPI collective I/O;two-phase I/O;non-contiguous I/O","Performance evaluation;Benchmark testing;Program processors;Libraries;Production;Aggregates;Writing","input-output programs;message passing;parallel processing","Haswell processors;Intel KNL;two-phase I/O;internode communication contention;intranode aggregation;MPI collective I/O;Cray XC40 parallel computers;MPI processes;parallel jobs;request redistribution;collective MPI-IO functions;high volume noncontiguous requests","",3.0,"",39.0,"IEEE","5 Jun 2020","","","IEEE","IEEE Journals"
"Countdown Slack: A Run-Time Library to Reduce Energy Footprint in Large-Scale MPI Applications","D. Cesarini; A. Bartolini; A. Borghesi; C. Cavazzoni; M. Luisier; L. Benini","Department of SuperComputing Applications and Innovation, CINECA, Casalecchio di Reno (BO), Italy; Department of Electrical, Electronic and Information Engineering “Guglielmo Marconi”, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Chief Technology and Innovation Officer, Leonardo S.p.A., Roma, Italy; Department of Information Technology and Electrical Engineering, Swiss Federal Institute of Technology in Zurich, Zurich, Switzerland; Department of Information Technology and Electrical Engineering, Swiss Federal Institute of Technology in Zurich, Zurich, Switzerland","IEEE Transactions on Parallel and Distributed Systems","24 Jun 2020",2020,31.0,11.0,2696,2709,"The power consumption of supercomputers is a major challenge for system owners, users, and society. It limits the capacity of system installations, it requires large cooling infrastructures, and it is the cause of a large carbon footprint. Reducing power during application execution without changing the application source code or increasing time-to-completion is highly desirable in real-life high-performance computing scenarios. The power management run-time frameworks proposed in the last decade are based on the assumption that the duration of communication and application phases in an MPI application can be predicted and used at run-time to trade-off communication slack with power consumption. In this article, we first show that this assumption is too general and leads to mispredictions, slowing down applications, thereby jeopardizing the claimed benefits. We then propose a new approach based on (i) the separation of communication phases and slack during MPI calls and (ii) a timeout algorithm to cope with the hardware power management latency, which jointly makes it possible to achieve performance-neutral power saving in MPI applications without requiring labor-intensive and risky application source code modifications. We validate our approach in a tier-1 production environment with widely adopted scientific applications. Our approach has a time-to-completion overhead lower than 1 percent, while it successfully exploits slack in communication phases to achieve an average energy saving of 10 percent. If we focus on a large-scale application runs, the proposed approach achieves 22 percent energy saving with an overhead of only 0.4 percent. With respect to state-of-the-art approaches, COUNTDOWN Slack is the only that always leads to an energy saving with negligible overhead (<; 3 percent).","1558-2183","","10.1109/TPDS.2020.3000418","EU H2020(grant numbers:857191); EU H2020-INFRAEDI-2018-1 MaX(grant numbers:824143); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9109637","HPC;MPI;DVFS;power management;DVFS;P-states;energy saving;power saving;reactive policy","Power system management;Hardware;Power demand;Cooling;Runtime;Task analysis;Monitoring","application program interfaces;energy conservation;message passing;parallel processing;power aware computing;power consumption","run-time library;COUNTDOWN Slack;time-to-completion overhead;risky application source code modifications;performance-neutral power saving;hardware power management;communication phases;communication slack;MPI application;application phases;power management run-time frameworks;real-life high-performance computing scenarios;application execution;carbon footprint;cooling infrastructures;system installations;system owners;power consumption;large-scale MPI applications;energy footprint reduction;efficiency 22.0 percent;efficiency 0.4 percent;efficiency 3.0 percent;efficiency 1.0 percent;efficiency 10.0 percent","",5.0,"",60.0,"IEEE","5 Jun 2020","","","IEEE","IEEE Journals"
"High-Quality Shared-Memory Graph Partitioning","Y. Akhremtsev; P. Sanders; C. Schulz","Google, Zurich, Switzerland; Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Faculty of Computer Science, University of Vienna, Vienna, Austria","IEEE Transactions on Parallel and Distributed Systems","24 Jun 2020",2020,31.0,11.0,2710,2722,"Partitioning graphs into blocks of roughly equal size such that few edges run between blocks is a frequently needed operation in processing graphs. Recently, size, variety, and structural complexity of these networks has grown dramatically. Unfortunately, previous approaches to parallel graph partitioning have problems in this context since they often show a negative trade-off between speed and quality. We present an approach to multi-level shared-memory parallel graph partitioning that produces balanced solutions, shows high speedups for a variety of large graphs and yields very good quality independently of the number of cores used. For example, in an extensive experimental study, at 79 cores, one of our closest competitors is faster but fails to meet the balance criterion in the majority of cases and another is mostly slower and incurs about 13 percent larger cut size. Important ingredients include parallel label propagation for both coarsening and refinement, parallel initial partitioning, a simple yet effective approach to parallel localized local search, and fast locality preserving hash tables.","1558-2183","","10.1109/TPDS.2020.3001645","DFG(grant numbers:SA 933/10-2,SCHU 2567/1-2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115228","Parallel graph partitioning;shared-memory parallelism;local search;label propagation","Partitioning algorithms;Clustering algorithms;Program processors;Complex networks;Contracts;Parallel algorithms","graph theory;shared memory systems","locality preserving hash tables;parallel localized local search;multilevel shared-memory parallel graph partitioning;high-quality shared-memory graph partitioning;parallel initial partitioning;parallel label propagation","",14.0,"",54.0,"IEEE","11 Jun 2020","","","IEEE","IEEE Journals"
"System Error Prediction for Business Support Systems in Telecommunications Networks","E. -H. Yeh; P. Lin; X. -X. Lin; J. -Y. Jeng; Y. Fang","Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Information System Department General Headquarters, Chunghwa Telecom Company, Ltd., Taipei, Taiwan; Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA","IEEE Transactions on Parallel and Distributed Systems","25 Jun 2020",2020,31.0,11.0,2723,2733,"Reliability and stability have been treated as the major requirements for the Business Support System (BSS) in telecommunications networks. It is crucial and essential for service providers to maintain good operating state of the BSS. In this article, we aim at system error prediction for a BSS, i.e., we predict occurrences of the abnormal state or behavior of the BSS. Because the occurrences of system errors are rare events in the BSS (i.e., the dataset of system status is highly imbalanced), it is highly challenging to use machine learning or deep learning algorithms to predict system error for the BSS. To address this challenge, we propose a machine learning-based framework for the system error prediction and a Frequency-based Feature Creation (FFC) algorithm to create new features to improve prediction. By adding the time-series information created by the existing features, the proposed FFC can amplify the effects of important features. Our experimental results show that the FFC significantly improves the prediction performance for the Random Forest algorithm.","1558-2183","","10.1109/TPDS.2020.3001593","Ministry of Science and Technology, Taiwan(grant numbers:MOST 108-2823-8-002-007-,MOST 107-2221-E-002-042-MY3,MOST 106-2923-E-002-005-MY3); Chunghwa Telecom in Taiwan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115291","System error prediction;business support system;machine learning;telecommunications network","Machine learning algorithms;Monitoring;Prediction algorithms;Databases;Training;Anomaly detection;Business","learning (artificial intelligence);local area networks;random forests;telecommunication network reliability;time series","random forest algorithm;time-series information;FFC algorithm;frequency-based feature creation algorithm;deep learning algorithms;machine learning;system status dataset;service providers;business support system;BSS;telecommunications networks;system error prediction","",1.0,"",44.0,"IEEE","11 Jun 2020","","","IEEE","IEEE Journals"
"A Truthful and Efficient Incentive Mechanism for Demand Response in Green Datacenters","Z. Zhou; F. Liu; S. Chen; Z. Li","Guangdong Key Laboratory of Big Data Analysis and Processing, School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","18 Dec 2019",2020,31.0,1.0,1,15,"Datacenter demand response is envisioned as a promising tool for mitigating operational stability issues faced by smart grids. It enables significant potentials in peak load reduction and facilitates the incorporation of distributed generation. Monetary refund from the smart grid can also alleviate the cloud's burden in escalating electricity cost. However, the current demand response paradigm is inefficient towards incentivizing a cloud service provider (CSP) that operates geo-distributed datacenters. To incentivize CSP participation, this work presents an auction mechanism that enables smart grids to voluntarily submit bids to the CSP to procure diverse amounts of demand response with different payments. To maximize the social welfare of the auction, the CSP that acts as the auctioneer needs to solve the winner determination problem at large-scale. By applying the proximal Jacobian alternating direction method of multipliers, we propose a distributed algorithm for each datacenter to solve a small-scale problem in a parallel fashion. Desirable properties of the proposed auction, such as social welfare maximization and truthfulness are achieved through Vickrey-Clarke-Groves (VCG) payment. Through extensive evaluations based on real datacenter workload traces and IEEE 14-bus test systems, we demonstrate that our incentive mechanism constitutes a win-win mechanism for both the geo-distributed cloud and the smart grid.","1558-2183","","10.1109/TPDS.2018.2882174","National Key Research & Development (R&D)(grant numbers:2017YFB1001703); National Natural Science Foundation of China(grant numbers:61722206,61802449,61761136014 (NSFC-DFG),61520106005); Fundamental Research Funds for the Central Universities(grant numbers:2017KFKJXX009,2017LGJC40); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540414","Geo-distributed datacenters;smart grid;demand response;incentive mechanism;distributed algorithm","Load management;Smart grids;Power demand;Cloud computing;Economics;Electric potential;Distributed power generation","cloud computing;computer centres;demand side management;distributed power generation;green computing;power engineering computing;smart power grids","VCG payment;Vickrey-Clarke-Groves payment;geo-distributed cloud;win-win mechanism;datacenter workload traces;truthfulness;social welfare maximization;distributed algorithm;auctioneer;auction mechanism;CSP participation;geo-distributed datacenters;cloud service provider;current demand response paradigm;distributed generation;peak load reduction;smart grid;operational stability issues;datacenter demand response;green datacenters;efficient incentive mechanism;truthful incentive mechanism","",23.0,"",42.0,"IEEE","18 Nov 2018","","","IEEE","IEEE Journals"
"Adaptive Alert Management for Balancing Optimal Performance among Distributed CSOCs using Reinforcement Learning","A. Shah; R. Ganesan; S. Jajodia; P. Samarati; H. Cam","Center for Secure Information Systems, George Mason University, Fairfax, USA; Center for Secure Information Systems, George Mason University, Fairfax, USA; Center for Secure Information Systems, George Mason University, Fairfax, USA; Computer Science Department, Università degli Studi di Milano, Milan, Italy; U.S. Army Research Laboratory, Adelphi, USA","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,16,33,"Large organizations typically have Cybersecurity Operations Centers (CSOCs) distributed at multiple locations that are independently managed, and they have their own cybersecurity analyst workforce. Under normal operating conditions, the CSOC locations are ideally staffed such that the alerts generated from the sensors in a work-shift are thoroughly investigated by the scheduled analysts in a timely manner. Unfortunately, when adverse events such as increase in alert arrival rates or alert investigation rates occur, alerts have to wait for a longer duration for analyst investigation, which poses a direct risk to organizations. Hence, our research objective is to mitigate the impact of the adverse events by dynamically and autonomously re-allocating alerts to other location(s) such that the performances of all the CSOC locations remain balanced. This is achieved through the development of a novel centralized adaptive decision support system whose task is to re-allocate alerts from the affected locations to other locations. This re-allocation decision is non-trivial because the following must be determined: (1) timing of a re-allocation decision, (2) number of alerts to be reallocated, and (3) selection of the locations to which the alerts must be distributed. The centralized decision-maker (henceforth referred to as agent) continuously monitors and controls the level of operational effectiveness-LOE (a quantified performance metric) of all the locations. The agent's decision-making framework is based on the principles of stochastic dynamic programming and is solved using reinforcement learning (RL). In the experiments, the RL approach is compared with both rule-based and load balancing strategies. By simulating real-world scenarios, learning the best decisions for the agent, and applying the decisions on sample realizations of the CSOC's daily operation, the results show that the RL agent outperforms both approaches by generating (near-) optimal decisions that maintain a balanced LOE among the CSOC locations. Furthermore, the scalability experiments highlight the practicality of adapting the method to a large number of CSOC locations.","1558-2183","","10.1109/TPDS.2019.2927977","Army Research Office(grant numbers:W911NF-13-1-0421,W911NF-15-1-0576); Office of Naval Research(grant numbers:N00014-15-1-2007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762232","Distributed cybersecurity operations center (CSOC);centralized alert management;level of operational effectiveness;reinforcement learning;adaptive resource allocation","Measurement;Organizations;Computer security;Monitoring;Sensors;Reinforcement learning;Decision making","decision making;decision support systems;dynamic programming;learning (artificial intelligence);resource allocation;scheduling;security of data;stochastic processes","CSOC locations;adaptive decision support system;affected locations;re-allocation decision;centralized decision-maker;reinforcement learning;CSOC's daily operation;optimal decisions;adaptive alert management;balancing optimal performance;distributed CSOCs;Cybersecurity Operations Centers;cybersecurity analyst workforce;normal operating conditions;alert arrival rates;alert investigation rates","",1.0,"",41.0,"IEEE","15 Jul 2019","","","IEEE","IEEE Journals"
"Deep Learning Research and Development Platform: Characterizing and Scheduling with QoS Guarantees on GPU Clusters","Z. Chen; W. Quan; M. Wen; J. Fang; J. Yu; C. Zhang; L. Luo","Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,34,50,"Deep learning (DL) has been widely adopted in various domains of artificial intelligence (AI), achieving dramatic developments in industry and academia. Besides giant AI companies, numerous small and medium-sized enterprises, institutes, and universities (EIUs) have focused on the research and development (R&D) of DL. Considering the high cost of datacenters and high performance computing (HPC) systems, EIUs prefer adopting off-the-shelf GPU clusters as a DL R&D platform for multiple users and developers to process diverse DL workloads. In such scenarios, the scheduling of multiple DL tasks on a shared GPU cluster is both significant and challenging in terms of efficiently utilizing limited resources. Existing schedulers cannot predict the resource requirements of diverse DL workloads, leading to the under-utilization of computing resources and a decline in user satisfaction. This paper proposes GENIE, a QoS-aware dynamic scheduling framework for a shared GPU cluster, which achieves users' QoS guarantee and high system utilization. In accordance with an exhaustive characterization, GENIE analyzes the key factors that affect the performance of DL tasks and proposes a prediction model derived from lightweight profiling to estimate the processing rate and response latency for diverse DL workloads. Based on the prediction models, we propose a QoS-aware scheduling algorithm to identify the best placements for DL tasks and schedule them on the shared cluster. Experiments on a GPU cluster and large-scale simulations demonstrate that GENIE achieves a QoS-guarantee percentage improvement of up to 67.4 percent and a makespan reduction of up to 28.2 percent, compared to other baseline schedulers.","1558-2183","","10.1109/TPDS.2019.2931558","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000400,2018YFB0204300); National Natural Science Foundation of China(grant numbers:61872377,61802417,61802420); NUDT Science Foundation(grant numbers:ZK18-03-40); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778770","DL research and development platform;characterizing;scheduling;QoS-aware;GPU clusters","Task analysis;Graphics processing units;Research and development;Quality of service;Job shop scheduling;Training;Predictive models","dynamic scheduling;graphics processing units;learning (artificial intelligence);power aware computing;quality of service;small-to-medium enterprises","large-scale simulations;GENIE;QoS-guarantee percentage improvement;baseline schedulers;QoS guarantees;deep learning;artificial intelligence;giant AI companies;medium-sized enterprises;EIUs;high performance computing systems;off-the-shelf GPU clusters;DL R&D platform;multiple users;diverse DL workloads;multiple DL tasks;shared GPU cluster;existing schedulers;resource requirements;user satisfaction;QoS-aware dynamic scheduling framework;exhaustive characterization;prediction model;processing rate;QoS-aware scheduling algorithm;shared cluster;efficiency 67.4 percent;efficiency 28.2 percent","",17.0,"",42.0,"IEEE","29 Jul 2019","","","IEEE","IEEE Journals"
"Designing Energy-Efficient MPSoC with Untrustworthy 3PIP Cores","Y. Sun; G. Jiang; S. -K. Lam; F. Ning","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,51,63,"The adoption of large-scale MPSoCs and the globalization of the IC design flow give rise to two major concerns: high power density due to continuous technology scaling and security due to the untrustworthiness of the third-party intellectual property (3PIP) cores. However, little work has been undertaken to consider these two critical issues jointly during the design stage. In this paper, we propose a design methodology that minimizes the energy consumption while simultaneously protecting the MPSoC against the effects of hardware trojans. The proposed methodology consists of three main stages: 1) Task scheduling to introduce core diversity in the MPSoC in order to detect the presence of malicious modifications in the cores, or mute their effects at runtime, 2) Vendor assignment to the cores using a novel heuristic that chooses vendor-specific cores with operating speed that minimizes the total energy consumption of the MPSoC, and 3) Explore optimization opportunities for further energy savings by minimizing idle periods on the cores, which are caused by the inter-task data dependencies. Experimental results show that our solutions consume only 1/3 energy of existing solutions without increasing schedule length while satisfying the security constraints.","1558-2183","","10.1109/TPDS.2019.2926721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758837","3PIP cores;untrustworthy;energy efficient;scheduling;vendor assignment;speed optimization","Trojan horses;Security;Task analysis;Energy consumption;Schedules;Hardware;Optimization","circuit optimisation;energy conservation;energy consumption;industrial property;integrated circuit design;multiprocessing systems;processor scheduling;system-on-chip","energy-efficient MPSoC design;untrustworthy 3PIP cores;large-scale MPSoCs;high power density;continuous technology scaling;third-party intellectual property cores;design stage;design methodology;core diversity;vendor-specific cores;total energy consumption;energy savings;inter-task data dependencies;IC design flow globalization;hardware trojan effect;task scheduling;heuristic","",2.0,"",47.0,"IEEE","10 Jul 2019","","","IEEE","IEEE Journals"
"EEPC: A Framework for Energy-Efficient Parallel Control of Connected Cars","M. Shen; G. Luo; N. Xiao","Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Guangzhou, China; Center for Energy-Efficient Computing and Applications, School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,64,79,"With the advanced communication sensors are deployed into the modern connected vehicles (CVs), large amounts of traffic information can be collected in real-time, which gives the chance to explore the various techniques to control the routing of CVs in a ground traffic network. However, the control of CVs often suffers from energy inefficiency due to the constant changes of network capacity and traffic demand. In this paper, we propose a cost-based iterative framework, named EEPC, to explore the energy-efficient parallel control of connected vehicles. EEPC enables the control of CVs to iteratively generate a feasible solution, where the control of each vehicle is guided in an energy-efficient way routing on its own trajectory. EEPC eliminates the conflicts between CVs with a limited number of iterations and in each iteration, EEPC enables each vehicle to coordinate with other vehicles for a same road resource of the traffic network, further determining which vehicle needs the resource most. Note that at each iteration, the imposed cost is updated to guide the coordination between CVs while the energy is always used to guide the control of CVs in EEPC. In addition, we also explore the parallel control of CVs to improve the real-time performance of EEPC. We provide two parallel approaches, one is fine grain and the other is coarse grain. The fine grain performs the parallel control of single-vehicle routing while the coarse grain performs the parallel control of multi-vehicle routing. Note that fine grain adopts multi-threading techniques and coarse grain adopts MPI techniques. The simulation results show that the proposed EEPC can generate a feasible control solution. Notably, we also demonstrate that the generated solution is effective in eliminating the resource conflicts between CVs and in suggesting an energy-efficient route to each vehicle. To the best of our knowledge, this is the first work to explore energy-efficient parallel control of CVs.","1558-2183","","10.1109/TPDS.2019.2930500","National Natural Science Foundation of China(grant numbers:61433019,61802446); Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); Guangdong Basic and Application Basic Research Teams(grant numbers:2018B030312002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770106","Cyber-physical system applications;connected vehicles;control and parallel control;energy-efficient control;real-time control","Energy consumption;Roads;Control systems;Real-time systems;Routing;Task analysis","energy conservation;iterative methods;message passing;multi-threading;optimisation;parallel processing;road traffic;search problems;transportation","energy-efficient parallel control;modern connected vehicles;CVs;ground traffic network;energy-efficient way;iteration;single-vehicle routing;multivehicle routing;feasible control solution;energy-efficient route;EEPC","","","",19.0,"IEEE","23 Jul 2019","","","IEEE","IEEE Journals"
"Enabling Runtime SpMV Format Selection through an Overhead Conscious Method","W. Zhou; Y. Zhao; X. Shen; W. Chen","Computer Science, North Carolina State University College of Engineering, Raleigh, USA; Department of Computer, North Carolina State University, Raleigh, USA; Computer Science, North Carolina State University, Raleigh, USA; CAS, IBM Canada Ltd, Markham, Canada","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,80,93,"Sparse matrix-vector multiplication (SpMV) is an important kernel and its performance is critical for many applications. Storage format selection is to select the best format to store a sparse matrix; it is essential for SpMV performance. Prior studies have focused on predicting the format that helps SpMV run fastest, but have ignored the runtime prediction and format conversion overhead. This work shows that the runtime overhead makes the predictions from previous solutions frequently sub-optimal and sometimes inferior regarding the end-to-end time. It proposes a new paradigm for SpMV storage selection, an overhead-conscious method. Through carefully designed regression models and neural network-based time series prediction models, the method captures the influence imposed on the overall program performance by the overhead and the benefits of format prediction and conversions. The method employs a novel two-stage lazy-and-light scheme to help control the possible negative effects of format predictions, and at the same time, maximize the overall format conversion benefits. Experiments show that the technique outperforms previous techniques significantly. It improves the overall performance of applications by 1.21X to 1.53X, significantly larger than the 0.83X to 1.25X upper-bound speedups overhead-oblivious methods could give.","1558-2183","","10.1109/TPDS.2019.2932931","DOE Early Career Award(grant numbers:DE-SC0013700); National Science Foundation(grant numbers:CCF-1455404,CCF-1525609,CNS-1717425,CCF-1703487); IBM; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787872","SpMV;high performance computing;program optimization;sparse matrix format;prediction model","Sparse matrices;Runtime;Predictive models;Matrix converters;Buildings;Kernel;Time series analysis","mathematics computing;matrix multiplication;multiprocessing systems;neural nets;parallel processing;regression analysis;sparse matrices;time series;vectors","program performance;format prediction;format predictions;format conversion benefits;1.25X upper-bound speedups overhead-oblivious methods;enabling runtime SpMV format;overhead conscious method;sparse matrix-vector multiplication;storage format selection;SpMV performance;runtime prediction;runtime overhead;end-to-end time;SpMV storage selection;overhead-conscious method;regression models;neural network-based time series prediction models","",2.0,"",53.0,"IEEE","5 Aug 2019","","","IEEE","IEEE Journals"
"Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect","A. Li; S. L. Song; J. Chen; J. Li; X. Liu; N. R. Tallent; K. J. Barker","High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA; Computer Science and Mathematics Department, Oak Ridge National Laboratory, Oak Ridge, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA; Computer Science Department, College of William and Mary, Williamsburg, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,94,110,"High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.","1558-2183","","10.1109/TPDS.2019.2928289","Exascale Computing Project(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; U.S. Department of Energy(grant numbers:66150); Pacific Northwest National Laboratory; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); U.S. Department of Energy(grant numbers:DE-AC05-76RL01830); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8763922","Performance evaluation;GPU;interconnect;NUMA;PCIe;NVLink;NVSwitch;SLI;GPUDirect;RDMA;NCCL","Graphics processing units;Bandwidth;Topology;Peer-to-peer computing;Network topology;Switches;Routing","graphics processing units;parallel processing","GPU communication network NUMA effects;PCIe chipset design issue;multiGPU node;GPU combination;GPU communication efficiency;GPU task allocation;communication-oriented performance tuning;NVSwitch;high performance multiGPU computing;computation capability;deep learning;planet-scale simulations;interconnect technology;multiGPU application performance;NVLink-V1;NVLink-V2;NVLink-SLI;NVIDIA P100-DGX-1;V100-DGX-1;DGX-2;empirical evaluation;NVIDIA Turing RTX-2080 GPU","",63.0,"",60.0,"IEEE","15 Jul 2019","","","IEEE","IEEE Journals"
"Exploiting Parallelism and Vectorisation in Breadth-First Search for the Intel Xeon Phi","M. Paredes; G. Riley; M. Luján","School of Computer Science, Kilburn Building, University of Manchester, Manchester, United Kingdom; School of Computer Science, Kilburn Building, University of Manchester, Manchester, United Kingdom; School of Computer Science, Kilburn Building, University of Manchester, Manchester, United Kingdom","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,111,128,"Modern applications generate massive amounts of data that is challenging to process or analyse. Graph algorithms have emerged as a solution for the analysis of such data because they can represent the entities participating in the generation of large-scale datasets in terms of vertices and their relationships in terms of edges. Graph analysis algorithms are used for finding patterns within these relationships, aiming to extract information to be further analysed. The breadth-first search (BFS) is one of the main graph search algorithms used for graph analysis and its optimisation has been widely researched using different parallel computers. However, the parallelisation of BFS has been shown to be challenging because of its inherent characteristics, including irregular memory access patterns, data dependencies and workload imbalance, that limit its scalability. This paper investigates the optimisation of the BFS on the Xeon Phi (Knights Corner), a modern parallel architecture provided with an advanced vector processor supporting the AVX-512 instruction set, using a bespoke development framework integrated with the Graph 500 benchmark. In addition, to demonstrate portability, we show results for a direct port of the algorithms to a more recent version of the Xeon Phi (Knights Landing) and to a Skylake CPU which supports most of the AVX-512 instruction set. Optimised parallel versions of two high-level algorithms for BFS were created using vectorisation, starting with the conventional top-down BFS algorithm and, building on this, a hybrid BFS algorithm. On the KNC our best implementations result in speedups of 1.37x (top-down) and 1.37x (hybrid), for a one million vertices graph, compared to the state-of-the-art. On the KNL and Skylake, the performance is higher than on KNC. In addition, we show results of our best hybrid algorithm on real-world graphs from the SNAP datasets with speedups up to 1.3x on KNC. Performance on KNL and Skylake is again higher, demonstrating the robustness and portability of our algorithm. The hybrid BFS algorithm can be further used to speed up other graph analysis algorithms and the lessons learned from vectorisation can be applied to other algorithms targeting existing and future models of the Xeon Phi and other advanced vector architectures.","1558-2183","","10.1109/TPDS.2019.2927451","Engineering and Physical Sciences Research Council(grant numbers:EP/L000725/1,PAMELA EP/K008730/1); The National Council for Science and Technology of Mexico; Royal Society University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758397","Breadth-first search;graph algorithms;hybrid BFS;vectorisation;parallel architecture;Graph 500;Xeon Phi","Distance measurement;Optical transmitters;Optical receivers;Light emitting diodes;Avalanche photodiodes;Optical noise","data structures;instruction sets;multiprocessing systems;optimisation;parallel architectures;tree searching","Graph 500 benchmark;AVX-512 instruction;modern parallel architecture;data dependencies;irregular memory access patterns;parallel computers;optimisation;main graph search algorithms;Graph algorithms;intel Xeon Phi;breadth-first search;parallelism;graph analysis algorithms;real-world graphs;hybrid algorithm;hybrid BFS algorithm;vectorisation;high-level algorithms;optimised parallel versions","",1.0,"",27.0,"CCBY","9 Jul 2019","","","IEEE","IEEE Journals"
"GA-Par: Dependable Microservice Orchestration Framework for Geo-Distributed Clouds","Z. Wen; T. Lin; R. Yang; S. Ji; R. Ranjan; A. Romanovsky; C. Lin; J. Xu","Newcastle University, Newcastle upon Tyne, United Kingdom; EPFL, Lausanne, Switzerland; BDBC, Beihang University, Beijing, China; Zhejiang University, Hangzhou Shi, China; Newcastle University, Newcastle upon Tyne, United Kingdom; Newcastle University, Newcastle upon Tyne, United Kingdom; Zhejiang University, Hangzhou Shi, China; BDBC, Beihang University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,129,143,"Recent advances in composing Cloud applications have been driven by deployments of inter-networking heterogeneous microservices across multiple Cloud datacenters. System dependability has been of the upmost importance and criticality to both service vendors and customers. Security, a measurable attribute, is increasingly regarded as the representative example of dependability. Literally, with the increment of microservice types and dynamicity, applications are exposed to aggravated internal security threats and externally environmental uncertainties. Existing work mainly focuses on the QoS-aware composition of native VM-based Cloud application components, while ignoring uncertainties and security risks among interactive and interdependent container-based microservices. Still, orchestrating a set of microservices across datacenters under those constraints remains computationally intractable. This paper describes a new dependable microservice orchestration framework GA-Par to effectively select and deploy microservices whilst reducing the discrepancy between user security requirements and actual service provision. We adopt a hybrid (both whitebox and blackbox based) approach to measure the satisfaction of security requirement and the environmental impact of network QoS on system dependability. Due to the exponential grow of solution space, we develop a parallel Genetic Algorithm framework based on Spark to accelerate the operations for calculating the optimal or near-optimal solution. Large-scale real world datasets are utilized to validate models and orchestration approach. Experiments show that our solution outperforms the greedy-based security aware method with 42.34 percent improvement. GA-Par is roughly 4× faster than a Hadoop-based genetic algorithm solver and the effectiveness can be constantly guaranteed under different application scales.","1558-2183","","10.1109/TPDS.2019.2929389","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000103); National Natural Science Foundation of China(grant numbers:61421003); Beijing Advanced Innovation Center for Big Data and Brain Computing (BDBC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8766876","Service orchestration;dependability;microservice","Security;Quality of service;Uncertainty;Optimization;Cloud computing;Genetic algorithms;Silicon","cloud computing;computer centres;genetic algorithms;quality of service;security of data;virtual machines","geo-distributed clouds;Cloud applications;inter-networking heterogeneous microservices;multiple Cloud datacenters;system dependability;service vendors;measurable attribute;microservice types;aggravated internal security threats;externally environmental uncertainties;QoS-aware composition;interactive container-based microservices;interdependent container-based microservices;dependable microservice orchestration framework GA-Par;user security requirements;actual service provision;security requirement;network QoS;orchestration approach;greedy-based security aware method;Hadoop-based genetic algorithm solver;application scales;parallel genetic algorithm framework;native VM-based cloud application components","",16.0,"",53.0,"IEEE","19 Jul 2019","","","IEEE","IEEE Journals"
"Minimizing Tardiness for Data-Intensive Applications in Heterogeneous Systems: A Matching Theory Perspective","K. Xu; L. Lv; T. Li; M. Shen; H. Wang; K. Yang","Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Protocol Research Lab, Huawei, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Department of Computer Science and Engineering, University of Minnesota at Duluth, Duluth, USA; School of Computer Science and Electronic Engineering, University of Essex, Colchester, United Kingdom","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,144,158,"The increasing data requirements of Internet applications have driven a dramatic surge in developing new programming paradigms and complex scheduling algorithms to handle data-intensive workloads. Due to the expanding volume and the variety of such flows, their raw data are often processed on Intermediate Processing Nodes (IPNs) before being sent to servers. However, the intermediate processing constraint is rarely considered in existing flow computing models. This paper aims to minimize the tardiness of data-intensive applications in the presence of intermediate processing constraint. Motivating cases show that the tardiness is affected by both IPN locations and flow dispatching strategies. Based on the observation that dispatching flows to IPNs is essentially building a matching between flows and IPNs, a novel solution is proposed based on matching theory. In the deployment phase, a tardiness-aware deferred acceptance algorithm is developed to optimize IPN locations. In the operation phase, the Power-of-D paradigm and matching theory are combined together to dispatch flows efficiently. Evaluation results show that our solution effectively minimizes the total tardiness of data-intensive applications in heterogeneous systems.","1558-2183","","10.1109/TPDS.2019.2930992","National Key R&D Program of China(grant numbers:2018YFB0803405); China National Funds for Distinguished Young Scientists(grant numbers:61825204); National Natural Science Foundation of China(grant numbers:61602039); Natural Science Foundation of Beijing Municipality(grant numbers:4192050); CCF-Tencent Open Fund WeBank; UK EPSRC Project NIRVANA(grant numbers:EP/L026031/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772187","Heterogeneous system;data-intensive application;matching theory;power-of-D","Routing;Delays;Dispatching;Cameras;Relays;Job shop scheduling","distributed processing;Internet","minimizing tardiness;data-intensive applications;heterogeneous systems;matching theory perspective;data requirements;Internet applications;complex scheduling algorithms;data-intensive workloads;raw data;Intermediate Processing Nodes;intermediate processing constraint;IPN locations;tardiness-aware deferred acceptance algorithm;low computing models;flow computing models;flow computing models","",6.0,"",69.0,"IEEE","25 Jul 2019","","","IEEE","IEEE Journals"
"Quantum Game Analysis on Extrinsic Incentive Mechanisms for P2P Services","S. Wang; W. Sun; L. Ma; W. Lv; X. Cheng","College of Information Science and Technology, Beijing Normal University, Beijing, P.R.China; Department of Computer Science, Columbia University, New York, USA; Department of Computer Science, Texas Christian University, Fort Worth, USA; School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science, George Washington University, Washington, USA","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,159,170,"Peer-to-peer (P2P) services such as mobile P2P transmissions and resource sharing, provide efficient methods to deliver data without the deployment of any central server. Nevertheless, the free-riding phenomenon inherit in such services presses a need for incentive mechanisms to stimulate contributions of data transmissions or sharing. As a result, it is imperative to answer the following questions: whether, and if so to what extent, an incentive mechanism can invoke such contributions? To answerthese questions, we employ an n-player continuous quantum game model to analyze the general extrinsic incentive mechanisms as well as the reputation-based incentive mechanisms, a typical class of extrinsic incentive mechanisms. We focus on studying the extrinsic incentive mechanisms in this paper due to their wide scope of applications stemming from the fact that they promote cooperative behaviors by offering rewards rather than depending on the internal bounds (e.g., social ties) among peers, which may not always exist between any pair of peers. To the best of our knowledge, we are the first to analyze the extrinsic incentive mechanisms for P2P services from a quantum game perspective. Such a perspective is adopted because the extended strategy space in the quantum game broadens the range for searching optimal strategies and the introduction of entanglement makes the proposed analytical frameworks more practical due to the consideration of the peers' relationships imposed by the rewards in extrinsic incentive mechanisms. Our quantum game-based analytical framework is generic because it is compatible with classic game-based schemes. The analytical results can provide a straightforward insight on evaluating the potential of the extrinsic incentive mechanisms and can serve as important references for designing new extrinsic incentive mechanisms.","1558-2183","","10.1109/TPDS.2019.2933416","National Natural Science Foundation of China(grant numbers:61772080,U1811463); National Science Foundation(grant numbers:OAC-1829553,CNS-1912755,IIS- 1741279,CNS-1704397); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8788575","P2P services;incentive mechanisms;quantum game","Games;Peer-to-peer computing;Quantum entanglement;Game theory;Resource management;Analytical models","game theory;peer-to-peer computing","reputation-based incentive mechanisms;Peer-to-peer services;general extrinsic incentive mechanisms;central server;free-riding phenomenon;n-player continuous quantum game mode","",4.0,"",33.0,"IEEE","5 Aug 2019","","","IEEE","IEEE Journals"
"Scheduling Parallel Real-Time Tasks on the Minimum Number of Processors","H. Cho; C. Kim; J. Sun; A. Easwaran; J. -D. Park; B. -C. Choi","Department of Computer and Information Science, Korea University, Sejong, South Korea; Department of Computer and Information Science, Korea University, Sejong, South Korea; Department of Computer and Information Science, Korea University, Sejong, South Korea; School of Computer Engineering, Nanyang Technological University, Singapore; ETRI, Daejeon, South Korea; ETRI, Daejeon, South Korea","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,171,186,"Recently, several parallel frameworks have emerged to utilize the increasing computational capacity of multiprocessors. Parallel tasks are distinguished from traditional sequential tasks in that the subtasks contained in a single parallel task can simultaneously execute on multiple processors. In this study, we consider the scheduling problem of minimizing the number of processors on which the parallel real-time tasks feasibly run. In particular, we focus on scheduling sporadic parallel real-time tasks, in which precedence constraints between subtasks of each parallel task are expressed using a directed acyclic graph (DAG). To address the problem, we formulate an optimization problem that aims to minimize the maximum processing capacity for executing the given tasks. We then suggest a polynomial solution consisting of three steps: (1) transform each parallel real-time task into a series of multithreaded segments, while respecting the precedence constraints of the DAG; (2) selectively extend the segment lengths; and (3) interpret the problem as a flow network to balance the flows on the terminal edges. We also provide the schedulability bound of the proposed solution: it has a capacity augmentation bound of 2. Our experimental results show that the proposed approach yields higher performance than one developed in a recent study.","1558-2183","","10.1109/TPDS.2019.2929048","National Research Foundation of Korea; Ministry of Education(grant numbers:NRF-2015R1D1A1A01057018,NRF-2018R1D1A1B07049078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770295","Real-time scheduling;multicores;multiprocessors;linear programming;flow networks;maximum flow problem;minimum cost flow problem","Task analysis;Processor scheduling;Real-time systems;Program processors;Computational modeling;Scheduling;Multicore processing","directed graphs;multiprocessing systems;parallel processing;processor scheduling;real-time systems","parallel frameworks;subtasks;single parallel task;multiple processors;sporadic parallel real-time tasks;precedence constraints;optimization problem;schedulability;parallel real-time task scheduling","",12.0,"",51.0,"IEEE","23 Jul 2019","","","IEEE","IEEE Journals"
"Single Restart with Time Stamps for Parallel Task Processing with Known and Unknown Processors","J. P. Champati; B. Liang","Division of Information Science and Engineering, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,187,200,"We study the problem of scheduling n tasks on m + m' parallel processors, where the processing times on m processors are known while those on the remaining m' processors are not known a priori. This semi-online model is an abstraction of certain heterogeneous computing systems, e.g., with them known processors representing local CPU cores and the unknown processors representing remote servers with uncertain availability of computing cycles. Our objective is to minimize the makespan of all tasks. We initially focus on the case m' = 1 and propose a semi-online algorithm termed Single Restart with Time Stamps (SRTS), which has time complexity O(nlogn). We derive its competitive ratio in comparison with the optimal offline solution. If the unknown processing times are deterministic, the competitive ratio of SRTS is shown to be either always constant or asymptotically constant in practice, respectively in cases where the processing times are independent and dependent on m. A similar result is obtained when the unknown processing times are random. Furthermore, extending the ideas of SRTS, we propose a heuristic algorithm termed SRTS-Multiple (SRTS-M) for the case m' > 1. Finally, where tasks arrive dynamically with unknown arrival times, we extend SRTS to Dynamic SRTS (DSRTS) and find its competitive ratio. Besides the proven competitive ratios, simulation results further suggest that SRTS and SRTS-M give superior performance on average over randomly generated task processing times, substantially reducing the makespan over the best known alternatives. Interestingly, the performance gain is more significant for task processing times sampled from heavy-tailed distributions.","1558-2183","","10.1109/TPDS.2019.2929173","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765409","Computational offloading;edge computing;mobile cloud computing;opportunistic computing;unknown processing times;task restart;semi-online algorithms","Program processors;Task analysis;Processor scheduling;Scheduling;Heuristic algorithms;Servers;Schedules","computational complexity;microprocessor chips;optimisation;parallel processing;processor scheduling","computing cycles;semionline algorithm;time stamps;competitive ratio;unknown processing times;heuristic algorithm termed SRTS-Multiple;unknown arrival times;proven competitive ratios;SRTS-M;randomly generated task processing times;parallel task processing;unknown processors;parallel processors;semionline model;known processors;local CPU cores;dynamic SRTS;single restart","",3.0,"",36.0,"IEEE","17 Jul 2019","","","IEEE","IEEE Journals"
"The Existence of Completely Independent Spanning Trees for Some Compound Graphs","X. -W. Qin; R. -X. Hao; J. -M. Chang","Department of Mathematics, Beijing Jiaotong University, Beijing, P.R. China; Department of Mathematics, Beijing Jiaotong University, Beijing, P.R. China; Institute of Information and Decision Sciences, National Taipei University of Business, Taipei, Taiwan","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,201,210,"Given two regular graphs G and H such that the vertex degree of G is equal to the number of vertices in H, the compound graph G(H) is constructed by replacing each vertex of G by a copy of Hand replacing each edge of G by an additional edge connecting random vertices in two corresponding copies of H, respectively, under the constraint that each vertex in G(H) is incident with only one additional edge, exactly. L-HSDCm is a compound graph G(H), where G is a hypercube Qm and H is a complete graph Km, which is defined by focusing on the connected relation between servers in the novel data center network HSDCm proposed in [30]. A set of k spanning trees in a graph G are called completely independent spanning trees (CISTs for short) if the paths joining every pair of vertices x and yin any two trees have neither vertex nor edge in common, except for x and y. In this paper, we give a sufficient condition for the existence of k CISTs in a kind of compound graph. Furthermore, a specific construction algorithm is provided. As corollaries of the main results, the existences of two CISTs form m ≥ 4; three CISTs form m ≥ 8 and four CISTs form m ≥ 10 in L-HSDCm(m) are gotten directly.","1558-2183","","10.1109/TPDS.2019.2931904","National Natural Science Foundation of China(grant numbers:11731002); 111 Project of China(grant numbers:B16002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8781860","Completely independent spanning tree;data center network architecture;compound graph;fault-tolerance","Compounds;Data centers;Hypercubes;Servers;Network architecture;Bipartite graph;Routing","computer centres;graph theory;trees (mathematics)","completely independent spanning trees;compound graph;vertex degree;complete graph K;k spanning trees;CISTs;data center network","",21.0,"",30.0,"IEEE","30 Jul 2019","","","IEEE","IEEE Journals"
"WPaxos: Wide Area Network Flexible Consensus","A. Ailijiang; A. Charapko; M. Demirbas; T. Kosar","State University of New York at Buffalo, Buffalo, USA; State University of New York at Buffalo, Buffalo, USA; State University of New York at Buffalo, Buffalo, USA; State University of New York at Buffalo, Buffalo, USA","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,211,223,"WPaxos is a multileader Paxos protocol that provides low-latency and high-throughput consensus across wide-area network (WAN) deployments. WPaxos uses multileaders, and partitions the object-space among these multileaders. Unlike statically partitioned multiple Paxos deployments, WPaxos is able to adapt to the changing access locality through object stealing. Multiple concurrent leaders coinciding in different zones steal ownership of objects from each other using phase-1 of Paxos, and then use phase-2 to commit update-requests on these objects locally until they are stolen by other leaders. To achieve fast phase-2 commits, WPaxos adopts the flexible quorums idea in a novel manner, and appoints phase-2 acceptors to be close to their respective leaders. We implemented WPaxos and evaluated it over WAN deployments across 5 AWS regions. The dynamic partitioning of the objectspace and emphasis on zone-local commits allow WPaxos to significantly outperform both partitioned Paxos deployments and leaderless Paxos approaches.","1558-2183","","10.1109/TPDS.2019.2929793","National Science Foundation(grant numbers:CNS-1527629,XPS-1533870); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765834","Distributed systems;distributed applications;wide-area networks;fault-tolerance","Protocols;Wide area networks;Safety;Throughput;Indexes;Fault tolerance;Fault tolerant systems","distributed processing;fault tolerant computing;protocols;wide area networks","multiple concurrent leaders;object stealing;statically partitioned multiple Paxos deployments;object-space;multileaders;wide-area network deployments;multileader Paxos protocol;wide area network flexible consensus;partitioned Paxos deployments;appoints phase-2 acceptors;WPaxos","",12.0,"",34.0,"IEEE","18 Jul 2019","","","IEEE","IEEE Journals"
"A Survey of Phase Classification Techniques for Characterizing Variable Application Behavior","K. Criswell; T. Adegbija","Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA; Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,224,236,"Adaptable computing is an increasingly important paradigm that specializes system resources to variable application requirements, environmental conditions, or user requirements. Adapting computing resources to variable application requirements (or application phases) is otherwise known as phase-based optimization. Phase-based optimization takes advantage of application phases, or execution intervals of an application that behave similarly, to enable effective and beneficial adaptability. In order for phase-based optimization to be effective, the phases must first be classified to determine when application phases begin and end, and ensure that system resources are accurately specialized. In this paper, we present a survey of phase classification techniques that have been proposed to exploit the advantages of adaptable computing through phase-based optimization. We focus on recent techniques and classify these techniques with respect to several factors in orderto highlight their similarities and differences. We divide the techniques by their major defining characteristics-online/offline and serial/parallel. In addition, we discuss other characteristics such as prediction and detection techniques, the characteristics used for prediction, interval type, etc. We also identify gaps in the state-of-the-art and discuss future research directions to enable and fully exploit the benefits of adaptable computing.","1558-2183","","10.1109/TPDS.2019.2929781","NSF(grant numbers:1844952); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8769866","Phase classification;adaptable computing;workload characterization;variable program behavior;dynamic optimization;edge computing;multithreaded applications;big data;emerging applications","Optimization;Hardware;Computational modeling;Multicore processing;Big Data;Clocks;Runtime","optimisation;pattern classification;power aware computing;telecommunication computing","effective adaptability;beneficial adaptability;variable application behavior;phase-based optimization;application phases;variable application requirements;system resources;adaptable computing;phase classification techniques","",4.0,"",108.0,"IEEE","23 Jul 2019","","","IEEE","IEEE Journals"
"Data-Parallel Hashing Techniques for GPU Architectures","B. Lessley; H. Childs","Department of Computer and Information Science, University of Oregon, Eugene, USA; Department of Computer and Information Science, University of Oregon, Eugene, USA","IEEE Transactions on Parallel and Distributed Systems","17 Dec 2019",2020,31.0,1.0,237,250,"Hash tables are a fundamental data structure for effectively storing and accessing sparse data, with widespread usage in domains ranging from computer graphics to machine learning. This study surveys the state-of-the-art research on data-parallel hashing techniques for emerging massively-parallel, many-core GPU architectures. This survey identifies key factors affecting the performance of different techniques and suggests directions for further research.","1558-2183","","10.1109/TPDS.2019.2929768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765787","Graphics processors;hash tables;parallel algorithms;search problems","Graphics processing units;Instruction sets;Parallel processing;Message systems;Hash functions;Data structures","data structures;graphics processing units;learning (artificial intelligence);multiprocessing systems;parallel processing","data-parallel hashing techniques;hash tables;fundamental data structure;sparse data;many-core GPU architectures;computer graphics;machine learning","",9.0,"",75.0,"IEEE","18 Jul 2019","","","IEEE","IEEE Journals"
"T-Caching: Enhancing Feasibility of In-Network Caching in ICN","S. Lee; I. Yeom; D. Kim","Department of Computer Engineering, Sungkyunkwan University, Seoul, South Korea; Department of Computer Engineering, Sungkyunkwan University, Seoul, South Korea; Department of Computer Science, Kangwon National University, Chuncheon-si, South Korea","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2020",2020,31.0,7.0,1486,1498,"In Information-Centric Networking (ICN), in-network caching is one of the core functions to implement efficient content distribution. As content is served from the network cache, redundant transmission across the same link is reduced and better quality services are provided to the user. However, caching operations (locating cached content, validating/writing content to the cache) may be a large burden on routers. In this article, we substantially reduce the caching overhead of routers by introducing T-Caching, a new in-network caching model in which routers selectively cache some of the most popular content recommended by content providers. In T-Caching, tokens are used 1) to enable routers to maximize caching benefits by simply controlling the amount of content inserted into the cache; 2) to allow content providers to recommend the most popular content as much as routers would actually cache. Our simulation and empirical studies using synthetic data, as well as real-world traces, show that T-Caching allows routers to insert much less than 1 percent of the content into the cache, compared to typical caching mechanisms. Nevertheless, the cache-hit ratio is improved by up to 2~9 times depending on the cache size. We believe that T-Caching can significantly contribute to improving feasibility and performance of in-network caching in ICN.","1558-2183","","10.1109/TPDS.2020.2970702","National Research Foundation of Korea(grant numbers:NRF-2018R1C1B4A01022931,NRF-2016M3C4A7952587); MSIT, Korea(grant numbers:IITP-2019-2018-0-01431); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8975984","Information-centric networking;network caching;feasibility;caching overhead;caching performance","Routing;Servers;Computer architecture;Topology;Network topology;Mathematical model;Logic gates","Internet;quality of service;telecommunication network routing","T-caching;ICN;information-centric networking;content distribution;in-network caching model;content providers;caching mechanisms;cache-hit ratio;cached content operations","",17.0,"",36.0,"IEEE","30 Jan 2020","","","IEEE","IEEE Journals"
"Performance-Aware Speculative Resource Oversubscription for Large-Scale Clusters","R. Yang; C. Hu; X. Sun; P. Garraghan; T. Wo; Z. Wen; H. Peng; J. Xu; C. Li","School of Computing, University of Leeds, Leeds, United Kingdom; Beihang University, Beijing, China; School of Computing, University of Leeds, Leeds, United Kingdom; Lancaster University, Lancaster, United Kingdom; Beihang University, Beijing, China; Newcastle University, Newcastle upon Tyne, United Kingdom; Beihang University, Beijing, China; School of Computing, University of Leeds, Leeds, United Kingdom; Alibaba Group, Hangzhou, China","IEEE Transactions on Parallel and Distributed Systems","20 Feb 2020",2020,31.0,7.0,1499,1517,"It is a long-standing challenge to achieve a high degree of resource utilization in cluster scheduling. Resource oversubscription has become a common practice in improving resource utilization and cost reduction. However, current centralized approaches to oversubscription suffer from the issue with resource mismatch and fail to take into account other performance requirements, e.g., tail latency. In this article we present ROSE, a new resource management platform capable of conducting performance-aware resource oversubscription. ROSE allows latency-sensitive long-running applications (LRAs) to co-exist with computation-intensive batch jobs. Instead of waiting for resource allocation to be confirmed by the centralized scheduler, job managers in ROSE can independently request to launch speculative tasks within specific machines according to their suitability for oversubscription. Node agents of those machines can however, avoid any excessive resource oversubscription by means of a mechanism for admission control using multi-resource threshold control and performance-aware resource throttle. Experiments show that in case of mixed co-location of batch jobs and latency-sensitive LRAs, the CPU utilization and the disk utilization can reach 56.34 and 43.49 percent, respectively, but the 95th percentile of read latency in YCSB workloads only increases by 5.4 percent against the case of executing the LRAs alone.","1558-2183","","10.1109/TPDS.2020.2970013","Alibaba Group; National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2016YFB1000503); National Natural Science Foundation of China(grant numbers:61421003); Engineering and Physical Sciences Research Council(grant numbers:EP/T01461X/1); Beijing Advanced Innovation Center for Big Data and Brain Computing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972553","Resource scheduling;oversubscription;cluster utilization;resource throttling;QoS","Task analysis;Resource management;Quality of service;Heart beat;Yarn;Decision making","cloud computing;computer centres;distributed processing;multiprocessing systems;power aware computing;processor scheduling;resource allocation","performance-aware speculative resource oversubscription;large-scale clusters;resource utilization;cluster scheduling;resource mismatch;ROSE;resource management platform;long-running applications;computation-intensive batch jobs;resource allocation;centralized scheduler;speculative tasks;excessive resource oversubscription;multiresource threshold control;performance-aware resource throttle;latency-sensitive LRAs;CPU utilization;disk utilization;efficiency 43.49 percent;efficiency 5.4 percent","",11.0,"",53.0,"IEEE","28 Jan 2020","","","IEEE","IEEE Journals"
"Modeling Analysis and Cost-Performance Ratio Optimization of Virtual Machine Scheduling in Cloud Computing","B. Wan; J. Dang; Z. Li; H. Gong; F. Zhang; S. Oh","Department of Computer Science and Technology, Xidian University, Xi'an, China; Department of Computer Science and Technology, Xidian University, Xi'an, China; Key Laboratory of Hunan Province for Internet of Things and Information Security and College of Information Engineering, Xiangtan University, Xiangtan, China; College of Mathematics and Statistics, Changsha University of Science and Technology, Changsha, China; DEKE Lab, School of Information, Renmin University of China, Beijing, China; Department of Computer and Information Engineering, Ajou University, Suwon, South Korea","IEEE Transactions on Parallel and Distributed Systems","20 Feb 2020",2020,31.0,7.0,1518,1532,"As an essential feature of cloud computing, dynamic scalability enables the cloud system to dynamically expand or shrink resources according to user needs at runtime. Effectively predicting and optimizing the cost and performance of cloud computing platforms have become one of the key research challenges in the field of cloud computing. In this article, to quantitatively predict the cost and performance of cloud computing platforms, we propose a cloud computing resource analysis model considering both hot/cold startup and hot/cold shutdown of virtual machines (VMs), and use the M/M/N/oo queuing model to analyze cloud computing platform and acquire accurate performance indicators, such as elasticity indicators, cost indicators, performance indicators, cost-performance ratios, etc. In addition, we establish a multi-objective optimization model to optimize both performance and cost of cloud computing platform. Then the optimal stopping and cost-performance optimization algorithm are applied to obtain the optimal configurations, including the number of hot startup VMs, the system service rate, the hot/cold startup rate of VMs, and the hot/cold shutdown rate. By comparing with existing optimization methods, we demonstrate the superiority of our cost-performance ratio optimization method.","1558-2183","","10.1109/TPDS.2020.2968913","Science and Technology Projects of Xi'an, China(grant numbers:201809170CX11JC12); National Natural Science Foundation of China(grant numbers:61972302); Key Research and Development Program of Shanxi Provence(grant numbers:2017ZDXM-GY-002); Hunan Provincial Natural Science Foundation of China for Distinguished Young Scholars(grant numbers:2018JJ1025); Hunan Science and Technology Planning(grant numbers:2019RS3019); National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB1003702); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967018","Cloud computing;queuing system;cost-performance ratio optimization","Cloud computing;Optimization;Computational modeling;Analytical models;Task analysis;Queueing analysis;Elasticity","cloud computing;optimisation;queueing theory;scheduling;virtual machines","cost-performance ratio optimization method;cloud computing platform;resource analysis model;performance indicators;cost indicators;cost-performance ratios;multiobjective optimization model;cost-performance optimization algorithm;virtual machine scheduling","",12.0,"",43.0,"IEEE","23 Jan 2020","","","IEEE","IEEE Journals"
"Partitioning Tree-Shaped Task Graphs for Distributed Platforms With Limited Memory","C. Gou; A. Benoit; L. Marchal","Univ. Lyon, CNRS, ENS de Lyon, Inria, Université Claude-Bernard Lyon 1, LIP UMR5668, Lyon, France; Univ. Lyon, CNRS, ENS de Lyon, Inria, Université Claude-Bernard Lyon 1, LIP UMR5668, Lyon, France; Univ. Lyon, CNRS, ENS de Lyon, Inria, Université Claude-Bernard Lyon 1, LIP UMR5668, Lyon, France","IEEE Transactions on Parallel and Distributed Systems","20 Feb 2020",2020,31.0,7.0,1533,1544,"Scientific applications are commonly modeled as the processing of directed acyclic graphs of tasks, and for some of them, the graph takes the special form of a rooted tree. This tree expresses both the computational dependencies between tasks and their storage requirements. The problem of scheduling/traversing such a tree on a single processor to minimize its memory footprint has already been widely studied. The present article considers the parallel processing of such a tree and studies how to partition it for a homogeneous multiprocessor platform, where each processor is equipped with its own memory. We formally state the problem of partitioning the tree into subtrees, such that each subtree can be processed on a single processor (i.e., it must fit in memory), and the goal is to minimize the total resulting processing time. We prove that this problem is NP-complete, and we design polynomial-time heuristics to address it. An extensive set of simulations demonstrates the usefulness of these heuristics.","1558-2183","","10.1109/TPDS.2020.2971200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8979173","Scheduling;tree partitioning;memory-aware;makespan minimization;parallel computing","Task analysis;Memory management;Computational modeling;Processor scheduling;Scheduling;Schedules","computational complexity;directed graphs;multiprocessing systems;parallel processing;scheduling;trees (mathematics)","polynomial-time heuristics;NP-complete problem;processing time;subtree;homogeneous multiprocessor platform;parallel processing;memory footprint;single processor;storage requirements;computational dependencies;rooted tree;directed acyclic graphs;scientific applications;distributed platforms;partitioning tree-shaped task graphs","",1.0,"",23.0,"IEEE","3 Feb 2020","","","IEEE","IEEE Journals"
"Reliability-Aware Network Service Provisioning in Mobile Edge-Cloud Networks","J. Li; W. Liang; M. Huang; X. Jia","Research School of Computer Science, The Australian National University, Canberra, Australia; Research School of Computer Science, The Australian National University, Canberra, Australia; Research School of Computer Science, The Australian National University, Canberra, Australia; Department of Computer Science, City University of Hong Kong, Hong Kong, P.R. China","IEEE Transactions on Parallel and Distributed Systems","27 Feb 2020",2020,31.0,7.0,1545,1558,"The Mobile Edge-Cloud (MEC) network has emerged as a promising networking paradigm to address the conflict between increasing computing-intensive applications and resource-constrained mobile Internet-of-Thing (IoT) devices with portable size and storage. In MEC environments, Virtualized Network Functions (VNFs) are deployed for provisioning network services to users to reduce the service cost on top of dedicated hardware infrastructures. However, VNFs may suffer from failures and malfunctions while network service providers have to guarantee continuously reliable services to their consumers to meet the ever-growing service demands of users, thereby securing their revenues for the service. In this article, we focus on reliable VNF service provisioning in MECs, by placing primary and backup VNF instances to cloudlets in an MEC network to meet the service reliability requirements of users. We first formulate a novel VNF service reliability problem with the aim to maximize the revenue collected by admitting as many as user requests while meeting their different reliability requirements, assuming that requests arrive into the system one by one without the knowledge of future arrivals, and the admission or rejection decision must be made immediately. We then develop two efficient online algorithms for the problem under two different backup schemes: the on-site (local) and off-site (remote) schemes, by adopting the primal-dual updating technique. Both algorithms achieve provable competitive ratios with bounded moderate resource capacity violations. We finally evaluate the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms are promising, compared with existing baseline algorithms.","1558-2183","","10.1109/TPDS.2020.2970048","Australian Research Council(grant numbers:DP200101985); Research Grants Council of Hong Kong(grant numbers:CityU11214316); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8974390","Mobile edge computing (MEC);virtualized network function (VNF);virtualized service functions (VNFs);revenue maximization;reliability-aware service provisioning;Fault-tolerance;software failure;cloudlet failure;online algorithms;the primal-dual dynamic updating technique;mobile edge-cloud networks;competitive ratio analysis;resource allocation and optimization","Cloud computing;Software reliability;Heuristic algorithms;Computer network reliability;Software;Software algorithms","cloud computing;Internet;mobile computing;resource allocation;virtualisation","user requests;mobile edge-cloud network;Internet-of-Things devices;MEC environments;virtualized network functions;network service providers;VNF instances;MEC network;service reliability requirements;VNF service reliability problem;VNF service;reliability-aware network service provisioning","",25.0,"",25.0,"IEEE","29 Jan 2020","","","IEEE","IEEE Journals"
"Simplified Workflow Simulation on Clouds based on Computation and Communication Noisiness","R. Mathá; S. Ristov; T. Fahringer; R. Prodan","Institute of Information Technology, University of Klagenfurt, Klagenfurt, Austria; Institute of Computer Science, University of Innsbruck, Innsbruck, Austria; Institute of Computer Science, University of Innsbruck, Innsbruck, Austria; Institute of Information Technology, University of Klagenfurt, Klagenfurt","IEEE Transactions on Parallel and Distributed Systems","27 Feb 2020",2020,31.0,7.0,1559,1574,"Many researchers rely on simulations to analyze and validate their researched methods on Cloud infrastructures. However, determining relevant simulation parameters and correctly instantiating them to match the real Cloud performance is a difficult and costly operation, as minor configuration changes can easily generate an unreliable inaccurate simulation result. Using legacy values experimentally determined by other researchers can reduce the configuration costs, but is still inaccurate as the underlying public Clouds and the number of active tenants are highly different and dynamic in time. To overcome these deficiencies, we propose a novel model that simulates the dynamic Cloud performance by introducing noise in the computation and communication tasks, determined by a small set of runtime execution data. Although the estimating method is apparently costly, a comprehensive sensitivity analysis shows that the configuration parameters determined for a certain simulation setup can be used for other simulations too, thereby reducing the tuning cost by up to 82.46 percent, while declining the simulation accuracy by only 1.98 percent on average. Extensive evaluation also shows that our novel model outperforms other state-of-the-art dynamic Cloud simulation models, leading up to 22 percent lower makespan inaccuracy.","1558-2183","","10.1109/TPDS.2020.2967662","European Commission(grant numbers:801091); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8964294","Cloud computing;simulation;workflow applications;burstable instances;performance instability and noisiness","Task analysis;Cloud computing;Computational modeling;Analytical models;Data models;Sensitivity analysis","cloud computing;resource allocation;scheduling","runtime execution data;workflow simulation;communication noisiness;dynamic cloud simulation models;sensitivity analysis;cloud computing;resource management;cloud noisiness;scheduling","",9.0,"",32.0,"CCBY","22 Jan 2020","","","IEEE","IEEE Journals"
"Scalable and Adaptive Data Replica Placement for Geo-Distributed Cloud Storages","K. Liu; J. Peng; J. Wang; W. Liu; Z. Huang; J. Pan","Department of Computer Science, University of Victoria, Victoria, Canada; School of Computer Science and Engineering, Central South University, Changsha, China; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; School of Computer Science and Engineering, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China; Department of Computer Science, University of Victoria, Victoria, Canada","IEEE Transactions on Parallel and Distributed Systems","27 Feb 2020",2020,31.0,7.0,1575,1587,"In geo-distributed cloud storage systems, data replication has been widely used to serve the ever more users around the world for high data reliability and availability. How to optimize the data replica placement has become one of the fundamental problems to reduce the inter-node traffic and the system overhead of accessing associated data items. In the big data era, traditional solutions may face the challenges of long running time and large overheads to handle the increasing scale of data items with time-varying user requests. Therefore, novel offline community discovery and online community adjustment schemes are proposed to solve the replica placement problem in a scalable and adaptive way. The offline scheme can find a replica placement solution based on the average read/write rates for a certain period of time. The scalability can be achieved as 1) the computation complexity is linear to the amount of data items and 2) the data-node communities can evolve in parallel for a distributed replica placement. Furthermore, the online scheme is adaptive to handle the bursty data requests, without the need to completely override the existing replica placement. Driven by real-world data traces, extensive performance evaluations demonstrate the effectiveness of our design to handle large-scale datasets.","1558-2183","","10.1109/TPDS.2020.2968321","National Natural Science Foundation of China(grant numbers:61672537,61672539,61873353); China Postdoctoral Science Foundation; Natural Sciences and Engineering Research Council of Canada; CFI; British Columbia Knowledge Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8964316","Geo-distributed storage system;data replica placement;scalability;adaptivity;community discovery","Distributed databases;Cloud computing;Scalability;Computer science;Adaptive systems;Reliability;Face","Big Data;cloud computing;storage management;telecommunication traffic","replica placement solution;data-node communities;distributed replica placement;real-world data traces;scalable data replica placement;adaptive data replica placement;geo-distributed cloud storages;data replication;Big Data era;online community adjustment;offline community discovery","",14.0,"",36.0,"IEEE","22 Jan 2020","","","IEEE","IEEE Journals"
"Distributed Graph Computation Meets Machine Learning","W. Xiao; J. Xue; Y. Miao; Z. Li; C. Chen; M. Wu; W. Li; L. Zhou","Alibaba Group, Hangzhou, China; Microsoft Research, China; Microsoft Research, China; Google, Mountain View, USA; ByteDance, Beijing, China; Conflux Foundation, Singapore; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; Microsoft Research, China","IEEE Transactions on Parallel and Distributed Systems","27 Feb 2020",2020,31.0,7.0,1588,1604,"TuX2 is a new distributed graph engine that bridges graph computation and distributed machine learning. TuX2 inherits the benefits of elegant graph computation model, efficient graph layout, and balanced parallelism to scale to billion-edge graphs, while extended and optimized for distributed machine learning to support heterogeneity in data model, Stale Synchronous Parallel in scheduling, and a new Mini-batch, Exchange, GlobalSync, and Apply (MEGA) model for programming. TuX2 further introduces a hybrid vertex-cut graph optimization and supports various consistency models in fault tolerance for machine learning. We have developed a set of representative distributed machine learning algorithms in TuX2, covering both supervised and unsupervised learning. Compared to the implementations on distributed machine learning platforms, writing those algorithms in TuX2 takes only about 25 percent of the code: our graph computation model hides the detailed management of data layout, partitioning, and parallelism from developers. The extensive evaluation of TuX2, using large datasets with up to 64 billion of edges, shows that TuX2 outperforms PowerGraph/PowerLyra, the state-of-the-art distributed graph engines, by an order of magnitude, while beating two state-of-the-art distributed machine learning systems by at least 60 percent.","1558-2183","","10.1109/TPDS.2020.2970047","National Natural Science Foundation of China(grant numbers:61472009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8974443","Graph computing;distributed machine learning;heterogeneity;stale synchronous parallel;MEGA model","Machine learning;Data models;Computational modeling;Engines;Machine learning algorithms;Layout;Parallel processing","distributed processing;graph theory;unsupervised learning","stale synchronous parallel;minibatch exchange globalsync and apply model;distributed graph engines;distributed machine learning;graph computation model;distributed machine learning platforms;unsupervised learning;supervised learning;TuX;consistency models;hybrid vertex-cut graph optimization;data model;distributed graph engine;efficiency 25.0 percent;efficiency 60.0 percent","",6.0,"",83.0,"IEEE","29 Jan 2020","","","IEEE","IEEE Journals"
"Combinatorial Auctions for Temperature-Constrained Resource Management in Manycores","H. Khdr; M. Shafique; S. Pagani; A. Herkersdorf; J. Henkel","Computer Science Department, Karlsruher Institut für Technologie,, Karlsruhe, Germany; Department of Computer Engineering, Technische Universitat Wien, Wien, Austria; Central Engineering – Media, Arm Ltd., Cambridge, United Kingdom; Lehrstuhl für Integrierte Systeme, Technische Universität München, München, Germany; Karlsruher Institut für Technologie, Karlsruhe, Germany","IEEE Transactions on Parallel and Distributed Systems","27 Feb 2020",2020,31.0,7.0,1605,1620,"Although manycore processors have plenty of cores, not all of them may run simultaneously at full speed and even some of them might need to be power-gated in order to keep the chip within safe temperature limits. Hence, a resource management technique, that allocates cores to application aiming at maximizing the system performance, will not be able to achieve its goal without taking into account the on-chip temperature and its impact on the availability of the chip's resources. However, considering a temperature constraint by the resource management will further increase its complexity, especially in manycores, and thus implementing it in a centralized scheme might lead to a computation bottleneck and a single point of failure. To avoid such scenarios, it is inevitable to distribute the computation required by the resource management technique throughout the chip. In this article, we propose a distributed resource management technique that considers temperature as an essential factor in allocating cores to applications and determining the power states of these cores and their voltage/frequency levels, while taking into account the performance models of the applications in order to maximize the overall system performance under a temperature constraint. Our proposed technique employs, for the first time, combinatorial auctions within an agent system to achieve the targeted goal in a distributed manner. The experimental evaluations show that our proposed technique achieves significant performance improvements with an average of 41% compared to several distributed resource management techniques.","1558-2183","","10.1109/TPDS.2020.2965523","Deutsche Forschungsgemeinschaft(grant numbers:146371743); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955960","Performance optimization;DVFS;temperature-aware design;system-level optimization;runtime resource management","Resource management;System performance;Distributed management;Optimization;Thermal management;Steady-state;Temperature","microprocessor chips;multiprocessing systems;power aware computing;resource allocation","agent system;core allocation;temperature-constrained resource management;combinatorial auctions;distributed resource management technique;temperature constraint;on-chip temperature;system performance;safe temperature limits;power-gated;manycore processors","",4.0,"",37.0,"IEEE","10 Jan 2020","","","IEEE","IEEE Journals"
"Compression Ratio Modeling and Estimation across Error Bounds for Lossy Compression","J. Wang; T. Liu; Q. Liu; X. He; H. Luo; W. He","Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA","IEEE Transactions on Parallel and Distributed Systems","2 Mar 2020",2020,31.0,7.0,1621,1635,"Scientific simulations on high-performance computing (HPC) systems generate vast amounts of floating-point data that need to be reduced in order to lower the storage and I/O cost. Lossy compressors trade data accuracy for reduction performance and have been demonstrated to be effective in reducing data volume. However, a key hurdle to wide adoption of lossy compressors is that the trade-off between data accuracy and compression performance, particularly the compression ratio, is not well understood. Consequently, domain scientists often need to exhaust many possible error bounds before they can figure out an appropriate setup. The current practice of using lossy compressors to reduce data volume is, therefore, through trial and error, which is not efficient for large datasets which take a tremendous amount of computational resources to compress. This paper aims to analyze and estimate the compression performance of lossy compressors on HPC datasets. In particular, we predict the compression ratios of two modern lossy compressors that achieve superior performance, SZ and ZFP, on HPC scientific datasets at various error bounds, based upon the compressors' intrinsic metrics collected under a given base error bound. We evaluate the estimation scheme using twenty real HPC datasets and the results confirm the effectiveness of our approach.","1558-2183","","10.1109/TPDS.2019.2938503","National Science Foundation(grant numbers:CCF-1718297,CCF-1812861); Nanjing Institute of Technology; National Science Foundation(grant numbers:1828363,1813081); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821342","High-performance computing;lossy compression;data reduction;performance modeling","Compressors;Data models;Measurement;Computational modeling;Estimation;Market research;Predictive models","data analysis;data compression;parallel processing","ZFP;SZ;base error bound;reduction performance;data accuracy;floating-point data;high-performance computing systems;scientific simulations;HPC scientific datasets;modern lossy compressors;data volume;compression ratio","",5.0,"",35.0,"IEEE","30 Aug 2019","","","IEEE","IEEE Journals"
"Accelerating Sparse Cholesky Factorization on Sunway Manycore Architecture","M. Li; Y. Liu; H. Yang; Z. Luan; L. Gan; G. Yang; D. Qian","Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Beihang University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","3 Mar 2020",2020,31.0,7.0,1636,1650,"To improve the performance of sparse Cholesky factorization, existing research divides the adjacent columns of the sparse matrix with the same nonzero patterns into supernodes for parallelization. However, due to the various structures of sparse matrices, the computation of the generated supernodes varies significantly, and thus hard to optimize when computed by dense matrix kernels. Therefore, how to efficiently map sparse Choleksy factorization to the emerging architectures, such as Sunway many-core processor, remains an active research direction. In this article, we propose swCholesky, which is a highly optimized implementation of sparse Cholesky factorization on Sunway processor. Specifically, we design three kernel task queues and a dense matrix library to dynamically adapt to the kernel characteristics and architecture features. In addition, we propose an auto-tuning mechanism to search for the optimal settings of the important parameters in swCholesky. Our experiments show that swCholesky achieves better performance than state-of-the-art implementations.","1558-2183","","10.1109/TPDS.2019.2953852","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2016YFB1000503,2016YFB0200100); National Natural Science Foundation of China(grant numbers:61502019,61732002); Center for High Performance Computing and System Simulation; Pilot National Laboratory for Marine Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903486","Sunway architecture;Sparse Cholesky factorization;performance optimization","Sparse matrices;Kernel;Computer architecture;Acceleration;Libraries;Task analysis;Linear algebra","matrix decomposition;multiprocessing systems;parallel architectures;sparse matrices","auto-tuning mechanism;architecture features;kernel characteristics;kernel task queues;swCholesky;generated supernode computation;nonzero patterns;dense matrix library;Sunway many-core processor;map sparse Choleksy factorization;dense matrix kernels;sparse matrices;Sunway manycore architecture","",7.0,"",56.0,"IEEE","18 Nov 2019","","","IEEE","IEEE Journals"
"Improving Restore Performance of Packed Datasets in Deduplication Systems via Reducing Persistent Fragmented Chunks","Y. Zhang; M. Fu; X. Wu; F. Wang; Q. Wang; C. Wang; X. Dong; H. Han","School of Computer Science, Hubei University of Technology, Wuhan, China; Sangfor Technologies Inc., Shenzhen, China; School of Computer Science, Hubei University of Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Division of Data Storage System, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Division of Data Storage System, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","4 Mar 2020",2020,31.0,7.0,1651,1664,"Data deduplication, though being efficient for redundancy elimination in storage systems, introduces chunk fragmentation which severely decreases restore performance. Rewriting algorithms are proposed to reduce the chunk fragmentation. Typically, the backup software aggregates files into larger “tar” type files for storage. We observe that, in tar type datasets, a large number of Persistent Fragmented Chunks (PFCs) are repeatedly rewritten by state-of-the-art rewriting algorithms in every backup, which severely impacts restore performance. We found that the existence of PFCs is due to the traditional strategy of storing PFCs along with other chunks in the containers to preserve the stream locality, rendering them always stored in the containers with low utilization. We propose DePFC to reduce PFCs. DePFC identifies and removes PFCs from the containers preserving the stream locality, and groups them together, to increase the utilization of containers holding them for the subsequent backup, thus preventing them from being rewritten again. We further propose an FC Buffer to avoid mistaken rewrites of PFCs and grouping PFCs that cause restore cache thrashing together. Experimental results demonstrate that DePFC improves restore performance of state-of-the-art rewriting algorithms by 44.24-89.42 percent, while attaining comparable deduplication efficiency, and FC Buffer further improves restore performance.","1558-2183","","10.1109/TPDS.2020.2972898","National Natural Science Foundation of China(grant numbers:61902116,61772180,61832020,61772216); National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB10033005); The Scientific Research Fund of Hubei Provincial Department of Education(grant numbers:B2017042); Research Foundation for Advanced Talents of Hubei University of Technology(grant numbers:BSQD2019025,BSQD2019022,BSQD2019020,BSQD2019026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8994084","Data deduplication;storage system;chunk fragmentation;restore performance","Containers;Metadata;Redundancy;Software;Software algorithms;Indexes;Electromagnetic compatibility","cache storage;data handling;storage management","restore performance;packed datasets;deduplication systems;persistent fragmented chunks;data deduplication;storage systems;chunk fragmentation;tar type datasets;stream locality;DePFC;rewriting algorithms","",5.0,"",33.0,"IEEE","11 Feb 2020","","","IEEE","IEEE Journals"
"Performance Optimization for Relative-Error-Bounded Lossy Compression on Scientific Data","X. Zou; T. Lu; W. Xia; X. Wang; W. Zhang; H. Zhang; S. Di; D. Tao; F. Cappello","Peng Cheng Laboratory, Shenzhen, China; Marvell Technology Group, Santa Clara, USA; Wuhan National Laboratory for Optoelectronics, Wuhan, China; Peng Cheng Laboratory, Shenzhen, China; Peng Cheng Laboratory, Shenzhen, China; Peng Cheng Laboratory, Shenzhen, China; Argonne National Laboratory, Lemont, USA; University of Alabama, Tuscaloosa, USA; Argonne National Laboratory, Lemont, USA","IEEE Transactions on Parallel and Distributed Systems","4 Mar 2020",2020,31.0,7.0,1665,1680,"Scientific simulations in high-performance computing (HPC) environments generate vast volume of data, which may cause a severe I/O bottleneck at runtime and a huge burden on storage space for postanalysis. Unlike traditional data reduction schemes such as deduplication or lossless compression, not only can error-controlled lossy compression significantly reduce the data size but it also holds the promise to satisfy user demand on error control. Pointwise relative error bounds (i.e., compression errors depends on the data values) are widely used by many scientific applications with lossy compression since error control can adapt to the error bound in the dataset automatically. Pointwise relative-error-bounded compression is complicated and time consuming. We develop efficient precomputation-based mechanisms based on the SZ lossy compression framework. Our mechanisms can avoid costly logarithmic transformation and identify quantization factor values via a fast table lookup, greatly accelerating the relative-error-bounded compression with excellent compression ratios. In addition, we reduce traversing operations for Huffman decoding, significantly accelerating the decompression process in SZ. Experiments with eight well-known real-world scientific simulation datasets show that our solution can improve the compression and decompression rates (i.e., the speed) by about 40 and 80 p, respectively, in most of cases, making our designed lossy compression strategy the best-in-class solution in most cases.","1558-2183","","10.1109/TPDS.2020.2972548","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB1003800,2018YFB1003805); National Natural Science Foundation of China(grant numbers:61972441,61832004,61972112,61672186,61872110); Wuhan National Laboratory for Optoelectronics(grant numbers:2018WNLOKF008); Key R&D Program for Guangdong Province(grant numbers:2019B010136001); Shenzhen Science and Technology Program(grant numbers:JCYJ20170413105929681,JCYJ20170811161545863); U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:1619253); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8989806","Lossy compression;high-performance computing;scientific data;compression rate","Compressors;Quantization (signal);Data models;Distributed databases;Error correction;Decoding;Acceleration","data compression;data reduction;Huffman codes;natural sciences computing;parallel processing;table lookup","Huffman decoding;fast table lookup;quantization factor values;I/O bottleneck;HPC environment;precomputation-based mechanisms;data size reduction;error-controlled lossy compression;lossless compression;data reduction;high-performance computing environments;scientific simulations;scientific data;relative-error-bounded lossy compression;performance optimization;lossy compression strategy;decompression rates;real-world scientific simulation datasets;compression ratios;SZ lossy compression framework;pointwise relative-error-bounded compression;error control;scientific applications;data values;compression errors","",7.0,"",33.0,"IEEE","10 Feb 2020","","","IEEE","IEEE Journals"
"Replica Exchange MCMC Hardware With Automatic Temperature Selection and Parallel Trial","K. Dabiri; M. Malekmohammadi; A. Sheikholeslami; H. Tamura","Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto, Toronto, Canada; Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto, Toronto, Canada; Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto, Toronto, Canada; Fujitsu Laboratories Ltd., Kawasaki, Kanagawa, Japan","IEEE Transactions on Parallel and Distributed Systems","5 Mar 2020",2020,31.0,7.0,1681,1692,"A replica exchange Markov Chain Monte Carlo (MCMC) engine is developed with automatic temperature adjustment for solving combinatorial optimization problems by minimizing the energy of the Ising model. The automatic temperature adjustment scheme ensures that the MCMC process is optimized at every stage of the execution. This approach is performed by dynamically adjusting temperatures of all replicas, based on the properties of any given problem, in addition to the capability of automatically inserting new replicas or removing any existing replicas to achieve the best possible resource efficiency and execution time. The proposed algorithm is integrated with parallel evaluation of energy increment and update scheme. The engine is implemented on the FPGA platform with a capacity of running up to 42 replicas in pipeline, each running 1024 fully-connected Ising spins in parallel. The performance of the hardware is examined with three different classes of problems, Vertex Cover, Maximum-Cut, and Travelling Salesman using the engine in three modes, simulated annealing, with replica exchange while the adjustments are turned on or off. Up to 16x speedup is observed by turning on the replica exchange capability in addition to the advantage of eliminating the challenging process of finding an optimal annealing schedule for simulated annealing process.","1558-2183","","10.1109/TPDS.2020.2972359","Fujitsu Laboratories Ltd.; Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025708","Replica exchange;combinatorial optimization;Markov chain Monte Carlo;hardware implementation;ising model;FPGA;parallel computing;simulated annealing;Boltzmann machine;neural network;openCL","Hardware;Engines;Monte Carlo methods;Field programmable gate arrays;Simulated annealing;Probability distribution","field programmable gate arrays;Ising model;Markov processes;Monte Carlo methods;simulated annealing;travelling salesman problems","simulated annealing process;travelling salesman;maximum-cut;vertex cover;FPGA;fully-connected Ising spins;resource efficiency;MCMC process;automatic temperature adjustment scheme;Ising model;combinatorial optimization problems;replica exchange Markov Chain Monte Carlo engine;automatic temperature selection;replica exchange MCMC hardware;optimal annealing schedule;replica exchange capability;energy increment","",9.0,"",39.0,"IEEE","5 Mar 2020","","","IEEE","IEEE Journals"
"Exact Distributed Load Centrality Computation: Algorithms, Convergence, and Applications to Distance Vector Routing","L. Maccari; L. Ghiro; A. Guerrieri; A. Montresor; R. L. Cigno","University of Venice, Venice, Italy; University of Trento, Trento, Italy; SpazioDati srl, Cascina, Italy; University of Trento, Trento, Italy; University of Brescia, Brescia, Italy","IEEE Transactions on Parallel and Distributed Systems","6 Mar 2020",2020,31.0,7.0,1693,1706,"Many optimization techniques for networking protocols take advantage of topological information to improve performance. Often, the topological information at the core of these techniques is a centrality metric such as the Betweenness Centrality (BC) index. BC is, in fact, a centrality metric with many well-known successful applications documented in the literature, from resource allocation to routing. To compute BC, however, each node must run a centralized algorithm and needs to have the global topological knowledge; such requirements limit the feasibility of optimization procedures based on BC. To overcome restrictions of this kind, we present a novel distributed algorithm that requires only local information to compute an alternative similar metric, called Load Centrality (LC). We present the new algorithm together with a proof of its convergence and the analysis of its time complexity. The proposed algorithm is general enough to be integrated with any distance vector (DV) routing protocol. In support of this claim, we provide an implementation on top of Babel, a real-world DV protocol. We use this implementation in an emulation framework to show how LC can be exploited to reduce Babel's convergence time upon node failure, without increasing control overhead. As a key step towards the adoption of centrality-based optimization for routing, we study how the algorithm can be incrementally introduced in a network running a DV routing protocol. We show that even when only a small fraction of nodes participate in the protocol, the algorithm accurately ranks nodes according to their centrality.","1558-2183","","10.1109/TPDS.2020.2973960","European Commission(grant numbers:688768); Horizon 2020 Framework Programme(grant numbers:645274); IEEE Smart Cities Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999507","Multi-hop networks;mesh networks;ad-hoc networks;bellman-ford;load centrality;distributed algorithms;failure recovery","Routing protocols;Measurement;Routing;Indexes;Distributed algorithms;Network topology","ad hoc networks;computational complexity;distributed algorithms;optimisation;resource allocation;routing protocols;telecommunication network reliability;telecommunication network topology","node failure;time complexity;distributed algorithm;performance improvement;Babel convergence time;betweenness centrality index;exact distributed load centrality computation;DV routing protocol;centrality-based optimization;real-world DV protocol;distance vector routing protocol;local information;optimization procedures;global topological knowledge;centralized algorithm;resource allocation;centrality metric;topological information;optimization techniques","",4.0,"",47.0,"IEEE","14 Feb 2020","","","IEEE","IEEE Journals"
"Cost-Aware Partitioning for Efficient Large Graph Processing in Geo-Distributed Datacenters","A. C. Zhou; B. Shen; Y. Xiao; S. Ibrahim; B. He","National Engineering Lab for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; National Engineering Lab for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; National Engineering Lab for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; IMT Atlantique, LS2N, Inria, Nantes, France; National University of Singapore, Singapore","IEEE Transactions on Parallel and Distributed Systems","13 Mar 2020",2020,31.0,7.0,1707,1723,"Graph processing is an emerging computation model for a wide range of applications and graph partitioning is important for optimizing the cost and performance of graph processing jobs. Recently, many graph applications store their data on geo-distributed datacenters (DCs) to provide services worldwide with low latency. This raises new challenges to existing graph partitioning methods, due to the multi-level heterogeneities in network bandwidth and communication prices in geo-distributed DCs. In this article, we propose an efficient graph partitioning method named Geo-Cut, which takes both the cost and performance objectives into consideration for large graph processing in geo-distributed DCs. Geo-Cut adopts two optimization stages. First, we propose a cost-aware streaming heuristic and utilize the one-pass streaming graph partitioning method to quickly assign edges to different DCs while minimizing inter-DC data communication cost. Second, we propose two partition refinement heuristics which identify the performance bottlenecks of geo-distributed graph processing and refine the partitioning result obtained in the first stage to reduce the inter-DC data transfer time while satisfying the budget constraint. Geo-Cut can be also applied to partition dynamic graphs thanks to its lightweight runtime overhead. We evaluate the effectiveness and efficiency of Geo-Cut using real-world graphs with both real geo-distributed DCs and simulations. Evaluation results show that Geo-Cut can reduce the inter-DC data transfer time by up to 79 percent (42 percent as the median) and reduce the monetary cost by up to 75 percent (26 percent as the median) compared to state-of-the-art graph partitioning methods with a low overhead.","1558-2183","","10.1109/TPDS.2019.2955494","National Natural Science Foundation of China(grant numbers:61802260); Natural Science Foundation of Guangdong Province(grant numbers:2018A030310440,2019A1515012053); Shenzhen Science and Technology Foundation(grant numbers:JCYJ20180305125737520); Natural Science Foundation of SZU(grant numbers:827-000370,827-000175,860-000002110319); Guangdong Province Key Laboratory of Popular High Performance Computers(grant numbers:2017B030314073); ANR KerStream project(grant numbers:ANR-16-CE25-0014-01); MoE AcRF Tier 1(grant numbers:T1 251RES1610); National Natural Science Foundation of China(grant numbers:61929103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911231","Graph processing;wide area network;geo-distributed datacenters","Bandwidth;Wide area networks;Data transfer;Downlink;Internet;Uplink","cloud computing;computer centres;graph theory","optimization stages;Geo-Cut;dynamic graphs;performance objectives;graph partitioning method;graph applications;graph processing jobs;geo-distributed datacenters;cost-aware partitioning;state-of-the-art graph partitioning;monetary cost;geo-distributed DC;real-world graphs;inter-DC data transfer time;partitioning result;geo-distributed graph processing;partition refinement heuristics;inter-DC data communication cost;one-pass streaming graph partitioning method;cost-aware streaming heuristic","",8.0,"",37.0,"IEEE","25 Nov 2019","","","IEEE","IEEE Journals"
"High Performance GPU Tensor Completion With Tubal-Sampling Pattern","T. Zhang; X. -Y. Liu; X. Wang","School of Computer Engineering and Science, Shanghai University, Shanghai, China; Department of Electrical Engineering, Columbia University, New York, USA; Department of Electrical Engineering, Columbia University, New York, USA","IEEE Transactions on Parallel and Distributed Systems","13 Mar 2020",2020,31.0,7.0,1724,1739,"Data completion is a problem of filling missing or unobserved elements of partially observed datasets. Data completion algorithms have received wide attention and achievements in diverse domains including data mining, signal processing, and computer vision. We observe a ubiquitous tubal-sampling pattern in big data and Internet of Things (IoT) applications, which is introduced by many reasons such as high data acquisition cost, downsampling for data compression, sensor node failures, and packet losses in low-power wireless transmissions. To meet the time and accuracy requirements of applications, data completion methods are expected to be accurate as well as fast. However, the existing methods for data completion with the tubal-sampling pattern are either accurate or fast, but not both. In this article, we propose high-performance graphics processing unit (GPU) tensor completion for data completion with the tubal-sampling pattern. First, by exploiting the convolution theorem, we split a tensor least-squares minimization problem into multiple least-squares sub-problems in the frequency domain. In this way, massive parallelisms are exposed for many-core GPU architectures while still preserving high recovery accuracy. Second, we propose computing slice-level and tube-level tasks in batches to improve GPU utilization. Third, we reduce the data transfer cost by eliminating the accesses to the CPU memory inside algorithm loop structures. The experimental results show that the proposed tensor completion is both fast and accurate. Using synthetic data of varying sizes, the proposed GPU tensor completion achieves maximum 248.18×, 7, 403.27×, and 33.27× speedups over the CPU MATLAB implementation, GPU element-sampling tensor completion in the cuTensor-tubal library, and GPU high-performance matrix completion, respectively. With a 50 percent sampling rate, the proposed GPU tensor completion achieves a recovery error of 1.40e-5, which is comparable with that of the GPU element-sampling tensor completion and three orders of magnitude better than that of the GPU high-performance matrix completion. To utilize multiple GPUs in servers, we design a multi-GPU scheme for tubal-sampling tensor completion. The multi-GPU tensor completion achieves maximum 1.89× speedup on two GPUs versus on a single GPU for medium or big tensors. We further evaluate the performance of the proposed GPU tensor completion in three real applications, namely, video transmission in wireless camera networks, RF fingerprint-based indoor localization, and seismic data completion, and it achieves maximum speedups of 448.68×, 24.63×, and 311.54×, respectively. We integrate this high-performance GPU tensor completion implementation into the cuTensor-tubal library to support various applications.","1558-2183","","10.1109/TPDS.2020.2975196","Natural Science Foundation of Shanghai(grant numbers:17ZR1409800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003201","Tensor completion;GPU;low-tubal-rank tensor model;tubal-sampling;alternating minimization","Tensile stress;Graphics processing units;Libraries;Wireless communication;Wireless sensor networks;Cameras;Big Data","data compression;data mining;graphics processing units;least squares approximations;Matlab;minimisation;tensors","synthetic data;data transfer cost;GPU utilization;many-core GPU architectures;tensor least-squares minimization problem;high-performance graphics processing unit tensor completion;data completion methods;data compression;high data acquisition cost;big data;data mining;data completion algorithms;tubal-sampling pattern;high performance GPU tensor completion;cuTensor-tubal library;high-performance GPU tensor completion implementation;seismic data completion;big tensors;multiGPU tensor completion;tubal-sampling tensor completion;multiGPU scheme;GPU element-sampling tensor completion;GPU high-performance matrix completion","",6.0,"",43.0,"IEEE","19 Feb 2020","","","IEEE","IEEE Journals"
"Large-Scale Automatic K-Means Clustering for Heterogeneous Many-Core Supercomputer","T. Yu; W. Zhao; P. Liu; V. Janjic; X. Yan; S. Wang; H. Fu; G. Yang; J. Thomson","University of St Andrews, St Andrews, United Kingdom; National Supercomputer Centre, Wuxi, China; National Supercomputer Centre, Wuxi, China; University of St Andrews, St Andrews, United Kingdom; University of California, Berkeley, Berkeley, USA; Wellcome Trust Sanger Institute, Saffron Walden, United Kingdom; National Supercomputer Centre, Wuxi, China; National Supercomputer Centre, Wuxi, China; University of St Andrews, St Andrews, United Kingdom","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2020",2020,31.0,5.0,997,1008,"This article presents an automatic k-means clustering solution targeting the Sunway TaihuLight supercomputer. We first introduce a multilevel parallel partition approach that not only partitions by dataflow and centroid, but also by dimension, which unlocks the potential of the hierarchical parallelism in the heterogeneous many-core processor and the system architecture of the supercomputer. The parallel design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability. Furthermore, we propose an automatic hyper-parameter determination process for k-means clustering, by automatically generating and executing the clustering tasks with a set of candidate hyper-parameter, and then determining the optimal hyper-parameter using a proposed evaluation method. The proposed autoclustering solution can not only achieve high performance and scalability for problems with massive high-dimensional data, but also support clustering without sufficient prior knowledge for the number of targeted clusters, which can potentially increase the scope of k-means algorithm to new application areas.","1558-2183","","10.1109/TPDS.2019.2955467","Engineering and Physical Sciences Research Council(grant numbers:EP/P020631/1); ABC: Adaptive Brokerage(grant numbers:EP/R010528/1); National Key R&D Program of China(grant numbers:2017YFE0123600); China Postdoctoral Science Foundation(grant numbers:2018M641359); Center for High Performance Computing and System Simulation of Pilot National Laboratory for Marine Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911258","Supercomputer;heterogeneous many-core processor;data partitioning;clustering;scheduling;AutoML","Supercomputers;Task analysis;Clustering algorithms;Parallel processing;Partitioning algorithms;Scalability;Graphics processing units","Bayes methods;data flow computing;multiprocessing systems;parallel machines;pattern clustering","targeting centroids;multilevel parallel partition approach;Sunway TaihuLight supercomputer;heterogeneous many-core supercomputer;targeted clusters;massive high-dimensional data;autoclustering solution;optimal hyper-parameter;candidate hyper-parameter;clustering tasks;automatic hyper-parameter determination process;large-scale clustering problems;parallel design;system architecture;heterogeneous many-core processor;hierarchical parallelism;centroid;dataflow","",4.0,"",44.0,"IEEE","25 Nov 2019","","","IEEE","IEEE Journals"
"Making Application-Level Crash Consistency Practical on Flash Storage","D. H. Kang; C. Min; S. -W. Lee; Y. I. Eom","Department of Computer Engineering, Dongguk University-Gyeongju, Gyeongju, South Korea; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA; College of Computing, Sungkyunkwan University, Suwon, South Korea; College of Computing, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1009,1020,"We present the design, implementation, and evaluation of a new file system, called ACCFS, supporting application-level crash consistency as its first-class citizen functionality. With ACCFS, application data can be correctly recovered in the event of system crashes without any complex update protocol at the application level. With the help of the SHARE interface supporting atomic address remapping at the flash storage layer, ACCFS can easily and efficiently achieve crash consistency as well as single-write journaling. We prototyped ACCFS by slightly modifying the full data journal mode in ext4, implemented the SHARE interface as firmware in a commercial SSD available in the market, and carried out various experiments by running ACCFS on top of the SSD. Our preliminary experimental results are very promising. For instance, the performance of an OLTP benchmark using MySQL/InnoDB engine can be boosted by more than 2-6x by offloading the responsibility of guaranteeing the atomic write of MySQL data pages from the InnoDB engine's own journaling mechanism to ACCFS. This impressive performance gain is in part due to the single-write journaling in ACCFS and in part comes from the fact that the frequent fsync() calls caused by the complex update protocol at the application level can be avoided. ACCFS is a practical solution for the crash consistency problem in that (1) the SHARE interface can be, like the TRIM command, easily supported by commercial SSDs, (2) it can be embodied with a minor modification on the existing ext4 file system, and (3) the existing applications can be made crash consistent simply by opening files in O_ATOMIC mode while the legacy applications can be run without any change.","1558-2183","","10.1109/TPDS.2019.2959305","Institute for Information and communications Technology Promotion(grant numbers:IITP-2015-0-00284); NVRam Based High Performance Open Source DBMS Development(grant numbers:2015-0-00314); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936358","File system-level consistency;application-level consistency;flash storage device;address remapping","Computer crashes;Engines;Databases;Protocols;Atomic layer deposition;Benchmark testing;Metadata","data mining;file organisation;flash memories;SQL;storage management;system recovery;transaction processing","complex update protocol;crash consistency problem;SHARE interface;ext4 file system;legacy applications;application-level crash consistency practical;application data;system crashes;flash storage layer;data journal mode;ACCFS","",2.0,"",39.0,"IEEE","18 Dec 2019","","","IEEE","IEEE Journals"
"Approximate NoC and Memory Controller Architectures for GPGPU Accelerators","V. Y. Raparti; S. Pasricha","Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, USA; Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, USA","IEEE Transactions on Parallel and Distributed Systems","23 Jan 2020",2020,31.0,5.0,25,39,"High interconnect bandwidth is crucial for achieving better performance in many-core GPGPU architectures that execute highly data parallel applications. The parallel warps of threads running on shader cores generate a high volume of read requests to the main memory due to the limited size of data caches at the shader cores. This leads to a scenarios with rapid arrival of an even larger volume of reply data from the DRAM, which creates a bottleneck at memory controllers (MCs) that send reply packets back to the requesting cores over the network-on-chip (NoC). Coping with such high volumes of data requires intelligent memory scheduling and innovative NoC architectures. To mitigate memory bottlenecks in GPGPUs, we first propose a novel approximate memory controller architecture (AMC) that reduces the DRAM latency by opportunistically exploiting row buffer locality and bank level parallelism in memory request scheduling, and leverages approximability of the reply data from DRAM, to reduce the number of reply packets injected into the NoC. To further realize high throughput and low energy communication in GPGPUs, we propose a low power, approximate NoC architecture (Dapper) that increases the utilization of the available network bandwidth by using single cycle overlay circuits for the reply traffic between MCs and shader cores. Experimental results show that Dapper and AMC together increase NoC throughput by up to 21 percent; and reduce NoC latency by up to 45.5 percent and energy consumed by the NoC and MC by up to 38.3 percent, with minimal impact on output accuracy, compared to state-of-the-art approximate NoC/MC architectures.","1558-2183","","10.1109/TPDS.2019.2958344","NSF(grant numbers:CCF-1813370); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8928966","GPGPU;approximate computing;network-on-chip;memory controller","Computer architecture;Random access memory;Throughput;Approximate computing;Graphics processing units;Parallel processing;Energy consumption","cache storage;computer graphic equipment;DRAM chips;graphics processing units;microprocessor chips;multiprocessing systems;network-on-chip;parallel processing;processor scheduling;storage management","innovative NoC architectures;memory bottlenecks;GPGPU;approximate memory controller architecture;DRAM;bank level parallelism;memory request scheduling;leverages approximability;reply data;reply packets;approximate NoC architecture;available network bandwidth;reply traffic;MC;shader cores;NoC throughput;memory controller architectures;GPGPU accelerators;high interconnect bandwidth;many-core GPGPU architectures;read requests;data caches;larger volume;memory controllers;requesting cores;intelligent memory scheduling","",1.0,"",40.0,"IEEE","9 Dec 2019","","","IEEE","IEEE Journals"
"A General Design for a Scalable MPI-GPU Multi-Resolution 2D Numerical Solver","M. Turchetto; A. D. Palù; R. Vacondio","Engineering and Architecture Department, University of Parma, Parma, Italy; Mathematics Physics Computer Science Department, University of Parma, Parma, Italy; Engineering and Architecture Department, University of Parma, Parma, Italy","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1036,1047,"This article presents a multi-GPU implementation of a Finite-Volume solver on a multi-resolution grid. The implementation completely offloads the computation to the GPUs and communications between different GPUs are implemented by means of the Message Passing Interface (MPI) API. Different domain decomposition techniques have been considered and the one based on the Hilbert Space Filling Curves (HSFC) showed optimal scalability. Several optimizations are introduced: One-to-one MPI communications among MPI ranks are completely masked by GPU computations on internal cells and a novel dynamic load balancing algorithm is introduced to minimize the waiting times at global MPI synchronization barriers. Such algorithm adapts the computational load of ranks in response to dynamical changes in the execution time of blocks and in network performances; Its capability to converge to a balanced computation has been empirically shown by numerical experiments. Tests exploit up to 64 GPUs and 83M cells and achieve an efficiency of 90 percent in weak scalability and 85 percent for strong scalability. The framework is general and the results of the article can be ported to a wide range of explicit 2D Partial Differential Equations solvers.","1558-2183","","10.1109/TPDS.2019.2961909","the Italian MIUR(grant numbers:RBSI14R1GP,D92I15000190001); Italian INdAM – GNCS(grant numbers:2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941298","CUDA;multi-GPU;MPI;dynamic load balancing;hilbert space filling curves;multi-resolution grid;shallow water equations (SWE);AMR","Graphics processing units;Mathematical model;Computational modeling;Load management;Numerical models;Load modeling;Two dimensional displays","application program interfaces;finite volume methods;graphics processing units;Hilbert spaces;mathematics computing;message passing;partial differential equations;resource allocation","optimal scalability;83M cells;Hilbert space filling curves;explicit 2D partial differential equations solvers;finite-volume solver;message passing interface API;domain decomposition techniques;multiresolution grid;multiGPU implementation;scalable MPI-GPU multiresolution 2D numerical solver;strong scalability;weak scalability;balanced computation;global MPI synchronization barriers;dynamic load balancing algorithm;GPU computations;MPI ranks;MPI communications","",8.0,"",25.0,"IEEE","24 Dec 2019","","","IEEE","IEEE Journals"
"Proofs of Physical Reliability for Cloud Storage Systems","L. Li; L. Lazos","Department of Electrical and Computer Engineering, The University of Arizona, Tucson, USA; Department of Electrical and Computer Engineering, The University of Arizona, Tucson, USA","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1048,1065,"Cloud service providers (CSPs) promise to reliably store repositories outsourced by clients. Unfortunately, once files have left the client's control, he has no means to verify their redundant storage. In this article, we develop Proof of Physical Reliability (PoPR) auditing mechanisms that prove that a CSP stores an outsourced repository across multiple physical storage nodes. A PoPR complements the existing proof-of-retrievability (PoR) and proof-of-data possession (PDP) methods that are concerned with file retrievability, but without any verification of the fault-tolerance to physical storage nodes failures. A PoPR goes beyond retrievability by verifying that a file is redundantly stored across multiple physical storage nodes according to a pre-agreed layout and can, therefore, survive node failures. The verification mechanism relies on a combination of storage integrity and timing tests on the simultaneous retrieval of a collection of file symbols from multiple storage nodes. Compared to the state-of-the-art, our approach accommodates CSPs with heterogeneous storage devices (hard disks, SSDs, etc.) and does not assume constant data processing nor network delays. Instead, it can operate under any delayvariance, because it relies only on (loose) delay bounds. We analytically prove the security of our construction and experimentally validate its success in heterogeneous storage settings.","1558-2183","","10.1109/TPDS.2019.2958919","National Science Foundation(grant numbers:CNS1813401); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930269","Proof of reliability;fault tolerance;data integrity;data security and privacy;storage reliability;retrievability","Delays;Protocols;Security;Layout;Data centers;Fault tolerance","cloud computing;storage management","cloud storage systems;cloud service providers;store repositories;client;redundant storage;PoPR;CSP stores;outsourced repository;multiple physical storage nodes;proof-of-retrievability;proof-of-data possession methods;file retrievability;physical storage nodes failures;node failures;storage integrity;timing tests;simultaneous retrieval;file symbols;multiple storage nodes;heterogeneous storage devices;heterogeneous storage settings;physical reliability auditing mechanisms","",1.0,"",43.0,"IEEE","10 Dec 2019","","","IEEE","IEEE Journals"
"WEED-MC: Wavelet Transform for Energy Efficient Data Gathering and Matrix Completion","V. K. Singh; B. Nathani; M. Kumar","Department of Information Technology, Indian Institute of Information Technology, Lucknow, India; CISCO Systems, Bengaluru, India; Department of Information Technology, Indian Institute of Information Technology, Allahabad, India","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1066,1073,"Compressed sensing based data gathering is in-efficient in small scale wireless sensor networks because of uncorrelated observations at the sink. Failure to exploit the low rank and suitable transform domain to explore the correlation structure of sensor data, results in significantly low recovery accuracy in such matrix completion algorithms for small scale networks. Targeting the spatio temporal correlation structure of the data, a novel data gathering and matrix completion scheme, which exploits the low rank property and compactness of sensor data which exists in wavelet transform domain, is proposed in this work. The compactness of the sensor data in wavelet transform domain is used for recovering the missing entries of the matrix. Experiments and simulations, for single-node and multi-node scenarios, prove the efficacy of the proposed approach over existing schemes significantly in terms of recovery accuracy even at extremely low sampling rate.","1558-2183","","10.1109/TPDS.2019.2954902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908738","Compressed sensing;correlation;data gathering;discreet wavelet transform;energy efficiency;low rank;sparsity;wireless sensor networks","Wireless sensor networks;Wavelet transforms;Distributed databases;Correlation;Information technology;Sensors","complex networks;compressed sensing;correlation methods;energy conservation;matrix algebra;telecommunication power management;wavelet transforms;wireless sensor networks","small scale networks;low sampling rate;recovery accuracy;single-node scenarios;multinode scenarios;matrix completion algorithms;small scale wireless sensor networks;low recovery accuracy;compressed sensing based data gathering;energy efficient data gathering;wavelet transform domain;WEED-MC;sensor data;low rank property;spatio temporal correlation structure","",2.0,"",17.0,"IEEE","21 Nov 2019","","","IEEE","IEEE Journals"
"Customer Perceived Value- and Risk-Aware Multiserver Configuration for Profit Maximization","T. Wang; J. Zhou; G. Zhang; T. Wei; S. Hu","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Technology, East China Normal University, Shanghai, China; School of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1074,1088,"Along with the wide deployment of infrastructures and the rapid development of virtualization techniques in cloud computing, more and more enterprises begin to adopt cloud services, inspiring the emergence of various cloud service providers. The goal of cloud service providers is to pursue profit maximization. To achieve this goal, cloud service providers need to have a good understanding of the economics of cloud computing. However, the existing pricing strategies rarely consider the interaction between user requests for services and the cloud service provider and hence cannot accurately reflect the supply and demand law of the cloud service market. In addition, few previous pricing strategies take into account the risk involved in the pricing contract. In this article, we first propose a dynamic pricing strategy that is developed based on the customer perceived value (CPV) and is able to accurately capture the real situation of supply and demand in marketing. The strategy is utilized to estimate the user's demand for cloud services. We then design a profit maximization scheme that is developed based on the CPV-aware dynamic pricing strategy and considers the risk in the pricing contract. The scheme is utilized to derive the optimal multiserver configuration for maximizing the profit. Extensive simulations are carried out to verify the proposed customer perceived value and risk-aware profit maximization scheme. As compared to two state of the art benchmarking methods, the proposed scheme gains 31.6 and 30.8 percent more profit on average, respectively.","1558-2183","","10.1109/TPDS.2019.2960024","National Natural Science Foundation of China(grant numbers:61802185,61272420); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180470); Fundamental Research Funds for the Central Universities(grant numbers:30919011233); The Hubei Key Laboratory of Intelligent Geo-Information Processing(grant numbers:KLIGIP-2018A04); Natural Science Foundation of Shanghai(grant numbers:16ZR1409000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933484","Cloud computing;customer perceived value;dynamic pricing;multiserver configuration;profit maximization;risk","Cloud computing;Pricing;Servers;Contracts;Computational modeling;Task analysis;Supply and demand","cloud computing;optimisation;pricing;profitability;quality of service;virtualisation","cloud service provider;cloud computing;cloud service market;pricing contract;CPV-aware dynamic pricing strategy;risk-aware profit maximization scheme;customer perceived valueand risk-aware multiserver configuration","",25.0,"",42.0,"IEEE","16 Dec 2019","","","IEEE","IEEE Journals"
"Efficient Performance Estimation and Work-Group Size Pruning for OpenCL Kernels on GPUs","X. Wang; X. Qian; A. Knoll; K. Huang","Department of Informatics, Chair of Robotics, Artificial Intelligence and Real-time Systems, Technical University of Munich, Garching, Germany; Ming Hsieh Department of Electrical Engineering, Department of Computer Science, University of Southern California, Los Angeles, USA; Department of Informatics, Chair of Robotics, Artificial Intelligence and Real-time Systems, Technical University of Munich, Garching, Germany; Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, School of Data and Computer Science, Sun Yat-sen University, Guangzhou, P. R. China","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1089,1106,"Graphic Processing Units (GPUs) play a vital role in state-of-the-art high-performance scientific computing realm and research work towards its performance analysis is crucial but nontrivial. Extant GPU performance models are far from practical use, while fine-grained GPU simulation requires a considerably large time cost. Moreover, massive amounts of designs with various program inputs and parameter settings pose a challenge for efficient performance estimation and tuning of parallel GPU applications. To this end, this article presents a hybrid framework for the efficient performance estimation and work-group size pruning of OpenCL workloads on GPUs. The framework contains a static module used to extract the kernel execution trace from the high-level source code and a dynamical module used to mimic the kernel execution flow to estimate the runtime performance. For the design space pruning, an extra analysis is performed to filter out the redundant work-group sizes with duplicated execution traces and inferior pipelines. The proposed framework does not require any program runs to estimate the performance and find the optimal or near-optimal designs. Experiments on four Commercial Off-The-Shelf (COTS) Nvidia GPUs show that the framework can predict the runtime performance with an average error of 17.04 percent and reduce the program design space by an average of 78.47 percent.","1558-2183","","10.1109/TPDS.2019.2958343","China Scholarship Council(grant numbers:201506270152); National Natural Science Foundation of China(grant numbers:61872393); National Science Foundation(grant numbers:NSF-CCF-1657333,NSF-CCF-1717754,NSF-CNS-1717984,NSF-CCF-1750656); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8928962","GPU;performance estimation;OpenCL;work-group size;performance tuning","Kernel;Graphics processing units;Measurement;Runtime;Estimation;Hardware;Analytical models","graphics processing units;parallel architectures;power aware computing","efficient performance estimation;work-group size pruning;OpenCL kernels;state-of-the-art high-performance scientific computing realm;performance analysis;extant GPU performance models;fine-grained GPU simulation;parallel GPU applications;kernel execution trace;kernel execution flow;runtime performance;design space pruning;redundant work-group","",2.0,"",50.0,"IEEE","9 Dec 2019","","","IEEE","IEEE Journals"
"Architectural Support for NVRAM Persistence in GPUs","S. Chen; L. Liu; W. Zhang; L. Peng","Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, USA; SKLCA, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Software School, Fudan University, Shanghai, China; Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, USA","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1107,1120,"Non-volatile Random Access Memories (NVRAM) have emerged in recent years to bridge the performance gap between the main memory and external storage devices, such as Solid State Drives (SSD). In addition to higher storage density, NVRAM provides byte-addressability, higher bandwidth, near-DRAM latency, and easier access compared to block devices such as traditional SSDs. This enables new programming paradigms taking advantage of durability and larger memory footprint. With the range and size of GPU workloads expanding, NVRAM will present itself as a promising addition to GPU's memory hierarchy. To utilize the non-volatility of NVRAMs, programs should allow durable stores, maintaining consistency through a power loss event. This is usually done through a logging mechanism that works in tandem with a transaction execution layer which can consist of a transactional memory or a locking mechanism. Together, this results in a transaction processing system that preserves the ACID properties. GPUs are designed with high throughput in mind, leveraging high degrees of parallelism. Transactional memory proposals enable fine-grained transactions at the GPU thread-level. However, with lower write bandwidths compared to that of DRAMs, using NVRAM as-is may yield sub-optimal overall system performance when threads experience long latency. To address this problem, we propose using Helper Warps to move persistence out of the critical path of transaction execution, alleviating the impact of latencies. Our mechanism achieves a speedup of 4.4 and 1.5 under bandwidth limits of 1.6 GB/s and 12 GB/s and is projected to maintain speed advantage even when NVRAM bandwidth gets as high as hundreds of GB/s in certain cases. Due to the speedup, our proposed method also results in reduction in overall energy consumption.","1558-2183","","10.1109/TPDS.2019.2960233","National Science Foundation(grant numbers:CCF-1422408,CNS-1527318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935351","NVRAM;persistence;GPUs;helper warps","Random access memory;Nonvolatile memory;Bandwidth;Graphics processing units;Hardware;Performance evaluation;Instruction sets","cache storage;DRAM chips;flash memories;graphics processing units;integrated circuit design;transaction processing","ACID properties;DRAMs;Helper Warps;locking mechanism;logging mechanism;power loss event;system performance;GPU memory hierarchy;GPU workloads;memory footprint;programming paradigms;block devices;byte-addressability;storage density;Solid State Drives;external storage devices;performance gap;Nonvolatile Random Access Memories;NVRAM persistence;architectural support;NVRAM bandwidth;GPU thread-level;fine-grained transactions;transactional memory;transaction processing system;transaction execution layer;durable stores;bit rate 1.6 Gbit/s;bit rate 12.0 Gbit/s;SSDs","",2.0,"",49.0,"IEEE","17 Dec 2019","","","IEEE","IEEE Journals"
"Thread-Level Locking for SIMT Architectures","L. Gao; Y. Xu; R. Wang; Z. Luan; Z. Yu; D. Qian","Beijing Key Laboratory of Electronic System Reliability and Prognostics, College of Information Engineering, Capital Normal University, Beijing, China; Xi'an Jiaotong University; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; School of Computer Science and Engineering, Beihang University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1121,1136,"As more emerging applications are moving to GPUs, thread-level synchronization has become a requirement. However, GPUs only provide warp-level and thread-block-level rather than thread-level synchronization. Moreover, it is highly possible to cause live-locks by using CPU synchronization mechanisms to implement thread-level synchronization for GPUs. In this article, we first propose a software-based thread-level synchronization mechanism called lock stealing for GPUs to avoid live-locks. We then describe how to implement our lock stealing algorithm in mutual exclusive locks and readers-writer locks with high performance. Finally, by putting it all together, we develop a thread-level locking library (TLLL) for commercial GPUs. To evaluate TLLL and show its general applicability, we use it to implement six widely used programs. We compare TLLL against the state-of-the-art ad-hoc GPU synchronization, GPU software transactional memory (STM), and CPU hardware transactional memory (HTM), respectively. The results show that, compared with the ad-hoc GPU synchronization for Delaunay mesh refinement (DMR), TLLL improves the performance by 22 percent on average on a GTX970 GPU, and shows up to 11 percent of performance improvement on a Volta V100 GPU. Moreover, it significantly reduces the required memory size. Such low memory consumption enables DMR to successfully run on the GTX970 GPU with the 10-million mesh size, and the V100 GPU with the 40-million mesh size, with which the ad-hoc synchronization can not run successfully. In addition, TLLL outperforms the GPU STM by 65 percent, and the CPU HTM (running on a Xeon E5-2620 v4 CPU with 16 hardware threads) by 43 percent on average.","1558-2183","","10.1109/TPDS.2019.2955705","National Key R&D Program of China(grant numbers:2017YFB0202202,2018YFB0203901,2017YFC0820100,2016YFB1000204); National Natural Science Foundation of China(grant numbers:61732002,61672511,61702495,61772350); Shenzhen Technology Research Project(grant numbers:JSGG20160510154636747); outstanding technical talent program of CAS; Common Information System Equipment Research Program(grant numbers:JZX2017-0988/Y300); Beijing Nova program(grant numbers:Z181100006218093); Beijing Innovation Center for Future Chip(grant numbers:KYJJ2018008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911260","Deadlocks;parallelism and concurrency;runtime environments;SIMD processors;synchronization","Graphics processing units;Synchronization;Message systems;System recovery;Instruction sets;Hardware;Computer architecture","concurrency control;graphics processing units;mesh generation;multiprocessing systems;multi-threading;parallel architectures;synchronisation;transaction processing","readers-writer locks;thread-level locking library;TLLL;state-of-the-art ad-hoc GPU synchronization;GPU software transactional memory;GTX970 GPU;ad-hoc synchronization;hardware threads;warp-level;thread-block-level;live-locks;CPU synchronization mechanisms;software-based thread-level synchronization mechanism;lock stealing;mutual exclusive locks;V100 GPU;DMR;GPU STM;GPU HTM;Xeon E5-2620 v4 CPU","",2.0,"",38.0,"IEEE","25 Nov 2019","","","IEEE","IEEE Journals"
"REACT: Scalable and High-Performance Regular Expression Pattern Matching Accelerator for In-Storage Processing","W. S. Jeong; C. Lee; K. Kim; M. K. Yoon; W. Jeon; M. Jung; W. W. Ro","School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1137,1151,"This article proposes REACT, a regular expression matching accelerator, which can be embedded in a modern Solid-State Drive (SSD) and a novel data access scheduling algorithm for high matching throughput. Specifically, REACT, including our data access scheduling algorithm, increases the utilization of SSD and the degree of internal memory parallelism for pattern matching processes. While the low-level flash exhibits long latency, modern SSDs in practice achieve high I/O performance by utilizing the massive internal parallelism at the system-level. However, exploiting the parallelism is limited for pattern matching since the subblocks, which constitute an input data and can be placed in multiple flash pages, should be tested in a sequence to process the input correctly. This limitation can induce low utilization of the accelerator. To address this challenge, the proposed REACT simultaneously processes multiple input streams with a parallel processing architecture to maximize matching throughput by hiding the long and irregular latency. The scheduling algorithm finds a data stream which requires a sub-block in closest time and prioritizes the access request to reduce the data stall of REACT. REACT achieves maximum 22.6 percent of matching throughput improvement on a 16channel high-performance SSD compared to the accelerator without the proposed scheduling algorithm.","1558-2183","","10.1109/TPDS.2019.2953646","National Research Foundation of Korea(grant numbers:NRF-2018R1A2A2A05018941); Memory Division of Samsung Electronics Co., Ltd.(grant numbers:NRF 2016R1C1B2015312,NRF 2017R1A4A1015498); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8901167","In-storage processing (ISP);regular expression matching;accelerator;solid-state drive","Delays;Pattern matching;Scheduling algorithms;Throughput;Computer architecture;Performance evaluation","flash memories;parallel architectures;pattern matching;processor scheduling;solid state drives","REACT;multiple input streams;parallel processing architecture;data stream;data stall;high-performance SSD;In-Storage Processing;regular expression matching accelerator;solid-state drive;novel data access scheduling algorithm;high matching throughput;internal memory parallelism;pattern matching processes;massive internal parallelism;multiple flash pages;modern SSD;scheduling algorithm;matching throughput maximization;high I/O performance;low-level flash;low utilization;efficiency 22.6 percent","",5.0,"",45.0,"IEEE","14 Nov 2019","","","IEEE","IEEE Journals"
"Reducing the Impact of Intensive Dynamic Memory Allocations in Parallel Multi-Threaded Programs","D. Langr; M. Kočička","Department of Computer Systems, Faculty of Information Technology, Czech Technical University in Prague, Praha, Czech Republic; Department of Computer Systems, Faculty of Information Technology, Czech Technical University in Prague, Praha, Czech Republic","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1152,1164,"Frequent dynamic memory allocations (DyMAs) can significantly hinder the scalability of parallel multi-threaded programs. As the number of threads grows, DyMAs can even become the main performance bottleneck. We introduce modern tools and methods for evaluating the impact of DyMAs and present techniques for its reduction, which include scalable heap implementations, small buffer optimization, and memory pooling. Additionally, we provide a survey of state-of-the-art implementations of these techniques and study them experimentally by using a benchmark program, server simulator software, and a real-world high-performance computing application. As a result, we show that relatively small modifications in parallel program's source code or a way of its execution may substantially reduce the runtime overhead associated with the use of dynamic data structures.","1558-2183","","10.1109/TPDS.2019.2960514","Ministerstvo Školství, Mládeže a Tělovýchovy(grant numbers:CZ.02.1.01/0.0/0.0/16_019/0000765); Ministry of Education, Youth and Science; NSF(grant numbers:OCI-0725070,ACI-1238993); state of Illinois; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936486","Dynamic memory allocation;memory pooling;multi-threading;parallel program;scalable heap implementation;shared memory;small buffer optimization","Benchmark testing;Message systems;Data structures;C++ languages;Runtime;Libraries;Dynamic scheduling","data structures;multiprocessing systems;multi-threading;parallel programming;storage management","DyMAs;scalable heap implementations;memory pooling;state-of-the-art implementations;benchmark program;high-performance computing application;parallel program;dynamic data structures;intensive dynamic memory allocations;parallel multithreaded programs;frequent dynamic memory allocations;performance bottleneck","","","",38.0,"IEEE","18 Dec 2019","","","IEEE","IEEE Journals"
"Task Scheduling for Energy Consumption Constrained Parallel Applications on Heterogeneous Computing Systems","Z. Quan; Z. -J. Wang; T. Ye; S. Guo","College of Information Science and Engineering, Hunan University, Changsha, China; National Engineering Laboratory for Big Data Analysis and Applications, Beijing, China; College of Information Science and Engineering, Hunan University, Changsha, China; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1165,1182,"Power-aware task scheduling on processors has been a research hotspot in computing systems. Given an application G containing a set N of tasks {n1,...,n|N|}, and a system containing a set U of processors {u1,..., u|U|}, the power-aware task scheduling generally refers to finding the appropriate processor and frequency for each task ni, so as to make sure that all the tasks can be finished efficiently and the overall energy consumption is guaranteed. In this article, we study the problem of minimizing the schedule length for energy consumption constrained parallel applications on heterogeneous computing systems, where the schedule length refers to the time interval between starting the first task and finishing the last task. For this problem, existing work adopts a policy that preassigns the minimum energy consumption for each unassigned task. Nevertheless, our analysis reveals that, such a pre-assignment policy could be unfair for the low priority tasks, and it may not achieve an optimistic schedule length. Thereby, we propose a new task scheduling algorithm that suggests a weight-based mechanism to preassign energy consumption for unassigned tasks, and we provide the rigorous proof to show its feasibility. Further, we show that this idea can be extended to solve reliability maximization problems with energy consumption constraint or with both deadline and energy consumption constraints, where the reliability refers to the probability of executing application G without failures, and the deadline constraint refers to the “allowable” maximum schedule length. We have conducted extensive experiments based on real parallel applications. The experimental results consistently demonstrate that our proposed algorithms can achieve favourable performance, compared to state-of-the-art algorithms.","1558-2183","","10.1109/TPDS.2019.2959533","National Key R&D Program of China(grant numbers:2018YFB0204100); National Natural Science Foundation of China(grant numbers:61472124,61472453,61602166,61702320,U1401256,U1501252,U1611264,U1711261,U1711262,U1811264,61972425); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936469","heterogeneous systems;energy consumption;parallel application;task scheduling;reliability","Task analysis;Energy consumption;Reliability;Program processors;Schedules;Processor scheduling;Scheduling","optimisation;parallel processing;power aware computing;processor scheduling","allowable maximum schedule length;energy consumption constraint;task scheduling algorithm;optimistic schedule length;low priority tasks;unassigned task;minimum energy consumption;appropriate processor;power-aware task scheduling;heterogeneous computing systems;energy consumption constrained parallel applications","",24.0,"",55.0,"IEEE","18 Dec 2019","","","IEEE","IEEE Journals"
"Decentralized Utility- and Locality-Aware Replication for Heterogeneous DHT-Based P2P Cloud Storage Systems","Y. Hassanzadeh-Nazarabadi; A. Küpçü; O. Ozkasap","Department of Computer Engineering, Koç University, Istanbul, Turkey; Department of Computer Engineering, Koç University, Istanbul, Turkey; Department of Computer Engineering, Koç University, Istanbul, Turkey","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1183,1193,"As a Distributed Hash Table (DHT), Skip Graph routing overlays are exploited in several peer-to-peer (P2P) services, including P2P cloud storage. The fully decentralized replication algorithms that are applicable to the Skip Graph-based P2P cloud storage fail on improving the performance of the system with respect to both the availability of replicas as well as their response time. Additionally, they presume the system as homogeneous with respect to the nodes' latency distribution, availability behavior, and bandwidth, or storage. In this article, we propose Pyramid, which is the first fully decentralized utility- and locality-aware replication approach for Skip Graph-based P2P cloud storage systems. Pyramid considers the nodes as heterogeneous with respect to their latency distribution, availability behavior, bandwidth, and storage. Pyramid is utility-aware as it maximizes the average available bandwidth of replicas per time slot (e.g., per hour). Additionally, Pyramid is locality-aware as it minimizes the average latency between nodes and their closest replica. Our simulation results show that compared to the state-of-the-art solutions that either perform good in utility-awareness, or in locality-awareness, our proposed Pyramid improves both the utility- and locality-awareness of replicas with a gain of about 1.2 and 1.1 times at the same time, respectively.","1558-2183","","10.1109/TPDS.2019.2960018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933479","P2P systems;cloud storage;distributed hash tables;skip graphs;replication;availability;locality","Peer-to-peer computing;Cloud computing;Bandwidth;IP networks;Routing;Numerical models;Protocols","cloud computing;graph theory;peer-to-peer computing;storage management","heterogeneous DHT-Based P2P cloud storage systems;skip graph routing overlays;skip graph-based P2P cloud storage systems;locality-awareness;utility-awareness;average available bandwidth;utility-aware;latency distribution;locality-aware replication approach;fully decentralized utility;Pyramid;availability behavior;fully decentralized replication algorithms;peer-to-peer services;distributed hash table","",16.0,"",47.0,"IEEE","16 Dec 2019","","","IEEE","IEEE Journals"
"Massively Scaling Seismic Processing on Sunway TaihuLight Supercomputer","Y. Hu; H. Yang; Z. Luan; L. Gan; G. Yang; D. Qian","Sino-German Joint Software Institute, School of Computer Science and Engineering, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science and Engineering, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science and Engineering, Beihang University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1194,1208,"Common Midpoint (CMP) and Common Reflection Surface (CRS) are widely used methods for improving the signal-to-noise ratio in the field of seismic processing. These methods are computationally intensive and require high-performance computing. This article optimizes these methods on the Sunway many-core architecture and implements large-scale seismic processing on the Sunway Taihulight supercomputer. We propose the following three optimization techniques: 1) we propose a software cache method to reduce the overhead of memory accesses, and share data among CPEs via the register communication; 2) we re-design the semblance calculation procedure to further reduce the overhead of memory accesses; 3) we propose a vectorization method to improve the performance when processing the small volume of data within short loops. The experimental results show that our implementations of CMP and CRS methods on Sunway achieve 3.50× and 3.01× speedup on average compared to the-state-of-the-art implementations on CPU. In addition, our implementation is capable to run on more than one million cores of Sunway TaihuLight with good scalability.","1558-2183","","10.1109/TPDS.2019.2962395","National Key R&D Program of China(grant numbers:2016YFB1000503,2016YFB0200100); National Natural Science Foundation of China(grant numbers:61502019,61732002); State Key Laboratory of Software Development Environment(grant numbers:SKLSDE-2018ZX-19); Center for High Performance Computing and System Simulation; Pilot National Laboratory for Marine Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943329","Many-core architecture;sunway taihulight;seismic processing;common midpoint;common reflection surface;performance optimization","Computer architecture;Software;Surface treatment;Stacking;Acceleration;Supercomputers;Optimization","geophysical techniques;geophysics computing;seismology","high-performance computing;signal-to-noise ratio;common reflection surface;common midpoint;Sunway TaihuLight supercomputer;seismic processing;CRS method;CMP method;vectorization method;semblance calculation procedure;share data;memory accesses;software cache method;optimization techniques;Sunway Taihulight supercomputer;Sunway many-core architecture","",7.0,"",31.0,"IEEE","25 Dec 2019","","","IEEE","IEEE Journals"
"gMig: Efficient vGPU Live Migration with Overlapped Software-Based Dirty Page Verification","Q. Lu; X. Zheng; J. Ma; Y. Dong; Z. Qi; J. Yao; B. He; H. Guan","Intel Corporation; Alibaba Group, Hangzhou, China; Intel Corporation; Intel Corporation; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; National University of Singapore, Singapore; Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1209,1222,"This paper introduces gMig, an open-source and practical vGPU live migration solution for full virtualization. Taking the advantage of the dirty pattern of GPU workloads, gMig presents the One-Shot Pre-Copy mechanism combined with the hashing based Software Dirty Page technique to achieve efficient vGPU live migration. Particularly, we propose three core techniques for gMig: 1) Dynamic Graphics Address Remapping, which parses and manipulates GPU commands to adjust the address mapping and adapt to a different environment after migration, 2) Software Dirty Page, which utilizes a hashing based approach with sampling pre-filtering to detect page modification, overcomes the commodity GPU's hardware limitation, and speeds up the migration by only sending the dirtied pages, 3) Overlapped Migration Process, which significantly compresses the hanging overhead by overlapping the dirty page verification and transmission concurrently. Our evaluation shows that gMig achieves GPU live migration with an average downtime of 302 ms on Windows and 119 ms on Linux. With the help of Software Dirty Page, the number of GPU pages transferred during the downtime is effectively reduced by up to 80.0 percent . The design of sampling filter and overlapped processing can bring about further 30.0 and 10.0 percent improvements in page processing.","1558-2183","","10.1109/TPDS.2019.2947521","National Key Research & Development Program of China(grant numbers:2016YFB1000502); National Natural Science Foundation of China(grant numbers:61672344,61525204,61572322,61732010); Shanghai Key Laboratory of Scalable Computing and Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869867","GPU;virtualization;migration","Graphics processing units;Virtualization;Hardware;Cloud computing;Computer architecture;Performance evaluation","graphics processing units;Linux;storage management;virtual machines;virtualisation","one-shot pre-copy mechanism;vGPU;GPU workloads;dirty pattern;practical vGPU live migration;overlapped software-based dirty page verification;page processing;overlapped processing;GPU pages;gMig;migration process;dirtied pages;commodity GPU hardware limitation;page modification;sampling pre-filtering;hashing based approach;core techniques;software dirty page technique","",3.0,"",39.0,"IEEE","16 Oct 2019","","","IEEE","IEEE Journals"
"Exploring Token-Oriented In-Network Prioritization in Datacenter Networks","K. Liu; B. Tian; C. Tian; B. Li; Q. Wang; J. Zheng; J. Sun; Y. Gao; W. Wang; G. Chen; W. Dou; Y. Jiang; H. Zhou; J. Jiang; F. Zhang; G. Zhang","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Huawei Technologies Company, Ltd, Shenzhen, China; Huawei Technologies Company, Ltd, Shenzhen, China; Huawei Technologies Company, Ltd, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,5.0,1223,1238,"In memory computing and high-end distributed storage demand low latency, high throughput, and zero data loss simultaneously from datacenter networks. Existing reactive congestion control approaches cannot both minimize queuing latency and ensure zero data loss. A token-oriented proactive approach can achieve them together by controlling congestion even before sending data packets. However, state-of-the-art token-oriented approaches only strive to optimize network-level metrics: maximizing throughput while achieving flow-level fairness. This article answers the question of how to support objective-aware traffic scheduling in token-oriented approaches. The novelty of Token-Oriented in-network Prioritization (TOP) is that it prioritizes tokens instead of data packets. We make three contributions. Via simulations over a hypothetical TOP system, our first contribution is demonstrating the potential performance gain that can be brought by TOP. Second, we investigate the applicability of TOP. Although the overhead of enabling necessary TOP features in switches is trivial, we find that mainstream commodity datacenter switches do not support them. We hence propose a readily-deployable remedy to achieve in-network prioritization by pushing both switch and end-host hardware capacity to an extreme end. Lastly, we implement a running TOP system with Linux hosts and commodity switches, and evaluate TOP in testbeds and with large-scale simulations for various scenarios.","1558-2183","","10.1109/TPDS.2019.2958899","National Key R&D Program of China(grant numbers:2018YFB1003505); National Natural Science Foundation of China(grant numbers:61772265,61802172); Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930303","Token-oriented proactive congestion control;datacenters","Receivers;Delays;Throughput;Control systems;Distributed databases;Hardware","computer centres;computer networks;Linux;queueing theory;telecommunication congestion control;telecommunication switching;telecommunication traffic","memory computing;queuing latency minimization;throughput maximization;large-scale simulations;end-host hardware capacity;TOP system;objective-aware traffic scheduling;network-level metric optimization;flow-level fairness;token-oriented proactive approach;reactive congestion control approaches;zero data loss;high-end distributed storage;datacenter networks;mainstream commodity datacenter switches;token-oriented in-network prioritization;data packets","",2.0,"",60.0,"IEEE","10 Dec 2019","","","IEEE","IEEE Journals"
"GRP-HEFT: A Budget-Constrained Resource Provisioning Scheme for Workflow Scheduling in IaaS Clouds","H. R. Faragardi; M. R. Saleh Sedghpour; S. Fazliahmadi; T. Fahringer; N. Rasouli","Institute of Computer Sceience, University of Innsbruck, Innsbruck, Austria; Iran University of Science and Technology, Tehran, Iran; Iran University of Science and Technology, Tehran, Iran; Institute of Computer Sceience, University of Innsbruck, Innsbruck, Austria; Department of Computer Engineering, Technical and Vocational University, Alborz, Iran","IEEE Transactions on Parallel and Distributed Systems","22 Jan 2020",2020,31.0,6.0,1239,1254,"In Infrastructure as a Service (IaaS) Clouds, users are charged to utilize cloud services according to a pay-per-use model. If users intend to run their workflow applications on cloud resources within a specific budget, they have to adjust their demands for cloud resources with respect to this budget. Although several scheduling approaches have introduced solutions to optimize the makespan of workflows on a set of heterogeneous IaaS cloud resources within a certain budget, the hourly-based cost model of some well-known cloud providers (e.g., Amazon EC2 Cloud) can easily lead to a higher makespan and some schedulers may not find any feasible solution. In this article, we propose a novel resource provisioning mechanism and a workflow scheduling algorithm, named Greedy Resource Provisioning and modified HEFT (GRP-HEFT), for minimizing the makespan of a given workflow subject to a budget constraint for the hourly-based cost model of modern IaaS clouds. As a resource provisioning mechanism, we propose a greedy algorithm which lists the instance types according to their efficiency rate. For our scheduler, we modified the HEFT algorithm to consider a budget limit. GRP-HEFT is compared against state-of-the-art workflow scheduling techniques, including MOACS (MultiObjective Ant Colony System), PSO (Particle Swarm Optimization), and GA (Genetic Algorithm). The experimental results demonstrate that GRP-HEFT outperforms GA, PSO, and MOACS for several well-known scientific workflow applications for different problem sizes on average by 13.64, 19.77, and 11.69 percent, respectively. Also in terms of time complexity, GRP-HEFT outperforms GA, PSO and MOACS.","1558-2183","","10.1109/TPDS.2019.2961098","The Austrian Research Promotion Agency(grant numbers:868018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937813","Cloud computing;workflow scheduling;resource provisioning;budget-constrained scheduling","Scheduling;Cloud computing;Task analysis;Genetic algorithms;Schedules;Scheduling algorithms","budgeting;cloud computing;genetic algorithms;greedy algorithms;particle swarm optimisation;resource allocation;scheduling;workflow management software","budget constraint;modern IaaS clouds;HEFT algorithm;budget limit;state-of-the-art workflow;scientific workflow applications;cloud providers;budget-constrained resource provisioning scheme;greedy resource provisioning;workflow subject;workflow scheduling algorithm;resource provisioning mechanism;Amazon EC2 Cloud;hourly-based cost model;heterogeneous IaaS cloud resources;scheduling approaches;pay-per-use model;cloud services;GRP-HEFT","",63.0,"",33.0,"IEEE","20 Dec 2019","","","IEEE","IEEE Journals"
"NVGraph: Enforcing Crash Consistency of Evolving Network Analytics in NVMM Systems","S. Lim; T. Coy; Z. Lu; B. Ren; X. Zhang","School of Computer Science and Engineering, Washington State University, Vancouver, USA; School of Computer Science and Engineering, Washington State University, Vancouver, USA; School of Computer Science and Engineering, Washington State University, Vancouver, USA; Department of Computer Science, College of William and Mary, Williamsburg, USA; School of Computer Science and Engineering, Washington State University, Vancouver, USA","IEEE Transactions on Parallel and Distributed Systems","24 Jan 2020",2020,31.0,6.0,1255,1269,"Many complex networks can be modeled as evolving graphs. Existing in-memory graph data structures are designed for DRAM. Thus, they cannot effectively exploit the current and ongoing adoption of emerging non-volatile main memory (NVMM) for two reasons. (1) Ephemeral graph data structures are not crash-consistent for NVMM. (2) NVMM writes and reads and may incur higher latency than DRAM. In this article, we propose a novel persistent evolving graph data structure, named NVGRAPH, for both computing and in-memory storage of evolving graphs in NVMM. We devise NVGRAPH as a multi-version data structure, wherein a minimum of one version of its data is stored in NVMM to provide the desired durability at runtime for failure recovery, and another version is stored in both DRAM and NVMM to reduce the NVMM-induced memory latency. NVGRAPH is also a partitioned data structure. We dynamically transform the layout of NVGRAPH (e.g., changing the size of its partition in DRAM) exploiting network properties and data access patterns of workloads. For the evaluation of NVGRAPH, we implement four representative real-world graph applications: pagerank, BFS, influence maximization, and rumor source detection. The experimental results show that the performance of NVGRAPH is comparable to other in-memory data structure (e.g., CSR and LLAMA) while using 70 percent less DRAM. It scales well up to 10 billion edges and 201 snapshots and supports crash consistency. It offers up to the 21X speedup of execution time compared to the scale-up graph computation approaches (e.g., GraphChi and X-stream).","1558-2183","","10.1109/TPDS.2020.2965452","National Science Foundation(grant numbers:CNS-1906541); DoE Electricity Industry Technology and Practices Innovation Challenge Award; WSU Vancouver Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955949","Evolving graphs;crash consistency;non-volatile main memory;graph layout transformation","Nonvolatile memory;Random access memory;Computer crashes;Runtime;Layout;Arrays","data structures;DRAM chips;failure analysis;graph theory;integrated circuit reliability","rumor source detection;influence maximization;pagerank;representative real-world graph applications;data access patterns;failure recovery;ephemeral graph data structures;nonvolatile main memory;scale-up graph computation approaches;in-memory data structure;real-world graph applications;partitioned data structure;NVMM-induced memory latency reduction;multiversion data structure;in-memory storage;RAPH;NVG;DRAM;in-memory graph data structures;complex networks;NVMM systems;evolving network analytics;crash consistency;efficiency 70.0 percent","",3.0,"",75.0,"IEEE","10 Jan 2020","","","IEEE","IEEE Journals"
"Online Deadline-Aware Task Dispatching and Scheduling in Edge Computing","J. Meng; H. Tan; X. -Y. Li; Z. Han; B. Li","LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China; LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China; LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China; LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China; LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China","IEEE Transactions on Parallel and Distributed Systems","24 Jan 2020",2020,31.0,6.0,1270,1286,"In this article, we study online deadline-aware task dispatching and scheduling in edge computing. We jointly considerthe management of the networking and computing resources to meet the maximum number of deadlines. We propose an online algorithm, named Dedas, which greedily schedules newly arriving tasks and considers whether to replace some existing tasks in order to make the new deadlines satisfied. We derive a non-trivial competitive ratio of Dedas theoretically, and our analysis is asymptotically tight. Besides, we implement a distributed approximation D - Dedas with a better scalability and less than 10 percent performance loss compared with the centralized algorithm Dedas. We then build DeEdge, an edge computing testbed installed with typical latency-sensitive applications such as IoT sensor monitoring and face matching. We adopt a real-world data trace from the Google cluster for large-scale emulations. Extensive testbed experiments and simulations demonstrate that the deadline miss ratio of Dedas is stable for online tasks, which is reduced by up to 60 percent compared with state-of-the-art methods. Moreover, Dedas performs well in minimizing the average task completion time.","1558-2183","","10.1109/TPDS.2019.2961905","National Key R&D Program of China(grant numbers:2018YFB0803400); China National Funds for Distinguished Young Scientists(grant numbers:61625205); National Natural Science Foundation of China(grant numbers:61772489,61751211); Key Research Program of Frontier Sciences(grant numbers:QYZDY-SSW-JSC002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941266","Edge computing;task dispatching and scheduling;deadline-aware tasks;online algorithm","Task analysis;Servers;Bandwidth;Processor scheduling;Computational modeling;Cloud computing;Edge computing","computational complexity;distributed processing;power aware computing;processor scheduling;resource allocation;scheduling","Google cluster;face matching;IoT sensor monitoring;DeEdge;computing resources;online deadline-aware task dispatching;average task completion time;online tasks;edge computing;centralized algorithm Dedas;greedily schedules;online algorithm","",49.0,"",31.0,"IEEE","24 Dec 2019","","","IEEE","IEEE Journals"
"Faster Parallel Core Maintenance Algorithms in Dynamic Graphs","Q. -S. Hua; Y. Shi; D. Yu; H. Jin; J. Yu; Z. Cai; X. Cheng; H. Chen","National Engineering Research Center-Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China; National Engineering Research Center-Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China; School of Computer Science and Technology, Shandong University, Qingdao, P.R. China; National Engineering Research Center-Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China; School of Computer Science and Technology, Shandong Academy of Sciences, Jinan, Shandong, P.R. China; Department of Computer Science, Georgia State University, Atlanta, USA; School of Computer Science and Technology, Shandong University, Qingdao, P.R. China; National Engineering Research Center-Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China","IEEE Transactions on Parallel and Distributed Systems","24 Jan 2020",2020,31.0,6.0,1287,1300,"This article studies the core maintenance problem for dynamic graphs which requires to update each vertex's core number with the insertion/deletion of vertices/edges. Previous algorithms can either process one edge associated with a vertex in each iteration or can only process one superior edge associated with the vertex (an edge 〈u; v〉 is a superior edge of vertex u if v' core number is no less than u's core number) in each iteration. Thus for high superior-degree vertices (the vertices associated with many superior edges) insertions/deletions, previous algorithms become very inefficient. In this article, we discovered a new structure called joint edge set whose insertions/deletions make each vertex's core number change at most one. The joint edge set mainly contains all the superior edges associated with the high superior-degree vertices as long as these vertices are 3+-hop independent. Based on this discovery, faster parallel algorithms are devised to solve the core maintenance problems. In our algorithms, we can process all edges in the joint edge set in one iteration and thus can greatly increase the parallelism and reduce the processing time. The results of extensive experiments conducted on various types of real-world, temporal, and synthetic graphs illustrate that the proposed algorithms achieve good efficiency, stability and scalability. Specifically, the new algorithms can outperform the single-edge processing algorithms by up to four orders of magnitude. Compared with the matching based algorithm and the superior edge based algorithm, our algorithms show a significant speedup up to 60× in the processing time.","1558-2183","","10.1109/TPDS.2019.2960226","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1003203); National Natural Science Foundation of China(grant numbers:61572216,61832006,61971269,61672321,61832012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935160","Graph analysis;core maintenance problem;parallel algorithm","Maintenance engineering;Heuristic algorithms;Multicore processing;Parallel algorithms;Computer science;Indexes;Stability analysis","computational complexity;graph theory;parallel algorithms","single-edge processing algorithms;matching based algorithm;superior edge based algorithm;dynamic graphs;superior-degree vertices;parallel core maintenance algorithms;vertex core number;joint edge set;synthetic graphs;temporal graphs","",37.0,"",26.0,"IEEE","17 Dec 2019","","","IEEE","IEEE Journals"
"Concurrent Irrevocability in Best-Effort Hardware Transactional Memory","R. Titos-Gil; R. Fernández-Pascual; A. Ros; M. E. Acacio","Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain","IEEE Transactions on Parallel and Distributed Systems","24 Jan 2020",2020,31.0,6.0,1301,1315,"Existing best-effort requester-wins implementations of transactional memory must resort to non-speculative execution to provide forward progress in the presence of transactions that exceed hardware capacity, experience page faults or suffer high-contention leading to livelocks. Current approaches to irrevocability employ lock-based synchronization to achieve mutual exclusion when executing a transaction non-speculatively, conservatively precluding concurrency with any other transactions in order to guarantee atomicity at the cost of degrading performance. In this article, we propose a new form of concurrent irrevocability whose goal is to minimize the loss of concurrency paid when transactions resort to irrevocability to complete. By enabling optimistic concurrency control also during non-speculative execution of a transaction, our proposal allows for higher parallelism than existing schemes. We describe the extensions to the instruction set to provide concurrent irrevocable transactions as well as the architectural extensions required to realize them on a best-effort HTM system without requiring any modification to the cache coherence protocol. Our evaluation shows that our proposal achieves an average reduction of 12.5 percent in execution time across the STAMP benchmarks, with 15.8 percent on average for highly contended workloads.","1558-2183","","10.1109/TPDS.2019.2963030","Spanish MCIU and AEI; European Commission(grant numbers:RTI2018-098156-B-C53); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945317","Parallel programming;multicore architectures;transactional memory","Hardware;Proposals;Benchmark testing;Synchronization;Concurrent computing;Software;Concurrency control","concurrency control;protocols;storage management;synchronisation;transaction processing","concurrency paid loss mimimization;lock-based synchronization;STAMP benchmarks;cache coherence protocol;instruction set;performance degradation cost;HTM system;hardware transactional memory;concurrent irrevocable transactions;optimistic concurrency control;concurrent irrevocability;irrevocability employ lock-based synchronization;experience page faults;hardware capacity;nonspeculative execution;efficiency 15.8 percent","",1.0,"",25.0,"IEEE","30 Dec 2019","","","IEEE","IEEE Journals"
"SLEEF: A Portable Vectorized Library of C Standard Mathematical Functions","N. Shibata; F. Petrogalli","Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan; ARM 110, Cambridge, United Kingdom","IEEE Transactions on Parallel and Distributed Systems","24 Jan 2020",2020,31.0,6.0,1316,1327,"In this article, we present techniques used to implement our portable vectorized library of C standard mathematical functions written entirely in C language. In order to make the library portable while maintaining good performance, intrinsic functions of vector extensions are abstracted by inline functions or preprocessor macros. We implemented the functions so that they can use sub-features of vector extensions such as fused multiply-add, mask registers, and extraction of mantissa. In order to make computation with SIMD instructions efficient, the library only uses a small number of conditional branches, and all the computation paths are vectorized. We devised a variation of the Payne-Hanek argument reduction for trigonometric functions and a floating point remainder, both of which are suitable for vector computation. We compare the performance with our library to Intel SVML.","1558-2183","","10.1109/TPDS.2019.2960333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936472","Parallel and vector implementations;SIMD processors;elementary functions;floating-point arithmetic","Libraries;Registers;Standards;Program processors;Optimization;Open source software;Computer architecture","computational complexity;digital arithmetic;floating point arithmetic;microprocessor chips;parallel processing;software libraries;vectors","C standard mathematical functions;portable vectorized library;intrinsic functions;vector extensions;inline functions;trigonometric functions;vector computation;Payne-Hanek argument reduction;Intel SVML;C language;SIMD instructions","",8.0,"",53.0,"CCBY","18 Dec 2019","","","IEEE","IEEE Journals"
"ERA-LSTM: An Efficient ReRAM-Based Architecture for Long Short-Term Memory","J. Han; H. Liu; M. Wang; Z. Li; Y. Zhang","Beijing National Research Center for Information Science and Technology, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China; Beijing National Research Center for Information Science and Technology, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","24 Jan 2020",2020,31.0,6.0,1328,1342,"Processing-in-memory (PIM) architecture based on resistive random access memory (ReRAM) crossbars is a promising solution to the memory bottleneck that long short-term memory (LSTM) faces. Based on the dataflow analysis of the LSTM computing paradigm, this article proposes to adopt the ReRAM-based analog approximate computing to conduct the LSTM-specific element-wise computation. Combined with the dot-product computation implemented with ReRAM crossbars, a new LSTM processing tile is designed to significantly reduce the demand for analog-to-digital converters (ADCs), which is the major part of power consumption of existing designs. Next, we elaborate on a mapping scheme to efficiently deploy large-scale LSTM onto multiple processing tiles. Finally, an architecture enhancement is proposed to support crossbar-friendly LSTM pruning to further improve efficiency. This overall design, named ERA-LSTM, is presented. Our evaluation shows that it can outperform two state-of-the-art FPGA-based LSTM accelerators by 103.6 and 35.9 times, respectively; compared with a state-of-the-art ReRAM-based LSTM accelerator with digital element-wise computation, it is 6.1 times more efficient. Moreover, our experiments demonstrate that the impact of hardware constraints and approximation errors on the inference accuracy can be effectively reduced by the proposed fine-tuning scheme and by optimizing the design of the approximator.","1558-2183","","10.1109/TPDS.2019.2962806","Beijing Academy of Artificial Intelligence; Beijing Innovation Center for Future Chip; Tsinghua University; Science and Technology Innovation Special Zone Project, China; Tsinghua University(grant numbers:2018Z05JDX005); China Postdoctoral Science Foundation(grant numbers:2019M650030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944023","Long short-term memory (LSTM);resistive random-access memory (ReRAM);processing in memory (PIM);approximate computing;accelerator","Computer architecture;Logic gates;Artificial neural networks;Microprocessors;Hardware;Training;Field programmable gate arrays","analogue-digital conversion;approximation theory;data flow analysis;field programmable gate arrays;integrated circuit design;neural chips;neural net architecture;power consumption;recurrent neural nets;resistive RAM","approximation errors;hardware constraints;inference accuracy;fine-tuning scheme;crossbar-friendly LSTM pruning;mapping scheme;power consumption;PIM architecture;ERA-LSTM;FPGA-based LSTM accelerators;ReRAM-based LSTM accelerator;digital element-wise computation;crossbar-friendly LSTM;architecture enhancement;multiple processing tiles;large-scale LSTM;analog-to-digital converters;LSTM processing tile;ReRAM crossbars;dot-product computation;LSTM-specific element-wise computation;ReRAM-based analog approximate computing;dataflow analysis;memory bottleneck;resistive random access memory crossbars;processing-in-memory architecture;long short-term memory;efficient ReRAM-based architecture","",17.0,"",64.0,"IEEE","27 Dec 2019","","","IEEE","IEEE Journals"
"Performance Analysis of Trial and Error Algorithms","J. Gaveau; C. J. Le Martret; M. Assaad","DGA-MI, Bruz, France; Waveform Design Laboratory, Thales SIX GTS France, Gennevilliers, France; Laboratoire des Signaux et Systèmes (L2S), CentraleSupelec & Université Paris-Saclay, Gif sur Yvette, France","IEEE Transactions on Parallel and Distributed Systems","24 Jan 2020",2020,31.0,6.0,1343,1356,"Model-free decentralized optimizations and learning are receiving increasing attention from theoretical and practical perspectives. In particular, two fully decentralized learning algorithms, namely Trial and Error Learning (TEL) and Optimal Dynamical Learning (ODL), are very appealing for a broad class of games. Indeed, ODL has the property to spend a high proportion of time in an optimum state that maximizes the sum of the utilities of all players, whereas, TEL has the property to spend a high proportion of time in an optimum state that maximizes the sum of the utilities of all players if there is a pure Nash equilibrium, otherwise, it spends a high proportion of time in a state that maximizes a trade-off between the sum of the utilities of the players and a predefined stability function. On the other hand, estimating the mean fraction of time spent in the optimum state (as well as the mean time duration to reach it) is challenging due to the high complexity and dimension of the inherent Markov chains. In this article, under some specific system model, an evaluation of the above performance metrics is provided by proposing an approximation of the considered Markov chains, which allows overcoming the problem of high dimensionality. A comparison between the two algorithms is then performed which allows a better understanding of their performance.","1558-2183","","10.1109/TPDS.2020.2964256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950102","Model-free optimization;markov chain;multi-agent system;game theory;trial and error","Games;Convergence;Markov processes;Optimization;Approximation algorithms;Mathematical model;Resource management","game theory;learning (artificial intelligence);Markov processes;optimisation","performance analysis;theoretical perspectives;practical perspectives;fully decentralized learning algorithms;trial and error learning;TEL;ODL;optimum state;mean time duration;optimal dynamical learning;trial and error algorithms","",2.0,"",20.0,"IEEE","6 Jan 2020","","","IEEE","IEEE Journals"
"On-Edge Multi-Task Transfer Learning: Model and Practice With Data-Driven Task Allocation","Q. Chen; Z. Zheng; C. Hu; D. Wang; F. Liu","National Engineering Research Center for Big Data Technology and System, Key Laboratory of Services Computing Technology and System, Ministry of Education, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Edge Cloud Innovation Lab, Technical Innovation Department, Cloud BU, Huawei Technologies Co., Ltd., Shenzhen, China; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong; National Engineering Research Center for Big Data Technology and System, Key Laboratory of Services Computing Technology and System, Ministry of Education, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2020",2020,31.0,6.0,1357,1371,"On edge devices, data scarcity occurs as a common problem where transfer learning serves as a widely-suggested remedy. Nevertheless, transfer learning imposes heavy computation burden to the resource-constrained edge devices. Existing task allocation works usually assume all submitted tasks are equally important, leading to inefficient resource allocation at a task level when directly applied in Multi-task Transfer Learning (MTL). To address these issues, we first reveal that it is crucial to measure the impact of tasks on overall decision performance improvement and quantify task importance. We then show that task allocation with task importance for MTL (TATIM) is a variant of NP-complete Knapsack problem, where the complicated computation to solve this problem needs to be conducted repeatedly under varying contexts. To solve TATIM with high computational efficiency, we propose a Data-driven Cooperative Task Allocation (DCTA) approach. Finally, we evaluate the performance of DCTA by not only a trace-driven simulation, but also a new comprehensive real-world AIOps case study which bridges model and practice via a new architecture and main components design within AIOps system. Extensive experiments show that our DCTA reduces 3.24 times of processing time, and saves 48.4 percent energy consumption compared with the state-of-the-art when solving TATIM.","1558-2183","","10.1109/TPDS.2019.2962435","National Natural Science Foundation of China(grant numbers:61722206,61761136014,61520106005); NSFC-DFG(grant numbers:392046569); National Key Research and Development (R&D)(grant numbers:2017YFB1001703); Fundamental Research Funds for the Central Universities(grant numbers:2017KFKJXX009,3004210116); RGC GRF(grant numbers:PolyU 15210119,CRF C5026-18G,ITF UIM/363,ITF ITS/070/19FP,PolyU 1-ZVPZ); Huawei Collaborative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943178","Edge computing;transfer learning;data-driven task allocation;real-world application","Task analysis;Resource management;Image edge detection;Data models;Machine learning;Performance evaluation;Computational modeling","computational complexity;greedy algorithms;knapsack problems;learning (artificial intelligence);power aware computing;resource allocation","DCTA;AIOps system;submitted tasks;NP-complete Knapsack problem;task importance;task level;resource-constrained edge devices;data-driven task allocation;edge multitask transfer learning","",33.0,"",69.0,"IEEE","25 Dec 2019","","","IEEE","IEEE Journals"
"Turbo: Dynamic and Decentralized Global Analytics via Machine Learning","H. Wang; D. Niu; B. Li","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada","IEEE Transactions on Parallel and Distributed Systems","28 Jan 2020",2020,31.0,6.0,1372,1386,"Big data analytics are practiced in many fields to extract insights from massive amounts of data. With exponential growth in both the volume and variety of data, analytic queries have expanded from those executed in a single datacenter to those requiring inputs from multiple datacenters that are geographically separate or even globally distributed. Unfortunately, the software stack that supports data analytics is designed originally for a cluster environment and is not tailored to execute global analytic queries, where resources such as inter-datacenter networks may vary on the fly. Existing optimization strategies that determine the query execution plan before its execution are not able to adapt to resource variations at query runtime. In this article, we present Turbo, a lightweight and non-intrusive global analytics system that can dynamically adjust query execution plans for geo-distributed analytics in the presence of time-varying resources and network bandwidth across datacenters. Turbo uses machine learning to accurately predict the time cost of a query execution plan so that dynamic adjustments can be made to it when necessary. Turbo is non-intrusive in the sense that it does not require modifications to the existing software stack for data analytics. We have implemented a real-world prototype of Turbo, and evaluated it on a cluster of 33 instances across eight regions in the Google Cloud Platform. Our experimental results have shown that Turbo can achieve an accuracy of 95 percent for estimating time costs, and can reduce the query completion time by 41 percent.","1558-2183","","10.1109/TPDS.2020.2964667","Huawei Technologies; NSERC Discovery Research Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951093","Data analytics;distributed systems;machine learning","Bandwidth;Optimization;Data analysis;Machine learning;Runtime;Task analysis;Sparks","Big Data;cloud computing;computer centres;data analysis;learning (artificial intelligence);query processing","Turbo;machine learning;big data analytics;single datacenter;multiple datacenters;global analytic queries;inter-datacenter networks;query execution plan;query runtime;geo-distributed analytics;time-varying resources;dynamic adjustments;query completion time;Google Cloud Platform","",3.0,"",39.0,"IEEE","7 Jan 2020","","","IEEE","IEEE Journals"
"RIVA: Robust Integrity Verification Algorithm for High-Speed File Transfers","B. Charyyev; E. Arslan","Stevens Institute of Technology, Hoboken, USA; University of Nevada, Reno, USA","IEEE Transactions on Parallel and Distributed Systems","29 Jan 2020",2020,31.0,6.0,1387,1399,"End-to-end integrity verification is designed to protect file transfers against silent data corruption by comparing checksum of files at source and destination end points using cryptographic hash functions such as MD5 and SHA1. However, existing implementations of end-to-end integrity verification for file transfers fall short to detect undetected disk errors that causes inconsistency between disk and cache memory. In this article, we propose Robust Integrity Verification Algorithm (RIVA) to strengthen the integrity of file transfers by forcing checksum computation tasks to read files directly from disk. RIVA achieves this by invalidating memory mappings of file pages after their transfer such that when the file is read again for checksum calculation, it will be fetched from disk and silent disk errors will be captured. We design and conduct extensive fault resilience experiments to evaluate the robustness of integrity verification algorithms against undetected disk write errors. The results indicate that while the state-of-the-art integrity verification algorithms fail to detect the injected errors for almost all file sizes, RIVA captures all of them with the help of cache invalidation. We further run statistical analysis to assess the probability of missing silent disk errors and find that RIVA reduces the likelihood by 10 to 15 orders of magnitude compared to the existing approaches. Finally, enforcing disk read in integrity verification introduces an inevitable overhead in exchange of increased robustness against silent disk errors, but RIVA keeps its overhead below 15 percent in most cases by running transfer, cache invalidation, and checksum computation processes concurrently for different portions of the same file.","1558-2183","","10.1109/TPDS.2020.2966616","National Science Foundation(grant numbers:OAC-1850353); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8959149","End-to-end integrity verification;file transfers;high performance networks;undetected disk errors;silent data corruption","Receivers;Servers;Disk drives;Data transfer;Target tracking;Robustness;Distributed databases","cache storage;cryptography;data integrity;probability;RAID;statistical analysis","RIVA;cache invalidation;silent disk errors;robust integrity verification algorithm;high-speed file transfers;end-to-end integrity verification;destination end points;undetected disk errors;file pages;fault resilience experiments","",8.0,"",45.0,"IEEE","14 Jan 2020","","","IEEE","IEEE Journals"
"Towards Distributed SDN: Mobility Management and Flow Scheduling in Software Defined Urban IoT","D. Wu; X. Nie; E. Asmare; D. I. Arkhipov; Z. Qin; R. Li; J. A. McCann; K. Li","Department of Computing, Imperial College London, London; Department of Computer Engineering, Hunan University, Changsha, China; Department of Computing, Imperial College London, London, United Kingdom; Department of Computer Science, University of California, Irvine, USA; Department of Computer Science, University of California, Irvine, USA; Department of Computer Engineering, Hunan University, Changsha, China; Department of Computing, Imperial College London, London, United Kingdom; Department of Computer Science, Hunan University, Changsha, China","IEEE Transactions on Parallel and Distributed Systems","29 Jan 2020",2020,31.0,6.0,1400,1418,"The growth of Internet of Things (IoT) devices with multiple radio interfaces has resulted in a number of urban-scale deployments of IoT multinetworks, where heterogeneous wireless communication solutions coexist (e.g., WiFi, Bluetooth, Cellular). Managing the multinetworks for seamless IoT access and handover, especially in mobile environments, is a key challenge. Software-defined networking (SDN) is emerging as a promising paradigm for quick and easy configuration of network devices, but its application in urban-scale multinetworks requiring heterogeneous and frequent IoT access is not well studied. In this paper we present UbiFlow, the first software-defined IoT system for combined ubiquitous flow control and mobility management in urban heterogeneous networks. UbiFlow adopts multiple controllers to divide urban-scale SDN into different geographic partitions (assigning one controller per partition) and achieve distributed control of IoT flows. A distributed hashing based overlay structure is proposed to maintain network scalability and consistency. Based on this UbiFlow overlay structure, the relevant issues pertaining to mobility management such as scalable control, fault tolerance, and load balancing have been carefully examined and studied. The UbiFlow controller differentiates flow scheduling based on per-device requirements and whole-partition capabilities. Therefore, it can present a network status view and optimized selection of access points in multinetworks to satisfy IoT flow requests, while guaranteeing network performance for each partition. Simulation and realistic testbed experiments confirm that UbiFlow can successfully achieve scalable mobility management and robust flow scheduling in IoT multinetworks; e.g., 67.21 percent throughput improvement, 72.99 percent reduced delay, and 69.59 percent jitter improvements, compared with alternative SDN systems.","1558-2183","","10.1109/TPDS.2018.2883438","National Natural Science Foundation of China(grant numbers:61602168,61972145); Intel Collaborative Research Institute for Sustainable Connected Cities; University of California Center on Economic Competitiveness in Transportation; Hu-Xiang Youth Talent Program(grant numbers:2018RS3040); research project OrganiCity(grant numbers:645198); European Unions Horizon 2020 research and innovation program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550695","Distributed control;flow scheduling;Internet of Things;mobility management;software defined networking","Switches;Software;Internet of Things;Wireless communication;Wireless fidelity;Decentralized control","cryptography;Internet of Things;jitter;mobility management (mobile radio);overlay networks;software defined networking;telecommunication scheduling","multiple radio interfaces;urban-scale deployments;heterogeneous wireless communication solutions;handover;mobile environments;software-defined networking;network devices;urban-scale multinetworks;geographic partitions;Internet of Things devices;software defined urban IoT;distributed SDN;SDN systems;IoT multinetworks;robust flow scheduling;scalable mobility management;network performance;IoT flow requests;access points;network status view;per-device requirements;UbiFlow controller;scalable control;UbiFlow overlay structure;network scalability;distributed hashing;urban-scale SDN;multiple controllers;urban heterogeneous networks;combined ubiquitous flow control;software-defined IoT system;frequent IoT access;heterogeneous IoT access;efficiency 67.21 percent;efficiency 72.99 percent;efficiency 69.59 percent","",32.0,"",63.0,"IEEE","28 Nov 2018","","","IEEE","IEEE Journals"
"A Value-Oriented Job Scheduling Approach for Power-Constrained and Oversubscribed HPC Systems","N. Kumbhare; A. Marathe; A. Akoglu; H. J. Siegel; G. Abdulla; S. Hariri","Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA; Lawrence Livermore National Laboratory, Livermore, USA; Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA; Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, USA; Lawrence Livermore National Laboratory, Livermore, USA; Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA","IEEE Transactions on Parallel and Distributed Systems","3 Feb 2020",2020,31.0,6.0,1419,1433,"In this article, we investigate limitations in the traditional value-based algorithms for a power-constrained HPC system and evaluate their impact on HPC productivity. We expose the trade-off between allocating system-wide power budget uniformly and greedily under different system-wide power constraints in an oversubscribed system. We experimentally demonstrate that, under the tightest power constraint, the mean productivity of the greedy allocation is 38 percent higher than the uniform allocation whereas, under the intermediate power constraint, the uniform allocation has a mean productivity of 6 percent higher than the greedy allocation. We then propose a new algorithm that adapts its behavior to deliver the combined benefits of the two allocation strategies. We design a methodology with online retraining capability to create application-specific power-execution time models for a class of HPC applications. These models are used in predicting the execution time of an application on the available resources at the time of making scheduling decisions in the power-aware algorithms. We evaluate the proposed algorithm using emulation and simulation environments, and show that our adaptive strategy results in improving HPC resource utilization while delivering a mean productivity that is almost the same as the best performing algorithm across various system-wide power constraints.","1558-2183","","10.1109/TPDS.2020.2967373","National Science Foundation(grant numbers:NSF CNS-1624668,CCF-1302693); Lawrence Livermore National Laboratory(grant numbers:DE-AC52-07NA27344 (LLNL-JRNL-775437)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961147","High performance computing;power-constrained computing;power-aware scheduling;value heuristics;HPC productivity","Productivity;Resource management;Mathematical model;Measurement;Power demand;Adaptation models;Time factors","parallel processing;power aware computing;resource allocation;scheduling","performing algorithm;improving HPC resource utilization;power-aware algorithms;HPC applications;application-specific power-execution time models;allocation strategies;intermediate power constraint;uniform allocation;greedy allocation;mean productivity;tightest power constraint;oversubscribed system;different system-wide power constraints;allocating system-wide power budget;HPC productivity;power-constrained HPC system;traditional value-based algorithms;oversubscribed HPC systems;value-oriented job scheduling approach","",7.0,"",47.0,"IEEE","16 Jan 2020","","","IEEE","IEEE Journals"
"An Approximate Communication Framework for Network-on-Chips","Y. Chen; A. Louri","Department of Electrical and Computer Engineering, George Washington University, Washington, USA; Department of Electrical and Computer Engineering, George Washington University, Washington, USA","IEEE Transactions on Parallel and Distributed Systems","3 Feb 2020",2020,31.0,6.0,1434,1446,"Current multi-/many-core systems spend large amounts of time and power transmitting data across on-chip interconnects. This problem is aggravated when data-intensive applications, such as machine learning and pattern recognition, are executed in these systems. Recent studies show that some data-intensive applications can tolerate modest errors, thus opening a new design dimension, namely, trading result quality for better system performance. In this article, we explore application error tolerance and propose an approximate communication framework to reduce the power consumption and latency of network-on-chips (NoCs). The proposed framework incorporates a quality control method and a data approximation mechanism to reduce the packet size to decrease network power consumption and latency. The quality control method automatically identifies the error-resilient variables that can be approximated during transmission and calculates their error thresholds based on the quality requirements of the application by analyzing the source code. The data approximation method includes a lightweight lossy compression scheme, which significantly reduces packet size when the error-resilient variables are transmitted. This framework results in fewer flits in each data packet and reduces traffic in NoCs while guaranteeing the quality requirements of applications. Our cycle-accurate simulation using the AxBench benchmark suite shows that the proposed approximate communication framework achieves 62 percent latency reduction and 43 percent dynamic power reduction compared to previous approximate communication techniques while ensuring 95 percent result quality.","1558-2183","","10.1109/TPDS.2020.2968068","National Science Foundation(grant numbers:CCF-1812495,CCF-1740249,CCF-1513923); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966491","Approximate communication;error control;power consumption;network-on-chips (NoCs)","Power demand;Quality control;Multicore processing;Network interfaces;Approximate computing;Data compression;Approximation methods","approximation theory;data compression;integrated circuit design;integrated circuit interconnections;learning (artificial intelligence);multiprocessing systems;network-on-chip;pattern recognition;source coding","approximate communication techniques;data packet;data approximation method;network power consumption;quality control method;application error tolerance;pattern recognition;data-intensive applications;on-chip interconnects;power transmitting data;network-on-chips;approximate communication framework","",12.0,"",60.0,"IEEE","22 Jan 2020","","","IEEE","IEEE Journals"
"P-PFC: Reducing Tail Latency with Predictive PFC in Lossless Data Center Networks","C. Tian; B. Li; L. Qin; J. Zheng; J. Yang; W. Wang; G. Chen; W. Dou","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","10 Feb 2020",2020,31.0,6.0,1447,1459,"Remote Direct Memory Access(RDMA) technology rapidly changes the landscape of nowadays datacenter applications. Congestion control for RDMA networking is a critical challenge. As an end-to-end layer 3 congestion control mechanism, Datacenter QCN (DCQCN) alleviates the unfairness and head-of-the-line blocking problems of Priority-based Flow Control (PFC). However, a lossless network does not guarantee low latency even with DCQCN enabled. When network congestion happens, switch queues still build-up due to the response latency of end-to-end solutions. In this article, we propose Predictive PFC (P-PFC) to reduce tail latency in RDMA networks. P-PFC monitors the derivative of buffer occupation, predicts the happening of PFC trigger in the future, and proactively triggers PFC pause in advance. The benefit is that buffer usage can be maintained at a low level, hence the tail latency can be controlled. Preliminary evaluation results demonstrate that P-PFC can reduce tail latency by more than half of that in standard PFC in many scenarios, without hurting the throughput and average latency. P-PFC can also protect innocent flows compared with standard PFC according to our experiments. To our best knowledge, this is the first work of using derivative to improve PFC in lossless RDMA networks.","1558-2183","","10.1109/TPDS.2020.2969182","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB1003505); National Natural Science Foundation of China(grant numbers:61772265,61802172); Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship (Shuangchuang) Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967139","Data center networks;congestion control;PFC;RDMA","Switches;Standards;Data centers;Throughput;Monitoring;Receivers;Prediction algorithms","buffer storage;computer centres;computer networks;telecommunication congestion control","remote direct memory access technology;lossless RDMA networks;standard PFC;PFC trigger;P-PFC monitors;end-to-end solutions;network congestion;lossless network;Priority-based Flow Control;head-of-the-line blocking problems;unfairness;DCQCN;Datacenter QCN;end-to-end layer 3 congestion control mechanism;RDMA networking;lossless data center networks;Predictive PFC;tail latency reduction","",6.0,"",37.0,"IEEE","23 Jan 2020","","","IEEE","IEEE Journals"
"Errata to “Exploring Fault-Tolerant Erasure Codes for Scalable All-Flash Array Clusters”","S. Koh; J. Zhang; M. Kwon; J. Yoon; D. Donofrio; N. S. Kim; M. Jung","NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Parallel and Distributed Systems","12 Feb 2020",2020,31.0,6.0,1460,1460,"Presents corrections to author affiliation information in the above mentioned article.","1558-2183","","10.1109/TPDS.2020.2971074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995737","","Fault tolerance;Fault tolerant systems;Computer architecture;Convergence;Telecommunications","","","",1.0,"",1.0,"IEEE","12 Feb 2020","","","IEEE","IEEE Journals"
"Reduce Operations: Send Volume Balancing While Minimizing Latency","M. O. Karsavuran; S. Acer; C. Aykanat","Computer Engineering Department, Bilkent University, Ankara, Turkey; Sandia National Laboratories, Center for Computing Research, Albuquerque, USA; Computer Engineering Department, Bilkent University, Ankara, Turkey","IEEE Transactions on Parallel and Distributed Systems","10 Feb 2020",2020,31.0,6.0,1461,1473,"Communication hypergraph model was proposed in a two-phase setting for encapsulating multiple communication cost metrics (bandwidth and latency), which are proven to be important in parallelizing irregular applications. In the first phase, computational-task-to-processor assignment is performed with the objective of minimizing total volume while maintaining computational load balance. In the second phase, communication-task-to-processor assignment is performed with the objective of minimizing total number of messages while maintaining communication-volume balance. The reduce-communication hypergraph model suffers from failing to correctly encapsulate send-volume balancing. We propose a novel vertex weighting scheme that enables part weights to correctly encode send-volume loads of processors for send-volume balancing. The model also suffers from increasing the total communication volume during partitioning. To decrease this increase, we propose a method that utilizes the recursive bipartitioning framework and refines each bipartition by vertex swaps. For performance evaluation, we consider column-parallel SpMV, which is one of the most widely known applications in which the reduce-task assignment problem arises. Extensive experiments on 313 matrices show that, compared to the existing model, the proposed models achieve considerable improvements in all communication cost metrics. These improvements lead to an average decrease of 30 percent in parallel SpMV time on 512 processors for 70 matrices with high irregularity.","1558-2183","","10.1109/TPDS.2020.2964536","National Center for High Performance Computing of Turkey(grant numbers:4005072018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951280","Communication hypergraph;communication cost;maximum communication volume;communication volume;latency;recursive bipartitioning;hypergraph partitioning;sparse matrix;sparse matrix-vector multiplication","Program processors;Task analysis;Computational modeling;Load modeling;Sparse matrices;Measurement;Solid modeling","communication complexity;graph theory;matrix multiplication;message passing;resource allocation;sparse matrices","column-parallel SpMV;recursive bipartitioning framework;send-volume loads;vertex weighting scheme;reduce-communication hypergraph model;reduce-task assignment problem;send-volume balancing;computational load balancing;computational-task-to-processor assignment;multiple communication cost metrics","",1.0,"",21.0,"IEEE","7 Jan 2020","","","IEEE","IEEE Journals"
"Efficient Compute-Intensive Job Allocation in Data Centers via Deep Reinforcement Learning","D. Yi; X. Zhou; Y. Wen; R. Tan","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Parallel and Distributed Systems","10 Feb 2020",2020,31.0,6.0,1474,1485,"Reducing the energy consumption of the servers in a data center via proper job allocation is desirable. Existing advanced job allocation algorithms, based on constrained optimization formulations capturing servers' complex power consumption and thermal dynamics, often scale poorly with the data center size and optimization horizon. This article applies deep reinforcement learning to build an allocation algorithm for long-lasting and compute-intensive jobs that are increasingly seen among today's computation demands. Specifically, a deep Q-network is trained to allocate jobs, aiming to maximize a cumulative reward over long horizons. The training is performed offline using a computational model based on long short-term memory networks that capture the servers' power and thermal dynamics. This offline training approach avoids slow online convergence, low energy efficiency, and potential server overheating during the agent's extensive state-action space exploration if it directly interacts with the physical data center in the usually adopted online learning scheme. At run time, the trained Q-network is forward-propagated with little computation to allocate jobs. Evaluation based on eight months' physical state and job arrival records from a national supercomputing data center hosting 1,152 processors shows that our solution reduces computing power consumption by more than 10 percent and processor temperature by more than 4°C without sacrificing job processing throughput.","1558-2183","","10.1109/TPDS.2020.2968427","Nation Research Foundation, Prime Minister's Office(grant numbers:NRF2015ENC-GBICRD001-012); Green Data Centre Research(grant numbers:NRF2015ENC-GDCR01001-003); Alibaba Group(grant numbers:M4062352); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8964275","Job allocation;data center;energy efficiency;deep reinforcement learning","Servers;Data centers;Training;Resource management;Computational modeling;Optimization;Power demand","computer centres;learning (artificial intelligence);optimisation;parallel machines;power aware computing;processor scheduling;resource allocation","optimization horizon;deep reinforcement learning;allocation algorithm;compute-intensive jobs;computation demands;deep Q-network;long horizons;computational model;short-term memory networks;thermal dynamics;offline training approach;low energy efficiency;potential server;agent;physical data center;learning scheme;trained Q-network;national supercomputing data center hosting;power consumption;job processing throughput;efficient compute-intensive job allocation;energy consumption;job allocation;advanced job allocation algorithms;constrained optimization formulations;data center size","",16.0,"",35.0,"IEEE","22 Jan 2020","","","IEEE","IEEE Journals"
"A Black-Box Fork-Join Latency Prediction Model for Data-Intensive Applications","M. Nguyen; S. Alesawi; N. Li; H. Che; H. Jiang","Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, USA; Faculty of Computing and Information Technology in Rabigh, King Abdulaziz University, Jeddah, Saudi Arabia; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, USA","IEEE Transactions on Parallel and Distributed Systems","14 Apr 2020",2020,31.0,9.0,1983,2000,"The workflows of the predominant datacenter services are underlaid by various Fork-Join structures. Due to the lack of good understanding of the performance of Fork-Join structures in general, today's datacenters often operate under low resource utilization to meet stringent service level objectives (SLOs), e.g., in terms of tail and/or mean latency, for such services. Hence, to achieve high resource utilization, while meeting stringent SLOs, it is of paramount importance to be able to accurately predict the tail and/or mean latency for a broad range of Fork-Join structures of practical interests. In this article, we propose a black-box Fork-Join model that covers a wide range of Fork-Join structures for the prediction of tail and mean latency, called ForkTail and ForkMean, respectively. We derive highly computational effective, empirical expressions for tail and mean latency as functions of means and variances of task response times. Our extensive testing results based on model-based and trace-driven simulations, as well as a real-world case study in a cloud environment demonstrate that the models can consistently predict the tail and mean latency within 20 and 15 percent prediction errors at 80 and 90 percent load levels, respectively, for heavy-tailed workloads, and at any load levels for light-tailed workloads. Moreover, our sensitivity analysis demonstrates that such errors can be well compensated for with no more than 7 percent resource overprovisioning. Consequently, the proposed prediction model can be used as a powerful tool to aid the design of tail-and-mean-latency guaranteed job scheduling and resource provisioning, especially at high load, for datacenter applications.","1558-2183","","10.1109/TPDS.2020.2982137","NSF(grant numbers:CCF XPS 1629625,CCF 1704504); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043685","Tail latency;mean response time;Fork Join queuing networks;datacenter resource provisioning","Task analysis;Load modeling;Time factors;Servers;Predictive models;Computational modeling;Resource management","cloud computing;computer centres;resource allocation;scheduling","data-intensive applications;black-box Fork-Join latency prediction model;tail-and-mean-latency;prediction errors;Fork-Join structures;datacenter services;efficiency 15.0 percent;efficiency 90.0 percent","",5.0,"",52.0,"IEEE","20 Mar 2020","","","IEEE","IEEE Journals"
"ESetStore: An Erasure-Coded Storage System With Fast Data Recovery","C. Liu; Q. Wang; X. Chu; Y. -W. Leung; H. Liu","College of Big Data and Internet, Shenzhen Technology University, Shenzhen, China; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Computing, Hang Seng University of Hong Kong, Siu Lek Yuen, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","15 Apr 2020",2020,31.0,9.0,2001,2016,"Erasure codes have been used extensively in large-scale storage systems to reduce the storage overhead of triplication-based storage systems. One key performance issue introduced by erasure codes is the long time needed to recover from a single failure, which occurs constantly in large-scale storage systems. We present ESetStore, a prototype erasure-coded storage system that aims to achieve fast recovery from failures. ESetStore is novel in the following aspects. We proposed a data placement algorithm named ESet for our ESetStore that can aggregate adequate I/O resources from available storage servers to recover from each single failure. We designed and implemented efficient read and write operations on our erasure-coded storage system via effective use of available I/O and computation resources. We evaluated the performance of ESetStore with extensive experiments on a cluster with 50 storage servers. The evaluation results demonstrate that our recovery performance can obtain linear performance growth by harvesting available I/O resources. With our defined parameter recovery I/O parallelism under some mild conditions, we can achieve optimal recovery performance, in which ESet enables minimal recovery time. Rather than being an alternative to improve recovery performance, our work can be an enhancement for existing solutions, such as Partial-parallel-repair (PPR), to further improve recovery performance.","1558-2183","","10.1109/TPDS.2020.2983411","Hong Kong Innovation and Technology(grant numbers:ITS/443/16FX); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9051846","ESetStore;ESet;Erasure coded storage systems;Fast data recovery","Servers;Reliability;Encoding;Distributed databases;Performance evaluation;Containers;Bandwidth","security of data;storage management","ESetStore;optimal recovery performance;data recovery;large-scale storage systems;triplication-based storage systems;prototype erasure-coded storage system","",10.0,"",44.0,"IEEE","31 Mar 2020","","","IEEE","IEEE Journals"
"The Design of Fast Content-Defined Chunking for Data Deduplication Based Storage Systems","W. Xia; X. Zou; H. Jiang; Y. Zhou; C. Liu; D. Feng; Y. Hua; Y. Hu; Y. Zhang","Wuhan National Laboratory for Optoelectronics, Wuhan, China; Peng Cheng Laboratory, Cyberspace Security Research Center, Shenzhen, China; Department of Computer Science and Engineering, University of Texas at Arlington, USA; Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China; Peng Cheng Laboratory, Cyberspace Security Research Center, Shenzhen, China; Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","15 Apr 2020",2020,31.0,9.0,2017,2031,"Content-Defined Chunking (CDC) has been playing a key role in data deduplication systems recently due to its high redundancy detection ability. However, existing CDC-based approaches introduce heavy CPU overhead because they declare the chunk cut-points by computing and judging the rolling hashes of the data stream byte by byte. In this article, we propose FastCDC, a Fast and efficient Content-Defined Chunking approach, for data deduplication-based storage systems. The key idea behind FastCDC is the combined use of five key techniques, namely, gear based fast rolling hash, simplifying and enhancing the Gear hash judgment, skipping sub-minimum chunk cut-points, normalizing the chunk-size distribution in a small specified region to address the problem of the decreased deduplication ratio stemming from the cut-point skipping, and last but not least, rolling two bytes each time to further speed up CDC. Our evaluation results show that, by using a combination of the five techniques, FastCDC is 3-12X faster than the state-of-the-art CDC approaches, while achieving nearly the same and even higher deduplication ratio as the classic Rabin-based CDC. In addition, our study on the deduplication throughput of FastCDC-based Destor (an open source deduplication project) indicates that FastCDC helps achieve 1.2-3.0X higher throughput than Destor based on state-of-the-art chunkers.","1558-2183","","10.1109/TPDS.2020.2984632","National Natural Science Foundation of China(grant numbers:61972441,61872110,61872414,61772212,61821003,61772222,61832007); National Science and Technology of China(grant numbers:2017ZX01032-101); Wuhan National Laboratory for Optoelectronics(grant numbers:2018WNLOKF008); Shenzhen Science and Technology Program(grant numbers:JCYJ20190806143405318); Key R&D Program for Guangdong Province(grant numbers:2019B010136001); National Science Foundation(grant numbers:CCF-1704504,CCF-1629625); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055082","Data deduplication;content-defined chunking;storage system;performance evaluation","Microsoft Windows;Gears;Power capacitors;Redundancy;Acceleration;Throughput;Distributed databases","data handling;storage management","content-defined chunking;gear hash;Rabin-based CDC;deduplication ratio;cut-point skipping;chunk-size distribution;sub-minimum chunk cut-points;data deduplication-based storage systems;data stream byte;rolling hashes;data deduplication systems;data deduplication based storage systems;open source deduplication project;FastCDC-based Destor","",27.0,"",53.0,"IEEE","2 Apr 2020","","","IEEE","IEEE Journals"
"An Event-Driven Approach to Serverless Seismic Imaging in the Cloud","P. A. Witte; M. Louboutin; H. Modzelewski; C. Jones; J. Selvage; F. J. Herrmann","School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA; School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA; Department of Earth, Ocean and Atmospheric Sciences, University of British Columbia, Vancouver, Canada; Osokey Ltd., Henley-on-Thames, United Kingdom; Osokey Ltd., Henley-on-Thames, United Kingdom; School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA","IEEE Transactions on Parallel and Distributed Systems","15 Apr 2020",2020,31.0,9.0,2032,2049,"Adapting the cloud for high-performance computing (HPC) is a challenging task, as software for HPC applications hinges on fast network connections and is sensitive to hardware failures. Using cloud infrastructure to recreate conventional HPC clusters is therefore in many cases an infeasible solution for migrating HPC applications to the cloud. As an alternative to the generic lift and shift approach, we consider the specific application of seismic imaging and demonstrate a serverless and event-driven approach for running large-scale instances of this problem in the cloud. Instead of permanently running compute instances, our workflow is based on a serverless architecture with high throughput batch computing and event-driven computations, in which computational resources are only running as long as they are utilized. We demonstrate that this approach is very flexible and allows for resilient and nested levels of parallelization, including domain decomposition for solving the underlying partial differential equations. While the event-driven approach introduces some overhead as computational resources are repeatedly restarted, it inherently provides resilience to instance shut-downs and allows a significant reduction of cost by avoiding idle instances, thus making the cloud a viable alternative to on-premise clusters for large-scale seismic imaging.","1558-2183","","10.1109/TPDS.2020.2982626","Georgia Research Alliance; Georgia Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9044390","","Cloud computing;Imaging;Mathematical model;Computational modeling;Numerical models;Propagation;Benchmark testing","cloud computing;geophysical image processing;geophysical techniques;parallel processing;partial differential equations;resource allocation;seismology","large-scale seismic imaging;serverless seismic imaging;high-performance computing;fast network connections;hardware failures;cloud infrastructure;HPC applications;shift approach;serverless event-driven approach;large-scale instances;serverless architecture;high throughput batch computing;event-driven computations;computational resources;instance shut-downs;parallelization;partial differential equations","",9.0,"",74.0,"IEEE","23 Mar 2020","","","IEEE","IEEE Journals"
"Efficient Algorithms for Delay-Aware NFV-Enabled Multicasting in Mobile Edge Clouds With Resource Sharing","H. Ren; Z. Xu; W. Liang; Q. Xia; P. Zhou; O. F. Rana; A. Galis; G. Wu","Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China; Research School of Computer Science, Australian National University, Canberra, Australia; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, International School of Information Science and Engineering, Dalian University of Technology, Dalian, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Cardiff University, Cardiff, United Kingdom; University College London, London, United Kingdom; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China","IEEE Transactions on Parallel and Distributed Systems","20 Apr 2020",2020,31.0,9.0,2050,2066,"Stringent delay requirements of many mobile applications have led to the development of mobile edge clouds, to offer low latency network services at the network edges. Most conventional network services are implemented via hardware-based network functions, including firewalls and load balancers, to guarantee service security and performance. However, implementing hardware-based network functions usually incurs both a high capital expenditure (CAPEX) and operating expenditure (OPEX). Network Function Virtualization (NFV) exhibits a potential to reduce CAPEX and OPEX significantly, by deploying software-based network functions in virtual machines (VMs) on edge-clouds. We consider a fundamental problem of NFV-enabled multicasting in a mobile edge cloud, where each multicast request has both service function chain and end-to-end delay requirements. Specifically, each multicast request requires chaining of a sequence of network functions (referred to as a service function chain) from a source to a set of destinations within specified end-to-end delay requirements. We devise an approximation algorithm with a provable approximation ratio for a single multicast request admission if its delay requirement is negligible; otherwise, we propose an efficient heuristic. Furthermore, we also consider admissions of a given set of the delay-aware NFV-enabled multicast requests, for which we devise an efficient heuristic such that the system throughput is maximized, while the implementation cost of admitted requests is minimized. We finally evaluate the performance of the proposed algorithms in a real test-bed, and experimental results show that our algorithms outperform other similar approaches reported in literature.","1558-2183","","10.1109/TPDS.2020.2983918","National Natural Science Foundation of China(grant numbers:61802048,61802047); Fundamental Research Funds for the Central Universities(grant numbers:DUT17RC(3)061,DUT17RC(3)070,DUT19RC(4)035,DUT19GJ204); Australian Research Council(grant numbers:DP200101985); National Natural Science Foundation of China(grant numbers:61972448); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050847","Mobile edge clouds;network function virtualization;multicasting;approximation algorithms;algorithm design","Multicast communication;Delays;Multicast algorithms;Approximation algorithms;Cloud computing;Optical switches;Network function virtualization","cloud computing;mobile computing;multicast communication;quality of service;resource allocation;software defined networking;telecommunication network routing;virtual machines;virtualisation","network edges;latency network services;mobile applications;stringent delay requirements;delay-aware NFV-enabled multicasting;delay-aware NFV-enabled multicast requests;delay requirement;single multicast request admission;specified end-to-end delay requirements;service function chain;mobile edge cloud;edge-clouds;software-based network;network function virtualization;operating expenditure;service security;hardware-based network functions;conventional network services","",36.0,"",51.0,"IEEE","30 Mar 2020","","","IEEE","IEEE Journals"
"Safety Enhancement for Real-Time Parallel Applications in Distributed Automotive Embedded Systems: A Stable Stopping Approach","G. Xie; G. Zeng; R. Li","Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; Graduate School of Engineering, Nagoya University, Nagoya, Japan; Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, China","IEEE Transactions on Parallel and Distributed Systems","21 Apr 2020",2020,31.0,9.0,2067,2080,"In distributed automotive embedded systems, safety issues run through the entire life cycle, and safety mechanisms for error handling are desirable for risk control. This article focuses on safety enhancement (i.e., safety mechanisms for error handling) for a safety-critical automotive application within its deadline. A stable stopping approach used for safety enhancement for an automotive application is proposed based on the static recovery mechanism provided in ISO 26262. The Stable Stopping-based Safety Enhancement (SSSE) approach is proposed by combining known backward recovery, proposed forward recovery, and proposed forward-and-backward recovery through primary-backup repetition. The stable stopping (i.e., SSSE) approach is a convergence algorithm, which means that when the reliability value reaches a steady state and the algorithm can stop. Experimental results reveal that the exposure level defined in ISO 26262 drops from E3 to E1 after using SSSE, and such improvement enables a safety guarantee of higher level.","1558-2183","","10.1109/TPDS.2020.2984719","National Natural Science Foundation of China(grant numbers:61702172,61932010,61672217,61972139); CCF-Tencent Open Fund(grant numbers:CCF-TecentRAGR20190119); Northeastern University(grant numbers:PAL-N201803); Natural Science Foundation of Hunan Province(grant numbers:2018JJ3076); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055151","Distributed automotive embedded systems;safety enhancement;stable stopping","Safety;Task analysis;Reliability;Automotive applications;Real-time systems;Time factors","automotive electronics;automotive engineering;embedded systems;ISO standards;safety-critical software;security of data","stable stopping-based safety enhancement approach;distributed automotive embedded systems;real-time parallel applications;safety guarantee;backward recovery;forward recovery;static recovery mechanism;safety-critical automotive application;error handling","",2.0,"",30.0,"IEEE","2 Apr 2020","","","IEEE","IEEE Journals"
"Customizable Scale-Out Key-Value Stores","A. Anwar; Y. Cheng; H. Huang; J. Han; H. Sim; D. Lee; F. Douglis; A. R. Butt","IBM Research-Almaden, San Jose, USA; George Mason University, Fairfax, USA; IBM Research–T.J. Watson, Ossining, USA; Virginia Tech, Blacksburg, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Virginia Tech, Blacksburg, USA; Perspecta Labs, Basking Ridge, USA; Virginia Tech, Blacksburg, USA","IEEE Transactions on Parallel and Distributed Systems","24 Apr 2020",2020,31.0,9.0,2081,2096,"Enterprise KV stores are often not well suited for HPC applications, and thus cumbersome end-to-end KV design customization is required to meet the needs of modern HPC applications. To this end, in this article we present bespoKV, an adaptive, extensible, and scale-out KV store framework. bespoKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. For the control plane, bespoKVprovides pre-built modules, called controlets, supporting common distributed functionalities (e.g., replication, consistency, and topology) and their various combinations. This decoupling allows bespoKV to take a user-provided single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Such specializations enable innovative uses of KV stores in HPC applications, especially for emerging applications that utilize KV-friendly workloads. We evaluate bespoKV in a local testbed as well as in a public cloud settings. Experiments show that bespoKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes 1.2× to 2.6× better than the state-of-the-art systems.","1558-2183","","10.1109/TPDS.2020.2982640","National Science Foundation(grant numbers:CNS-1565314,CNS-1405697,CNS-1615411,CCF-1919075,CCF-1919113,CNS-1814430); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050561","Key-value stores;HPC KV stores;scale-out KV stores;application tailored storage","Topology;Distributed databases;Peer-to-peer computing;Fault tolerance;Fault tolerant systems;Nonvolatile memory;Cloud computing","application program interfaces;cloud computing;disc storage;fault tolerant computing;parallel processing;storage management","extensible,scale-out KV store framework;controlets;scalable fault-tolerant;user-provided single-server KV store;distributed functionalities;local data store;data plane;distributed management;control plane;KV store design;decouples;adaptive scale-out KV store framework;HPC applications;design customization;customizable scale-out key-value stores","",4.0,"",81.0,"IEEE","30 Mar 2020","","","IEEE","IEEE Journals"
"Energy-Efficient Parallel Real-Time Scheduling on Clustered Multi-Core","A. Bhuiyan; D. Liu; A. Khan; A. Saifullah; N. Guan; Z. Guo","Department of Electrical and Computer Engineering, University of Central Florida, Orlando, USA; Yunnan University, Kunming, China; Brainco Inc., Somerville, USA; Department of Computer Science, Wayne State University, Detroit, USA; Department of Computing, Hong Kong Polytechnic University, Hong Kong; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, USA","IEEE Transactions on Parallel and Distributed Systems","22 Apr 2020",2020,31.0,9.0,2097,2111,"Energy-efficiency is a critical requirement for computation-intensive real-time applications on multi-core embedded systems. Multi-core processors enable intra-task parallelism, and in this work, we study energy-efficient real-time scheduling of constrained deadline sporadic parallel tasks, where each task is represented as a directed acyclic graph (DAG). We consider a clustered multi-core platform where processors within the same cluster run at the same speed at any given time. A new concept named speed-profile is proposed to model per-task and per-cluster energy-consumption variations during run-time to minimize the expected long-term energy consumption. To our knowledge, no existing work considers energy-aware real-time scheduling of DAG tasks with constrained deadlines, nor on a clustered multi-core platform. The proposed energy-aware real-time scheduler is implemented upon an ODROID XU-3 board to evaluate and demonstrate its feasibility and practicality. To complement our system experiments in large-scale, we have also conducted simulations that demonstrate a CPU energy saving of up to 67 percent through our proposed approach compared to existing methods.","1558-2183","","10.1109/TPDS.2020.2985701","National Science Foundation(grant numbers:CNS-1850851,CNS-1742985); National Natural Science Foundation of China(grant numbers:61801418); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066868","Parallel task;real-time scheduling;energy minimization;cluster-based platform;heterogeneous platform","Task analysis;Real-time systems;Program processors;Processor scheduling;Power demand;Multicore processing;Energy consumption","directed graphs;embedded systems;multiprocessing systems;parallel processing;power aware computing;processor scheduling","clustered multicore;energy-efficiency;computation-intensive real-time applications;multicore embedded systems;multicore processors;intra-task parallelism;long-term energy consumption;energy-aware real-time scheduler;CPU energy saving;per-cluster energy-consumption;ODROID XU-3 board","",18.0,"",52.0,"IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs","K. Li; J. Chen; W. Chen; J. Zhu","Department of Computer Sciences and Technology, Tsinghua University, Beijing, China; Department of Computer Sciences and Technology, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist); Department of Computer Sciences and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Apr 2020",2020,31.0,9.0,2112,2124,"Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images, which are required to model datasets and a large number of topics, e.g., tens of thousands of topics for industry scale applications. Although distributed CPU systems have been used to address this problem, they are slow and resource inefficient. GPU-based systems have emerged as a promising alternative because of their high computational power and memory bandwidth. However, existing GPU-based LDA systems can only learn thousands of topics, because they use dense data structures, and have linear time complexity to the number of topics. In this article, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a warp-based sampling kernel, an efficient sparse matrix counting method, and a fine-grained load balancing strategy. SaberLDA achieves linear speedup on 4 GPUs and is 6-10 times faster than existing GPU systems in thousands of topics. It can learn 40,000 topics from a dataset of billions of tokens in two hours, which was previously only achievable using clusters of tens of CPU servers.","1558-2183","","10.1109/TPDS.2020.2979702","National Natural Science Foundation of China(grant numbers:61525202); Beijing Academy of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9061024","GPU acceleration;latent dirichlet allocation;topic models;machine learning","Graphics processing units;Sparse matrices;Data models;Computational modeling;Time complexity;Resource management;Vocabulary","data structures;graphics processing units;learning (artificial intelligence);resource allocation;sparse matrices","efficient sparse matrix counting method;data layout;sublinear time complexity;sparsity-aware algorithm;linear time complexity;dense data structures;GPU-based LDA system;memory bandwidth;high computational power;GPU-based systems;distributed CPU systems;industry scale applications;discrete count data;latent Dirichlet allocation;sparsity-aware learning;SaberLDA;GPU systems","","","",42.0,"CCBY","8 Apr 2020","","","IEEE","IEEE Journals"
"CURE: A High-Performance, Low-Power, and Reliable Network-on-Chip Design Using Reinforcement Learning","K. Wang; A. Louri","Department of Electrical and Computer Engineering, George Washington University, Washington, USA; Department of Electrical and Computer Engineering, George Washington University, Washington, USA","IEEE Transactions on Parallel and Distributed Systems","1 May 2020",2020,31.0,9.0,2125,2138,"We propose CURE, a deep reinforcement learning (DRL)-based NoC design framework that simultaneously reduces network latency, improves energy-efficiency, and tolerates transient errors and permanent faults. CURE has several architectural innovations and a DRL-based hardware controller to manage design complexity and optimize trade-offs. First, in CURE, we propose reversible multi-function adaptive channels (RMCs) to reduce NoC power consumption and network latency. Second, we implement a new fault-secure adaptive error correction hardware in each router to enhance reliability for both transient errors and permanent faults. Third, we propose a router power-gating and bypass design that powers off NoC components to reduce power and extend chip lifespan. Further, for the complex dynamic interactions of these techniques, we propose using DRL to train a proactive control policy to provide improved fault-tolerance, reduced power consumption, and improved performance. Simulation using the PARSEC benchmark shows that CURE reduces end-to-end packet latency by 39 percent, improves energy efficiency by 92 percent, and lowers static and dynamic power consumption by 24 and 38 percent, respectively, over conventional solutions. Using mean-time-to-failure, we show that CURE is 7.7× more reliable than the conventional NoC design.","1558-2183","","10.1109/TPDS.2020.2986297","National Science Foundation(grant numbers:CCF-1420718,CCF1513606,CCF-1703013,CCF-1547034,CCF-1547035,CCF-1540736,CCF-1702980); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9061016","Computer architecture;network-on-chip(NoC);reliability;deep reinforcement learning","Buffer storage;Hardware;Circuit faults;Reliability;Error correction;Power demand;Transistors","electronic engineering computing;integrated circuit reliability;learning (artificial intelligence);network-on-chip;power consumption","adaptive error correction;permanent faults;router power-gating;NoC components;complex dynamic interactions;reduced power consumption;dynamic power consumption;network-on-chip;energy-efficiency;tolerates transient errors;DRL-based hardware controller;NoC power consumption;deep reinforcement learning;PARSEC benchmark;reversible multifunction adaptive channels","",14.0,"",45.0,"IEEE","8 Apr 2020","","","IEEE","IEEE Journals"
"Heterogeneous Edge Offloading With Incomplete Information: A Minority Game Approach","M. Hu; Z. Xie; D. Wu; Y. Zhou; X. Chen; L. Xiao","Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; Peng Cheng Laboratory, Shenzhen, China; Guangdong Key Laboratory of Big Data Analysis and Processing, Guangzhou, China; Department of Communication Engineering, Xiamen University, Xiamen, China","IEEE Transactions on Parallel and Distributed Systems","1 May 2020",2020,31.0,9.0,2139,2154,"Task offloading is one of key operations in edge computing, which is essential for reducing the latency of task processing and boosting the capacity of end devices. However, the heterogeneity among tasks generated by various users makes it challenging to design efficient task offloading algorithms. In addition, the assumption of complete information for offloading decision-making does not always hold in a distributed edge computing environment. In this article, we formulate the problem of heterogeneous task offloading in a distributed environment as a minority game (MG), in which each player must make decisions independently in each turn and the players who end up on the minority side win. The multi-player MG incentivizes players to cooperate with each other in the scenarios with incomplete information, where players don't have full information about other players (e.g., the number of tasks, the required resources). To address the challenges incurred by task heterogeneity and the divergence of naive MG approaches, we propose an MG based scheme, in which tasks are divided into subtasks and instructed to form into a set of groups as possible, and the left ones are scheduled to perform decision adjustment in a probabilistic manner. We prove that our proposed algorithm can converge to a near-optimal point, and also investigate its stability and price of anarchy in terms of task processing time. Finally, we conduct a series of simulations to evaluate the effectiveness of our proposed scheme and the results indicate that our scheme can achieve around 30% reduction of task processing time compared with other approaches. Moreover, our proposed scheme can converge to a near-optimal point, which cannot be guaranteed by naive MG approaches.","1558-2183","","10.1109/TPDS.2020.2988161","National Natural Science Foundation of China(grant numbers:61802452,U1911201,61972432,61971366,61872420); Guangdong Special Support Program(grant numbers:2017TX04X148); Natural Science Foundation of Guangdong Province(grant numbers:2018A030310079); Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X355); Pearl River Talent Recruitment Program(grant numbers:2017GC010465); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070220","Heterogeneous edge offloading;incomplete information;minority game","Task analysis;Servers;Edge computing;Computational modeling;Cameras;Games;Analytical models","decision making;distributed processing;game theory;scheduling","MG based scheme;heterogeneous edge offloading;minority game approach;offloading decision-making;distributed edge computing environment;heterogeneous task offloading;distributed environment;multiplayer MG;task heterogeneity","",27.0,"",37.0,"IEEE","17 Apr 2020","","","IEEE","IEEE Journals"
"Minority Disk Failure Prediction Based on Transfer Learning in Large Data Centers of Heterogeneous Disk Systems","J. Zhang; K. Zhou; P. Huang; X. He; M. Xie; B. Cheng; Y. Ji; Y. Wang","Tencent, Shenzhen, China; Tencent, Shenzhen, China; Temple University, Philadelphia, USA; Temple University, Philadelphia, USA; Tencent Inc., Shenzhen, China; Tencent Inc., Shenzhen, China; Tencent Inc., Shenzhen, China; Tencent Inc., Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","4 May 2020",2020,31.0,9.0,2155,2169,"The storage system in large scale data centers is typically built upon thousands or even millions of disks, where disk failures constantly happen. A disk failure could lead to serious data loss and thus system unavailability or even catastrophic consequences if the lost data cannot be recovered. While replication and erasure coding techniques have been widely deployed to guarantee storage availability and reliability, disk failure prediction is gaining popularity as it has the potential to prevent disk failures from occurring in the first place. Recent trends have turned toward applying machine learning approaches based on disk SMART attributes for disk failure predictions. However, traditional machine learning (ML) approaches require a large set of training data in order to deliver good predictive performance. In large-scale storage systems, new disks enter gradually to augment the storage capacity or to replace failed disks, leading storage systems to consist of small amounts of new disks from different vendors and/or different models from the same vendor as time goes on. We refer to this relatively small amount of disks as minority disks. Due to the lack of sufficient training data, traditional ML approaches fail to deliver satisfactory predictive performance in evolving storage systems which consist of heterogeneous minority disks. To address this challenge and improve the predictive performance for minority disks in large data centers, we propose a minority disk failure prediction model named TLDFP based on a transfer learning approach. Our evaluation results in two realistic datasets have demonstrated that TLDFP can deliver much more precise results and lower additional maintenance cost, compared to four popular prediction models based on traditional ML algorithms and two state-of-the-art transfer learning methods.","1558-2183","","10.1109/TPDS.2020.2985346","National Natural Science Foundation of China(grant numbers:61821003); National Basic Research Program of China (973 Program)(grant numbers:2016YFB0800402); National Science Foundation(grant numbers:CCF-1717660,CCF-1813081); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9057467","Disk failure;machine learning;transfer learning;cloud computing;data center","Data centers;Servers;Data models;Predictive models;Training data;Support vector machines;Reliability","computer centres;disc storage;learning (artificial intelligence);storage management","large-scale storage systems;failed disks;storage system;heterogeneous minority disks;transfer learning;heterogeneous disk systems;large scale data centers;data loss;system unavailability;disk SMART;minority disk failure prediction","",14.0,"",52.0,"IEEE","6 Apr 2020","","","IEEE","IEEE Journals"
"The Workflow Trace Archive: Open-Access Data From Public and Private Computing Infrastructures","L. Versluis; R. Mathá; S. Talluri; T. Hegeman; R. Prodan; E. Deelman; A. Iosup","Computer Science, Vrije Universiteit Amsterdam, HV Amsterdam, Netherlands; Institute of Computer Science, Universitat Innsbruck, Innsbruck, Austria; Computer Science, Vrije Universiteit Amsterdam, HV Amsterdam, Netherlands; Computer Science, Vrije Universiteit Amsterdam, HV Amsterdam, Netherlands; Institute of Software Technology, University of Klagenfurt, Klagenfurt am, Austria; Information Sciences Institute, University of Southern California, Los Angeles, USA; Computer Science, Vrije Universiteit Amsterdam, HV Amsterdam, Netherlands","IEEE Transactions on Parallel and Distributed Systems","4 May 2020",2020,31.0,9.0,2170,2184,"Realistic, relevant, and reproducible experiments often need input traces collected from real-world environments. In this work, we focus on traces of workflows-common in datacenters, clouds, and HPC infrastructures. We show that the state-of-the-art in using workflow-traces raises important issues: (1) the use of realistic traces is infrequent and (2) the use of realistic, open-access traces even more so. Alleviating these issues, we introduce the Workflow Trace Archive (WTA), an open-access archive of workflow traces from diverse computing infrastructures and tooling to parse, validate, and analyze traces. The WTA includes > 48 million workflows captured from > 10 computing infrastructures, representing a broad diversity of trace domains and characteristics. To emphasize the importance of trace diversity, we characterize the WTA contents and analyze in simulation the impact of trace diversity on experiment results. Our results indicate significant differences in characteristics, properties, and workflow structures between workload sources, domains, and fields.","1558-2183","","10.1109/TPDS.2020.2984821","Vidi MagnaData; European Union's Horizon 2020 Research and Innovation Programme(grant numbers:801091); National Science Foundation(grant numbers:1664162); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066946","Workflow;open-source;open-access;traces;characterization;archive;survey;simulation","Cloud computing;Open source software;Testing;Labeling;Computational modeling;Task analysis;Tools","cloud computing;public domain software;resource allocation;workflow management software","datacenters;clouds infrastructures;workflow trace archive;workflow structures;trace diversity;trace domains;computing infrastructures;open-access traces;HPC infrastructures;open-access data","",8.0,"",60.0,"CCBY","14 Apr 2020","","","IEEE","IEEE Journals"
"Boosting the Performance of SSDs via Fully Exploiting the Plane Level Parallelism","C. Gao; L. Shi; K. Liu; C. J. Xue; J. Yang; Y. Zhang","University of Pittsburgh, Pittsburgh, USA; School of Computer Science and Technology, East China Normal University, Shanghai, P.R. China; College of Computer Science, Chongqing University, Chongqing, P.R. China; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong; University of Pittsburgh, Pittsburgh, USA; University of Pittsburgh, Pittsburgh, USA","IEEE Transactions on Parallel and Distributed Systems","5 May 2020",2020,31.0,9.0,2185,2200,"Solid state drives (SSDs) are constructed with multiple level parallel organization, including channels, chips, dies, and planes. Among these parallel levels, plane level parallelism, which is the last level parallelism of SSDs, has the most strict restrictions. Only the same type of operations that access the same address in different planes can be processed in parallel. In order to maximize the access performance, several previous works have been proposed to exploit the plane level parallelism for host accesses and internal operations of SSDs. However, our preliminary studies show that the plane level parallelism is farfrom well utilized and should be further improved. The reason is that the strict restrictions of plane level parallelism are hard to be satisfied. In this article, a from plane to die parallel optimization framework is proposed to exploit the plane level parallelism through smartly satisfying the strict restrictions all the time. In order to achieve the objective, there are at least two challenges. First, due to that host access patterns are always complex, receiving multiple same-type requests to different planes at the same time is uncommon. Second, there are many internal activities, such as garbage collection (GC), which may destroy the restrictions. In order to solve above challenges, two schemes are proposed in the SSD controller: First, a die level write construction scheme is designed to make sure there are always N pages of data written by each write operation. Second, in a further step, a die level GC scheme is proposed to activate GC in the unit of all planes in the same die. Combing the die level write and die level GC, write accesses from both host write operations and GC induced valid page movements can be processed in parallel at all time. To further improve the performance of SSDs, host write operations blocked by GCs are suggested to be processed in parallel with GC induced valid page movements, bringing lesser waiting time cost of host write operations. As a result, the GC cost and average write latency can be significantly reduced. Experiment results show that the proposed framework is able to significantly improve the write performance without read performance impact.","1558-2183","","10.1109/TPDS.2020.2987894","National Natural Science Foundation of China(grant numbers:61772092,61872049); NSF(grant numbers:1910413,1617071,1725657,1718080); Research Grants Council of the Hong Kong Special Administrative Region, China(grant numbers:CityU 11219319); Frontier Interdisciplinary Research Funds for the Central Universities(grant numbers:2018CDQYJSJ0034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068501","SSD;parallelism;storage;scheduling, performance improvement","Parallel processing;Organizations;Random access memory;Computer science;Resource management;Optimization;Solid state drives","optimisation;solid state drives;storage management","SSDs;plane level parallelism;die level write construction scheme;die level GC;garbage collection;solid state drives;die parallel optimization framework","",7.0,"",55.0,"IEEE","15 Apr 2020","","","IEEE","IEEE Journals"
"Towards Higher Performance and Robust Compilation for CGRA Modulo Scheduling","Z. Zhao; W. Sheng; Q. Wang; W. Yin; P. Ye; J. Li; Z. Mao","Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Nvidia, China; Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","7 May 2020",2020,31.0,9.0,2201,2219,"Coarse-Grained Reconfigurable Architectures (CGRA) is a promising solution for accelerating computation intensive tasks due to its good trade-off in energy efficiency and flexibility. One of the challenging research topic is how to effectively deploy loops onto CGRAs within acceptable compilation time. Modulo scheduling (MS) has shown to be efficient on deploying loops onto CGRAs. Existing CGRA MS algorithms still suffer from the challenge of mapping loop with higher performance under acceptable compilation time, especially mapping large and irregular loops onto CGRAs with limited computational and routing resources. This is mainly due to the under utilization of the available buffer resources on CGRA, unawareness of critical mapping constraints and time consuming method of solving temporal and spatial mapping. This article focus on improving the performance and compilation robustness of the modulo scheduling mapping algorithm for CGRAs. We decomposes the CGRA MS problem into the temporal and spatial mapping problem and reorganize the processes inside these two problems. For the temporal mapping problem, we provide a comprehensive and systematic mapping flow that includes a powerful buffer allocation algorithm, and efficient interconnection & computational constraints solving algorithms. For the spatial mapping problem, we develop a fast and stable spatial mapping algorithm with backtracking and reordering mechanism. Our MS mapping algorithm is able to map loops onto CGRA with higher performance and faster compilation time. Experiment results show that given the same compilation time budget, our mapping algorithm generates higher compilation success rate. Among the successfully compiled loops, our approach can improve 5.4 to 14.2 percent performance and takes x24 to x1099 less compilation time in average comparing with state-of-the-art CGRA mapping algorithms.","1558-2183","","10.1109/TPDS.2020.2989149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9075353","CGRA;modulo scheduling;temporal mapping;spatial mapping","Routing;Registers;Signal processing algorithms;System-on-chip;Robustness;Processor scheduling;Computer architecture","program compilers;program control structures;reconfigurable architectures;scheduling","energy efficiency;computational constraints;acceptable compilation time;CGRA MS algorithms;mapping loop;irregular loops;computational resources;routing resources;available buffer resources;critical mapping constraints;compilation robustness;modulo scheduling mapping algorithm;CGRA MS problem;temporal mapping problem;spatial mapping problem;comprehensive mapping flow;systematic mapping flow;powerful buffer allocation algorithm;fast mapping algorithm;stable spatial mapping algorithm;MS mapping algorithm;map loops;faster compilation time;compilation time budget;higher compilation success rate;successfully compiled loops;compilation time;state-of-the-art CGRA mapping algorithms;efficient interconnection constraints;good trade-off;computation intensive tasks;coarse-grained reconfigurable architectures;CGRA modulo scheduling;robust compilation","",16.0,"",75.0,"IEEE","21 Apr 2020","","","IEEE","IEEE Journals"
"Lock-Free Parallelization for Variance-Reduced Stochastic Gradient Descent on Streaming Data","Y. Peng; Z. Hao; X. Yun","Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","7 May 2020",2020,31.0,9.0,2220,2231,"Stochastic Gradient Descent (SGD) is an iterative algorithm for fitting a model to the training dataset in machine learning problems. With low computation cost, SGD is especially suited for learning from large datasets. However, the variance of SGD tends to be high because it uses only a single data point to determine the update direction at each iteration of gradient descent, rather than all available training data points. Recent research has proposed variance-reduced variants of SGD by incorporating a correction term to approximate full-data gradients. However, it is difficult to parallelize such variants with high performance and accuracy, especially on streaming data. As parallelization is a crucial requirement for large-scale applications, this article focuses on the parallel setting in a multicore machine and presents LFS-STRSAGA, a lock-free approach to parallelizing variance-reduced SGD on streaming data. LFS-STRSAGA embraces a lock-free data structure to process the arrival of streaming data in parallel, and asynchronously maintains the essential information to approximate full-data gradients with low cost. Both our theoretical and empirical results show that LFS-STRSAGA matches the accuracy of the state-of-the-art variance-reduced SGD on streaming data under sparsity assumption (common in machine learning problems), and that LFS-STRSAGA reduces the model update time by over 98 percent.","1558-2183","","10.1109/TPDS.2020.2987867","National Natural Science Foundation of China(grant numbers:61702499); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068443","Gradient descent methods;machine learning;parallel computing;multicore;streaming data","Computational modeling;Machine learning;Data models;Training data;Multicore processing;Training;Mathematical model","data handling;data structures;gradient methods;iterative methods;learning (artificial intelligence);multiprocessing systems;stochastic processes","lock-free parallelization;streaming data;machine learning problems;single data point;available training data points;variance-reduced variants;approximate full-data gradients;parallel setting;lock-free approach;LFS-STRSAGA;lock-free data structure;state-of-the-art variance-reduced SGD;variance-reduced stochastic gradient descent","",6.0,"",30.0,"IEEE","15 Apr 2020","","","IEEE","IEEE Journals"
"Challenges and Opportunities in Designing High-Performance and Scalable Middleware for HPC and AI: Past, Present, and Future","D. K. Panda","","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1","1","This talk focuses on challenges and opportunities emerging over the years (past, present, and future) in designing middleware for HPC and AI (Deep/Machine Learning) workloads on modern high-end computing systems. The talk initially presents the challenges in designing HPC runtime environments with MPI+X programming models by considering support for dense multi-core CPUs, high-performance interconnects, GPUs, and emerging DPUs. Advanced designs and solutions (such as RDMA, in-network computing, GPUDirect RDMA, on-the-fly compression) to exploit novel features of these emerging technologies and their benefits in the context of MVAPICH2 libraries are presented. Next, the talk focuses on MPI-driven solutions for the Deep/Machine Learning domains to extract performance and scalability for popular Deep Learning frameworks, large out-of-core models, GPUs, and DPUs. MPI-driven solutions to accelerate data science applications like Dask are highlighted. Challenges and experiences in deploying this middleware to the HPC cloud environments for Azure, AWS, and Oracle Cloud are presented. The talk concludes with an overview of the newly established NSF-AI Institute ICICLE (https://icicle.osu.edu/) to address challenges in designing future high-performance edge-to-HPC/ cloud middleware for AI-driven data-intensive applications.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820631","","Middleware;Libraries;Distributed processing;Deep learning;Computational modeling;Scalability;Runtime environment","application program interfaces;cloud computing;learning (artificial intelligence);message passing;middleware;parallel processing;power aware computing","designing high-performance;scalable middleware;modern high-end computing systems;HPC runtime environments;MPI+X programming models;multicore CPUs;high-performance interconnects;GPUs;emerging DPUs;in-network computing;GPUDirect RDMA;MVAPICH2 libraries;MPI-driven solutions;popular Deep Learning frameworks;out-of-core models;HPC cloud environments;newly established NSF-AI Institute ICICLE;AI-driven data-intensive applications","","","","","IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"HTS: A Threaded Multilevel Sparse Hybrid Solver","J. D. Booth","University of Alabama in Huntsville, Huntsville, AL, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","2","12","Large shared-memory many-core nodes have become the norm in scientific computing, and therefore the sparse linear solver stack must adapt to the multilevel structure that exists on these nodes. One adaption is the development of hybrid-solvers at the node level. We present HTS as a hybrid threaded solver that aims to provide a finer-grain algorithm to keep an increased number of threads actively working on these larger shared-memory environments without the overheads of message passing implementations. Additionally, HTS aims at utilizing the additional shared memory that may be available to improve performance, i.e., reducing iteration counts when used as a preconditioner and speeding up calculations. HTS is built around the Schur complement framework that many other hybrid solver packages already use. However, HTS uses a multilevel structure in dealing with the Schur complement and allows for fill-in in certain off-diagonal submatrices to allow for a faster and more accurate solve phase. These modifications allow for a tasking thread library, namely Cilk, to be used to speed up performance while still reducing peak memory by more than 20% on average compared to an optimized direct factorization method. We show that HTS can outperform the MPI-based hybrid solver ShyLU on a suite of sparse matrices by as much as 2×, and show that HTS can scale well on three-dimensional finite difference problems.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820665","Linear algebra;Parallel algorithms","High-temperature superconductors;Scientific computing;Scalability;Particle separators;Parallel processing;Libraries;Sparse matrices","application program interfaces;iterative methods;matrix algebra;message passing;multi-threading;physics computing;shared memory systems;sparse matrices","hybrid-solvers;node level;HTS;hybrid threaded solver;shared-memory environments;additional shared memory;Schur complement framework that many other hybrid solver packages;multilevel structure;tasking thread library;peak memory;MPI-based hybrid solver ShyLU;threaded multilevel sparse hybrid solver;shared-memory many-core nodes;sparse linear solver stack","","","",25.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"A scalable adaptive-matrix SPMV for heterogeneous architectures","H. D. Tran; M. Fernando; K. Saurabh; B. Ganapathysubramanian; R. M. Kirby; H. Sundar","University of Utah, USA; University of Texas at Austin, USA; Iowa State University, USA; Iowa State University, USA; University of Utah, USA; University of Utah, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","13","24","In most computational codes, the core computational kernel is the Sparse Matrix-Vector product (SpMV) that enables specialized linear algebra libraries like PETSc to be used, especially in the distributed memory setting. However, optimizing SpMvperformance and scalability at all levels of a modern heterogeneous architecture can be challenging as it is characterized by irregular memory access. This work presents a hybrid approach (HyMV) for evaluating SpMV for matrices arising from PDE discretization schemes such as the finite element method (FEM). The approach enables localized structured memory access that provides improved performance and scalability. Additionally, it simplifies the programmability and portability on different architectures. The developed HyMV approach enables efficient parallelization using MPI, SIMD, OpenMP, and CUDA with minimum programming effort. We present a detailed comparison of HyMV with the two traditional approaches in computational code, matrix-assembled and matrix-free approaches, for structured and unstructured meshes. Our results demonstrate that the HyMV approach achieves excellent scalability and outperforms both approaches, e.g., achieving average speedups of 11x for matrix setup, 1.7x for SpMV with structured meshes, 3.6x for SpMV with unstructured meshes, and 7.5x for GPU SpMV.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00011","National Science Foundation(grant numbers:OAC-1808652); Air Force Research Laboratory(grant numbers:FA8650-19-C-5212,FA8650-17-C-5269); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820649","adaptive-matrix;matrix-assembled;matrix-free;element-by-element;FEM;parallel computing;heterogeneous architectures","Linear systems;Codes;Scalability;Graphics processing units;Computer architecture;Linear algebra;Programming","coprocessors;finite element analysis;graphics processing units;linear algebra;mathematics computing;matrix algebra;matrix multiplication;message passing;multiprocessing systems;parallel algorithms;parallel architectures;parallel processing;parallel programming;partial differential equations;sparse matrices;vectors","adaptive-matrix SPMV;heterogeneous architectures;computational code;core computational kernel;Sparse Matrix-Vector product;specialized linear algebra libraries;distributed memory setting;modern heterogeneous architecture;irregular memory access;hybrid approach;PDE discretization schemes;finite element method;developed HyMV approach;matrix-free approaches;excellent scalability;matrix setup;structured meshes;GPU SpMV","",1.0,"",31.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Direct solution of larger coupled sparse/dense linear systems using low-rank compression on single-node multi-core machines in an industrial context","E. Agullo; M. Felšöci; G. Sylvand","HiePACS team, Inria Bordeaux Sud-Ouest, Bordeaux, France; HiePACS team, Inria Bordeaux Sud-Ouest, Bordeaux, France; HiePACS team, Airbus Central R&T, Issy-les-Moulineaux, France","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","25","35","While hierarchically low-rank compression methods are now commonly available in both dense and sparse direct solvers, their usage for the direct solution of coupled sparse/dense linear systems has been little investigated. The solution of such systems is though central for the simulation of many important physics problems such as the simulation of the propagation of acoustic waves around aircrafts. Indeed, the heterogeneity of the jet flow created by reactors often requires a Finite Element Method (FEM) discretization, leading to a sparse linear system, while it may be reasonable to assume as homogeneous the rest of the space and hence model it with a Boundary Element Method (BEM) discretization, leading to a dense system. In an industrial context, these simulations are often operated on modern multicore workstations with fully-featured linear solvers. Exploiting their low-rank compression techniques is thus very appealing for solving larger coupled sparse/dense systems (hence ensuring a finer solution) on a given multicore workstation, and - of course - possibly do it fast. The standard method performing an efficient coupling of sparse and dense direct solvers is to rely on the Schur complement functionality of the sparse direct solver. However, to the best of our knowledge, modern fully-featured sparse direct solvers offering this functionality return the Schur complement as a non compressed matrix. In this paper, we study the opportunity to process larger systems in spite of this constraint. For that we propose two classes of algorithms, namely multi-solve and multi-factorization, consisting in composing existing parallel sparse and dense methods on well chosen submatrices. An experimental study conducted on a 24 cores machine equipped with 128 GiB of RAM shows that these algorithms, implemented on top of state-of-the-art sparse and dense direct solvers, together with proper low-rank assembly schemes, can respectively process systems of 9 million and 2.5 million total unknowns instead of 1.3 million unknowns with a standard coupling of compressed sparse and dense solvers.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820683","sparse and dense matrices;large linear systems;direct method;parallel solvers;low-rank compression;Finite Elements Method (FEM);Boundary Elements Method (BEM);FEM/BEM coupling","Couplings;Linear systems;Multicore processing;Atmospheric modeling;Random access memory;Workstations;Finite element analysis","aircraft manufacture;boundary-elements methods;finite element analysis;matrix decomposition;multiprocessing systems;parallel processing;production engineering computing;sparse matrices","finite element method discretization;industrial context;fully-featured linear solvers;low-rank compression;single-node multicore machines;sparse and dense direct solvers;BEM discretization;boundary element method discretization;sparse-dense linear systems;FEM discretization;Schur complement functionality;multisolve;multifactorization;parallel sparse and dense methods;industrial aeroacoustic problems","",1.0,"",27.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"I/O-Optimal Cache-Oblivious Sparse Matrix-Sparse Matrix Multiplication","N. Gleinig; M. Besta; T. Hoefler","ETH Zurich; ETH Zurich; ETH Zurich","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","36","46","Data movements between different levels of the memory hierarchy (I/O-transitions, or simply I/O s) are a critical performance bottleneck in modern computing. Therefore it is a problem of high practical relevance to find algorithms that use a minimal number of I/O s. We present a cache-oblivious sparse matrix-sparse matrix multiplication algorithm that uses a worst-case number of I/O s that matches a previously established lower bound for this problem (0 (N2/B.M) read-I/Os and 0 (N2/B) write-I/Os, where $N$ is the size of the problem instance, $M$ is the size of the fast memory and $B$ is the size of the cache lines). When the output does not need to be stored, also the number of write-I/Os can be reduced to 0 (N2/B.M). This improves the worst-case I/O-complexity of the previously best known algorithm for this problem (which is cache-aware) by a logarithmic multiplicative factor. Compared to other cache-oblivious algorithms our algorithm improves the worst-case number of I/Os by a multiplicative factor of Θ(M. N). We show how the algorithm can be applied to produce the first I/O-efficient solution for the sparse 2- vs 3-diameter problem on sparse directed graphs.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820740","","Distributed processing;Costs;Scientific computing;Directed graphs;Complexity theory;Sparse matrices","cache storage;computational complexity;data structures;directed graphs;graph theory;matrix multiplication;sparse matrices","cache-oblivious sparse matrix-sparse matrix multiplication algorithm;worst-case number;cache-oblivious algorithms","","","",55.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Distributed-Memory Sparse Kernels for Machine Learning","V. Bharadwaj; A. Buluç; J. Demmel","EECS Department, University of California, Berkeley, USA; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, USA; EECS Department, University of California, Berkeley, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","47","58","Sampled Dense Times Dense Matrix Multiplication (SDDMM) and Sparse Times Dense Matrix Multiplication (SpMM) appear in diverse settings, such as collaborative filtering, document clustering, and graph embedding. Frequently, the SDDMM output becomes the input sparse matrix for a sub-sequent SpMM operation. Existing work has focused on shared memory parallelization of these primitives. While there has been extensive analysis of communication-minimizing distributed 1.5D algorithms for SpMM, no such analysis exists for SDDMM or the back-to-back sequence of SDDMM and SpMM, termed FusedMM. We show that distributed memory 1.5D and 2.5D algorithms for SpMM can be converted to algorithms for SD-DMM with identical communication costs and input / output data layouts. Further, we give two communication-eliding strategies to reduce costs further for FusedMM kernels: either reusing the replication of an input dense matrix for the SDDMM and SpMM in sequence, or fusing the local SDDMM and SpMM kernels. We benchmark FusedMM algorithms on Cori, a Cray XC40 at LBNL, using Erdos-Renyi random matrices and large real-world sparse matrices. On 256 nodes with 68 cores each, 1.5D FusedMM algorithms using either communication eliding approach can save at least 30% of time spent exclusively in communication compared to executing a distributed-memory SpMM and SDDMM kernel in sequence. Our 2.5D communication-eliding algorithms save 21 % of communication time compared to the unoptimized sequence. On real-world matrices with hundreds of millions of edges, all of our algorithms exhibit at least a 10x speedup over the SpMM algorithm in PETSc. On these matrices, our communication-eliding techniques exhibit runtimes up to 1.6 times faster than an unoptimized sequence of SDDMM and SpMM. We embed and test the scaling of our algorithms in real-world applications, including collaborative filtering via alternating-least-squares and inference for attention-based graph neural networks.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00014","Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820738","SDDMM;SpMM;FusedMM;Communication Avoiding Algorithms","Machine learning algorithms;Costs;Runtime;Collaborative filtering;Layout;Machine learning;Inference algorithms","distributed memory systems;graph theory;learning (artificial intelligence);mathematics computing;matrix algebra;matrix multiplication;neural nets;sparse matrices","collaborative filtering;distributed-memory Sparse kernels;Dense Times Dense Matrix Multiplication;Sparse Times Dense Matrix Multiplication;SDDMM output;input sparse matrix;sub-sequent SpMM operation;shared memory parallelization;communication-minimizing;1.5D algorithms;distributed memory 1.5D;SD-DMM;identical communication costs;communication-eliding strategies;FusedMM kernels;input dense matrix;local SDDMM;SpMM kernels;real-world sparse matrices;1.5D FusedMM algorithms;communication eliding approach;distributed-memory SpMM;SDDMM kernel;communication time;unoptimized sequence;SpMM algorithm;communication-eliding techniques","","","",24.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"PokéMem: Taming Wild Memory Consumers in Apache Spark","M. Kweun; G. Kim; B. Oh; S. Jung; T. Um; W. -Y. Lee","Samsung Research, Korea, Republic of; Samsung Research, Korea, Republic of; Samsung Research, Korea, Republic of; Kakao Corp.; Samsung Research, Korea, Republic of; Samsung Research, Korea, Republic of","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","59","69","Apache Spark is a widely used in-memory processing system due to its high performance. For fast data processing, Spark manages in-memory data such as cached or shuffling (aggregate and sorting) data in its own managed memory pools. However, despite its sophisticated memory management scheme, we found that Spark still suffers from out-of-memory (OOM) exceptions and high garbage collection (GC) overheads when wild memory consumers, who are not tracked by Spark and execute external codes, use a large amount of memory. To resolve the problems, we propose PokéMem, which is an enhanced Spark that incorporates wild memory consumers into the managed ones to prevent them from taking up memory spaces excessively in stealth. Our main idea is to open the black-box of unmanaged memory regions in external codes by providing customized data collections. PokéMem enables fine-grained controls of created objects within running tasks, by spilling and reloading the objects of custom data collections based on the memory pressure and access patterns. To further reduce memory pressures, PokéMem exploits pre-built memory estimation models to predict the external code's memory usage and proactively acquires memory before the execution of external code, and also performs JVM heap-usage monitoring to avoid critical memory pressures. With the help of these techniques, our evaluations show that PokéMem outperforms vanilla Spark with at most 3× faster execution with 3.9× smaller GC overheads, and successfully runs workloads without OOM exception that vanilla Spark has failed to run.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820670","distributed framework;Apache Spark;memory management","Codes;Runtime;Machine learning algorithms;Memory management;Cluster computing;Predictive models;Sparks","cluster computing;storage management;tree data structures","memory spaces;unmanaged memory regions;external code;customized data collections;memory pressure;access patterns;PokéMem;memory estimation models;critical memory pressures;vanilla Spark;Apache Spark;in-memory processing system;fast data processing;in-memory data;managed memory pools;sophisticated memory management scheme;out-of-memory exceptions;high garbage collection;enhanced Spark;wild memory consumers;JVM heap-usage monitoring;GC overheads;OOM exception","","","",30.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"SSB-Tree: Making Persistent Memory B+- Trees Crash-Consistent and Concurrent by Lazy-Box","T. Li; H. Wang; A. Shao; D. Wang","Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","70","80","The 8-byte granularity of failure-atomicity brings two challenges to Persistent Memory (PM) B+-tree designs. The first is how to insert a key in a sorted node atomically. The second is how to atomically perform structural modification operations that involve multiple nodes, such as node splits and merges. The majority of current designs either have huge consistency cost or compromise recovery time. In this paper, we propose an in-node logging technology, named Lazy-Box, to update multiple parts of a node without exposing intermediate states. Lazy-Box aggregates successive modifications and selectively uses Copy-on-Write (CoW) to reduce consistency cost. Based on Lazy-Box, we propose a new variant of B+-tree on PM, named Side-to-Side B+-Tree (SSB- Tree). SSB- Tree relaxes the tree-structure requirement so that node splits and merges only modify one node. Taking advantage of Lazy-Box, all modification operations of SSB-Tree are committed through a single 8-byte write. Therefore, SSB- Tree not only enables efficient concurrency protocol but also achieves instant recovery. Last but most important, SSB- Tree doubles the node space and reuses the extra space to avoid expensive node allocations when performing CoW. Our experimental results show that SSB-Tree achieves up to 29%, 40%, and 14% higher throughput in Insert, Delete, and Scan benchmark respectively than other state-of-the-art PM B+-trees.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00016","National Key Research and Development Program of China(grant numbers:2021YFB3100902); National Natural Science Foundation of China(grant numbers:62072263,62102214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820732","Persistent Memory;B+- Tree;Crash Consistency;Concurrency Protocol","Concurrent computing;Distributed processing;Protocols;Costs;Cows;Throughput;Computer crashes","cache storage;system recovery;tree data structures","SSB-Tree;SSB- Tree;tree-structure requirement;Lazy-Box;persistent memory B+-tree;side-to-side B+-tree;copy-on-write;in-node logging technology","","","",23.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"FAM-Graph: Graph Analytics on Disaggregated Memory","D. Zahka; A. Gavrilovska","Georgia Institute of Technology; Georgia Institute of Technology","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","81","92","Disaggregated memory is being proposed as a way to provide efficient memory scaling for data intensive applications. High performance interconnect technologies, such as CXL, make disaggregated, fabric-attached-memory (FAM) a viable secondary tier of memory. Previous work on remote memory relies on extending kernel level paging to utilize FAM as an additional storage tier after local memory. These approaches have the advantage of exposing remote memory in application transparent ways that do not require code changes, but they incur large overheads due to the mismatch between the abstraction of a flat virtual address space and the reality of the tiered nature of FAM. In this paper, we present an alternative approach to remote memory based on application-specific objects. We design FAM-Graph - a semi-external graph processing system that leverages application-level properties, such as read only edge data, to efficiently tier data between local and remote memory, and prefetch remote data for local computation. Using several graph algorithms and datasets, we demonstrate that FAM-Graph achieves end-to-end performance within factors of 1–6× of Galois, the state of the art shared memory graph processing system, while using up to 20× less local memory. When Galois is used in conjunction with an OS-level FAM solution, we show that FAM-Graph achieves better end-to-end performance by up to 9× when both systems are configured with the same amount of local memory.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00017","NSF(grant numbers:SPX-1822972,CNS-2016701); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820652","Disaggregated Memory;Fabric Attached Memory;Graph Analytics","Distributed processing;Runtime;Costs;Prefetching;Memory management;Software algorithms;Semantics","graph theory;paged storage;shared memory systems","graph analytics;disaggregated memory;data intensive applications;fabric-attached-memory;remote memory;local memory;semiexternal graph processing system;graph algorithms;end-to-end performance;OS-level FAM solution;FAM-graph;memory scaling;high performance interconnect technologies;CXL;kernel level paging;storage tier;flat virtual address space;application-specific objects;application-level properties;remote data prefetching;shared memory graph processing system","",1.0,"",45.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Scalable Multi-Versioning Ordered Key-Value Stores with Persistent Memory Support","B. Nicolae","Argonne National Laboratory, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","93","103","Ordered key-value stores (or sorted maps/dictionaries) are a fundamental building block in a large variety of both sequential and parallel/distributed algorithms. However, most state-of-art approaches are either based on ephemeral in-memory representations that are difficult to persist and/or not scalable enough under concurrent access (e.g., red-black trees, skip lists), and/or not lightweight enough (e.g. database engines). Furthermore, there is an increasing need to provide versioning support, which is needed in a variety of scenarios: introspection, provenance tracking, revisiting previous intermediate results. To address these challenges, we propose a new lightweight dictionary data structure that simultaneously provides support for multi-versioning, persistency and scalability under concurrent access. We demonstrate its effectiveness through a series of experiments, in which it outperforms several state-of-art approaches, both in terms of vertical and horizontal scalability.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820695","key-value store;ordered dictionary;versioning control;scalable access under concurrency;persistent memory","Concurrent computing;Distributed processing;Dictionaries;Databases;Scalability;Data structures;Planning","data structures;database management systems;storage management;tree data structures;trees (mathematics)","horizontal scalability;scalable multiversioning;key-value stores;persistent memory support;fundamental building block;state-of-art approaches;in-memory representations;concurrent access;red-black trees;skip lists;versioning support;lightweight dictionary data structure;persistency;vertical scalability","","","",23.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"In-Memory Indexed Caching for Distributed Data Processing","A. Uta; B. Ghit; A. Dave; J. Rellermeyer; P. Boncz","LIACS, Leiden University; Databricks; UC Berkeley; TU Delft; CWI","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","104","114","Powerful abstractions such as dataframes are only as efficient as their underlying runtime system. The de-facto distributed data processing framework, Apache Spark, is poorly suited for the modern cloud-based data-science workloads due to its outdated assumptions: static datasets analyzed using coarse-grained transformations. In this paper, we introduce the Indexed DataFrame, an in-memory cache that supports a dataframe abstraction which incorporates indexing capabilities to support fast lookup and join operations. Moreover, it supports appends with multi-version concurrency control. We implement the Indexed DataFrame as a lightweight, standalone library which can be integrated with minimum effort in existing Spark programs. We analyze the performance of the Indexed DataFrame in cluster and cloud deployments with real-world datasets and benchmarks using both Apache Spark and Databricks Runtime. In our evaluation, we show that the Indexed DataFrame significantly speeds-up query execution when compared to a non-indexed dataframe, incurring modest memory overhead.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00019","The Dutch National Science Foundation NWO Veni(grant numbers:VI.202.195); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820662","","Distributed processing;Runtime;Data analysis;Cluster computing;Relational databases;Libraries;Concurrency control","cloud computing;concurrency control;data structures;query processing","dataframes;underlying runtime system;de-facto distributed data processing framework;Apache Spark;modern cloud-based data-science workloads;Indexed DataFrame;in-memory cache;dataframe abstraction;indexing capabilities;nonindexed dataframe;memory Indexed caching;powerful abstractions","","","",62.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Landau collision operator in the CUDA programming model applied to thermal quench plasmas","M. F. Adams; D. P. Brennan; M. G. Knepley; P. Wang","Lawrence Berkeley National Laboratory; Princeton University; University at Buffalo; NVIDIA Corporation","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","115","123","Collisional processes are critical in the understanding of non-Maxwellian plasmas. The Landau form of the Fokker-Planck equation is the gold standard for modeling collisions in most plasmas, however $\mathcal{O}(N^{2})$ work complexity inhibits its widespread use. We show that with advanced numerical methods and GPU hardware this cost can be effectively mitigated. This paper extends previous work on a conservative, high order accurate, finite element discretization with adaptive mesh refinement of the Landau operator, with extensions to GPU hardware and implementations in both the CUDA and Kokkos programming languages. This work focuses on the Landau kernels and on NVIDIA hardware, however preliminary results on AMD and Fujitsu/ARM hardware, as well as end-to-end performance of a velocity space model of a plasma thermal quench, are also presented. Both the fully implicit Landau time integrator and the plasma thermal quench model are publicly available in PETSc (Portable, Extensible, Toolkit for Scientific computing).","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00020","U.S. Department of Energy(grant numbers:DE-AC02-05CH11231); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820672","Plasma physics;Fokker-Planck-Landau collision operator;runaway electrons;GPU;CUDA;Kokkos","Adaptation models;Thermal quenching;Computational modeling;Graphics processing units;Hardware;Mathematical models;Plasmas","finite element analysis;Fokker-Planck equation;graphics processing units;parallel architectures;physics computing;plasma collision processes;quenching (thermal)","end-to-end performance;velocity space model;fully implicit Landau time integrator;plasma thermal quench model;Landau collision operator;CUDA programming model;thermal quench plasmas;collisional processes;nonMaxwellian plasmas;Landau form;Fokker-Planck equation;collision modeling;GPU hardware;finite element discretization;adaptive mesh refinement;Landau operator;Landau kernels;NVIDIA hardware;Kokkos programming languages;AMD;Fujitsu hardware;ARM hardware;PETSc","","","",24.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Exploiting Reduced Precision for GPU-based Time Series Mining","Y. Ju; A. Raoofy; D. Yang; E. Laure; M. Schulz","Max Plank Computing and Data Facility; Technical University of Munich; NVIDIA GmbH; Max Plank Computing and Data Facility; Technical University of Munich","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","124","134","The mining of multi-dimensional time series is a crucial step in gaining insights into data obtained from physical systems and from monitoring infrastructures. A widely accepted approach for this challenge is the matrix profile, which, however, is computationally very expensive. It relies on calculating large correlation matrices coupled with sort operations across all dimensions of the data, as well as on performing inclusive scans. All of these steps are inherently data parallel and can, therefore, benefit from execution on GPUs, and even more so from horizontal scaling on multiple GPUs. In addition, the nature of the matrix profile calculation allows the exploitation of reduced precision on GPUs. This offers further improvements to enable the analysis of ever growing data sets in real-world scenarios. Based on these motivations, we introduce the first parallel algorithm for multi-dimensional matrix profile on multiple GPUs exploiting reduced precision modes and provide a highly opti-mized implementation using novel optimization techniques. On one NVIDIA A100 GPU, our implementation achieves a 54x performance improvement in comparison to an optimized single-node execution on a state-of-the-art CPU-based implementation relying on double-precision computation and an additional factor of 1.4x when switching to reduced precision while maintaining sufficient accuracy. We study the accuracy and performance trade-offs for our proposed algorithm in detail and present synthetic and real-world case studies to demonstrate how the reduced precision improves the performance, while accomplishing sufficiently accurate results.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00021","Bayerische Forschungsstiftung(grant numbers:AZ-1214-16); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820614","Multi-GPU algorithms;data mining;reduced precision;multi-dimensional time series;matrix profile","Time series analysis;Layout;Graphics processing units;Switches;Data mining;Synchronization;Parallel algorithms","data mining;graphics processing units;matrix algebra;optimisation;parallel algorithms;time series","GPU-based time series mining;multiple GPUs;matrix profile calculation;data sets;multidimensional matrix profile;NVIDIA A100 GPU;CPU-based implementation;double-precision computation;reduced precision mode;multidimensional time series mining;sort operations;parallel algorithm;novel optimization techniques","","","",28.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"MICCO: An Enhanced Multi-GPU Scheduling Framework for Many-Body Correlation Functions","Q. Wang; B. Ren; J. Chen; R. G. Edwards","Department of Computer Science, William & Mary, Williamsburg, VA; Department of Computer Science, William & Mary, Williamsburg, VA; Jefferson Lab, Newport News, VA; Jefferson Lab, Newport News, VA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","135","145","Calculation of many-body correlation functions is one of the critical kernels utilized in many scientific computing areas, especially in Lattice Quantum Chromodynamics (Lattice QCD). It is formalized as a sum of a large number of contraction terms each of which can be represented by a graph consisting of vertices describing quarks inside a hadron node and edges designating quark propagations at specific time intervals. Due to its computation- and memory-intensive nature, real-world physics systems (e.g., multi-meson or multi-baryon systems) explored by Lattice QCD prefer to leverage multi-GPUs. Different from general graph processing, many-body correlation function calculations show two specific features: a large number of computation-/data-intensive kernels and frequently repeated appearances of original and intermediate data. The former results in expensive memory operations such as tensor movements and evictions. The latter offers data reuse opportunities to mitigate the data-intensive nature of many-body correlation function calculations. However, existing graph-based multi-GPU schedulers cannot capture these data-centric features, thus resulting in a sub-optimal performance for many-body correlation function calculations. To address this issue, this paper presents a multi-GPU scheduling framework, MICCO, to accelerate contractions for correlation functions particularly by taking the data dimension (e.g., data reuse and data eviction) into account. This work first performs a comprehensive study on the interplay of data reuse and load balance, and designs two new concepts: local reuse pattern and reuse bound to study the opportunity of achieving the optimal trade-off between them. Based on this study, MICCO proposes a heuristic scheduling algorithm and a machine-learning-based regression model to generate the optimal setting of reuse bounds. Specifically, MICCO is integrated into a real-world Lattice QCD system, Redstar, for the first time running on multiple GPUs. The evaluation demonstrates MICCO outperforms other state-of-art works, achieving up to 2.25× speedup in synthesized datasets, and 1.49× speedup in real-world correlation functions.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00022","NSF(grant numbers:CCF-2047516); US Department Of Energy; Office of Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820666","","Distributed processing;Correlation;Tensors;Quantum computing;Scientific computing;Scheduling algorithms;Lattices","graph theory;graphics processing units;learning (artificial intelligence);multiprocessing systems;processor scheduling;quantum chromodynamics;regression analysis;scheduling","MICCO;data dimension;data eviction;load balance;real-world Lattice QCD system;real-world correlation functions;multiGPU scheduling framework;many-body correlation functions;Lattice Quantum Chromodynamics;memory-intensive nature;multimeson;multibaryon systems;leverage multiGPUs;many-body correlation function calculations;original data;intermediate data;offers data reuse opportunities;data-intensive nature;existing graph-based multiGPU schedulers;data-centric features","","","",35.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Unlocking Personalized Healthcare on Modern CPUs/GPUs: Three-way Gene Interaction Study","D. Marques; R. Campos; S. Santander-Jiménez; Z. Matveev; L. Sousa; A. Ilic","INESC-ID Instituto Superior Técnico, Universidade de Lisboa, Portugal; INESC-ID Instituto Superior Técnico, Universidade de Lisboa, Portugal; Polytechnic School, University of Extremadura, Spain; Intel Corporation, Russia; INESC-ID Instituto Superior Técnico, Universidade de Lisboa, Portugal; INESC-ID Instituto Superior Técnico, Universidade de Lisboa, Portugal","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","146","156","Developments in Genome-Wide Association Studies have led to the increasing notion that future healthcare techniques will be personalized to the patient, by relying on genetic tests to determine the risk of developing a disease. To this end, the detection of gene interactions that cause complex diseases constitutes an important application. Similarly to many applications in this field, extensive data sets containing genetic information for a series of patients are used (such as Single-Nucleotide Polymorphisms), leading to high computational complexity and memory utilization, thus constituting a major challenge when targeting high-performance execution in modern computing systems. To close this gap, this work proposes several novel approaches for the detection of three-way gene interactions in modern CPUs and GPUs, making use of different optimizations to fully exploit the target architectures. Crucial insights from the Cache-Aware Roofline Model are used to ensure the suitability of the applications to the computing devices. An extensive study of the architectural features of 13 CPU and GPU devices from all main vendors is also presented, allowing to understand the features relevant to obtain high-performance in this bioinformatics domain. To the best of our knowledge, this study is the first to perform such evaluation for epistasis detection. The proposed approaches are able to surpass the performance of state-of-the-art works in the tested platforms, achieving an average speedup of 3.9× (7.3× on CPUs and 2.8× on GPUs) and maximum speedup of 10.6× on Intel UHD P630 GPU.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00023","FCT (Fundação para a Ciência e a Tecnologia, Portugal); ERDF (European Regional Development Fund, EU); Intel Corporation; Research Council of Norway(grant numbers:270053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820625","Epistasis detection;CPU;GPU;CARM","Performance evaluation;Distributed processing;Computational modeling;Graphics processing units;Genomics;Medical services;Computer architecture","bioinformatics;biology computing;cache storage;diseases;genetics;genomics;graphics processing units;medical computing;molecular biophysics;multiprocessing systems;polymorphism","modern computing systems;gene interactions;target architectures;Cache-Aware Roofline Model;computing devices;architectural features;epistasis detection;tested platforms;personalized healthcare;three-way gene interaction study;Genome-Wide Association Studies;increasing notion;future healthcare techniques;genetic tests;disease;complex diseases;extensive data sets;genetic information;Single-Nucleotide Polymorphisms;high computational complexity;memory utilization;targeting high-performance execution","","","",34.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Batched sparse iterative solvers on GPU for the collision operator for fusion plasma simulations","A. Kashi; P. Nayak; D. Kulkarni; A. Scheinberg; P. Lin; H. Anzt","Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Lawrence Berkeley National Laboratory (LBNL), Berkeley, USA; Jubilee Development, Cambridge, USA; Lawrence Berkeley National Laboratory (LBNL), Berkeley, USA; University of Tennessee (UTK), Knoxville, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","157","167","Batched linear solvers, which solve many small related but independent problems, are important in several applications. This is increasingly the case for highly parallel processors such as graphics processing units (GPUs), which need a substantial amount of work to keep them operating efficiently and solving smaller problems one-by-one is not an option. Because of the small size of each problem, the task of coming up with a parallel partitioning scheme and mapping the problem to hardware is not trivial. In recent history, significant attention has been given to batched dense linear algebra. However, there is also an interest in utilizing sparse iterative solvers in a batched form, and this presents further challenges. An example use case is found in a gyrokinetic Particle-In-Cell (PIC) code used for modeling magnetically confined fusion plasma devices. The collision operator has been identified as a bottleneck, and a proxy app has been created for facilitating optimizations and porting to GPUs. The current collision kernel linear solver does not run on the GPU-a major bottleneck. As these matrices are well-conditioned, batched iterative sparse solvers are an attractive option. A batched sparse iterative solver capability has recently been developed in the Ginkgo library. In this paper, we describe how the software architecture can be used to develop an efficient solution for the XGC collision proxy app. Comparisons for the solve times on NVIDIA V100 and A100 GPUs and AMD MI100 GPUs with one dual-socket Intel Xeon Skylake CPU node with 40 OpenMP threads are presented for matrices representative of those required in the collision kernel of XGC. The results suggest that GINKGO's batched sparse iterative solvers are well suited for efficient utilization of the GPU for this problem, and the performance portability of Ginkgo in conjunction with Kokkos (used within XGC as the heterogeneous programming model) allows seamless execution for exascale oriented heterogeneous architectures at the various leadership supercomputing facilities.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00024","National Nuclear Security Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820663","Sparse linear systems;batched solvers;GPU;performance portability;Ginkgo;Xgc;Iter;WDMApp;fusion;simulation","Leadership;Plasma simulation;Software architecture;Graphics processing units;Production;Programming;Iterative methods","batch processing (computers);computational electromagnetics;graphics processing units;iterative methods;linear algebra;matrix algebra;message passing;multiprocessing systems;parallel processing;plasma simulation","collision operator;fusion plasma simulations;parallel processors;parallel partitioning;batched dense linear algebra;GPU;XGC collision proxy app;collision kernel linear solver;Ginkgo library;batched sparse iterative solver;gyrokinetic particle-in-cell code","",1.0,"",18.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"PARSEC: PARallel Subgraph Enumeration in CUDA","V. Dodeja; M. Almasri; R. Nagi; J. Xiong; W. -m. Hwu","C3SR, University of Illinois Urbana-Champaign, Urbana, IL, USA; C3SR, University of Illinois Urbana-Champaign, Urbana, IL, USA; C3SR, University of Illinois Urbana-Champaign, Urbana, IL, USA; University at Buffalo, Buffalo, NY, USA; Nvidia Corporation, Santa Clara, CA, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","168","178","Subgraph enumeration is an important problem in the field of Graph Analytics with numerous applications. The problem is provably NP-complete and requires sophisticated heuristics and highly efficient implementations to be feasible on problem sizes of realistic scales. Parallel solutions have shown a lot of promise on CPUs and distributed environments. Recently, GPU-based parallel solutions have also been proposed to take advantage of the massive execution resources in modern GPUs. Subgraph enumeration involves traversing a search tree for each vertex of the data graph to find matches of a query in a graph. Most GPU-based solutions traverse the tree in breadth-first manner that exploits parallelism at the cost of high memory requirement and presents a formidable challenge for processing large graphs with high-degree vertices since the memory capacity of GPUs is significantly lower than that of CPUs. In this work, we propose a novel GPU solution based on a hybrid BFS and DFS approach where the top level(s) of the search trees are traversed in a fully parallel, breadth-first manner while each subtree is traversed in a more space-efficient, depth-first manner. The depth-first traversal of subtrees requires less memory but presents more challenges for parallel execution. To overcome the less parallel nature of depth-first traversal, we exploit fine-grained parallelism in each step of the depth-first traversal of sub-trees. We further identify and implement various optimizations to efficiently utilize memory and compute resources of the GPUs. We evaluate our performance in comparison with the state-of-the-art GPU and CPU implementations. We outperform the GPU and CPU implementations with a geometric mean speedup of 9.47× (up to 92.01×) and 2.37× (up to 12.70×), respectively. We also show that the proposed approach can efficiently process the graphs that previously cannot be processed by the state-of-the-art GPU solutions due to their excessive memory requirement.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820623","Nvidia GPU;CUDA;subgraph matching;subgraph isomorphism;graph pattern matching;parallel;hybrid BFS+DFS;DFS;tree traversal","Distributed processing;Instruction sets;Memory management;Graphics processing units;Process control;Organizations;Parallel processing","approximation theory;computational complexity;graph theory;graphics processing units;multiprocessing systems;parallel algorithms;parallel architectures;parallel processing;tree searching;trees (mathematics)","Graph Analytics;sophisticated heuristics;highly efficient implementations;problem sizes;realistic scales;CPUs;GPU-based parallel solutions;massive execution resources;search tree;data graph;GPU-based solutions;breadth-first manner;high memory requirement;formidable challenge;high-degree vertices;memory capacity;novel GPU solution;fully parallel breadth;subtree;space-efficient;depth-first manner;depth-first traversal;parallel execution;parallel nature;fine-grained parallelism;sub-trees;state-of-the-art GPU solutions;excessive memory requirement;PARallel subgraph enumeration","","","",26.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Top-Down Performance Profiling on NVIDIA's GPUs","A. Saiz; P. Prieto; P. Abad; J. A. Gregorio; V. Puente","Computer Engineering Group, Universidad de Cantabria, Santander, Spain; Computer Engineering Group, Universidad de Cantabria, Santander, Spain; Computer Engineering Group, Universidad de Cantabria, Santander, Spain; Computer Engineering Group, Universidad de Cantabria, Santander, Spain; Computer Engineering Group, Universidad de Cantabria, Santander, Spain","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","179","189","The rise of data-intensive algorithms, such as Machine Learning ones, has meant a strong diversification of Graphics Processing Units (GPU) in fields with intensive Data-Level Parallelism. This trend, known as general-purpose computing on GPU (GP-GPU), makes the execution process on a GPU (seemingly simple in its architecture) far from trivial when targeting performance for many dissimilar applications. A proof of this is the existence of many profiling tools that help programmers to understand how to maximize hardware utilization. In contrast, this paper proposes a profiling tool focused on microarchitecture analysis under large sets of dissimilar applications. Therefore, the tool has a double objective. On the one hand, to check the suitability of a GPU for diverse sets of application kernels. On the other hand, to identify possible bottlenecks in a given GPU microarchitecture, facilitating the improvement of subsequent designs. For this purpose, using Top-Down methodology proposed by Intel for their CPUs as inspiration, we have defined a hierarchical organization for the execution pipeline of the GPU. The proposal makes use of the available hardware performance counters to identify how each component contributes to performance losses. We demonstrate the feasibility of the proposed methodology, analyzing how different modern NVIDIA architectures behave running relevant benchmarks, assessing in which microarchitecture component performance losses are the most significant.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820717","GPU;NVIDIA;Top-Down;Hardware Event Counters;Performance Profiling","Microarchitecture;Machine learning algorithms;Pipelines;Graphics processing units;Computer architecture;Organizations;Parallel processing","graphics processing units;parallel architectures;performance evaluation","data-intensive algorithms;graphics processing units;data-level parallelism;general-purpose computing on GPU;GP-GPU;GPU microarchitecture;hardware performance counters;microarchitecture component performance losses;top-down performance profiling;NVIDIA GPUs","","","",28.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Scaling and Selecting GPU Methods for All Pairs Shortest Paths (APSP) Computations","Y. Xia; P. Jiang; G. Agrawal; R. Ramnath","Computer Science and Engineering, The Ohio State University; Computer Science Department, University of Iowa; Computer and Cyber Sciences, Augusta University; Computer Science and Engineering, The Ohio State University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","190","200","All Pairs Shortest Path (APSP) is one of the graph problems where the output size is significantly larger than the input size. This paper examines the issues in scaling GPU implementations for this problem beyond the memory limits. Because the existing (in-core) methods offer a complex trade-off between the overall computation complexity and the available parallelism, choosing the best out-of-core version for a given matrix is challenging. We develop three efficient out-of-core implementations, which are based on the blocked Floyd-Warshall algorithm, Johnson's algorithm, and the boundary algorithm, respectively. Next, we develop a methodology to select the best implementation for a given graph. Experimental results show that compared with an efficient multi-core APSP implementation, the out-of-core version achieves speedups of 8.22 to 12.40 for graphs with a small separator, and speedups of 2.23 to 2.79 for other sparse graphs, and our models can select the best implementation in most cases.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00027","NSF(grant numbers:2131509,2034850,2007793); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820615","","Distributed processing;Costs;Particle separators;Computational modeling;Graphics processing units;Parallel processing;Distance measurement","computational complexity;graph theory;graphics processing units;multiprocessing systems;parallel architectures;parallel processing","existing methods;in-core;complex trade-off;computation complexity;available parallelism;out-of-core version;given matrix;out-of-core implementations;blocked Floyd-Warshall algorithm;Johnson's algorithm;boundary algorithm;multicore APSP implementation;sparse graphs;graph problems;output size;input size;scaling GPU implementations;memory limits","","","",37.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Parallel Vertex Cover Algorithms on GPUs","P. Yamout; K. Barada; A. Jaljuli; A. E. Mouawad; I. El Hajj","American University of Beirut, Lebanon; American University of Beirut, Lebanon; American University of Beirut, Lebanon; American University of Beirut, Lebanon; American University of Beirut, Lebanon","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","201","211","Finding small vertex covers in a graph has applications in numerous domains such as scheduling, computational biology, telecommunication networks, artificial intelligence, social science, and many more. Two common formulations of the problem include: Minimum Vertex Cover (MVC), which finds the smallest vertex cover in a graph, and Parameterized Vertex Cover (PVC), which finds a vertex cover whose size is less than or equal to some parameter $k$. Algorithms for both formulations involve traversing a search tree, which grows exponentially with the size of the graph or the value of $k$. Parallelizing the traversal of the vertex cover search tree on GPUs is challenging for multiple reasons. First, the search tree is a narrow binary tree which makes it difficult to extract enough sub-trees to process in parallel to fully utilize the GPU's massively parallel execution resources. Second, the search tree is highly imbalanced which makes load balancing across a massive number of parallel GPU workers especially challenging. Third, keeping around all the intermediate state needed to traverse many sub-trees in parallel puts high pressure on the GPU's memory resources and may act as a limiting factor to parallelism. To address these challenges, we propose an approach to traverse the vertex cover search tree in parallel using GPUs while handling dynamic load balancing. Each thread block traverses a different sub-tree using a local stack, however, we use a global worklist to balance the load to ensure that all blocks remain busy. Blocks contribute branches of their sub-trees to the global worklist on an as-needed basis, while blocks that finish their sub-trees pick up new ones from the global worklist. We use degree arrays to represent intermediate graphs so that the representation is compact in memory to avoid limiting parallelism, but self-contained which is necessary for the load balancing process. Our evaluation shows that compared to approaches used in prior work, our hybrid approach of using local stacks and a global worklist substantially improves performance and reduces load imbalance, especially on difficult instances of the problem. Our implementations have been open sourced to enable further research on parallel solutions to the vertex cover problem and other similar problems involving parallel traversal of narrow and highly imbalanced search trees.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00028","American University of Beirut; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820627","vertex cover;graphs;parallel computing;GPU","Limiting;Processor scheduling;Instruction sets;Heuristic algorithms;Social sciences;Memory management;Graphics processing units","graphics processing units;multi-threading;resource allocation;trees (mathematics)","minimum vertex cover;smallest vertex cover;vertex cover search tree;narrow binary tree;sub-trees;parallel GPU workers;parallelism;thread block;parallel traversal;parallel vertex cover;parameterized vertex cover;GPU massively parallel execution resources","",1.0,"",53.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"SecFortress: Securing Hypervisor using Cross-layer Isolation","Q. Zhou; X. Jia; S. Zhang; N. Jiang; J. Chen; W. Zhang","School of Cyber Security, University of Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; Metropolitan College, Boston University, USA; School of Cyber Security, University of Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","212","222","Virtualization is the corner stone of cloud computing, but the hypervisor, the crucial software component that enables virtualization, is known to suffer from various attacks. It is challenging to secure the hypervisor due to at least two reasons. On one hand, commercial hypervisors are usually integrated into a privileged Operating System (OS), which brings in a larger attack surface. On the other hand, multiple Virtual Machines (VM) share a single hypervisor, thus a malicious VM could leverage the hypervisor as a bridge to launch “cross-VM” attacks. In this work, we propose SecFortress, a dependable hypervisor design that decouples the virtualization layer into a mediator, an outerOS, and multiple HypBoxes through a cross-layer isolation approach. SecFortress extends the nested kernel approach to de-privilege the outerOS from accessing the mediator's memory and creates an isolated hypervisor instance, HypBox, to confine the impacts from the untrusted VMs. We implemented SecFortress based on KVM and evaluated its effectiveness and efficiency through case studies and performance evaluation. Experimental results show that SecFortress can significantly improve the security of the hypervisor with negligible runtime overhead.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00029","Chinese Academy of Sciences(grant numbers:XDC02010900); National Natural Science Foundation of China(grant numbers:61772078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820660","","Performance evaluation;Distributed processing;Virtual machine monitors;Runtime;Prototypes;Virtual machining;Software","cloud computing;operating systems (computers);security of data;virtual machines;virtualisation","SecFortress;virtual machines;hypervisor design;virtualization layer;cross-layer isolation approach;HypBox;operating system;mediator;outerOS","","","",39.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Exploring Efficient Microservice Level Parallelism","X. Wang; C. Li; L. Zhang; X. Hou; Q. Chen; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University; ACCESS, Hong Kong University of Science and Technology; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science and Engineering, Shanghai Jiao Tong University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","223","233","The microservice architecture has recently become a driving trend in the cloud by disaggregating a monolithic application into many scenario-oriented service blocks (microservices). The decomposition process results in a highly dynamic execution scenario, in which various chained microservices contend for computing resources in different ways. While parallelism has been exploited at both the instruction/thread level and the task/request level, very limited work has been done with the grain-size of a microservice. Current parallel processing solutions are sub-optimal as they neither capture the unique characteristics of microservices nor consider the uncertainty arises in the microservice environment. In this work we introduce microservice level parallelism (MLP), a technique that aims to precisely coalesce and align parallel microservice chains for better system performance and resource utilization. We identify major issues that prevent servers from effectively exploiting MLP and we define metrics that can guide MLP optimization. We propose v-MLP, a volatility-aware MLP that is able to adapt to a highly heterogeneous and dynamic microservice environment. We show that v-MLP can reduce tail latency by up to 50% and improve resource utilization by up to 15 % under various scenarios.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00030","National Natural Science Foundation of China(grant numbers:61832006); Shanghai S&T Committee Rising-Star Program(grant numbers:21QAI404400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820743","Microservice;Parallelism;Request Management","Uncertainty;System performance;Microservice architectures;Computer architecture;Tail;Parallel processing;Throughput","cloud computing;parallel programming;resource allocation;service-oriented architecture","v-MLP;volatility-aware MLP;highly heterogeneous microservice environment;dynamic microservice environment;resource utilization;microservice architecture;monolithic application;scenario-oriented service blocks;decomposition process;parallel processing;MLP optimization;parallel microservice chain alignment;highly dynamic execution;microservice level parallelism;tail latency reduction;cloud computing;computing resources","","","",46.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Scalable Low-Latency Inter-FPGA Networks","K. T. Pham; T. T. Nguyen; H. Yamaguchi; Y. Urino; M. Koibuchi","National Institute of Informatics / SOKENDAI, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; Photonics Electronics Technology Research Association, Japan; Photonics Electronics Technology Research Association, Japan; JST, PRESTO","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","234","245","A cutting-edge FPGA card can be equipped with many high-bandwidth I/Os by means of high-density optical integration, e.g., onboard Si-photonics transceivers, to provide high network bandwidth for memory-to-memory inter-FPGA communication. This study presents its scalable switchless net-work architecture by exploiting an indirect path, consisting of two one-hop paths, for enabling a diameter-2 network topology. It then takes a Kautz network topology with a diameter of two for connecting d(d + 1) FPGAs with a degree of $d$, which is close to the theoretical upper bound. The Kautz network topologies have bi-directional links and uni-directional links which form triangles. Uni-directional links introduce difficulty in avoiding channel buffer overflow because the existing link-level flow control assumes a bi-directional link. This study presents an indirect flow control along a uni-directional triangle embedded in the Kautz network topology. It then develops a combination of unicasts that forms multi-port collective communications to mitigate the influence of the startup latency on the execution time. Since a high-degree FPGA card introduces difficulty in storing many I/O ports at the panel of a 1- U compute server, we propose using WDM (Wavelength Division Multiplexing) as an alternative and present its efficient mapping onto arrayed waveguide grating (AWG). The required number of wavelengths becomes d on d+ 1 AWG equipments. Based on our experimental results with OPTWEB of custom Stratix10 FPGA cards, SimGrid simulation results show that our collective communication is 7 × faster than that of Dragonfly with 272 FPGAs.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00031","JST(grant numbers:JPM-JAX190C,JST PRESTO,JPMJPR19M1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820730","collective communication;Kautz graphs;low latency","Upper bound;Network topology;Simulation;Optical buffering;Bidirectional control;Wavelength division multiplexing;Routing","field programmable gate arrays;network topology;wavelength division multiplexing","Stratix10 FPGA cards;link-level flow control;high-degree FPGA card;multiport collective communications;uni-directional triangle;indirect flow control;uni-directional links;bi-directional link;Kautz network topology;diameter-2 network topology;indirect path;memory-to-memory inter-FPGA communication;high network bandwidth;onboard Si-photonics transceivers;cutting-edge FPGA card;scalable low-latency inter-FPGA networks;Si","","","",41.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"A General Offloading Approach for Near-DRAM Processing-In-Memory Architectures","D. Chen; H. Jin; L. Zheng; Y. Huang; P. Yao; C. Gui; Q. Wang; H. Liu; H. He; X. Liao; R. Zheng","National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Clusters and Grid Computing Lab, Huazhong University of Science and Technology, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","246","257","Processing-in-memory (PIM) is promising to solve the well-known data movement challenge by performing in-situ computations near the data. Leveraging PIM features is pretty profitable to boost the energy efficiency of applications. Early studies mainly focus on improving the programmability for computation offloading on PIM architectures. They lack a comprehensive analysis of computation locality and hence fail to accelerate a wide variety of applications. In this paper, we present a general-purpose instruction-level offloading technique for near-DRAM PIM architectures, namely IOTPIM, to exploit PIM features comprehensively. IOTPIM is novel with two technical advances: 1) a new instruction offloading policy that fully considers the locality of the whole on-chip cache hierarchy, and 2) an offloading performance benefit prediction model that directly predicts offloading performance benefits of an instruction based on the input dataset characterizes, preserving low analysis overheads. The evaluation demonstrates that IOTPIM can be applied to accelerate a wide variety of applications, including graph processing, machine learning, and image processing. IOT-PIM outperforms the state-of-the-art PIM offloading techniques by 1.28×-1.51× while ensuring offloading accuracy as high as 91.89% on average.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00032","National Natural Science Foundation of China(grant numbers:61832006,62072195,61825202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820674","processing-in-memory;PIM;offloading technique;data locality","Analytical models;Technological innovation;Distributed processing;Systematics;Image processing;Computer architecture;Machine learning","cache storage;DRAM chips;image processing;learning (artificial intelligence);memory architecture;microprocessor chips;mobile computing;parallel architectures","IOTPIM;PIM features;instruction offloading policy;on-chip cache hierarchy;offloading performance benefit prediction model;offloading performance benefits;low analysis overheads;graph processing;image processing;IOT-PIM;state-of-the-art PIM;offloading accuracy;general offloading approach;Near-DRAM Processing-In-Memory Architectures;data movement challenge;in-situ computations;early studies;computation offloading;computation locality;general-purpose instruction-level offloading technique;near-DRAM PIM architectures","","","",41.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Minerva: Rethinking Secure Architectures for the Era of Fabric-Attached Memory Architectures","M. Alwadi; R. Wang; D. Mohaisen; C. Hughes; S. D. Hammond; A. Awad","Jordan University of Science and Technology; Illinois Institute of Technology; University of Central Florida; Sandia National Laboratories; Sandia National Laboratories; North Carolina State University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","258","268","Fabric-attached memory (FAM) is proposed to enable the seamless integration of directly accessible memory modules attached to the shared system fabric, which will provide future systems with flexible memory integration options, mitigate underutilization, and facilitate data sharing. Recently proposed interconnects, such as Gen-Z and Compute Express Link (CXL), define security, correctness, and performance requirements of fabric-attached devices, including memory. These initiatives are supported by most major system and processor vendors, bringing widespread adoption of FAM-enabled systems one step closer to reality and security concerns to the forefront. This paper discusses the challenges for adapting secure memory implementations to FAM-enabled systems for the first time in literature. Specifically, we observe that handling the security metadata used to protect fabric-attached memories needs to be done deliberately to eliminate unintentional integrity check failures and/or security vulnerabilities, caused by an inconsistent view of the shared security metadata across nodes. Our scheme, Minerva, elegantly adapts secure memory implementations to support FAM-enabled systems with negligible performance over-heads (3.8% of an ideal scheme), compared to the performance overhead (99.5% of an ideal scheme) for a scheme that uses conventional invalidation-based cache coherence to ensure the consistency of security metadata across nodes.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00033","Defense Advanced Research Projects Agency (DARPA); Office of Naval Research (ONR); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820629","Secure-memory;Fabric-Attached-Memory;Memory-Encryption;Memory-Integrity;Security-metadata-Coherence","Performance evaluation;Nonvolatile memory;Memory management;Memory architecture;Coherence;Memory modules;Metadata","cache storage;data integrity;memory architecture;meta data;security of data","fabric-attached memory;directly accessible memory modules;shared system fabric;memory integration;fabric-attached devices;FAM-enabled systems;secure memory implementation;security vulnerability;shared security metadata;secure architecture;Minerva;data sharing;Gen-Z;Compute Express Link;CXL;performance requirements;unintentional integrity check failure;performance overhead;invalidation-based cache coherence","","","",41.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Parallel Global Edge Switching for the Uniform Sampling of Simple Graphs with Prescribed Degrees","D. Allendorf; U. Meyer; M. Penschuck; H. Tran","Goethe University, Frankfurt, Germany; Goethe University, Frankfurt, Germany; Goethe University, Frankfurt, Germany; Goethe University, Frankfurt, Germany","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","269","279","The uniform sampling of simple graphs matching a prescribed degree sequence is an important tool in network science, e.g., to construct graph generators or null-models. Here, the Edge Switching Markov Chain (ES-MC) is a common choice. Given an arbitrary simple graph with the required degree sequence, ES-MC carries out a large number of small changes involving at most four edges to eventually obtain a uniform sample. In practice, reasonably short runs efficiently yield approximate uniform samples. We first engineer a simple sequential ES-MC implementation representing the graph in a hash-set. Despite its simplicity and to the best of our knowledge, our implementation significantly outperforms all openly available solutions. Secondly, we propose the Global Edge Switching Markov Chain (G-ES-MC) and show that it, too, converges to a uni-form distribution. We provide empirical evidence that G-ES-MC requires not more switches than ES-MC (and often fewer). Thirdly, we engineer shared-memory parallel algorithms for ES-MC and G-ES-MC; we find that they benefit from the easier dependency structure of the G-ES-MC. In an empirical evaluation, we demonstrate the scalability of our implementations.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820710","random graphs;degrees;edge switching;markov chain;parallelism","Distributed processing;Runtime;Scalability;Switches;Markov processes;Data structures;Generators","graph theory;Markov processes;network theory (graphs);parallel algorithms;sampling methods;set theory;shared memory systems;statistical distributions","dependency structure;shared-memory parallel algorithms;uniform distribution;hash-set;null-models;network science;sequential ES-MC;global edge switching Markov chain;parallel global edge switching;approximate uniform samples;graph generators;prescribed degree sequence;simple graphs;uniform sampling;G-ES-MC","",1.0,"",61.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Parallel, Portable Algorithms for Distance-2 Maximal Independent Set and Graph Coarsening","B. Kelley; S. Rajamanickam","Sandia National Laboratories, Albuquerque, New Mexico, U.S.A; Sandia National Laboratories, Albuquerque, New Mexico, U.S.A","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","280","290","Given a graph, finding the distance-2 maximal independent set (MIS-2) of the vertices is a problem that is useful in several contexts such as algebraic multigrid coarsening or multilevel graph partitioning. Such multilevel methods rely on finding the independent vertices so they can be used as seeds for aggregation in a multilevel scheme. We present a parallel MIS-2 algorithm to improve performance on modern accelerator hardware. This algorithm is implemented using the Kokkos programming model to enable performance portability. We demonstrate the portability of the algorithm and the performance on a variety of architectures (x86/ARM CPUs and NVIDIA/AMD GPUs). The resulting algorithm is also deterministic, producing an identical result for a given input across all of these platforms. The new MIS-2 implementation outperforms implementations in state of the art libraries like CUSP and ViennaCL by 3-8x while producing similar quality results. We further demonstrate the benefits of this approach by developing parallel graph coarsening scheme for two different use cases. First, we develop an algebraic multigrid (AMG) aggregation scheme using parallel MIS-2 and demonstrate the benefits as opposed to previous approaches used in the MueLu multigrid package in Trilinos. We also describe an approach for implementing a parallel multicolor “cluster” Gauss-Seidel preconditioner using this MIS-2 coarsening, and demonstrate better performance with an efficient, parallel, mul-ticolor Gauss-Seidel algorithm.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00035","Sandia National Laboratories; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820696","graph algorithms;preconditioners;performance portability","Distributed processing;Clustering algorithms;Programming;Libraries;Hardware;Partitioning algorithms","differential equations;graph theory;graphics processing units;iterative methods;mathematics computing;multiprocessing systems;parallel algorithms","parallel algorithms;portable algorithms;distance-2 maximal independent set;algebraic multigrid coarsening;multilevel graph partitioning;multilevel methods;parallel MIS-2 algorithm;parallel graph coarsening scheme;parallel multicolor cluster Gauss-Seidel preconditioner;MIS-2 coarsening;multicolor Gauss-Seidel algorithm;CUSP library;ViennaCL library;MueLu multigrid package","",1.0,"",30.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Asynchronous Distributed-Memory Triangle Counting and LCC with RMA Caching","A. Strausz; F. Vella; S. Di Girolamo; M. Besta; T. Hoefler","Dept. of Computer Science, ETH Zurich, Zurich, Switzerland; Dept. of Engineering and Computer Science, University of Trento, Trento, Italy; Dept. of Computer Science, ETH Zurich, Zurich, Switzerland; Dept. of Computer Science, ETH Zurich, Zurich, Switzerland; Dept. of Computer Science, ETH Zurich, Zurich, Switzerland","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","291","301","Triangle count and local clustering coefficient are two core metrics for graph analysis. They find broad application in analyses such as community detection and link recommen-dation. To cope with the computational and memory demands that stem from the size of today's graph datasets, distributed-memory algorithms have to be developed. Current state-of-the-art solutions suffer from synchronization overheads or expensive pre-computations needed to distribute the graph, achieving limited scaling capabilities. We propose a fully asynchronous implementation for triangle counting and local clustering coef-ficient based on 1D partitioning, using remote memory accesses for transferring data and avoid synchronization. Additionally, we show how these algorithms present data reuse on remote memory accesses and how the overall communication time can be improved by caching these accesses. Finally, we extend CLaMPI, a software-layer caching system for MPI RMA, to include application-specific scores for cached entries and influence the eviction procedure to improve caching efficiency. Our results show improvements on shared memory, and we achieve 14x speedup from 4 to 64 nodes for the LiveJoumal 1 graph on distributed memory. Moreover, we demonstrate how caching remote accesses reduces total running time by up to 73 % with respect to a non-cached version. Finally, we compare our implementation to TriC, the 2020 graph champion paper, and achieve up to 100x faster results for scale-free graphs.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00036","European Research Council (ERC)(grant numbers:101002047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820724","asynchronous computing;caching;distributed computing;local clustering coefficient;RDMA;triangle counting","Measurement;Distributed processing;Costs;Clustering algorithms;Partitioning algorithms;Peer-to-peer computing;Synchronization","application program interfaces;cache storage;distributed memory systems;graph theory;message passing;parallel algorithms;shared memory systems","current state-of-the-art solutions;synchronization overheads;expensive pre-computations;fully asynchronous implementation;local clustering coef;remote memory accesses;software-layer caching system;MPI RMA;application-specific scores;cached entries;caching efficiency;shared memory;LiveJoumal 1 graph;distributed memory;caching remote accesses;2020 graph champion paper;scale-free graphs;distributed-memory triangle counting;RMA caching;local clustering coefficient;core metrics;graph analysis;broad application;community detection;link recommen-dation;computational memory demands;graph datasets;distributed-memory algorithms","",2.0,"",46.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Communication-efficient Massively Distributed Connected Components","S. Lamm; P. Sanders","Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Theoretical Informatics, Karlsruhe Institute of Technology, Karlsruhe, Germany","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","302","312","Finding the connected components of an undirected graph is one of the most fundamental graph problems. Connected components are used in a wide spectrum of applications including VLSI design, machine learning and image analysis. Sequentially, one can easily find all connected components in linear time using breadth-first traversal. However, in a massively distributed setting, finding connected components in a scalable way becomes much harder due to data irregularities and the overhead associated with the increased need for communication. In this work, we present a communication-efficient distributed graph algorithm for finding connected components that scales to massively parallel machines. Our algorithm is based on a recent linear-work shared-memory parallel algorithm by Blelloch et al. [1] and refines it for a distributed memory setting. This includes a communication-efficient graph contraction procedure, as well as a distributed variant of the low diameter decomposition by Miller et al. [2]. We tackle the data irregularities introduced by high degree vertices by using an efficient procedure for distributing their incident edges. Our experimental evaluation on up to 16384 cores indicates a good weak scaling behavior that outperforms current state-of-the-art algorithms.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820735","graph algorithms;distributed algorithms;communication efficiency;graph connectivity","Distributed processing;Image analysis;Fluctuations;Scalability;Machine learning;Very large scale integration;Parallel machines","computational complexity;distributed shared memory systems;graph theory;parallel algorithms;parallel machines;tree searching","communication-efficient massively distributed connected components;communication-efficient distributed graph algorithm;undirected graph;graph problems;breadth-first traversal;data irregularity;massively parallel machines;linear-work shared-memory parallel algorithm;distributed memory setting;communication-efficient graph contraction;low diameter decomposition","","","",55.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Mnemonic: A Parallel Subgraph Matching System for Streaming Graphs","B. Bhattarai; H. Huang","George Washington University; George Washington University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","313","323","Finding patterns in large highly connected datasets is critical for value discovery in business development and scientific research. This work focuses on the problem of subgraph matching on streaming graphs, which provides utility in a myriad of real-world applications ranging from social network analysis to cybersecurity. Each application poses a different set of control parameters, including the restrictions for a match, type of data stream, and search granularity. The problem-driven design of existing subgraph matching systems makes them challenging to apply for different problem domains. This paper presents Mnemonic, a programmable system that provides a high-level API and democratizes the development of a wide variety of subgraph matching solutions. Importantly, Mnemonic also delivers key data management capabilities and optimizations to support real-time processing on long-running, high-velocity multi-relational graph streams. The experiments demonstrate the versatility of Mnemonic, as it outperforms several state-of-the-art systems by up to two orders of magnitude.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00038","DARPA(grant numbers:N66001-18-C-4033); National Science Foundation(grant numbers:1618706,1717774,2127207); Department of Defense; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820731","subgraph;graph pattern;matching;isomorphism;streaming","Distributed processing;Social networking (online);Real-time systems;Distance measurement;Computer security;Pattern matching;Optimization","application program interfaces;data mining;graph theory;pattern matching;security of data;social networking (online)","Mnemonic;parallel subgraph matching system;streaming graphs;highly connected datasets;value discovery;business development;scientific research;real-world applications;social network analysis;different set;control parameters;data stream;search granularity;problem-driven design;subgraph matching systems;different problem domains;programmable system;high-level API;subgraph matching solutions;key data management capabilities;high-velocity multirelational graph streams","","","",35.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"QoS-awareness of Microservices with Excessive Loads via Inter-Datacenter Scheduling","J. Shi; J. Wang; K. Fu; Q. Chen; D. Zeng; M. Guo","Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; China University of Geosciences; Shanghai Jiao Tong University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","324","334","User-facing applications often experience excessive loads and are shifting towards microservice software architecture. While the local datacenter may not have enough resources to host the excessive loads, a reasonable solution is moving some microservices of the applications to remote datacenters. However, it is nontrivial to identify the appropriate migration decision, as the microservices show different characteristics, and the local datacenter also shows different resource contention situations. We therefore propose ELIS, an inter-datacenter scheduling system that ensures the required Quality-of-Service (QoS) of the microservice application with excessive loads, while minimizing the resource usage of the remote datacenter. ELIS comprises a resource manager and a reward-based microservice migrator. The resource manager finds the near-optimal resource configurations for different microservices to minimize resource usage while ensuring QoS. The microservice migrator migrates some microservices to remote datacenters when local resources cannot afford the excessive loads. Our experimental results show that ELIS ensures the required QoS of user-facing applications at excessive loads. Meanwhile, it reduces overall/remote resource usage by 13.1% and 58.1% on average, respectively.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00039","National Natural Science Foundation of China(grant numbers:62022057,61832006,61872240); Shanghai international science and technology collaboration project(grant numbers:21510713600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820678","Microservices;QoS;Inter datacenter;Resource management","Distributed processing;Costs;Software architecture;Microservice architectures;Quality of service;Performance gain;Throughput","computer centres;quality of service;resource allocation;scheduling;service-oriented architecture","excessive loads;user-facing applications;microservice software architecture;local datacenter;remote datacenter;resource contention situations;inter-datacenter scheduling system;microservice application;resource manager;reward-based microservice migrator;near-optimal resource configurations;local resources;ELIS;quality-of-service;QoS","",1.0,"",42.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Resource Utilization Aware Job Scheduling to Mitigate Performance Variability","D. Nichols; A. Marathe; K. Shoga; T. Gamblin; A. Bhatele","Department of Computer Science, University of Maryland, College Park, Maryland, USA; Lawrence Livermore National Laboratory, Livermore, California, USA; Lawrence Livermore National Laboratory, Livermore, California, USA; Lawrence Livermore National Laboratory, Livermore, California, USA; Lawrence Livermore National Laboratory, Livermore, California, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","335","345","Resource contention on high performance computing (HPC) platforms can lead to significant variation in application performance. When several jobs experience such large variations in run times, it can lead to less efficient use of system resources. It can also lead to users over-estimating their job's expected run time, which degrades the efficiency of the system scheduler. Mitigating performance variation on HPC platforms benefits end users and also enables more efficient use of system resources. In this paper, we present a pipeline for collecting and analyzing system and application performance data for jobs submitted over long periods of time. We use a set of machine learning (ML) models trained on this data to classify performance variation using current system counters. Additionally, we present a new resource-aware job scheduling algorithm that utilizes the ML pipeline and current system state to mitigate job variation. We evaluate our pipeline, ML models, and scheduler using various proxy applications and an actual implementation of the scheduler on an Infiniband-based fat-tree cluster.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00040","National Science Foundation(grant numbers:2047120); Lawrence Livermore National Laboratory(grant numbers:DE-AC52-07NA27344 (LLNL-CONF-825754)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820723","performance variability;data analytics;machine learning;prediction models;scheduling","Measurement;Schedules;Distributed processing;Scheduling algorithms;High performance computing;Pipelines;Machine learning","computer centres;learning (artificial intelligence);parallel processing;resource allocation;scheduling;trees (mathematics)","high performance computing;HPC;application performance;run times;system resources;system scheduler;mitigating performance variation;benefits end users;collecting analyzing system;machine learning models;current system counters;resource-aware job scheduling algorithm;ML pipeline;current system state;job variation;proxy applications;resource utilization aware job scheduling;mitigate performance variability;resource contention","","","",29.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Dynamic Task Shaping for High Throughput Data Analysis Applications in High Energy Physics","B. Tovar; B. Lyons; K. Mohrman; B. Sly-Delgado; K. Lannon; D. Thain","Department of Computer Science and Engineering, University of Notre Dame; Department of Computer Science and Engineering, University of Notre Dame; Department of Physics, University of Notre Dame; Department of Computer Science and Engineering, University of Notre Dame; Department of Physics, University of Notre Dame; Department of Computer Science and Engineering, University of Notre Dame","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","346","356","Distributed data analysis frameworks are widely used for processing large datasets generated by instruments in scientific fields such as astronomy, genomics, and particle physics. Such frameworks partition petabyte-size datasets into chunks and execute many parallel tasks to search for common patterns, locate unusual signals, or compute aggregate properties. When well-configured, such frameworks make it easy to churn through large quantities of data on large clusters. However, configuring frameworks presents a challenge for end users, who must select a variety of parameters such as the blocking of the input data, the number of tasks, the resources allocated to each task, and the size of nodes on which they run. If poorly configured, the result may perform many orders of magnitude worse than optimal, or the application may even fail to make progress at all. Even if a good configuration is found through painstaking observations, the performance may change drastically when the input data or analysis kernel changes. This paper considers the problem of automatically configuring a data analysis application for high energy physics (TopEFT) built upon standard frameworks for physics analysis (Coffea) and distributed tasking (Work Queue). We observe the inherent variability within the application, demonstrate the problems of poor configuration, and then develop several techniques for automatically sizing tasks to meet goals of resource consumption, and overall application completion.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00041","National Science Foundation(grant numbers:OAC-1931348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820639","high throughput;hep;coffea;task shaping;dynamic resource allocations;topEFT","Distributed processing;High energy physics;Data analysis;Instruments;Genomics;Throughput;Task analysis","data analysis;high energy physics instrumentation computing","high energy physics;physics analysis;dynamic task shaping;high throughput data analysis;distributed data analysis frameworks;parallel tasks;TopEFT;Coffea;Work Queue","","","",27.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Multi-Phase Task-Based HPC Applications: Quickly Learning how to Run Fast","L. L. Nesi; L. M. Schnorr; A. Legrand","Institute of Informatics, PPGC/UFRGS Porto Alegre, Brazil Univ. Grenoble Alpes, Grenoble, France; Institute of Informatics, PPGC/UFRGS, Porto Alegre, Brazil; Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, Grenoble, France","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","357","367","Parallel applications performance strongly depends on the number of resources. Although adding new nodes usually reduces execution time, excessive amounts are often detrimental as they incur substantial communication overhead, which is difficult to anticipate. Characteristics like network contention, data distribution methods, synchronizations, and how communications and computations overlap generally impact the performance. Finding the correct number of resources can thus be particularly tricky for multi-phase applications as each phase may have very different needs, and the popularization of hybrid ($C$ PU+GPU) machines and heterogeneous partitions makes it even more difficult. In this paper, we study and propose, in the context of a task-based GeoStatistic application, strategies for the application to actively learn and adapt to the best set of heterogeneous nodes it has access to. We propose strategies that use the Gaussian Process method with trends, bound mechanisms for reducing the search space, and heterogeneous behavior modeling. We compare these methods with traditional exploration strategies in 16 different machines scenarios. In the end, the proposed strategies are able to gain up to ≈51% compared to the standard case of using all the nodes while having low overhead.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00042","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES)(grant numbers:001); National Council for Scientific and Technological Development (CNPq)(grant numbers:141971/2020-7); FAPERGS(grant numbers:19/711-6); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820620","HPC;Heterogeneous;Task-Based;Distribution;Load Balancing","Distributed processing;Adaptation models;Gaussian processes;Market research;Behavioral sciences;Task analysis;Standards","Gaussian processes;learning (artificial intelligence);parallel processing;statistical analysis","hybrid machines;heterogeneous partitions;task-based GeoStatistic application;Gaussian Process method;heterogeneous behavior modeling;exploration strategies;multiphase task-based HPC applications;parallel applications;network contention;data distribution methods;multiphase applications","","","",28.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"DFMan: A Graph-based Optimization of Dataflow Scheduling on High-Performance Computing Systems","F. Chowdhury; F. Di Natale; A. Moody; K. Mohror; W. Yu","Florida State University; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Lawrence Livermore National Laboratory; Florida State University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","368","378","Scientific research and development campaigns are materialized by workflows of applications executing on high-performance computing (HPC) systems. These applications con-sist of tasks that can have inter- or intra-application flows of data to achieve the research goals successfully. These dataflows create dependencies among the tasks and cause resource con-tention on shared storage systems, thus limiting the aggregated I/O bandwidth achieved by the workflow. However, these I/O performance issues are often solved by tedious and manual efforts that demand holistic knowledge about the data dependencies in the workflow and the information about the infrastructure being utilized. Taking this into consideration, we design DFMan, a graph-based dataflow management and optimization framework for maximizing I/O bandwidth by leveraging the powerful storage stack on HPC systems to manage data sharing optimally among the tasks in the workflows. In particular, we devise a graph-based optimization algorithm that can leverage an intuitive graph representation of dataflow- and system-related information, and automatically carry out co-scheduling of task and data placement. According to our experiments, DFMan optimizes a wide variety of scientific workflows such as Hurricane 3D on Cloud Model 1 (CM1), Montage Carina Nebula (NGC3372), and an emulated dataflow kernel of the Multiscale Machine-learned Modeling Infrastructure (MuMMI I/O) on the Lassen supercomputer, and improves their aggregated I/O bandwidth by up to 5.42 x, 2.12 x and 1.29 x, respectively, compared to the baseline bandwidth.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00043","U.S. Department of Energy; Lawrence Livermore National Laboratory(grant numbers:DE-AC52-07NA27344,LLNL-CONF-827797); Office of Science; National Science Foundation(grant numbers:1561041,1564647,1763547); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820699","Graph based optimization;Task data co scheduling;HPC workflow;HPC Dataflow;HPC storage systems;Scientific-application-workflow","Solid modeling;Three-dimensional displays;Processor scheduling;High performance computing;Bandwidth;Manuals;Supercomputers","data flow computing;data flow graphs;data handling;graph theory;natural sciences computing;optimisation;parallel machines;scheduling;shared memory systems;storage management","dataflow scheduling;high-performance computing systems;resource contention;shared storage systems;holistic knowledge;data dependency;graph-based dataflow management;storage stack;HPC systems;graph-based optimization algorithm;intuitive graph representation;system-related information;scientific workflows;multiscale machine-learned modeling infrastructure;dataflow kernel emulation;DFMan;I/O performance;dataflow optimization;I/O bandwidth maximization;optimal data sharing management;data placement;Hurricane 3D;Cloud Model 1;Montage Carina Nebula;MuMMI I/O;Lassen supercomputer","","","",50.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Parallel Approximations of the Tukey g-and-h Likelihoods and Predictions for Non-Gaussian Geostatistics","S. Mondal; S. Abdulah; H. Ltaief; Y. Sun; M. G. Genton; D. E. Keyes","Statistics Program, King Abdullah University of Science and Technology; Extreme Computing Research Center, King Abdullah University of Science and Technology; Extreme Computing Research Center, King Abdullah University of Science and Technology; Statistics Program, King Abdullah University of Science and Technology; Extreme Computing Research Center, King Abdullah University of Science and Technology; Extreme Computing Research Center, King Abdullah University of Science and Technology","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","379","389","Maximum likelihood estimation is an essential tool in the procedure to impute missing data in climate/weather applications. By defining a particular statistical model, the maximum likelihood estimation can be used to understand the underlying structure of given geospatial data. The Gaussian random field has been widely used to describe geospatial data, as one of the most popular models under the hood of maximum likelihood estimation. Computation of Gaussian log-likelihood demands operations on a dense symmetric positive definite matrix, often parameterized by the Matérn correlation function. This computation of the log-likelihood requires $\mathcal{O}(n^{2})$ storage and $\mathcal{O}(n^{3})$ operations, which can be a huge task considering that the number of geographical locations, $n$, now commonly reaches into the millions. However, despite its appealing theoretical properties, the assumptions of Gaussianity may be unrealistic since real data often show signs of skewness or have some extreme values. Herein, we consider the Tukey ${g-}$ and $-h$ (TGH) random field as an example of a non-Gaussian random field that shows more robustness in modeling geospatial data by including two more parameters to incorporate skewness and heavy tail features in the model. This work provides the first HPC implementation of the TGH random field's inference on parallel hardware architectures. Using task-based programming models associated with dynamic runtime systems, our implementation leverages the high concurrency of current parallel systems. This permits to run the exact log-likelihood evaluation of the Tukey g-and-h (TGH) random fields for a decent number of geospatial locations. To tackle large-scale problems, we provide additionally an implementation of the given model using two different low-rank approximations. We compress the aforementioned positive-definite symmetric matrix for computing the log-likelihood and rely on the Tile Low-Rank (TLR) and the Hierarchical Off-Diagonal Low-Rank (HODLR) matrix approximations. We assess the performance and accuracy of the proposed implementations using synthetic datasets up to $800K$ and a $300K$ precipitation data of Germany to demonstrate the advantage of using non-Gaussian over Gaussian random fields. Moreover, by relying on TLR/HODLR matrix computations, we can now solve for larger matrix sizes while preserving the required accuracy for prediction. We show the performance superiority of TLR over HODLR matrix computations when calculating the TGH likelihoods and predictions. Our TLR-based approximation shows a speedup up to $7.29X$ and $2.96X$ on shared-memory and distributed-memory systems, respectively, compared to the exact implementation.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00044","Intel; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820617","Hierarchical Off Diagonal Low Rank approximation;Large scale geo statistical application;Likelihood approximation;Non Gaussian random field;Tile low rank approximation","Maximum likelihood estimation;Symmetric matrices;Runtime;Computational modeling;Linear algebra;Tail;Predictive models","computational complexity;data analysis;distributed memory systems;Gaussian processes;geophysics computing;inference mechanisms;matrix algebra;maximum likelihood estimation;parallel architectures;random processes;shared memory systems","parallel approximations;nonGaussian geostatistics;maximum likelihood estimation;missing data imputation;statistical model;dense symmetric positive definite matrix;nonGaussian random field;task-based programming models;log-likelihood evaluation;geospatial locations;positive-definite symmetric matrix;precipitation data;TGH likelihood;hierarchical off-diagonal low-rank matrix approximation;Gaussian log-likelihood;Tukey g-and-h likelihood;geospatial data modeling;Matérn correlation function;geographical locations;HPC;TGH random field inference;parallel hardware architecture;dynamic runtime systems;parallel system concurrency;tile low-rank matrix approximation;shared-memory system;distributed-memory system","","","",49.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Parallelizing and Balancing Coupled DSMC/PIC for Large-scale Particle Simulations","H. Qiu; C. Xu; D. Li; H. Wang; J. Li; Z. Wang","College of Computer, National University of Defense Technology; College of Computer, National University of Defense Technology; College of Aerospace Science and Engineer, National University of Defense Technology; College of Aerospace Science and Engineer, National University of Defense Technology; College of Aerospace Science and Engineer, National University of Defense Technology; School of Computing, University of Leeds","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","390","401","In high-performance and parallel computing, an important application class is particle simulation. Due to massive particle migration among distributed simulation workers across simulation iterations, achieving balanced runtime work distribution is vital for accelerating large-scale realistic particle simulations. This paper proposes a novel approach to enable dynamic load balance for distributed numerical particle simulations, specifically targeting the latest coupled DSMC/PI C method. Unlike prior work, our approach adopts a dual, nested unstructured grid organization to facilitate coupled DSMC/PIC computation and runtime grid distribution. Our implementation leverages both centralized and distributed communication strategies to dynamically migrate particles among arbitrary parallel processes. It then employs a load balancer - driven by a carefully designed analytical model and a grid remapping mechanism - to dynamically redistribute the simulation workloads among parallel simulation workers. By constantly monitoring and redis-tributing the simulation work across workers, our approach can adapt to the change of particle distribution across simulation iterations, avoiding a few workers becoming the performance bottleneck of the entire simulation process. We integrate our techniques into a coupled DSMC/PIC solver and apply them to simulate the plasma plume with hydrogen atoms and ions. Experimental results show that our approach can scale well up to 1500+ processes with billions of particles, exhibiting the state-of-the-art parallel simulation scalability and efficiency for plasma plume simulation.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00045","National Natural Science Foundation of China(grant numbers:U173024,61772542,61872294); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820729","Coupled DSMC/PIC;Particle simulation;Dynamic load balance","Adaptation models;Analytical models;Solid modeling;Runtime;Three-dimensional displays;Computational modeling;Numerical models","computer simulation;Monte Carlo methods;parallel processing;physics computing;plasma simulation;resource allocation","arbitrary parallel processes;grid remapping mechanism;simulation workloads;particle distribution;simulation iterations;parallel simulation;plasma plume simulation;large-scale particle simulations;parallel computing;massive particle migration;balanced runtime work distribution;dynamic load balance;distributed numerical particle simulations;runtime grid distribution;distributed communication strategies;high performance computing;nested unstructured grid organization;centralized communication strategies;coupled DSMC/PIC solver;direct simulation Monte Carlo;particle in cell","",3.0,"",37.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Next-Generation Local Time Stepping for the ADER-DG Finite Element Method","A. Breuer; A. Heinecke","Friedrich Schiller University Jena, Jena, Germany; Intel Corporation, Santa Clara, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","402","413","High-frequency ground motion simulations pose a grand challenge in computational seismology. Two main factors drive this challenge. First, to account for higher frequencies, we have to extend our numerical models, e.g., by considering anelasticity, or by including mountain topography. Second, even if we were able to keep our models unchanged, simply doubling the frequency content of a seismic wave propagation solver requires a sixteen-fold increase in computational resources due to the used four-dimensional space-time domains. This work presents the Extreme Scale Discontinuous Galerkin Environment (EDGE) in the context of high-frequency ground motion simulations. Our presented enhancements cover the entire spectrum of the unstructured finite element solver. This includes the incorporation of anelasticity, the introduction of a next-generation clustered local time stepping scheme, and the introduction of a completely revised communication scheme. We close the modeling and simulation loop by presenting our new and rich preprocessing, which drives the high problem-awareness and numerical efficiency of the core solver. In summary, the presented work allows us to conduct large scale high-frequency ground motion simulations efficiently, rou-tinely and conveniently. The soundness of our work is underlined by a set of high-frequency verification runs using a realistic setting. We conclude the presentation by studying EDGE's com-bined algorithmic and computational efficiency in a demanding setup of the 2014 Mw 5.1 La Habra earthquake. Our results are compelling and show an improved time-to-solution by over 10 x while scaling strongly from 256 to 1,536 nodes of the Frontera supercomputer with a sustained non-zero performance of 1.91 FP32-PFLOPS.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00046","Carl Zeiss Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820689","local time stepping;ADER-DG;unstructured meshes;large scale simulations;seismic wave propagation;anelasticity","Computational modeling;Seismology;Surfaces;Supercomputers;Numerical models;Finite element analysis;Seismic waves","earthquakes;finite element analysis;Galerkin method;geophysics computing;parallel machines;seismic waves;seismology;wave propagation","high problem-awareness;scale high-frequency ground motion simulations;high-frequency verification runs;computational efficiency;generation local time stepping;ADER-DG finite element method;computational seismology;anelasticity;frequency content;computational resources;four-dimensional space-time domains;Extreme Scale Discontinuous Galerkin Environment;presented enhancements;unstructured finite element;next-generation clustered local time;simulation loop","","","",38.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"A Framework to Exploit Data Sparsity in Tile Low-Rank Cholesky Factorization","Q. Cao; R. Alomairy; Y. Pei; G. Bosilca; H. Ltaief; D. Keyes; J. Dongarra","Innovative Computing Laboratory, University of Tennessee, US; Division of Computer, Electrical, and Mathematical Sciences and Engineering, Extreme Computing Research Center, King Abdullah University of Science and Technology, KSA; Innovative Computing Laboratory, University of Tennessee, US; Innovative Computing Laboratory, University of Tennessee, US; Division of Computer, Electrical, and Mathematical Sciences and Engineering, Extreme Computing Research Center, King Abdullah University of Science and Technology, KSA; Division of Computer, Electrical, and Mathematical Sciences and Engineering, Extreme Computing Research Center, King Abdullah University of Science and Technology, KSA; University of Manchester, UK","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","414","424","We present a general framework that couples the PaRSEC runtime system and the HiCMA numerical library to solve challenging 3D data-sparse problems. Though formally dense, many matrix operators possess a rank structured property that can be exploited during the most time-consuming computational phase, i.e., the matrix factorization. In particular, this work highlights how a software bundle powered by a task-based programming model can address the heterogeneous workloads engendered by compressing the dense operator. Using Tile Low-Rank (TLR) approximation, our approach consists in capturing the most significant information in each tile of the matrix using a threshold which satisfies the application's accuracy requirements. Matrix operations are performed on the compressed data layout, reducing memory footprint and algorithmic complexity. Our proposed software solution accommodates a range of traditional data structures of linear algebra, i.e., from dense and data-sparse to sparse, within a single matrix operation. Separation of concerns is at the heart: hardware-agnostic implementation, asynchronous execution with a dynamic runtime system, and high performance numerical kernels, to prepare scientific applications to embrace exascale opportunities. This ambition necessitates extensions to PaRSEC that incorporate information related to data structure and rank distribution into the runtime decision-making. We introduce two runtime optimizations to address the challenges encountered when confronted with a large rank disparity: (1) a trimming procedure performed at runtime to cut away data dependencies from the directed acyclic graph discovered to be no longer required after compression and (2) a rank-aware diamond-shaped data distribution to mitigate the load imbalance overheads, reduce data movement, and conserve memory foot-print. We assess our implementation using 3D unstructured mesh deformation based on Radial Basis Function (RBF) interpolation. We report performance results on two different high-performance supercomputers and compare against existing state-of-the-art implementation. Our implementation shows up to 7-fold on Shaheen II and 9-fold on Fugaku performance superiority in situations where the 3D unstructured mesh deformation application renders a matrix operator with low density. Our software framework solves a formally dense 3D problem with 52M mesh points on 65K cores in about half an hour. This multidisciplinary work emphasizes the need for runtime systems to go beyond their primary responsibility of task scheduling on massively parallel hardware system, by synergistically bridging matrix algebra libraries with scientific applications.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820680","Low-rank approximations;Task-based programming model;Dynamic runtime system;HPC;Mesh deformations","Runtime;Three-dimensional displays;Programming;Data structures;Software;Matrices;Libraries","approximation theory;computational complexity;data compression;data handling;data structures;decision making;directed graphs;interpolation;mathematical operators;mathematics computing;matrix decomposition;mesh generation;parallel processing;resource allocation;scheduling;solid modelling;sparse matrices","rank structured property;matrix factorization;software bundle;task-based programming;heterogeneous workloads;matrix operations;compressed data layout;memory footprint;hardware-agnostic implementation;dynamic runtime system;high performance numerical kernels;data structure;rank distribution;runtime decision-making;runtime optimizations;rank disparity;data dependencies;rank-aware diamond-shaped data distribution;data movement;high-performance supercomputers;Fugaku performance superiority;matrix operator;software framework;formally dense 3D problem;massively parallel hardware system;matrix algebra libraries;data sparsity;tile low-rank Cholesky factorization;PaRSEC runtime system;HiCMA numerical library;tile low-rank approximation;3D data-sparse problems;algorithmic complexity;linear algebra;asynchronous execution;directed acyclic graph;load imbalance overheads;3D unstructured mesh deformation;radial basis function;RBF interpolation;task scheduling","",1.0,"",40.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"On the Parallel Reconstruction from Pooled Data","O. Gebhard; M. Hahn-Klimroth; D. Kaaser; P. Loick","TU Dortmund University, Dortmund, Germany; TU Dortmund University, Dortmund, Germany; Universität Hamburg, Hamburg, Germany; Goethe University Frankfurt, Frankfurt, Germany","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","425","435","In the pooled data problem the goal is to efficiently reconstruct a binary signal from additive measurements. Given a signal $\sigma\in \{0, 1\}^{n}$, we can query multiple entries at once and get the total number of non-zero entries in the query as a result. We assume that queries are time-consuming and therefore focus on the setting where all queries are executed in parallel. For the regime where the signal is sparse such that $\Vert\sigma\Vert_{1}= o(n)$ our results are twofold: First, we propose and analyze a simple and efficient greedy reconstruction algorithm. Secondly, we derive a sharp information-theoretic threshold for the minimum number of queries required to reconstruct σ with high probability. Our first result matches the performance guarantees of much more involved constructions (Karimi et al. 2019). Our second result extends a result of Alaoui et al. (2014) and Scarlett & Cevher (2017) who studied the pooled data problem for dense signals. Finally, our theoretical findings are complemented with empirical simulations. Our data not only confirm the information-theoretic thresholds but also hint at the practical applicability of our pooling scheme and the simple greedy reconstruction algorithm.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820733","Reconstruction;Sparse Signal;Pooled Data;Information Theory;Phase Transitions","Distributed processing;Additives;Distributed databases;Reconstruction algorithms","approximation theory;graph theory;greedy algorithms;image reconstruction;matrix algebra;probability","parallel reconstruction;pooled data problem;binary signal;additive measurements;signal $\sigma\in;query multiple entries;nonzero entries;$\Vert\sigma\Vert;simple reconstruction algorithm;efficient greedy reconstruction algorithm;sharp information-theoretic threshold;dense signals;information-theoretic thresholds;pooling scheme;simple greedy reconstruction algorithm","","","",33.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"P-ckpt: Coordinated Prioritized Checkpointing","S. Behera; L. Wan; F. Mueller; M. Wolf; S. Klasky","North Carolina State University; Oak Ridge National Laboratory; North Carolina State University; Oak Ridge National Laboratory; Oak Ridge National Laboratory","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","436","446","Good prediction accuracy and adequate lead time to failure are key to the success of failure-aware Check-point/Restart (C/R) models on current and future large-scale High-Performance Computing (HPC) systems. This paper develops a novel checkpointing technique, called p-ckpt, that aims to maintain the performance efficiency of failure-aware C/R models even when failures are predicted with a small lead time. The p-ckpt technique is developed for HPC systems with multi-level memory systems to prioritize checkpoints from vulnerable nodes (nodes with predicted failure) in the event of failure prediction. It applies coordination among the nodes within an application so that vulnerable nodes' checkpoint data is stored to the Parallel File System (PFS) first by assigning priorities based on the lead time to failure. Vulnerable nodes thus have low-latency access on the critical path to the PFS before any failure happens. Further, we create the hybrid p-ckpt model by integrating Live Migration (LM) because of its cost-effectiveness and to reduce checkpoint frequency. Our hybrid p-ckpt C/R model considers prediction lead time and checkpoint latency to the PFS to decide on a feasible proactive action such as p-ckpt and LM. Simulations of six real-world applications for the Summit supercomputer show a ≈53-65% reduction in overhead due to the hybrid p-ckpt model compared to a ≈31-61% reduction in a state-of-the-art solution. We assess our C/R models against multiple failure distributions and consider lead time variability and failure prediction accuracy. Based on this evaluation and assessment, we discuss the trade-offs of using these models and their impact on application overhead.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00049","NSF(grant numbers:1525609,1813004,1818914); DOE(grant numbers:17-SC-20-SC,DE-AC05-000R22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820628","Fault Tolerance;High-Performance Computing;Failure Prediction;I/O subsystem;Checkpoint/Restart;Live Migration;Burst Buffers","Checkpointing;Fault tolerance;Distributed processing;File systems;Computational modeling;High performance computing;Fault tolerant systems","checkpointing;failure analysis;fault tolerant computing;parallel processing;software fault tolerance;system recovery","prediction accuracy;adequate lead time;large-scale high-performance computing;checkpointing technique;performance efficiency;p-ckpt technique;HPC systems;multilevel memory systems;vulnerable nodes;predicted failure;failure prediction;parallel file system;PFS;hybrid p-ckpt model;checkpoint frequency;prediction lead time;multiple failure distributions;lead time variability","","","",35.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"TEE-based decentralized recommender systems: The raw data sharing redemption","A. Dhasade; N. Dresevic; A. -M. Kermarrec; R. Pires","EPFL - Swiss Federal Institute of Technology, Lausanne, Switzerland; EPFL - Swiss Federal Institute of Technology, Lausanne, Switzerland; EPFL - Swiss Federal Institute of Technology, Lausanne, Switzerland; EPFL - Swiss Federal Institute of Technology, Lausanne, Switzerland","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","447","458","Recommenders are central in many applications today. The most effective recommendation schemes, such as those based on collaborative filtering (CF), exploit similarities between user profiles to make recommendations, but potentially expose private data. Federated learning and decentralized learning systems address this by letting the data stay on user's machines to preserve privacy: each user performs the training on local data and only the model parameters are shared. However, sharing the model parameters across the network may still yield privacy breaches. In this paper, we present Rex, the first enclave-based decentralized CF recommender. Rex exploits Trusted execution environments (TEE), such as Intel software guard extensions (SGX), that provide shielded environments within the processor to improve convergence while preserving privacy. Firstly, Rex enables raw data sharing, which ultimately speeds up convergence and reduces the network load. Secondly, Rex fully preserves privacy. We analyze the impact of raw data sharing in both deep neural network (DNN) and matrix factorization (MF) recommenders and showcase the benefits of trusted environments in a full-fledged implementation of Rex. Our experimental results demonstrate that through raw data sharing, Rex significantly decreases the training time by 18.3 x and the network load by 2 orders of magnitude over standard decentralized approaches that share only parameters, while fully protecting privacy by leveraging trustworthy hardware enclaves with very little overhead.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820716","privacy;security;recommender systems;SGX","Training;Data privacy;Neural networks;Telecommunication traffic;Collaborative work;Privacy breach;Software","data privacy;deep learning (artificial intelligence);matrix decomposition;recommender systems;trusted computing","Rex;network load;TEE;decentralized recommender systems;recommendation schemes;user profiles;private data;model parameters;enclave-based;CF recommender;collaborative filtering;federated learning;decentralized learning systems;enclave-based decentralized CF recommender;trusted execution environments;Intel software guard extensions;privacy preservation;deep neural network;matrix factorization;raw data sharing redemption","","","",63.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Accuracy vs. Cost in Parallel Fixed-Precision Low-Rank Approximations of Sparse Matrices","R. Ernstbrunner; V. Mayer; W. Gansterer","Faculty of Computer Science, University of Vienna, Vienna, Austria; Faculty of Computer Science, University of Vienna, Vienna, Austria; Faculty of Computer Science, University of Vienna, Vienna, Austria","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","459","469","We study a randomized and a deterministic algorithm for the fixed-precision low-rank approximation problem of large sparse matrices. The Randomized QB Factorization (RandQB_EI) constructs a reduced and dense representation of the originally sparse matrix based on randomization. The representation resulting from the deterministic Truncated LU Factorization with Column and Row Tournament Pivoting (LU_CRTP) is sparse, but fill-in introduced in the factorization process can affect sparsity and performance. We therefore attempt to mitigate fill-in with an incomplete LU_CRTP variant with thresholding (ILUT_CRTP). We analyze this approach and identify potential problems that may arise in practice. We design parallel implementations of RandQB_EI, LU_CRTP and ILUT_CRTP. We experimentally evaluate strong scaling properties for different problems and the runtime required for achieving a given approximation quality. Our results show that LU_CRTP tends to be particularly competitive for low approximation quality. However, when a lot of fill-in occurs, LU_CRTP is outperformed by RandQB_EI especially for higher approximation quality. ILUT_CRTP outperforms both LU_CRTP and RandQB_EI and can achieve speedups up to 40 over LU_CRTP, depending on the amount of fill-in.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00051","Vienna Science and Technology Fund (WWTF)(grant numbers:ICT15-113); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820630","Randomized low-rank approximations;Deterministic low-rank approximations;Randomized QB Factorization;Truncated LU Factorization with Tournament Pivoting","Distributed processing;Runtime;Costs;Scalability;Approximation algorithms;Prediction algorithms;Sparse matrices","approximation theory;deterministic algorithms;iterative methods;matrix algebra;matrix decomposition;sparse matrices","fixed-precision low-rank approximation problem;sparse matrices;Randomized QB Factorization;RandQB_EI;originally sparse matrix;deterministic Truncated LU Factorization;incomplete LU_CRTP variant;ILUT_CRTP;low approximation quality","","","",20.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Hybrid Workload Scheduling on HPC Systems","Y. Fan; Z. Lan; P. Rich; W. Allcock; M. E. Papka","Illinois Institute of Technology, Chicago, IL; Illinois Institute of Technology, Chicago, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Northern Illinois University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","470","480","Traditionally, on-demand, rigid, and malleable applications have been scheduled and executed on separate systems. The ever-growing workload demands and rapidly developing HPC infrastructure trigger the interest of converging these applications on a single HPC system. Although allocating the hybrid workloads within one system could potentially improve system efficiency, it is difficult to balance the tradeoff between the responsiveness of on-demand requests, incentive for malleable jobs, and the performance of rigid applications. In this study, we present several scheduling mechanisms to address the issues involved in co-scheduling on-demand, rigid, and malleable jobs on a single HPC system. We extensively evaluate and compare their performance under various configurations and workloads. Our experimental results show that our proposed mechanisms are capable of serving on-demand workloads with minimal delay, offering incentives for declaring malleability, and improving system performance.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00052","Office of Science(grant numbers:DE-AC02-06CHl1357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820734","cluster scheduling;high-performance computing;on-demand jobs;rigid jobs;malleable jobs","Distributed processing;System performance;Production;Delays","parallel processing;resource allocation;scheduling","on-demand workloads;malleability;system performance;hybrid workload scheduling;HPC systems;on-demand requests","","","",45.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"GSpecPal: Speculation-Centric Finite State Machine Parallelization on GPUs","Y. Wang; R. Watling; J. Qiu; Z. Wang","Department of Computer Science, Michigan Technological University; Department of Computer Science, Michigan Technological University; Department of Computer Science, Michigan Technological University; Department of Computer Science, Michigan Technological University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","481","491","Finite State Machine (FSM) plays a critical role in many real-world applications, ranging from pattern matching to network security. In recent years, significant research efforts have been made to accelerate FSM computations on different parallel platforms, including multicores, GPUs, and DRAM-based accelerators. A popular direction is the speculation-centric parallelization. Despite their abundance and promising results, the benefits of speculation-centric FSM parallelization on GPUs heavily depend on high speculation accuracy and are greatly limited by the inefficient sequential recovery. Inspired by speculative data forwarding used in Thread Level Speculation (TLS), this work addresses the existing bottlenecks by introducing speculative recovery with two heuristics for thread scheduling, which can effectively remove redundant computations and increase the GPU thread utilization. To maximize the performance of running FSMs on GPUs, this work integrates different speculative parallelization schemes into a latency-sensitive framework, GSpecPal, along with a scheme selector which aims to automatically configure the optimal GPU-based parallelization for a given FSM. Evaluation on a set of real-world FSMs with diverse characteristics confirms the effectiveness of GSpecPal. Experimental results show that GSpecPal can obtain 7.2× speedup on average (up to 20×) over the state-of-the-art on an Nvidia GeForce RTX 3090 GPU.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00053","National Science Foundation (NSF)(grant numbers:2105006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820624","Finite State Machine;Speculative Parallelization;GPU","Distributed processing;Processor scheduling;Multicore processing;Instruction sets;Graphics processing units;Automata;Network security","finite state machines;graphics processing units;multiprocessing systems;multi-threading;parallel architectures;parallel processing;pattern matching;shared memory systems","real-world applications;pattern matching;research efforts;FSM computations;parallel platforms;DRAM-based accelerators;speculation-centric parallelization;speculation-centric FSM parallelization;high speculation accuracy;inefficient sequential recovery;speculative data forwarding;thread level speculation;speculative recovery;thread scheduling;GPU thread utilization;GSpecPal;optimal GPU-based parallelization;FSM;speculation-centric finite state machine parallelization;speculative parallelization schemes;Nvidia GeForce RTX 3090 GPU","","","",40.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Lightning: Scaling the GPU Programming Model Beyond a Single GPU","S. Heldens; P. Hijma; B. Van Werkhoven; J. Maassen; R. V. van Nieuwpoort","University of Amsterdam; Vrije Universiteit Amsterdam; Netherlands eScience Center; Netherlands eScience Center; University of Amsterdam","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","492","503","The GPU programming model is primarily aimed at the development of applications that run one GPU. However, this limits the scalability of GPU code to the capabilities of a single GPU in terms of compute power and memory capacity. To scale GPU applications further, a great engineering effort is typically required: work and data must be divided over multiple GPUs by hand, possibly in multiple nodes, and data must be manually spilled from GPU memory to higher-level memories. We present Lightning: a framework that follows the common GPU programming paradigm but enables scaling to large problems with ease. Lightning supports multi-GPU execution of GPU kernels, even across multiple nodes, and seamlessly spills data to higher-level memories (main memory and disk). Existing CUDA kernels can easily be adapted for use in Lightning, with data access annotations on these kernels allowing Lightning to infer their data requirements and the dependencies between subsequent kernel launches. Lightning efficiently distributes the work/data across GPUs and maximizes efficiency by overlapping scheduling, data movement, and kernel execution when possible. We present the design and implementation of Lightning, as well as experimental results on up to 32 GPUs for eight benchmarks and one real-world application. Evaluation shows excellent performance and scalability, such as a speedup of 57.2 x over the CPU using Lighting with 16 GPUs over 4 nodes and 80 GB of data, far beyond the memory capacity of one GPU.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820612","GPU;distributed computing;CUDA;programmina model","Distributed processing;Codes;Computational modeling;Scalability;Memory management;Graphics processing units;Lightning","coprocessors;graphics processing units;object-oriented programming","GPU code;single GPU;memory capacity;GPU applications;GPU memory;higher-level memories;Lightning;common GPU programming paradigm;multiGPU execution;GPU kernels;data access annotations;data requirements;subsequent kernel launches;data movement;kernel execution;GPU programming model;memory size 80.0 GByte","","","",39.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Co-Designing an OpenMP GPU Runtime and Optimizations for Near-Zero Overhead Execution","J. Doerfert; A. Patel; J. Huber; S. Tian; J. M. M. Diaz; B. Chapman; G. Georgakoudis","Argonne National Laboratory, Lemont, USA; University of Waterloo, Waterloo, Ontario, Canada; Oak Ridge National Laboratory, Oak Ridge, USA; Stony Brook University, Stony Brook, USA; Argonne National Laboratory, Lemont, USA; Stony Brook University, Stony Brook, USA; Lawrence Livermore National Laboratory, Livermore, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","504","514","GPU accelerators are ubiquitous in modern HPC systems. To program them, users have the choice between vendor-specific, native programming models, such as CUDA, which provide simple parallelism semantics with minimal runtime support, or portable alternatives, such as OpenMP, which offer rich parallel semantics and feature an extensive runtime library to support execution. While the operations of such a runtime can easily limit performance and drain resources, it was to some degree regarded an unavoidable overhead. In this work we present a co-design methodology for optimizing applications using a specifically crafted OpenMP GPU runtime such that most use cases induce near-zero overhead. Specifically, our approach exposes runtime semantics and state to the compiler such that optimization effectively eliminating abstractions and runtime state from the final binary. With the help of user provided assumptions we can further optimize common patterns that otherwise increase resource consumption. We evaluated our prototype build on top of the LLVM/OpenMP GPU offloading infrastructure with multiple HPC proxy applications and benchmarks. Comparison of CUDA, the original OpenMP runtime, and our co-designed alternative show that, by our approach, performance is significantly improved and resource consumption is significantly lowered. Oftentimes we can closely match the CUDA implementation without sacrificing the versatility and portability of OpenMP.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00055","LLNL(grant numbers:DE-AC52-07NA27344,LLNL-CONF-826728); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820621","OpenMP;gpu;offloading;compiler optimization","Distributed processing;Runtime;Codes;Runtime library;Semantics;Graphics processing units;Prototypes","application program interfaces;graphics processing units;message passing;multiprocessing systems;parallel architectures;parallel processing;parallel programming;performance evaluation;program compilers","optimizations;near-zero overhead execution;GPU accelerators;modern HPC systems;native programming models;CUDA;simple parallelism semantics;minimal runtime support;portable alternatives;parallel semantics;extensive runtime library;drain resources;unavoidable overhead;co-design methodology;specifically crafted OpenMP GPU runtime;runtime semantics;runtime state;user provided assumptions;resource consumption;multiple HPC proxy applications;original OpenMP runtime","",2.0,"",23.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Bit-GraphBLAS: Bit-Level Optimizations of Matrix-Centric Graph Processing on GPU","J. -A. Chen; H. -H. Sung; X. Shen; N. Tallent; K. Barker; A. Li","Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","515","525","In a general graph data structure like an adjacency matrix, when edges are homogeneous, the connectivity of two nodes can be sufficiently represented using a single bit. This insight has, however, not yet been adequately exploited by the existing matrix-centric graph processing frameworks. This work fills the void by systematically exploring the bit-level representation of graphs and the corresponding optimizations to the graph operations. It proposes a two-level representation named Bit-Block Compressed Sparse Row (B2SR) and presents a series of optimizations to the graph operations on B2SR by leveraging the intrinsics of modern GPUs. Evaluations on NVIDIA Pascal and Volta GPUs show that the optimizations bring up to 40× and 6555× for essential GraphBLAS kernels SpMV and SpGEMM, respectively, making GraphBLAS-based BFS accelerate up to 433×, SSSP, PR, and CC up to 35×, and TC up to 52×.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00056","U.S. Department of Energy; Office of Science; NSF(grant numbers:CNS-1717425,CCF-1703487,CCF-2028850); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820669","","Distributed processing;Graphics processing units;Linear algebra;Performance gain;Parallel processing;Data structures;Sparse matrices","data structures;graph theory;graphics processing units;mathematics computing;matrix multiplication;multiprocessing systems;parallel processing;sparse matrices","Bit-GraphBLAS;bit-level optimizations;general graph data structure;adjacency matrix;single bit;existing matrix-centric graph processing frameworks;bit-level representation;corresponding optimizations;graph operations;two-level representation;Bit-Block Compressed Sparse Row;B2SR;GraphBLAS-based BFS","",1.0,"",50.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"CSMV: A Highly Scalable Multi-Versioned Software Transactional Memory for GPUs","D. Nunes; D. Castro; P. Romano","Instituto Superior Técnico & INESC-ID, Lisbon, Portugal; Instituto Superior Técnico & INESC-ID, Lisbon, Portugal; Instituto Superior Técnico & INESC-ID, Lisbon, Portugal","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","526","536","GPUs have traditionally focused on streaming applications with regular parallelism. Over the last years, though, GPUs have also been successfully used to accelerate irregular applications in a number of application domains by using fine grained synchronization schemes. Unfortunately, fine-grained synchronization strategies are notoriously complex and error-prone. This has motivated the search for alternative paradigms aimed to simplify concurrent programming and, among these, Transactional Memory (TM) is probably one of the most prominent proposals. This paper introduces CSMV (Client Server Multiversioned), a multi-versioned Software TM (STM) for GPUs that adopts an innovative client-server design. By decoupling the execution of transactions from their commit process, CSMV provides two main benefits: (i) it enables the use of fast on chip memory to access the global metadata used to synchronize transaction (ii) it allows for implementing highly efficient collaborative commit procedures, tailored to take full advantage of the architectural characteristics of GPUs. Via an extensive experimental study, we show that CSMV achieves up to 3 orders of magnitude speed-ups with respect to state of the art STMs for GPUs and that it can accelerate by up to 20× irregular applications running on state of the art STMs for CPUs.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00057","FCT(grant numbers:UIDB/50021/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820634","Multi-Version Concurrency Control;Synchronization;GPU;Transaction;Transactional Memory","Concurrent computing;Distributed processing;Memory management;Collaboration;Programming;Parallel processing;Metadata","client-server systems;concurrency control;graphics processing units;storage management;transaction processing","GPUs;CSMV;chip memory;fine grained synchronization schemes;fine-grained synchronization strategies;collaborative commit procedure;multiversioned software TM;client server multiversioned;multiversioned software transactional memory","","","",41.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Resilience at Extreme Scale and Connections with Other Domains","L. B. Gomez","Barcelona Supercomputing Center, Spain","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","537","537","Resilience has been one of the main research topics of the IPDPS community for decades. We have covered multiple types of failures and errors which has led to completely different fault tolerance techniques, some of them at the intersection of HPC and ML. The research work, carried out by researchers from many different institutions, shows a strong interaction between theoretical analysis and practical implementations. The results of this endeavor had led to many collaborations, publications and citations; but more interestingly, it has opened new questions and it has shown connections between HPC fields that we didn't know were connected before. In this talk, we will go over this trajectory and get a quick glance at what might come in the future for HPC resilience.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820736","","Resilience;High performance computing;Fault tolerant systems;Fault tolerance;Distributed processing;Trajectory;Thyristors","fault tolerant computing;parallel processing","IPDPS community;fault tolerance technique;HPC resilience","","","","","IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Colza: Enabling Elastic In Situ Visualization for High-performance Computing Simulations","M. Dorier; Z. Wang; U. Ayachit; S. Snyder; R. Ross; M. Parashar","Argonne National Laboratory, Lemont, Illinois; Rutgers University, New Jersey; Kitware, Inc.; Argonne National Laboratory, Lemont, Illinois; Argonne National Laboratory, Lemont, Illinois; University of Utah, Salt Lake City, Utah","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","538","548","In situ analysis and visualization have grown increasingly popular for enabling direct access to data from high-performance computing (HPC) simulations. As a simulation progresses and interesting physical phenomena emerge, however, the data produced may become increasingly complex, and users may need to dynamically change the type and scale of in situ analysis tasks being carried out and consequently adapt the amount of resources allocated to such tasks. To date, none of the production in situ analysis frameworks offer such an elasticity feature, and for good reason: the assumption that the number of processes could vary during run time would force developers to rethink software and algorithms at every level of the in situ analysis stack. In this paper we present Colza, a data staging service with elastic in situ visualization capabilities. Colza relies on the widely used ParaView Catalyst in situ visualization framework and enables elasticity by replacing MPI with a custom collective communication library based on the Mochi suite of libraries. To the best of our knowledge, this work is the first to enable elastic in situ visualization capabilities for HPC applications on top of existing production analysis tools.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00059","U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820656","HPC;In situ analysis;in situ visualization;elasticity","Analytical models;Adaptation models;Computational modeling;High performance computing;Data visualization;Production;Elasticity","application program interfaces;cloud computing;data analysis;data visualisation;message passing;parallel machines;parallel processing;resource allocation;software libraries;software maintenance","Colza;enabling elastic;high-performance computing simulations;simulation progresses;interesting physical phenomena;situ analysis tasks;situ analysis frameworks;elasticity feature;situ analysis stack;data staging service;situ visualization capabilities;situ visualization framework;production analysis tools","",1.0,"",49.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Towards Distributed 2-Approximation Steiner Minimal Trees in Billion-edge Graphs","T. Reza; G. Sanders; R. Pearce","Lawrence Livermore National Laboratory (LLNL), Center for Applied Scientific Computing (CASC); Lawrence Livermore National Laboratory (LLNL), Center for Applied Scientific Computing (CASC); Lawrence Livermore National Laboratory (LLNL), Center for Applied Scientific Computing (CASC)","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","549","559","Given an edge-weighted graph and a set of known seed vertices of interest, a network scientist often desires to understand the graph relationships to explain connections between the seed vertices. If the size of the seed set is 2, shortest path calculations are an attractive computational kernel to explore the connections between the two vertices. When the seed set is 3 or larger (say up to 1,000s) Steiner minimal tree – min-weight acyclic connected subgraph (of the input graph) that contains all the seed vertices – is an attractive generalization of shortest weighted paths. In general, computing a Steiner minimal tree is NP-hard, but decades ago several polynomial-time algorithms were designed and proven to yield Steiner trees whose total weight is bounded within 2 times the minimal Steiner tree. Despite its rich theoretical literature, works related to parallel Steiner minimal tree computation and their scalable implementations are rather scarce. In this paper, we present a parallel 2-approximation Steiner minimal tree algorithm (with theoretical guarantees) and its MPI-based distributed implementation. In place of distance computation between all pairs of seed vertices, an expensive phase in many approximation algorithms, the solution we employ, exploits Voronoi cell computation. Also, this approach has higher parallel efficiency than others that involve minimum spanning tree computation on the entire graph. Furthermore, our distributed design exploits asynchronous processing and a message prioritization scheme to accelerate convergence of distance computation, employs techniques to avoid inefficient distributed spanning tree computation on the entire graph, and harnesses a combination of vertex and edge centric processing to offer fast time-to-solution. We demonstrate scalability and performance of our solution using real-world graphs with up to 128 billion edges and 512 compute nodes (8K processes), show the ability to find Steiner trees with up to 10K seed vertices in under one minute, and present in-depth analyses that highlight the benefits of our design choices. Using four real-world graphs and three seed sets for each, we compare our solution with the state-of-the-art exact Steiner minimal tree solver, SCIP-Jack, and two sequential algorithms with the same approximation bound as our algorithm. Our distributed solution comfortably outperforms these related works on graphs with 10s million edges and offers decent strong scaling – up to 90% efficient. We empirically show that, on average, the total distance (sum of edge weights) of the Steiner tree identified by our solution is 1.0527 times greater than the Steiner minimal tree (i.e., the optimal solution) – well within the theoretical bound of less than equal to 2.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00060","U.S. Department of Energy; National Nuclear Security Administration(grant numbers:DE-AC52-07NA27344); LDRD(grant numbers:21-ERD-020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820640","Distributed computing;Graph algorithm;Steiner tree","Steiner trees;Distributed processing;Scalability;Approximation algorithms;Computational efficiency;Kernel;Convergence","application program interfaces;approximation theory;computational complexity;computational geometry;graph theory;mathematics computing;message passing;parallel processing;set theory;trees (mathematics)","billion-edge graphs;edge-weighted graph;attractive computational kernel;polynomial-time algorithms;MPI-based distributed implementation;Voronoi cell computation;asynchronous processing;edge centric processing;distributed 2-approximation Steiner minimal trees;NP-hard;message prioritization;distributed spanning tree computation;vertex centric processing;Steiner minimal tree solver;parallel Steiner minimal tree computation;SCIP-Jack","","","",54.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"As easy as ABC: Optimal (A)ccountable (B)yzantine (C)onsensus is easy!","P. Civit; S. Gilbert; V. Gramoli; R. Guerraoui; J. Komatovic","CNRS, Sorbonne University; NUS, Singapore; EPFL, University of Sydney; École Polytechnique Federale de Lausanne (EPFL); École Polytechnique Federale de Lausanne (EPFL)","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","560","570","It is known that the agreement property of the Byzantine consensus problem among $n$ processes can be violated in a non-synchronous system if the number of faulty processes exceeds $t_{0}$ = ┌$n$/3┐ − 1 [10], [19]. In this paper, we investigate the accountable Byzantine consensus problem in non-synchronous systems: the problem of solving Byzantine consensus whenever possible (e.g., when the number of faulty processes does not exceed $t_{0}$) and allowing correct processes to obtain proof of culpability of (at least) $t_{0}+ 1$ faulty processes whenever correct processes disagree. We present four complementary contributions: 1) We introduce ABC: a simple yet efficient transformation of any Byzantine consensus protocol to an accountable one. ABC introduces an overhead of only two all-to-all communication rounds and $O(n^{2})$ additional bits in executions with up to $t_{0}$ faults (i.e., in the common case). 2) We define the accountability complexity, a complex-ity metric representing the number of accountability-specific messages that correct processes must send. Fur-thermore, we prove a tight lower bound. In particular, we show that any accountable Byzantine consensus protocol incurs cubic accountability complexity. Moreover, we illustrate that the bound is tight by applying the ABC transformation to any Byzantine consensus protocol. 3) We demonstrate that, when applied to an optimal Byzan-tine consensus protocol, ABC constructs an accountable Byzantine consensus protocol that is (1) optimal with respect to the communication complexity in solving consensus whenever consensus is solvable, and (2) op-timal with respect to the accountability complexity in obtaining accountability whenever disagreement occurs. 4) We generalize ABC to other distributed computing prob-lems besides the classic consensus problem. We charac-terize a class of agreement tasks, including reliable and consistent broadcast [5], that ABC renders accountable.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820722","accountability;Byzantine agreement;fault detection","Measurement;Distributed processing;Lattices;Complexity theory;Consensus protocol;Reliability;Task analysis","communication complexity;distributed algorithms;fault tolerant computing;protocols","nonsynchronous system;accountable Byzantine consensus problem;accountability-specific messages;accountable Byzantine consensus protocol;cubic accountability complexity;ABC transformation;classic consensus problem;optimal Byzantine consensus protocol;optimal accountable byzantine consensus","",2.0,"",25.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Understanding the Design-Space of Sparse/Dense Multiphase GNN dataflows on Spatial Accelerators","R. Garg; E. Qin; F. Muñoz-Matrínez; R. Guirado; A. Jain; S. Abadal; J. L. Abellán; M. E. Acacio; E. Alarcón; S. Rajamanickam; T. Krishna","Georgia Institute of Technology; Georgia Institute of Technology; Universidad de Murcia; Universitat Politecnica de Catalunya; Neutroon; Universitat Politecnica de Catalunya; Universidad Católica de Murcia; Universidad de Murcia; Universitat Politecnica de Catalunya; Sandia National Laboratories; Georgia Institute of Technology","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","571","582","Graph Neural Networks (GNNs) have garnered a lot of recent interest because of their success in learning representations from graph-structured data across several critical applications in cloud and HPC. Owing to their unique compute and memory characteristics that come from an interplay between dense and sparse phases of computations, the emergence of recon-figurable dataflow (aka spatial) accelerators offers promise for acceleration by mapping optimized dataflows (i.e., computation order and parallelism) for both phases. The goal of this work is to characterize and understand the design-space of dataflow choices for running GNNs on spatial accelerators in order for mappers or design-space exploration tools to optimize the dataflow based on the workload. Specifically, we propose a taxonomy to describe all possible choices for mapping the dense and sparse phases of GNN inference, spatially and temporally over a spatial accelerator, capturing both the intra-phase dataflow and the inter-phase (pipelined) dataflow. Using this taxonomy, we do deep-dives into the cost and benefits of several dataflows and perform case studies on implications of hardware parameters for dataflows and value of flexibility to support pipelined execution.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00062","U.S. Department of Energy (DOE); National Nuclear Security Administration(grant numbers:DE-NA-0003525); Fundacion Seneca(grant numbers:MCIN/AEI/10.13039/501100011033,RTI2018-098156-B-C53 (MCIU/AEI/FEDER, UE),20749/FPI/18); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820725","Graph Neural Networks;Spatial Accelerators;Dataflows;Pipelined Parallelism","Distributed processing;Costs;Sensitivity;Taxonomy;Load management;Spatial databases;Hardware","data flow computing;data structures;graph theory;learning (artificial intelligence);neural nets;parallel processing;pipeline processing;system-on-chip","sparse phases;spatial accelerator;intra-phase dataflow;inter-phase;Graph Neural Networks;GNNs;graph-structured data;unique compute;memory characteristics;dense phases;recon-figurable dataflow accelerators;aka spatial;mapping optimized dataflows;computation order;dataflow choices;design-space exploration tools","","","",36.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"“Smarter” NICs for faster molecular dynamics: a case study","S. Karamati; C. Hughes; K. S. Hemmert; R. E. Grant; W. W. Schonbein; S. Levy; T. M. Conte; J. Young; R. W. Vuduc","Georgia Institute of Technology, Atlanta, Georgia, USA; Sandia National Laboratories, Albuquerque, New Mexico, USA; Sandia National Laboratories, Albuquerque, New Mexico, USA; Queen's University, Kingston, Ontario, Canada; Sandia National Laboratories, Albuquerque, New Mexico, USA; Sandia National Laboratories, Albuquerque, New Mexico, USA; Georgia Institute of Technology, Atlanta, Georgia, USA; Georgia Institute of Technology, Atlanta, Georgia, USA; Georgia Institute of Technology, Atlanta, Georgia, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","583","594","This work evaluates the benefits of using a “smart” network interface card (SmartNIC) as a compute accelerator for the example of the MiniMD molecular dynamics proxy application. The accelerator is NVIDIA's BlueField-2 card, which includes an 8-core Arm processor along with a small amount of DRAM and storage. We test the networking and data movement performance of these cards compared to a standard Intel server host using microbenchmarks and MiniMD. In MiniMD, we identify two distinct classes of computation, namely core computation and maintenance computation, which are executed in sequence. We restructure the algorithm and code to weaken this dependence and increase task parallelism, thereby making it possible to increase utilization of the BlueField-2 concurrently with the host. We evaluate our implementation on a cluster consisting of 16 dual-socket Intel Broadwell host nodes with one BlueField-2 per host-node. Our results show that while the overall compute performance of BlueField-2 is limited, using them with a modified MiniMD algorithm allows for up to 20% speedup over the host CPU baseline with no loss in simulation accuracy.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00063","Sandia National Laboratories(grant numbers:2200840); U.S. Department of Energy's; National Nuclear Security Administration(grant numbers:DE-NA0003525); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820686","","Distributed processing;Heuristic algorithms;Clustering algorithms;Random access memory;Parallel processing;Maintenance engineering;Servers","application program interfaces;coprocessors;message passing;microprocessor chips;multiprocessing systems;network interfaces;parallel processing","data movement performance;standard Intel server host;task parallelism;16 dual-socket Intel Broadwell host nodes;host-node;modified MiniMD algorithm;host CPU baseline;Smarter NICs;faster molecular dynamics;smart network interface card;SmartNIC;MiniMD molecular dynamics proxy application;8-core Arm processor;NVIDIA BlueField-2 card","","","",21.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"RLRP: High-Efficient Data Placement with Reinforcement Learning for Modern Distributed Storage Systems","K. Lu; N. Zhao; J. Wan; C. Fei; W. Zhao; T. Deng","Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; The School of Computer Science, Northwestern Polytechnical University, Xi'an, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; SenseTime Research, Shenzhen, China; SenseTime Research, Shenzhen, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","595","605","Modern distributed storage systems with massive data and storage nodes pose higher requirements to the data placement strategy. Furthermore, with emerged new storage devices, heterogeneous storage architecture has become increasingly common and popular. However, traditional strategies expose great limitations in the face of these requirements, especially do not well consider distinct characteristics of heterogeneous storage nodes yet, which will lead to suboptimal performance. In this paper, we present and evaluate the RLRP, a deep reinforcement learning (RL) based replica placement strategy. RLRP constructs placement and migration agents through the Deep-Q-Network (DQN) model to achieve fair distribution and adaptive data migration. Besides, RLRP provides optimal performance for heterogeneous environment by an attentional Long Short-term Memory (LSTM) model. Finally, RLRP adopts Stagewise Training and Model fine-tuning to accelerate the training of RL models with large-scale state and action space. RLRP is implemented on Park and the evaluation results indicate RLRP is a highly efficient data placement strategy for modern distributed storage systems. RLRP can reduce read latency by 10%∼50% in heterogeneous environment compared with existing strategies. In addition, RLRP is used in the real-world system Ceph, which improves the read performance of Ceph by 30%∼40%.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00064","National Natural Science Foundation of China(grant numbers:62072196,61821003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820675","Distributed Storage;Data Placement;Reinforcement Learning;Hashing;Heterogeneous Environment","Training;Performance evaluation;Adaptation models;Distributed processing;Distributed databases;Reinforcement learning;Data models","data handling;deep learning (artificial intelligence);distributed processing;recurrent neural nets;reinforcement learning;storage management","RLRP;high-efficient data placement;modern distributed storage systems;storage devices;heterogeneous storage architecture;migration agents;Deep-Q-Network model;adaptive data migration;heterogeneous environment;deep reinforcement learning based replica placement;attentional long short-term memory model;DQN model;LSTM model;Ceph;stagewise training;model fine-tuning","","","",30.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning","S. Singh; A. Bhatele","Department of Computer Science, University of Maryland, College Park, Maryland, USA; Department of Computer Science, University of Maryland, College Park, Maryland, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","606","616","In the last few years, the memory requirements to train state-of-the-art neural networks have far exceeded the DRAM capacities of modern hardware accelerators. This has necessitated the development of efficient algorithms to train these neural networks in parallel on large-scale GPU-based clusters. Since computation is relatively inexpensive on modern GPUs, designing and implementing extremely efficient communication in these parallel training algorithms is critical for extracting the maximum performance. This paper presents AxoNN, a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU, thereby reducing GPU idle time and maximizing hardware efficiency. By using the CPU memory as a scratch space for offloading data periodically during training, AxoNN is able to reduce GPU memory consumption by four times. This allows us to increase the number of parameters per GPU by four times, thus reducing the amount of communication and increasing performance by over 13%. When tested against large transformer models with 12–100 billion parameters on 48–384 NVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4–54.78% of theoretical peak and reduces the training time by 22-37 days (15–25% speedup) as compared to the state-of-the-art.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00065","U.S. Department of Energy(grant numbers:DE-AC05-000R22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820664","parallel deep learning;asynchrony;message driven scheduling;memory optimizations","Training;Deep learning;Schedules;Neural networks;Memory management;Graphics processing units;Clustering algorithms","convolutional neural nets;coprocessors;deep learning (artificial intelligence);graphics processing units;parallel processing;scheduling","AxoNN;asynchronous message-driven parallel framework;extreme-scale deep learning;memory requirements;DRAM capacities;large-scale GPU-based clusters;parallel training algorithms;parallel deep learning framework;message-driven execution;neural network operations;GPU idle time;hardware efficiency;CPU memory;GPU memory consumption;Tesla V100 GPU;time 22.0 d to 37.0 d","","","",30.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Fast Parallel Bayesian Network Structure Learning","J. Jiang; Z. Wen; A. Mian","Department of Computer Science and Software Engineering, The University of Western Australia; Department of Computer Science and Software Engineering, The University of Western Australia; Department of Computer Science and Software Engineering, The University of Western Australia","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","617","627","Bayesian networks (BNs) are a widely used graphical model in machine learning for representing knowledge with uncertainty. The mainstream BN structure learning methods require performing a large number of conditional independence (CI) tests. The learning process is very time-consuming, especially for high-dimensional problems, which hinders the adoption of BNs to more applications. Existing works attempt to accelerate the learning process with parallelism, but face issues including load unbalancing, costly atomic operations and dominant parallel overhead. In this paper, we propose a fast solution named Fast-BNS on multi-core CPUs to enhance the efficiency of the BN structure learning. Fast-Bns is powered by a series of efficiency optimizations including (i) designing a dynamic work pool to monitor the processing of edges and to better schedule the workloads among threads, (ii) grouping the CI tests of the edges with the same endpoints to reduce the number of unnecessary CI tests, (iii) using a cache-friendly data storage to improve the memory efficiency, and (iv) generating the conditioning sets on-the-fly to avoid extra memory consumption. A comprehensive experimental study shows that the sequential version of Fast-BNS is up to 50 times faster than its counterpart, and the parallel version of Fast-Bns achieves 4.8 to 24.5 times speedup over the state-of-the-art multi-threaded solution. Moreover, Fast-BNS has a good scalability to the network size as well as sample size.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00066","Australian Research Council Future Fellowship Award(grant numbers:FT210100268); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820657","","Schedules;Uncertainty;Instruction sets;Scalability;Memory management;Parallel processing;Dynamic scheduling","belief networks;learning (artificial intelligence);multiprocessing systems;multi-threading;optimisation;parallel processing","machine learning;mainstream BN structure learning methods;conditional independence tests;learning process;dominant parallel overhead;unnecessary CI tests;parallel version;Fast-Bns achieves;Fast parallel bayesian network structure learning;bayesian networks;widely used graphical model","","","",43.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Adaptive Verifiable Coded Computing: Towards Fast, Secure and Private Distributed Machine Learning","T. Tang; R. E. Ali; H. Hashemi; T. Gangwani; S. Avestimehr; M. Annavaram","Computer Science Department, University of Southern California, Los Angeles, USA; Electrical Engineering Department, University of Southern California, Los Angeles, USA; Electrical Engineering Department, University of Southern California, Los Angeles, USA; Electrical Engineering Department, University of Southern California, Los Angeles, USA; Electrical Engineering Department, University of Southern California, Los Angeles, USA; Electrical Engineering Department, University of Southern California, Los Angeles, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","628","638","Stragglers, Byzantine workers, and data privacy are the main bottlenecks in distributed cloud computing. Some prior works proposed coded computing strategies to jointly address all three challenges. They require either a large number of workers, a significant communication cost or a significant computational complexity to tolerate Byzantine workers. Much of the overhead in prior schemes comes from the fact that they tightly couple coding for all three problems into a single framework. In this paper, we propose Adaptive Verifiable Coded Computing (AVCC) framework that decouples the Byzantine node detection challenge from the straggler tolerance. AVCC leverages coded computing just for handling stragglers and privacy, and then uses an orthogonal approach that leverages verifiable computing to mitigate Byzantine workers. Furthermore, AVCC dynamically adapts its coding scheme to trade-off straggler tolerance with Byzantine protection. We evaluate AVCC on a compute-intensive distributed logistic regression application. Our experiments show that AVCC achieves up to 4.2× speedup and up to 5.1% accuracy improvement over the state-of-the-art Lagrange coded computing approach (LCC). AVCC also speeds up the conventional uncoded implementation of distributed logistic regression by up to 7.6×, and improves the test accuracy by up to 12.1%.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00067","Defense Advanced Research Projects Agency (DARPA)(grant numbers:HR001117C0053,FA8750-19-2-1005); ARO(grant numbers:W911NF1810400); NSF(grant numbers:CCF-1703575,CCF-1763673,MLWINS-2002874); ONR(grant numbers:N00014-16-1-2189); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820626","coded computing;verifiable computing;machine learning;straggler mitigation;Byzantine robustness;privacy","Deep learning;Privacy;Distributed processing;Costs;Neural networks;Encoding;Hardware","cloud computing;computational complexity;data privacy;formal verification;learning (artificial intelligence);regression analysis;security of data","private distributed machine learning;data privacy;distributed cloud computing;communication cost;computational complexity;straggler tolerance;AVCC;Byzantine protection;adaptive verifiable coded computing;Byzantine node detection;compute-intensive distributed logistic regression;Lagrange coded computing;fast distributed machine learning;secure distributed machine learning","",1.0,"",41.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"pFedGF: Enabling Personalized Federated Learning via Gradient Fusion","X. Wu; J. Niu; X. Liu; T. Ren; Z. Huang; Z. Li","School of Computer Science, Beihang University, Beijing, China; School of Computer Science, Beihang University, Beijing, China; School of Computer Science, Beihang University, Beijing, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China; College of Computer Science, Xiangtan University, Hunan, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","639","649","Data heterogeneity is one of the main challenges faced by federated learning (FL). Unlike traditional FL methods (e.g. FedAvg) which train a global model for all clients, personalized federated learning (PFL) can address the above problem by training a personalized model for each client. Current mainstream PFL researches first obtain a global model through collaborative training among all clients and then fine-tune the global model on each client's local data to obtain personalized models. However, this two-staged approach has a drawback: when the heterogeneity of different clients is large, the obtained final global model can deviate from the distributions of all clients, and therefore is not a good starting point for updating personalized models. In this paper, we propose pFedGF, a new PFL method based on gradient fusion. Different from traditional two-staged PFL, in each round of pFedGF, each client maintains two gradients simultaneously, a global gradient to capture information from all clients, and a local gradient that reflects the specific distribution of each client. The two gradients are fused to obtain the updated direction of the personalized model for each client. We carried out experiments on MNIST, FMNIST, and CIFAR-10 datasets. The results demonstrate that in the presence of data heterogeneity, pFedGF outperforms other PFL methods.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00068","National Natural Science Foundation of China(grant numbers:61976012,62032020); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY22F020006); National Key Research and Development Program of China(grant numbers:2021YFB3101200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820618","Non-IID;Personalized Federated Learning;Gradient Fusion","Training;Distributed processing;Fuses;Collaboration;Collaborative work;Data models","gradient methods;learning (artificial intelligence);sensor fusion","data heterogeneity;pFedGF;PFL;gradient fusion;personalized federated learning;global gradient;local gradient","",1.0,"",37.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"An Efficient Vectorization Scheme for Stencil Computation","K. Li; L. Yuan; Y. Zhang; Y. Yue; H. Cao","School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Institute of Computing Technology, Beijing; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Institute of Computing Technology, Beijing; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","650","660","Stencil computation is one of the most important kernels in various scientific and engineering applications. A variety of work has focused on vectorization and tiling techniques, aiming at exploiting the in-core data parallelism and data locality respectively. In this paper, the downsides of existing vectorization schemes are analyzed. Briefly, they either incur data alignment conflicts or hurt the data locality when integrated with tiling. Then we propose a novel transpose layout to preserve the data locality for tiling and reduce the data reorganization overhead for vectorization simultaneously. To further improve the data reuse at the register level, a time loop unroll-and-jam strategy is designed to perform multistep stencil computation along the time dimension. Experimental results on the AVX2 and AVX-S12 CPUs show that our approach obtains a competitive performance with the classic vectorization methods (Auto Vectorization and Data Reorganization), state-of-the-art compilers (Pluto and SDSL), and highly-optimized work (DLT and Tessellation).","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00069","National Natural Science Foundation of China(grant numbers:61972376,62072431,62032023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820707","Stencil;Vectorization;Data locality;Data alignment conflict","Concurrent computing;Distributed processing;Layout;Parallel processing;Registers;Computational efficiency;Kernel","jamming;optimisation;optimising compilers;parallel architectures;parallel processing;program compilers;program control structures","efficient Vectorization scheme;important kernels;scientific engineering applications;tiling techniques;in-core data parallelism;data locality;vectorization schemes;data alignment conflicts;data reorganization overhead;data reuse;multistep stencil computation;classic vectorization methods;Auto Vectorization","","","",39.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Scheduling on Uniform and Unrelated Machines with Bipartite Incompatibility Graphs","T. Pikies; H. Furmaǹczyk","Dept. of Algorithims and System Modelling, Faculty of Electronics, Telecommunications and Informatics, Gdansk University of Technology, Gdansk, Poland; Faculty of Mathematics, Physics and Informatics, Institute of Informatics, University of Gdansk, Gdansk, Poland","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","661","671","The problem of scheduling jobs on parallel machines under an incompatibility relation is considered in this paper. In this model, a binary relation between jobs is given and no two jobs that are in the relation can be scheduled on the same machine. We consider job scheduling under the incompatibility relation modeled by a bipartite graph, under the makespan optimality criterion, on uniform and unrelated machines. Unrelated machines are considered first. An FPTAS for $R2\vert G=bipartite\vert C_{\max}$ is provided. We also show that for any $\epsilon > 0, b > 0$ and $m\geq 3$, there is no polynomial-time algorithm of approximation ratio $\mathrm{O}(n^{b}p_{\max}^{1-\epsilon})$ for $Rm\vert G$ = bipartite $\vert C_{\max}$, unless P = NP. Uniform machines are considered as second. For any $\epsilon > 0$, we show that under P = NP assumption there is no polynomial-time $\mathrm{O}(n^{1/2-\epsilon}$)-approximation algorithm, even in the case of unit time jobs. We also provide a polynomial-time $\sqrt{\Sigma p_{j}}$ -approximation algorithm for the case of jobs of arbitrary lengths $p_{j}$, matching the established bound. To enrich the analysis, bipartite graphs generated randomly according to Gilbert's model $\mathbb{G}_{n,n,p(n)}$ are considered. We show that there exists an algorithm producing a schedule with makespan almost surely at most twice the optimum for a broad class of $p(n)$ functions. To the best of our knowledge, this is the first study of randomly generated graphs in the context of scheduling in the considered model.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820745","bipartite graphs;scheduling;uniform machines;unrelated machines;incompatibility graph;random bipartite graphs","Schedules;Distributed processing;Analytical models;Parallel machines;Approximation algorithms;Bipartite graph;Context modeling","approximation theory;computational complexity;graph theory;parallel machines;scheduling","polynomial-time algorithm;unit time jobs;randomly generated graphs;unrelated machines;bipartite incompatibility graphs;parallel machines;incompatibility relation;binary relation;job scheduling;makespan optimality criterion;uniform machine scheduling;approximation ratio;NP assumption","","","",35.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"SPIDER: An Effective, Efficient and Robust Load Scheduler for Real-time Split Frame Rendering","B. Ma; Z. Zhang; Y. Li; W. Cai; G. Wang; X. Liu","School of Computer Science, Nankai University, Tianjin, China; School of Computer Science, Nankai University, Tianjin, China; School of Computer Science, Nankai University, Tianjin, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science, Nankai University, Tianjin, China; School of Computer Science, Nankai University, Tianjin, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","672","682","Interactive graphics applications are generally latency-critical, while using multiple GPUs to accelerate such applications becomes possible recently with support from both hardware and software. Split frame rendering (SFR) is a popular approach for multi-GPU rendering, which splits a frame into disjoint regions and assigns the regions to different GPUs. Load scheduling of SFR is a crucial but challenging issue for achieving maximum rendering performance in real-time rendering, which is not well addressed by the existing solutions. In this paper, we propose SPIDER, a load scheduler which leverages fuzzy PID (proportional integral derivative) controller to schedule the rendering workload among GPUs. SPIDER has several distinguished properties: it is a feedback based mechanism which does not need a full knowledge of the dynamic system; it is very computationally efficient and easy to implement; it is highly robust to dynamic workload changes. Extensive experiments are conducted to evaluate SPIDER and the results show that SPIDER always achieves near-optimal performance for various workload patterns, which outperforms the state-of-the-art baselines signif-icantly.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00071","National Science Foundation of China(grant numbers:62141412,61872201); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820715","real-time rendering;multi-GPU rendering;split frame rendering;fuzzy PID controller;load scheduling","Graphics;Schedules;Distributed processing;Rendering (computer graphics);Real-time systems;Software;Hardware","control engineering computing;feedback;fuzzy control;graphics processing units;rendering (computer graphics);three-term control","SPIDER;real-time split frame rendering;interactive graphics applications;multiple GPUs;SFR;multiGPU rendering;disjoint regions;load scheduling;maximum rendering performance;real-time rendering;load scheduler;fuzzy PID controller;proportional integral derivative controller;rendering workload;dynamic workload changes","","","",30.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Bounding the Flow Time in Online Scheduling with Structured Processing Sets","L. -C. Canon; A. Dugois; L. Marchal","FEMTO-ST Institute, Univ. Franche-Comté; LIP, ENS Lyon, Inria; LIP, ENS Lyon, CNRS, Inria","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","683","693","Replication in distributed key-value stores makes scheduling more challenging, as it introduces processing set restrictions, which limits the number of machines that can process a given task. We focus on the online minimization of the maximum response time in such systems, that is, we aim at bounding the latency of each task. When processing sets have no structure, Anand et al. (Algorithmica, 2017) derive a strong lower bound on the competitiveness of the problem: no online scheduling algorithm can have a competitive ratio smaller than $\Omega(m)$, where $m$ is the number of machines. In practice, data replication schemes are regular, and structured processing sets may make the problem easier to solve. We derive new lower bounds for various common structures, including inclusive, nested or interval structures. In particular, we consider fixed sized intervals of machines, which mimic the standard replication strategy of key-value stores. We prove that EFT (Earliest Finish Time) scheduling is ($3-2/k$)-competitive when optimizing max-flow on disjoint intervals of size $k$. However, we show that the competitive ratio of EFT is at least $m-k+1$ when these intervals overlap, even when unit tasks are considered. We compare these two replication strategies in simulations and assess their efficiency when popularity biases are introduced, i.e., when some machines are accessed more frequently than others because they hold popular data. Even though overlapping intervals suffer from a bad worst-case in theory, they enable clusters to reach a maximum load that is up to 50% higher than with disjoint sets.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820638","Flow Time;Lower Bound;Restricted Assignment;Processing Set Restrictions;Replication;Key-Value Stores","Scheduling algorithms;Scalability;Throughput;Minimization;Data models;Scheduling;Servers","distributed processing;fault tolerant computing;minimisation;scheduling;storage management","distributed key-value stores;set restrictions;online minimization;maximum response time;online scheduling;data replication;lower bounds;nested structure;EFT scheduling;earliest finish time scheduling;inclusive structure;flow time;interval structure","","","",30.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Memory-Aware Scheduling of Tasks Sharing Data on Multiple GPUs with Dynamic Runtime Systems","M. Gonthier; L. Marchal; S. Thibault","ENS-Lyon, France; ENS-Lyon, France; University of Bordeaux, France","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","694","704","The use of accelerators such as GPUs has become mainstream to achieve high performance on modern computing systems. GPUs come with their own (limited) memory and are connected to the main memory of the machine through a bus (with limited bandwidth). When a computation is started on a GPU, the corresponding data needs to be transferred to the GPU before the computation starts. Such data movements may become a bottleneck for performance, especially when several GPUs have to share the communication bus. Task-based runtime schedulers have emerged as a convenient and efficient way to use such heterogeneous platforms. When processing an application, the scheduler has the knowledge of all tasks available for processing on a GPU, as well as their input data dependencies. Hence, it is able to choose which task to allocate to which GPU and to reorder tasks so as to minimize data movements. We focus on this problem of partitioning and ordering tasks that share some of their input data. We present a novel dynamic strategy based on data selection to efficiently allocate tasks to GPUs and a custom eviction policy, and compare them to existing strategies using either a well-known graph partitioner or standard scheduling techniques in runtime systems. We also improved an offline scheduler recently proposed for a single GPU, by adding load balancing and task stealing capabilities. All strategies have been implemented on top of the STARPU runtime, and we show that our dynamic strategy achieves better performance when scheduling tasks on multiple GPU s with limited memory.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820704","Memory-aware scheduling;Eviction policy;Tasks sharing data;Runtime systems","Distributed processing;Runtime;Graphics processing units;Distributed databases;Bandwidth;Dynamic scheduling;Load management","graphics processing units;multiprocessing systems;resource allocation;scheduling","single GPU;STARPU runtime;dynamic strategy;scheduling tasks;multiple GPU;memory-aware scheduling;dynamic runtime systems;data needs;data movements;communication bus;task-based runtime schedulers;input data dependencies;ordering tasks;data selection;offline scheduler;load balancing capability;task stealing capability","","","",21.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Accelerating Encrypted Computing on Intel GPUs","Y. Zhai; M. Ibrahim; Y. Qiu; F. Boemer; Z. Chen; A. Titov; A. Lyashevsky","University of California, Riverside, CA, USA; North Carolina State University, Raleigh, NC, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; University of California, Riverside, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","705","716","Homomorphic Encryption (HE) is an emerging encryption scheme that allows computations to be performed directly on encrypted messages. This property provides promising applications such as privacy-preserving deep learning and cloud computing. Prior works have been proposed to enable practical privacy-preserving applications with architectural-aware optimizations on CPUs, CUDA-enabled GPUs and FPGAs. However, there is no systematic optimization for the whole HE pipeline on Intel GPUs. In this paper, we present the first-ever SYCL-based GPU backend for Microsoft SEAL APIs. We perform optimizations from instruction level, algorithmic level and application level to accelerate our HE library based on the Cheon, Kim, Kim and Song (CKKS) scheme on Intel GPUs. The performance is validated on two latest Intel GPUs. Experimental results show that our staged optimizations together with optimizations including low-level optimizations and kernel fusion accelerate the Number Theoretic Transform (NTT), a key algorithm for HE, by up to 9.93X compared with the naive GPU baseline. The roofline analysis confirms that our optimized NTT reaches 79.8% and 85.7% of the peak performance on two GPU devices. Through the highly optimized NTT and the assembly-level optimization, we obtain 2.32X – 3.05X acceleration for HE evaluation routines. In addition, our all-together systematic optimizations improve the performance of encrypted element-wise polynomial matrix multiplication application by up to 3.11X.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820676","Homomorphic Encryption;Number Theoretic Transform;Intel GPU;CKKS;Privacy-Preserving Computing","Performance evaluation;Systematics;Graphics processing units;Seals;Transforms;Trademarks;Libraries","application program interfaces;cloud computing;cryptography;data privacy;deep learning (artificial intelligence);field programmable gate arrays;graphics processing units;matrix multiplication;optimisation;parallel processing","roofline analysis;number theoretic transform algorithm;kernel fusion;CKKS scheme;Cheon, Kim, Kim and Song scheme;application level;architectural-aware optimization;encrypted element-wise polynomial matrix multiplication application;assembly-level optimization;NTT algorithm;low-level optimization;Intel GPU;algorithmic level;instruction level;Microsoft SEAL APIs;SYCL-based GPU backend;systematic optimization;cloud computing;privacy-preserving deep learning;encrypted messages;homomorphic encryption;encrypted computing","",2.0,"",45.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Optimizing Huffman Decoding for Error-Bounded Lossy Compression on GPUs","C. Rivera; S. Di; J. Tian; X. Yu; D. Tao; F. Cappello","Department of Computer Science, University of Alabama, Tuscaloosa, AL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; School of EECS, Washington State University, Pullman, WA, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","717","727","More and more HPC applications require fast and effective compression techniques to handle large volumes of data in storage and transmission. Not only do these applications need to compress the data effectively during simulation, but they also need to perform decompression efficiently for post hoc analysis. SZ is an error-bounded lossy compressor for scientific data, and cuSZ is a version of SZ designed to take advantage of the GPU's power. At present, cuSZ's compression performance has been optimized significantly while its decompression still suffers considerably lower performance because of its sophisticated loss-less compression step-a customized Huffman decoding. In this work, we aim to significantly improve the Huffman decoding performance for cuSZ, thus improving the overall decompression performance in turn. To this end, we first investigate two state-of-the-art GPU Huffman decoders in depth. Then, we propose a deep architectural optimization for both algorithms. Specifically, we take full advantage of CUDA GPU architectures by using shared memory on decoding/writing phases, online tuning the amount of shared memory to use, improving memory access patterns, and reducing warp divergence. Finally, we evaluate our optimized decoders on an Nvidia V100 GPU using eight representative scientific datasets. Our new decoding solution obtains an average speedup of 3.64× over cuSZ's Huffman decoder and improves its overall decompression performance by 2.43× on average.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00075","U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:OAC-2003709,OAC-2034169,OAC-2042084,OAC-2104023,OAC-2104023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820677","Compression;Huffman Coding;Scientific Data Reduction;GPU;CUDA;Performance","Distributed processing;Analytical models;Memory management;Graphics processing units;Distributed databases;Throughput;Data models","data compression;decoding;graphics processing units;Huffman codes;image coding;parallel architectures;parallel processing","deep architectural optimization;state-of-the-art GPU Huffman decoders;Huffman decoding performance;compression step-a customized Huffman decoding;sophisticated loss-less;considerably lower performance;cuSZ's compression performance;GPU's power;scientific data;error-bounded lossy compressor;SZ;post hoc analysis;effective compression techniques;HPC applications;error-bounded lossy compression;decompression performance;cuSZ's Huffman decoder;decoding solution;representative scientific datasets;Nvidia V100 GPU;optimized decoders;memory access patterns;shared memory;CUDA GPU architectures","","","",46.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"SALoBa: Maximizing Data Locality and Workload Balance for Fast Sequence Alignment on GPUs","S. Park; H. Kim; T. Ahmad; N. Ahmed; Z. Al-Ars; H. P. Hofstee; Y. Kim; J. Lee","Yonsei University; Yonsei University; TU Delft; TU Delft; TU Delft; TU Delft; TU Delft; IBM","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","728","738","Sequence alignment forms an important backbone in many sequencing applications. A commonly used strategy for sequence alignment is an approximate string matching with a two-dimensional dynamic programming approach. Although some prior work has been conducted on GPU acceleration of a sequence alignment, we identify several shortcomings that limit exploiting the full computational capability of modern GPUs. This paper presents SALoBa, a GPU-accelerated sequence alignment library focused on seed extension. Based on the analysis of previous work with real-world sequencing data, we propose techniques to exploit the data locality and improve work-load balancing. The experimental results reveal that SALoBa significantly improves the seed extension kernel compared to state-of-the-art GPU-based methods.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00076","National Research Foundation of Korea (NRF)(grant numbers:2022R1C1C1008131,2022R1C1C1011307); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820739","Genome sequencing;Sequence alignment;Smith-Waterman;GPU acceleration","Performance evaluation;Sequential analysis;Distributed processing;Graphics processing units;Genomics;Performance gain;Software","bioinformatics;dynamic programming;graphics processing units;resource allocation;string matching","SALoBa;data locality;workload balance;fast sequence alignment;sequencing applications;two-dimensional dynamic programming approach;GPU-accelerated sequence alignment library;real-world sequencing data;work-load balancing;seed extension kernel","","","",62.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"DGSF: Disaggregated GPUs for Serverless Functions","H. Fingler; Z. Zhu; E. Yoon; Z. Jia; E. Witchel; C. J. Rossbach","Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Department of Computer Science, The University of Texas at Austin; Katana Graph; Katana Graph","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","739","750","Ease of use and transparent access to elastic resources have attracted many applications away from traditional platforms toward serverless functions. Many of these applications, such as machine learning, could benefit significantly from GPU acceleration. Unfortunately, GPUs remain inaccessible from serverless functions in modern production settings. We present DGSF, a platform that transparently enables serverless functions to use GPUs through general purpose APIs such as CUDA. DGSF solves provisioning and utilization challenges with disaggregation, serving the needs of a potentially large number of functions through virtual GPUs backed by a small pool of physical GPUs on dedicated servers. Disaggregation allows the provider to decouple GPU provisioning from other resources, and enables significant benefits through consolidation. We describe how DGSF solves GPU disaggregation challenges including supporting API transparency, hiding the latency of communication with remote GPUs, and load-balancing access to heavily shared GPUs. Evaluation of our prototype on six workloads shows that DGSF's API remoting optimizations can improve the runtime of a function by up to 50% relative to unoptimized DGSF. Such optimizations, which aggressively remove GPU runtime and object management latency from the critical path, can enable functions running over DGSF to have a lower end-to-end time than when running on a GPU natively. By enabling GPU sharing, DGSF can reduce function queueing latency by up to 53%. We use DGSF to augment AWS Lambda with GPU support, showing similar benefits.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00077","NSF(grant numbers:CNS-1846169,CNS-2006943,CNS-2008321,CNS-1900457); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820659","cloud computing;serverless;FaaS;GPU;API remoting","Distributed processing;Cloud computing;Runtime;Graphics processing units;Prototypes;Production;Machine learning","application program interfaces;cloud computing;graphics processing units;resource allocation;virtualisation","GPU disaggregation;API transparency;remote GPU;serverless functions;virtual GPU;physical GPU;transparent elastic resource access;DGSF API remoting optimization;GPU provisioning;general purpose API;CUDA;load-balancing;object management latency;GPU runtime latency;function queueing latency;AWS Lambda;cloud computing","","","",62.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Compiler-Directed Incremental Checkpointing for Low Latency GPU Preemption","Z. Ji; C. -L. Wang","Department of Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","751","761","GPUs are widely used in data centers to accelerate data-parallel applications. The multiuser and multitasking environment provides a strong incentive for preemptive GPU multitasking, especially for latency-sensitive jobs. Due to the large contexts of GPU kernels, preemptive GPU context switching is costly. Many novel GPU preemption techniques are proposed. Among them, checkpoint-based GPU preemption enables low latency GPU preemption but incurs a high runtime overhead. Prior studies propose to exclude dead registers from the checkpoint file to reduce the runtime overhead. It works well for CPUs, but it is not rare that a live register is not updated between two checkpoints for GPU kernels. This paper presents TripleC, a compiler-directed incremental checkpointing technique specially designed for GPU preemption. It further excludes the registers, which have not been overwritten since the last time they were spilled, from the checkpoint file with data flow analysis. The checkpoint placement algorithm of TripleC can properly estimate a checkpoint's cost under incremental checkpointing. It also considers the interaction among checkpoints so that the overall cost is minimized. Moreover, TripleC relaxes the conventional checkpointing constraint that the whole register context must be spilled before passing the checkpoint. Because of the diverse control flow, placing a register spilling instruction at different points incurs different costs. TripleC minimizes the cost with a two-phase algorithm that schedules these register spilling instructions at compilation time. Evaluations show that TripleC reduces the runtime overhead by 12.9 % on average compared with the state-of-the-art non-incremental checkpointing approach.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820705","GPU preemption;GPU multitasking;Incremental checkpointing;Compiler","Checkpointing;Schedules;Costs;Runtime;Graphics processing units;Switches;Multitasking","checkpointing;computer centres;data flow analysis;graphics processing units;parallel processing;program compilers","data-parallel applications;multitasking environment;preemptive GPU multitasking;latency-sensitive jobs;GPU kernels;preemptive GPU context switching;checkpoint-based GPU preemption;low latency GPU preemption;dead registers;checkpoint file;runtime overhead;TripleC;data flow analysis;checkpoint placement algorithm;register spilling instruction;data centers;compiler-directed incremental checkpointing;checkpointing constraint;nonincremental checkpointing;multiuser environment;checkpoint cost estimation;control flow","","","",37.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"ParaTreeT: A Fast, General Framework for Spatial Tree Traversal","J. Hutter; J. Szaday; J. Choi; S. Liu; L. Kale; S. Wallace; T. Quinn","Department of Computer Science, University of Illinois, Champaign, IL; Department of Computer Science, University of Illinois, Champaign, IL; Department of Computer Science, University of Illinois, Champaign, IL; Department of Computer Science, University of Illinois, Champaign, IL; Department of Computer Science, University of Illinois, Champaign, IL; Department of Astronomy, University of Washington, Seattle, WA; Department of Astronomy, University of Washington, Seattle, WA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","762","772","Tree-based algorithms for spatial domain applications scale poorly in the distributed setting without extensive experimentation and optimization. Reusability via well-designed parallel abstractions supported by efficient parallel algorithms is therefore desirable. We present ParaTreeT, a parallel tree toolkit for state-of-the-art performance and programmer productivity. ParaTreeT leverages a novel shared-memory software cache to reduce communication volume and idle time throughout traversal. By dividing particles and subtrees across processors independently, it improves decomposition and limits synchro-nization during tree build. Tree-node states are extracted from the particle set with the Data abstraction, and traversal work and pruning are defined by the Visitor abstraction. ParaTreeT provides built-in trees, decompositions, and traversals that offer application-specific customization. We demonstrate ParaTreeT's improved computational performance over even specialized codes with multiple applications on CPUs. We evaluate how several applications derive benefit from ParaTreeT's models while pro-viding new insights to these workloads through experimentation.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00079","National Science Foundation(grant numbers:ACI-1548562,1906892,1910428); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820692","N-body simulation;Tree traversals;Shared-memory models","Productivity;Multicore processing;Scalability;Software algorithms;Standardization;Software;Usability","cache storage;data structures;multiprocessing systems;parallel algorithms;shared memory systems;tree data structures;trees (mathematics)","shared-memory software cache;communication volume;dividing particles;decomposition;limits synchro-nization;tree-node states;Data abstraction;traversal work;pruning;Visitor abstraction;trees;application-specific customization;ParaTreeT's improved computational performance;applications derive benefit;ParaTreeT's models;spatial tree traversal;tree-based algorithms;spatial domain applications scale;distributed setting;extensive experimentation;well-designed parallel abstractions;efficient parallel algorithms;parallel tree toolkit;programmer productivity;ParaTreeT leverages","","","",33.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"An Integral-equation-oriented Vectorized SpMV Algorithm and its Application on CT Imaging Reconstruction","W. Ye; C. Huang; J. Huang; J. Li; Y. Lu; Y. Jiang","Guangdong Province Key Laboratory of Computational Science, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Province Key Laboratory of Computational Science, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Province Key Laboratory of Computational Science, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Province Key Laboratory of Computational Science, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Province Key Laboratory of Computational Science, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Guangdong Province Key Laboratory of Computational Science, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","773","783","Sparse-matrix vector multiplication (SpMV) is a core routine in many applications. Its performance is limited by memory bandwidth, which is for matrix transport between processors and memory, and instruction latency in computations. Vectorized operations (SIMD) can dramatically improve the execution efficiency, but irregular matrices' sparsity pattern is not compatible with the style of SIMD execution. We present a new matrix format, Compressed Sparse Column Vector (CSCV), and a corresponding vectorized SpMV algorithm for matrices arising from integral equations. This SpMV algorithm can inherently suit wide SIMD instructions and reduce the memory bandwidth used. We implement this algorithm for Computed Tomography (CT) imaging reconstructions on both Intel and AMD x86 platforms and compare it with seven state-of-the-art SpMV implementations using different CT imaging matrices. Experimental results show that CSCV can achieve up to 96.9 GFLOP/s in single-precision tests, with speedup 3.70× to MKL and 3.48× to the second place implementation. Furthermore, the implementation of CSCV SpMV is performance portable, which excludes almost all SIMD assemble code and has promising performance with compiler-assisted vectorization. Code Availability: https://github.com/sysu-compsci/cscv","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00080","NSFC(grant numbers:12126610,81971691); Sun Yat-sen University(grant numbers:2020B1212060032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820644","parallel SpMV;integral-operator-oriented vectorization;CT imaging reconstruction","Distributed processing;Codes;Program processors;Computed tomography;Integral equations;Imaging;Bandwidth","computerised tomography;image reconstruction;integral equations;matrix multiplication;microprocessor chips;parallel architectures;sparse matrices","memory bandwidth;matrix transport;vectorized operations;SIMD execution;matrix format;integral equations;SIMD instructions;computed tomography;SpMV implementation;CSCV SpMV;SIMD assemble code;compiler-assisted vectorization;integral-equation-oriented vectorized SpMV algorithm;CT imaging reconstruction;sparse-matrix vector multiplication;CT imaging matrices;vectorized SpMV algorithm;compressed sparse column vector;Intel platform;AMD x86 platform","","","",17.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"High-order Line Graphs of Non-uniform Hypergraphs: Algorithms, Applications, and Experimental Analysis","X. T. Liu; J. Firoz; S. Aksoy; I. Amburg; A. Lumsdaine; C. Joslyn; B. Praggastis; A. H. Gebremedhin","Washington State University, USA; Pacific Northwest National Lab, USA; Pacific Northwest National Lab, USA; Pacific Northwest National Lab, USA; Pacific Northwest National Lab, USA; Pacific Northwest National Lab, USA; Pacific Northwest National Lab, USA; Washington State University, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","784","794","Hypergraphs offer flexible and robust data representations for many applications, but methods that work directly on hypergraphs are not readily available and tend to be prohibitively expensive. Much of the current analysis of hypergraphs relies on first performing a graph expansion – either based on the nodes (clique expansion), or on the hyperedges (line graph) − and then running standard graph analytics on the resulting representative graph. However, this approach suffers from massive space complexity and high computational cost with increasing hypergraph size. Here, we present efficient, parallel algorithms to accelerate and reduce the memory footprint of higher-order graph expansions of hypergraphs. Our results focus on the hyperedge-based s-line graph expansion, but the methods we develop work for higher-order clique expansions as well. To the best of our knowledge, ours is the first framework to enable hypergraph spectral analysis of a large dataset on a single shared-memory machine. Our methods enable the analysis of datasets from many domains that previous graph-expansion-based models are unable to provide. The proposed s-line graph computation algorithms are orders of magnitude faster than state-of-the-art sparse general matrix-matrix multiplication methods, and obtain approximately 2–31× speedup over a prior state-of-the-art heuristic-based algorithm for $s$-line graph computation.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00081","Department of Energy's; Pacific Northwest National Laboratory; NSF(grant numbers:IIS-1553528,SI2-SSE 1716828); Battelle Memorial Institute(grant numbers:DE-ACO6-76RL01830); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820632","Hypergraphs;parallel hypergraph algorithms;line graphs;intersection graphs;clique expansion","Distributed processing;Distribution strategy;Heuristic algorithms;Memory management;Approximation algorithms;Sparse matrices;Parallel algorithms","computational complexity;graph theory;matrix multiplication;parallel algorithms;spectral analysis","graph-expansion-based models;sparse general matrix-matrix multiplication methods;standard graph analytics;clique expansion;nodes;current analysis;robust data representations;nonuniform hypergraphs;high-order line graphs;$s$-line graph computation;s-line graph computation algorithms;single shared-memory machine;hypergraph spectral analysis;higher-order clique expansions;hyperedge-based s-line graph expansion;higher-order graph expansions;parallel algorithms;hypergraph size;high computational cost;massive space complexity;representative graph","","","",38.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Topological Modeling and Parallelization of Multidimensional Data on Microelectrode Arrays","O. T. Tawose; B. Li; L. Yang; F. Yan; D. Zhao","Department of Computer Science and Engineering, University of Nevada, Reno, NV, United States of America; Department of Computer Science and Engineering, University of Nevada, Reno, NV, United States of America; Department of Computer Science and Engineering, University of Nevada, Reno, NV, United States of America; Department of Computer Science and Engineering, University of Nevada, Reno, NV, United States of America; Department of Computer Science and Engineering, University of Nevada, Reno, NV, United States of America","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","795","805","Microelectrode arrays (MEAs) are physical devices widely used in various science and engineering fields. One common computational challenge when applying a high-density MEA (i.e., a larger number of wires, more accurate locations of abnormal cells) is how to efficiently compute those resistance values provided the nonlinearity of the system of equations with the unknown resistance values per the Kirchhoff law. This paper proposes an algebraic-topological model for MEAs such that we can identify the intrinsic parallelism that cannot be identified by conventional approaches. We implement a system prototype called Parma based on the proposed topological methodology. Experimental results show that Parma outperforms the state-of-the-practice in time, scalability and memory usage: the computation time is two orders of magnitude faster on up to 1,024 cores with almost linear scalability and the memory is much better utilized with proportionally less warm-up time with respect to the number of concurrent threads.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820641","multidimensional data;parallel processing;scientific computing;applied topology","Resistance;Microelectrodes;Scalability;Computational modeling;Memory management;Wires;Prototypes","biomedical electrodes;microelectrodes","multidimensional data;microelectrode arrays;MEAs;physical devices;high-density MEA;abnormal cells;resistance values;Kirchhoff law;algebraic-topological model;intrinsic parallelism;topological methodology","","","",32.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Coupling streaming AI and HPC ensembles to achieve 100–1000× faster biomolecular simulations","A. Brace; I. Yakushin; H. Ma; A. Trifan; T. Munson; I. Foster; A. Ramanathan; H. Lee; M. Turilli; S. Jha","Department of Computer Science, University of Chicago, Chicago, USA; Data Science and Learning Division, Argonne National Laboratory, Lemont, USA; Data Science and Learning Division, Argonne National Laboratory, Lemont, USA; Center For Biophysics, University of Illinois Urbana-Champaign, Champaign, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, USA; Department of Computer Science, University of Chicago, Chicago, USA; Data Science and Learning Division, Argonne National Laboratory, Lemont, USA; RADICAL-Lab, Rutgers University, New Brunswick, USA; Computational Science Initiative, Brookhaven National Laboratory, Upton, USA; Computational Science Initiative, Brookhaven National Laboratory, Upton, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","806","816","Machine learning (ML)-based steering can improve the performance of ensemble-based simulations by allowing for online selection of more scientifically meaningful computations. We present DeepDriveMD, a framework for ML-driven steering of scientific simulations that we have used to achieve orders-of-magnitude improvements in molecular dynamics (MD) performance via effective coupling of ML and HPC on large parallel computers. We discuss the design of DeepDriveMD and characterize its performance. We demonstrate that DeepDriveMD can achieve between 100-1000× acceleration for protein folding simulations relative to other methods, as measured by the amount of simulated time performed, while covering the same conformational landscape as quantified by the states sampled during a simulation. Experiments are performed on leadership-class platforms on up to 1020 nodes. The results establish DeepDriveMD as a high-performance framework for ML-driven HPC simulation scenarios, that supports diverse MD simulation and ML back-ends, and which enables new scientific insights by improving the length and time scales accessible with current computing capacity.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00083","U.S. Department of Energy(grant numbers:17-SC-20-SC,DE-AC05-000R22725,DE-AC02-06CH11357,31975.2,DE-SC0019323); Lawrence Livermore National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820679","deep learning;molecular biophysics;adaptive simulations;protein folding;streaming data analytics;workflows","Proteins;Couplings;Computers;Accelerometers;Solid modeling;Distributed processing;Computational modeling","biology computing;computer simulation;deep learning (artificial intelligence);molecular biophysics;molecular dynamics method;parallel processing;proteins","computing capacity;biomolecular simulations;machine learning-based steering;ensemble-based simulations;online selection;DeepDriveMD;ML-driven steering;scientific simulations;orders-of-magnitude improvements;molecular dynamics performance;parallel computers;protein folding simulations;high-performance framework;ML-driven HPC simulation;AI;HPC ensembles;conformational landscape;leadership-class platforms","",1.0,"",62.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Neon: A Multi-GPU Programming Model for Grid-based Computations","M. Meneghin; A. H. Mahmoud; P. K. Jayaraman; N. J. W. Morris","Autodesk Research, Canada; University of California, Davis; Autodesk Research, Canada; Autodesk Research, Canada","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","817","827","We present Neon, a new programming model for grid-based computation with an intuitive, easy-to-use interface that allows domain experts to take full advantage of single-node multi-GPU systems. Neon decouples data structure from computation and back end configurations, allowing the same user code to operate on a variety of data structures and devices. Neon relies on a set of hierarchical abstractions that allow the user to write their applications as if they were sequential applications, while the runtime handles distribution across multiple GPUs and performs optimizations such as overlapping computation and communication without user intervention. We evaluate our programming model on several applications: a Lattice Boltzmann fluid solver, a finite-difference Poisson solver and a finite-element linear elastic solver. We show that these applications can be implemented concisely and scale well with the number of GPUs-achieving more than 99% of ideal efficiency.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820681","","Performance evaluation;Neon;Runtime;Fluids;Computational modeling;Programming;Data structures","graphics processing units;grid computing;multiprocessing systems;object-oriented programming;user interfaces","single-node multiGPU systems;data structure;user code;data structures;user intervention;finite-difference Poisson solver;finite-element linear elastic solver;multiGPU programming model;grid-based computation;Lattice Boltzmann fluid solver;Neon model","","","",23.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"OmpSs@cloudFPGA: An FPGA Task-Based Programming Model with Message Passing","J. M. de Haro; R. Cano; C. Álvarez; D. Jiménez-González; X. Martorell; E. Ayguadé; J. Labarta; F. Abel; B. Ringlein; B. Weiss","Universitat Politècnica de Catalunya; Barcelona Supercomputing Center; Universitat Politècnica de Catalunya; Universitat Politècnica de Catalunya; Universitat Politècnica de Catalunya; Universitat Politècnica de Catalunya; Universitat Politècnica de Catalunya; IBM Research Europe; IBM Research Europe; IBM Research Europe","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","828","838","Nowadays, a new parallel paradigm for energy-efficient heterogeneous hardware infrastructures is required to achieve better performance at a reasonable cost on high-performance computing applications. Under this new paradigm, some application parts are offloaded to specialized accelerators that run faster or are more energy-efficient than CPUs. Field-Programmable Gate Arrays (FPGA) are one of those types of accelerators that are becoming widely available in data centers. This paper proposes OmpSs@cloudFPGA, which includes novel extensions to parallel task-based programming models that enable easy and efficient programming of heterogeneous clusters with FPGAs. The programmer only needs to annotate, with OpenMP-like pragmas, the tasks of the application that should be accelerated in the cluster of FPGAs. Next, the proposed programming model framework automatically extracts parts annotated with High-Level Synthesis (HLS) pragmas and synthesizes them into hardware accelerator cores for FPGAs. Additionally, our extensions include and support two novel features: 1) FPGA-to-FPGA direct communication using a Message Passing Interface (MPI) similar Application Programming Interface (API) with one-to-one and collective communications to alleviate host communication channel bottleneck, and 2) creating and spawning work from inside the FPGAs to their own accelerator cores based on an MPI rank-like identification. These features break the classical host-accelerator model, where the host (typically the CPU) generates all the work and distributes it to each accelerator. We also present an evaluation of OmpSs@cloudFPGA for different parallel strategies of the N-Body application on the IBM cloudFPGA research platform. Results show that for cluster sizes up to 56 FPGAs, the performance scales linearly. To the best of our knowledge, this is the best performance obtained for N-body over FPGA platforms, reaching 344 Gpairs/s with 56 FPGAs. Finally, we compare the performance and power consumption of the proposed approach with the ones obtained by a classical execution on the MareNostrum 4 supercomputer, demonstrating that our FPGA approach reduces power consumption by an order of magnitude.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00085","Generalitat de Catalunya(grant numbers:2017-SGR-1414,2017-SGR-1328); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820636","FPGA;MPI;OpenMP;programming models;network-attached FPGA;stand-alone FPGA;High-Level Synthesis;heterogeneous programming;High-performance computing","Power demand;Costs;Message passing;Scalability;Programming;Supercomputers;Energy efficiency","application program interfaces;field programmable gate arrays;message passing;multiprocessing systems;parallel machines","hardware accelerator cores;extensions include;FPGA-to-FPGA direct communication;Message Passing Interface similar Application Programming Interface;collective communications;host communication channel bottleneck;classical host-accelerator model;OmpSs@cloudFPGA;different parallel strategies;N-Body application;IBM cloudFPGA research platform;56 FPGAs;performance scales;FPGA platforms;FPGA approach;FPGA task-based Programming model;parallel paradigm;energy-efficient heterogeneous hardware infrastructures;reasonable cost;high-performance computing applications;application parts;specialized accelerators;Field-Programmable Gate Arrays;task-based programming models;efficient programming;heterogeneous clusters;OpenMP-like pragmas;programming model framework;High-Level Synthesis pragmas","","","",20.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Generalized Flow-Graph Programming Using Template Task-Graphs: Initial Implementation and Assessment","J. Schuchart; P. Nookala; M. M. Javanmard; T. Herault; E. F. Valeev; G. Bosilca; R. J. Harrison","Innovative Computing Laboratory, The University of Tennessee, Knoxville, TN, USA; Institute for Advanced Computational Science, Stony Brook University, Stony Brook, NY, USA; Meta Platforms, Inc, New York, NY, USA; Innovative Computing Laboratory, The University of Tennessee, Knoxville, TN, USA; Department of Chemistry, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA; Innovative Computing Laboratory, The University of Tennessee, Knoxville, TN, USA; Institute for Advanced Computational Science, Stony Brook University, Stony Brook, NY, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","839","849","We present and evaluate TTG, a novel programming model and its C++ implementation that by marrying the ideas of control and data flowgraph programming supports compact specification and efficient distributed execution of dynamic and irregular applications. Programming interfaces that support task-based execution often only support shared memory parallel environments; a few support distributed memory environments, either by discovering the entire DAG of tasks on all processes, or by introducing explicit communications. The first approach limits scalability, while the second increases the complexity of programming. We demonstrate how TTG can address these issues without sacrificing scalability or programmability by providing higher-level abstractions than conventionally provided by task-centric programming systems, without impeding the ability of these runtimes to manage task creation and execution as well as data and resource management efficiently. TTG supports distributed memory execution over 2 different task runtimes, PaRSEC and MADNESS. Performance of four paradigmatic applications (in graph analytics, dense and block-sparse linear algebra, and numerical integrodifferential calculus) with various degrees of irregularity implemented in TTG is illustrated on large distributed-memory platforms and compared to the state-of-the-art implementations.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00086","NSF(grant numbers:1450300,1450344,1450262); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820613","Hybrid programming techniques in applications","Runtime;Processor scheduling;Scalability;Distributed databases;C++ languages;Programming;Data models","data flow computing;data flow graphs;data handling;distributed memory systems;formal specification;parallel programming;resource allocation;shared memory systems","generalized flow-graph programming;template task-graphs;TTG;compact specification;dynamic applications;irregular applications;programming interfaces;task-based execution;distributed memory environment;explicit communications;higher-level abstractions;task creation;task runtime;paradigmatic applications;graph analytics;distributed-memory platforms;task-centric programming system;distributed memory execution;data flowgraph programming;shared memory parallel environment;resource management;data management;PaRSEC;MADNESS;block-sparse linear algebra;numerical integrodifferential calculus","",2.0,"",40.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"PINT: Parallel INTerval-Based Race Detector","Y. Xu; A. Zhou; K. Agrawal; I. -T. A. Lee","Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis; Washington University in St. Louis","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","850","861","A race detector for task-parallel code typically consists of two main components - a reachability analysis component that checks whether two instructions are logically in parallel and an access history component that keeps track of memory locations accessed by previous instructions. Race detectors from prior work typically utilize a hashmap to maintain the access history, which provides asymptotically optimal overhead per operation but can incur significant overhead in practice, since the detector needs to insert into and query the hashmap for every memory access. An exception is STINT by Xu et al., which race detects task-parallel code by coalescing memory accesses into intervals, or continuous memory locations accessed within a sequence of instructions without any parallel construct. STINT utilizes a treap to manage access history that allows for insertions and queries of non-overlapping intervals. While a treap incurs higher asymptotic overhead per operation, this strategy works well in practice as the race detector performs operation on the access history with much lower frequency compared to the strategy that utilizes a hashmap. STINT only executes task-parallel code sequentially, however, due to the unique design of their treap that ensures no overlapping intervals exist in the tree. Parallelizing STINT efficiently is non-trivial, as it would require a concurrent treap that ensures no overlapping interval, which is challenging to design and likely incurs high synchronization overhead. This work proposes PINT, a race detector that, like STINT, race detects task-parallel code at the interval granularity and utilizes the same treap design to maintain access history. PINT executes the computation in parallel, however, while keeping the parallelization / synchronization overhead low. A key insight is that, PINT separates out operations needed for race detection into the core part (e.g., reachability maintenance) and the access history part. Doing so allows PINT to parallelize the core part efficiently and perform the access history part asynchronously, thereby incurring low overhead.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00087","National Science Foundation(grant numbers:CCF-1725647,CCF-1733873,CCF-1910568,CCF-1943456); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820653","shadow memory;determinacy race;access history;task parallelism","Distributed processing;Codes;Detectors;Maintenance engineering;History;Synchronization;Task analysis","concurrency control;file organisation;parallel programming;program debugging;reachability analysis;synchronisation","PINT;parallel INTerval-based race detector;task-parallel code;reachability analysis component;access history component;hashmap;asymptotically optimal overhead;memory access;continuous memory locations;STINT utilizes;treap;nonoverlapping intervals;overlapping interval;parallelizing STINT;race detection;access history part;asymptotic overhead","","","",49.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Frugal Decentralized Learning","A. -M. Kermarrec","Ecole Polytechnique Fédérale de Lausanne","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","862","862","Machine learning is currently shifting from a centralized paradigm to decentralized ones where machine learning models are trained collaboratively. In fully decentralized learning algorithms, data remains where it was produced, models are trained locally and only model parameters are exchanged among participating entities along an arbitrary network topology and aggregated over time until convergence. Not only this limits the cost of exchanging data but also exploits the growing capabilities of users' devices while mitigating privacy and confidentiality concerns. Such systems are significantly challenged by a potential high-level of heterogeneity both at the system level as participants may have differing capabilities of (e.g., computing power, memory and network connectivity) as well as data heterogeneity (a.k.a non-IIDness). The adoption of fully decentralized learning systems requires designing frugal systems that limit communication, energy and yet ensure convergences. Several avenues are promising from adapting the network topologies to compensate for data heterogeneity to exploiting the high levels of redundancy, both in data and computations, of ML algorithms to limit data and model sharing in such systems.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820694","","Biological system modeling;Machine learning;Data models;Network topology;Distributed processing;Technological innovation;Systems support","learning (artificial intelligence);topology","machine learning;data heterogeneity;fully decentralized learning systems;frugal systems;network topologies;frugal decentralized learning;non-IIDness","","","","","IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"A Fine-grained Prefetching Scheme for DGEMM Kernels on GPU with Auto-tuning Compatibility","J. Li; H. Ye; S. Tian; X. Li; J. Zhang","University of Chinese Academy of Sciences, Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Alibaba Group, Beijing, China; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","863","874","General Matrix Multiplication (GEMM) is one of the fundamental kernels for scientific and high-performance computing. When optimizing the performance of GEMM on GPU, the matrix is usually partitioned into a hierarchy of tiles to fit the thread hierarchy. In practice, the thread-level parallelism is affected not only by the tiling scheme but also by the resources that each tile consumes, such as registers and local data share memory. This paper presents a fine-grained prefetching scheme that improves the thread-level parallelism by balancing the usage of such resources. The gain and loss on instruction and thread level parallelism are analyzed and a mathematical model is developed to estimate the overall performance gain. Moreover, the proposed scheme is integrated into the open-source tool Tensile to automatically generate assembly and tune a collection of kernels to maximize the performance of DGEMM for a family of problem sizes. Experiments show about 1.10X performance speedup on a wide range of matrix sizes for both single and batched matrix-matrix multiplication.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00089","National Key R&D Program of China(grant numbers:2021YFB0300203); Natural Science Foundation of China (NSFC)(grant numbers:11871454); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820693","DGEMM;AMD GCN Architecture;Register;TLP;Workgroup Parallelism","Prefetching;High performance computing;Graphics processing units;Parallel processing;Performance gain;Mathematical models;Libraries","cache storage;graphics processing units;matrix multiplication;microprocessor chips;multiprocessing systems;multi-threading;optimisation;shared memory systems;storage management","high-performance computing;GEMM;GPU;thread hierarchy;thread-level parallelism;tiling scheme;local data share memory;fine-grained prefetching scheme;thread level parallelism;single matrix-matrix multiplication;batched matrix-matrix multiplication;DGEMM kernels;auto-tuning compatibility;general matrix multiplication","","","",32.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"StencilMART: Predicting Optimization Selection for Stencil Computations across GPUs","Q. Sun; Y. Liu; H. Yang; Z. Jiang; Z. Luan; D. Qian","School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","875","885","Stencil computations are widely used in high performance computing (HPC) applications. Many HPC platforms utilize the high computation capability of GPUs to accelerate stencil computations. In recent years, stencils have become more diverse in terms of stencil order, memory accesses and computation patterns. To adapt diverse stencils to GPUs, a variety of optimization techniques have been proposed such as streaming and retiming. However, due to the diversity of stencil patterns and GPU architectures, no single optimization technique fits all stencils. Besides, it is challenging to choose the most cost-efficient GPU for accelerating target stencils. To address the above problems, we propose StencilMART, an automatic optimization selection framework that predicts the best optimization combination and execution time under a certain parameter setting for stencils on GPUs. Specifically, the StencilMART represents the stencil patterns as binary tensors and neighboring features through tensor assignment and feature extraction. In addition, the StencilMART implements various machine learning methods such as classification and regression that utilize stencil representation and hardware characteristics for execution time prediction. The experiment results show that the StencilMART can achieve accurate optimization selection and performance prediction for various stencils across GPUs.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00090","National Key Research and Development Program of China(grant numbers:2020YFB1506703); National Natural Science Foundation of China(grant numbers:62072018); State Key Laboratory of Software Development Environment(grant numbers:SKLSDE-2021ZX-06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820650","Stencil Computation;GPU;Optimization Strategies;Performance Prediction;Machine Learning","Distributed processing;Tensors;High performance computing;Graphics processing units;Machine learning;Computer architecture;Feature extraction","coprocessors;feature extraction;graphics processing units;learning (artificial intelligence);parallel processing","StencilMART;stencil computations;GPUs;high performance computing applications;stencil order;computation patterns;diverse stencils;optimization techniques;stencil patterns;single optimization technique;automatic optimization selection framework;stencil representation;machine learning methods;classification method;regression method;tensor assignment;feature extraction","",1.0,"",29.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Mixed Precision $s$-step Conjugate Gradient with Residual Replacement on GPUs","I. Yamazaki; E. Carson; B. Kelley","Sandia National Laboratories, Albuquerque, New Mexico, U.S.A; Faculty of Mathematics and Physics, Charles University, Prague, Czech Republic; Sandia National Laboratories, Albuquerque, New Mexico, U.S.A","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","886","896","The $s$-step Conjugate Gradient (CG) algorithm has the potential to reduce the communication cost of standard CG by a factor of $s$. However, though mathematically equivalent, $s$-step CG may be numerically less stable compared to standard CG in finite precision, exhibiting slower convergence and decreased attainable accuracy. This limits the use of $s$-step CG in practice. To improve the numerical behavior of $s$-step CG and overcome this potential limitation, we incorporate two techniques. First, we improve convergence behavior through the use of higher precision at critical parts of the $s$-step iteration and second, we integrate a residual replacement strategy into the resulting mixed precision $s$-step CG to improve attainable accuracy. Our experimental results on the Summit Supercomputer demonstrate that when the higher precision is implemented in hardware, these techniques have virtually no overhead on the iteration time while improving both the convergence rate and the attainable accuracy of $s$-step CG. Even when the higher precision is implemented in software, these techniques may still reduce the time-to-solution (speedups of up to $1.8\times$ in our experiments), especially when $s$-step CG suffers from numerical instability with a small step size and the latency cost becomes a significant part of its iteration time.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00091","Charles University(grant numbers:PRIMUS/19/SCI/11,UNCE/SCI/023); DOE Office of Science(grant numbers:DE-AC05-000R22725); U.S. Department of Energy; National Nuclear Security Administration(grant numbers:DE-NA0003525); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820637","","Distributed processing;Costs;Supercomputers;Software;Hardware;Behavioral sciences;Numerical stability","conjugate gradient methods;graphics processing units;iterative methods","s-step iteration;s-step CG;s-step conjugate gradient;GPUs;residual replacement strategy","","","",25.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Degree-Aware Kernels for Computing Jaccard Weights on GPUs","A. A. Aljundi; T. A. Akyildiz; K. Kaya","Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","897","907","Graphs provide the ability to extract valuable met-rics from the structural properties of the underlying data they represent. One such metric is the Jaccard Weight of an edge, which is the ratio of the number of common neighbors of the edge's endpoints to the union of the endpoints' neighborhood. A naive implementation of Jaccard Weights computation has a complexity that scales with the number of edges in the graph times the square of the maximum degree. Recently, GPU-based parallel algorithms have been proposed for this problem. How-ever, these algorithms cannot overcome the structural variance within a graph, i.e., the sparsity pattern and degree imbalance, which directly translates to unbalanced work distribution across threads. In this work, we propose an optimized GPU-based algorithm with an ML-based work distribution model that mitigates the unbalanced work distribution. Our algorithm is shown to be up to 35x and on average 12x faster than the state of the art in practice while using less memory. In fact, we show that by manually tweaking the load distribution, a state-of-the-art implementation can be 5x faster. In addition, we propose a multi-core, shared-memory algorithm that applies a traditional but effective technique to improve the computation asymptotically and perform comparably to the GPU algorithms. Our code is available at https://github.com/SU-HPC/Jaccard-ML.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00092","Scientific and Technological Research Council of Turkey(grant numbers:220N254,956213); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820701","Jaccard Weights;parallel graph algorithms;GPU;work distribution","Measurement;Distributed processing;Instruction sets;Memory management;Graphics processing units;Complexity theory;Data mining","graph theory;graphics processing units;learning (artificial intelligence);mathematics computing;multi-threading;parallel algorithms;resource allocation","GPU-based parallel algorithms;load distribution;shared-memory algorithm;degree-aware kernels;Jaccard weight compunting;multicore algorithm;ML-based work distribution;graphs","","","",26.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Fast and High-Quality Influence Maximization on Multiple GPUs","G. Göktürk; K. Kaya","Computer Science and Engineering, Sabancı University, Istanbul, Turkey; Computer Science and Engineering, Sabancı University, Istanbul, Turkey","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","908","918","Influence Maximization (IM) is a popular problem focusing on finding a seed vertex set in a graph that maximizes the expected number of vertices affected via diffusion under a given, usually probabilistic model. For most diffusion models used in practice, finding an optimal seed set of a given size is NP-Hard. Hence, approximation algorithms and heuristics are often proposed and used. The Greedy approach is one of the most frequently applied approximation approach employed for IM. Indeed, this Monte-Carlo-based approach performs remarkably well in terms of seed set quality, i.e., the number of affected vertices. However, it is impractical for real-life networks containing tens of millions of vertices due to its expensive simulation costs. Recently, parallel IM kernels running on CPUs and GPUs have been proposed in the literature. In this work, we propose SUPERFUSER, a blazing-fast, sketch-based Influence Maximization algorithm developed for multiple GPUs. SUPERFUSER uses hash-based fused sampling to process multiple simulations at the same time with minimal overhead. In addition, we propose a Sampling-Aware Sample-Space Split approach to partition the edges to multiple GPUs efficiently by exploiting the unique characteristics of the sampling process. Based on our experiments, SUPERFUSER is up to 6.31× faster than its nearest competitor on a single GPU. Furthermore, we achieve 6.8× speed-up on average using 8 GPUs over a single GPU performance, and thanks to our novel partitioning scheme, we can process extremely large-scale graphs in practice without sacrificing quality too much. As an example, SUPERFUSER can generate a high-quality seed set with 50 vertices for a graph having 1.8B edges in less than 15 seconds on 2 GPUs.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00093","Scientific and Technological Research Council of Turkey (TÜBİTAK)(grant numbers:220N254); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820685","Influence maximization;Data sketches;GPU;Fused sampling","Distributed processing;Monte Carlo methods;Heuristic algorithms;Memory management;Graphics processing units;Focusing;Probabilistic logic","approximation theory;computational complexity;graph theory;graphics processing units;Monte Carlo methods;optimisation","diffusion models;NP-hard problem;approximation algorithms;greedy approach;frequently applied approximation approach;Monte-Carlo-based approach;seed set quality;parallel IM kernels;blazing-fast;hash-based;GPU performance;high-quality seed;sampling-aware sample-space split approach;sketch-based influence maximization algorithm;SUPERFUSER algorithm;time 15.0 s","","","",27.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Traffic-Optimal Virtual Network Function Placement and Migration in Dynamic Cloud Data Centers","V. Tran; J. Sun; B. Tang; D. Pan","Department of Computer Science, California State University Dominguez Hills; Department of Computer Science, California State University Dominguez Hills; Department of Computer Science, California State University Dominguez Hills; School of Computing and Information Sciences, Florida International University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","919","929","We propose a new algorithmic framework for traffic-optimal virtual network function (VNF) placement and migration for policy-preserving data centers (PPDCs). As dynamic virtual machine (VM) traffic must traverse a sequence of VNFs in PPDCs, it generates more network traffic, consumes higher bandwidth, and causes additional traffic delays than a traditional data center. We design optimal, approximation, and heuristic traffic-aware VNF placement and migration algorithms to minimize the total network traffic in the PPDC. In particular, we propose the first traffic-aware constant-factor approximation algorithm for VNF placement, a Pareto-optimal solution for VNF migration, and a suite of efficient dynamic-programming (DP)-based heuristics that further improves the approximation solution. At the core of our framework are two new graph-theoretical problems that have not been studied. Using flow characteristics found in production data centers and realistic traffic patterns, we show that a) our VNF migration techniques are effective in mitigating dynamic traffic in PPDCs, reducing the total traffic cost by up to 73%, b) our VNF placement algorithms yield traffic costs 56% to 64% smaller than those by existing techniques, and c) our VNF migration algorithms outperform the state-of-the-art VM migration algorithms by up to 63% in reducing dynamic network traffic.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00094","NSF(grant numbers:CNS-1911191); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820622","Policy-Preserving Data Centers;Dynamic Cloud Traffic;VNF Placement and Migration;Algorithms","Data centers;Cloud computing;Distributed processing;Costs;Heuristic algorithms;Telecommunication traffic;Production","approximation theory;cloud computing;computational complexity;computer centres;dynamic programming;Pareto optimisation;telecommunication traffic;virtual machines;virtualisation","traffic-optimal virtual network function placement;dynamic cloud data centers;algorithmic framework;policy-preserving data centers;PPDCs;dynamic virtual machine traffic;additional traffic delays;traditional data center;heuristic traffic-aware VNF placement;total network traffic;traffic-aware constant-factor approximation algorithm;Pareto-optimal solution;dynamic-programming-based heuristics;approximation solution;production data centers;realistic traffic patterns;VNF migration techniques;dynamic traffic;total traffic cost;VNF placement algorithms;traffic costs;VNF migration algorithms;state-of-the-art VM migration algorithms;dynamic network traffic","",1.0,"",56.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Parallel Tensor Train Rounding using Gram SVD","H. Al Daas; G. Ballard; L. Manning","Rutherford Appleton Laboratory, Computational Mathematics Group, Didcot, Oxfordshire, UK; Department of Computer Science, Wake Forest University, Winston-Salem, NC, USA; Department of Computer Science, Wake Forest University, Winston-Salem, NC, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","930","940","Tensor Train (TT) is a low-rank tensor representation consisting of a series of three-way cores whose dimensions specify the TT ranks. Formal tensor train arithmetic often causes an artificial increase in the TT ranks. Thus, a key operation for applications that use the TT format is rounding, which truncates the TT ranks subject to an approximation error guarantee. Truncation is performed via SVD of a highly structured matrix, and current rounding methods require careful orthogonalization to compute an accurate SVD. We propose a new algorithm for TT-Rounding based on the Gram SVD algorithm that avoids the expensive orthogonalization phase. Our algorithm performs less computation and can be parallelized more easily than existing approaches, at the expense of a slight loss of accuracy. We demonstrate that our implementation of the rounding algorithm is efficient, scales well, and consistently outperforms the existing state-of-the-art parallel implementation in our experiments.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820742","","Distributed processing;Tensors;Scalability;Approximation algorithms;Approximation error;Matrix decomposition;Iterative methods","approximation theory;matrix algebra;singular value decomposition;tensors","rounding algorithm;parallel tensor Train Rounding;low-rank tensor representation;TT ranks;formal tensor train arithmetic;TT format;current rounding methods;accurate SVD;TT-Rounding;Gram SVD algorithm","","","",40.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Task-based Acceleration of Bidirectional Recurrent Neural Networks on Multi-core Architectures","R. K. Sharma; M. Casas","Computer Science Department, Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC); Computer Science Department, Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC)","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","941","951","This paper proposes a novel parallel execution model for Bidirectional Recurrent Neural Networks (BRNNs), B-Par (Bidirectional-Parallelization), which exploits data and control dependencies for forward and reverse input computations. B-Par divides BRNN workloads across different parallel tasks by defining input and output dependencies for each RNN cell in both forward and reverse orders. B-Par does not require per-layer barriers to synchronize the parallel execution of BRNNs. We evaluate B-Par considering the TIDIGITS speech database and the Wikipedia data-set. Our experiments indicate that B-Par outperforms the state-of-the-art deep learning frameworks TensorFlow-Keras and Pytorch by achieving up to 2.34× and 9.16× speed-ups, respectively, on modern multi-core CPU architectures while preserving accuracy. Moreover, we analyze in detail aspects like task granularity, locality, or parallel efficiency to illustrate the benefits of B-Par.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00096","Generalitat de Catalunya(grant numbers:2017-SGR-1414); Spanish Ministry of Science and Technology(grant numbers:PID2019-107255GB); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820648","Deep neural network (DNN);Bidirectional re-current neural networks (BRNNs);Long-short term memory (LSTM);Gated Recurrent Units (GRU);Task Parallelism","Deep learning;Training;Recurrent neural networks;Scalability;Computer architecture;Parallel processing;Transformers","deep learning (artificial intelligence);multiprocessing systems;parallel processing;recurrent neural nets","Wikipedia dataset;multicore CPU architecture;task granularity;task-based acceleration;bidirectional recurrent neural networks;multicore architectures;parallel execution model;bidirectional-parallelization;TIDIGITS speech database;BRNN workload;TensorFlow-Keras framework;Pytorch framework","","","",58.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Sparsity-Aware Tensor Decomposition","S. E. Kurt; S. Raje; A. Sukumaran-Rajam; P. Sadayappan","School of Computing, University of Utah, Salt Lake City, Utah; School of Computing, University of Utah, Salt Lake City, Utah; Department of EECS, Washington State University, Pullman, Washington; School of Computing, University of Utah, Salt Lake City, Utah","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","952","962","Sparse tensor decomposition, such as Canonical Polyadic Decomposition (CPD), is a key operation for data analytics and machine learning. Its computation is dominated by a set of MTTKRP (Matricized Tensor Times Khatri Rao Product) operations. The collection of required MTTKRP operations for sparse CPD include common sub-computations across them and many approaches exist to factorize and reuse common sub-expressions. Prior work on sparse CPD has focused on minimizing the number of high-level operators. In this paper, we consider a design space that covers whether the partial MTTKRP results should be saved, different mode permutations and model the total volume of data movement to/from memory. Also, we propose a fine-grained load balancing method that supports higher levels of parallelization.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820702","CPD;MTTKRP;sparse tensor factorization","Solid modeling;Distributed processing;Tensors;Data analysis;Machine learning;Load management;Data models","data analysis;learning (artificial intelligence);mathematical operators;mathematics computing;matrix algebra;matrix decomposition;parallel processing;resource allocation;tensors","data analytics;machine learning;matricized tensor times Khatri Rao product;MTTKRP;sparse CPD;high-level operators;data movement;sparse tensor decomposition;sparsity-aware tensor decomposition;fine-grained load balancing;parallelization;sparse canonical polyadic decomposition","","","",22.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Coloring the Vertices of 9-pt and 27-pt Stencils with Intervals","D. Durrman; E. Saule","Dept. Mathematics, UNC Charlotte, Charlotte, NC; Dept. Computer Science, UNC Charlotte, Charlotte, NC","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","963","973","Graph coloring is commonly used to schedule computations on parallel systems. Given a good estimation of the computational requirement for each task, one can refine the model by adding a weight to each vertex. Instead of coloring each vertex with a single color, the problem is to color each vertex with an interval of colors. In this paper, we are interested in studying this problem for particular classes of graphs, namely stencil graphs. Stencil graphs appear naturally in the parallelisation of applications where the location of an object in a space affects the state of neighboring objects. Rectilinear decompositions of a space generate conflict graphs that are 9-pt stencils for 2D problems and 27-pt stencils for 3D problems. We show that the 5-pt stencil and 7-pt stencil relaxations of the problem can be solved in polynomial time. We prove that the decision problem on 27-pt stencil is NP-Complete. We discuss approximation algorithms with a ratio of 2 for the 9-pt stencil case, and 4 for the 27-pt stencil case. We identify two lower bounds for the problem that are used to design heuristics. We evaluate the effectiveness of several different algorithms experimentally on a set of real instances. Furthermore, these algorithms are integrated into a real application to demonstrate the soundness of the approach.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00098","National Science Foundation(grant numbers:CCF-1652442); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820709","interval vertex coloring;stencils;np completeness;approximation algorithms;heuristics","Schedules;Distributed processing;Three-dimensional displays;Runtime;Heuristic algorithms;Computational modeling;Estimation","approximation theory;computational complexity;graph colouring","graph coloring;parallel systems;computational requirement;vertex;stencil graphs;conflict graphs;7-pt stencil relaxations;9-pt stencil case;27-pt stencil case;approximation algorithm;NP-complete problem;polynomial time","","","",15.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Falcon: A Timestamp-based Protocol to Maximize the Cache Efficiency in the Distributed Shared Memory","J. Zhang; X. Yu; Z. Qi; H. Guan","Shanghai Jiao Tong University, Shanghai, China; University of Wisconsin-Madison, Madison, WI, USA; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","974","984","Distributed shared memory (DSM) systems can handle data-intensive applications and recently receiving more attention. A majority of existing DSM implementations are based on write-invalidation (WI) protocols, which achieve sub-optimal performance when the cache size is small. Specifically, the vast majority of invalidation messages become useless when evictions are frequent. The problem is troublesome regarding scarce memory resources in data centers. To this end, we propose a self-invalidation protocol Falcon to eliminate invalidation messages. It relies on per-operation timestamps to achieve the global memory order required by sequential consistency (SC). Furthermore, we conduct a comprehensive discussion on the two protocols with an emphasis on the cache size impact. We also implement both protocols atop a recent DSM system, Grappa. The evaluation shows that the optimal protocol can improve the performance of a KV database by 27% and a graph processing application by 71.4% against the vanilla cache-free scheme.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00099","National NSF of China(grant numbers:61732010,62141218); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820697","Distributed shard memory;cache coherence;self-invalidation","Distributed processing;Data centers;Protocols;Memory management;Distributed databases","distributed shared memory systems;graph theory;protocols","self-invalidation protocol Falcon;per-operation timestamps;global memory order;optimal protocol;vanilla cache-free scheme;timestamp-based protocol;cache efficiency;distributed shared memory;write-invalidation protocols","","","",41.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"HACCS: Heterogeneity-Aware Clustered Client Selection for Accelerated Federated Learning","J. Wolfrath; N. Sreekumar; D. Kumar; Y. Wang; A. Chandra","Department of Computer Science and Engineering, University of Minnesota, Minneapolis, USA; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, USA; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, USA; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, USA; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","985","995","Federated Learning is a machine learning paradigm where a global model is trained in-situ across a large number of distributed edge devices. While this technique avoids the cost of transferring data to a central location and achieves a strong degree of privacy, it presents additional challenges due to the heterogeneous hardware resources available for training. Furthermore, data is not independent and identically distributed (IID) across all edge devices, resulting in statistical heterogeneity across devices. Due to these constraints, client selection strategies play an important role for timely convergence during model training. Existing strategies ensure that each individual device is included, at least periodically, in the training process. In this work, we propose HACCS, a Heterogeneity-Aware Clustered Client Selection system that identifies and exploits the statistical heterogeneity by representing all distinguishable data distributions instead of individual devices in the training process. HACCS is robust to individual device dropout, provided other devices in the system have similar data distributions. We propose privacy-preserving methods for estimating these client distributions and clustering them. We also propose strategies for leveraging these clusters to make scheduling decisions in a federated learning system. Our evaluation on real-world datasets suggests that our framework can provide 18% −38% reduction in time to convergence compared to the state of the art without any compromise in accuracy.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00100","NSF(grant numbers:CNS-1717834); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820684","Federated Learning;Non-IID data;Clustering;Scheduling","Training;Distributed processing;Data privacy;Costs;Distributed databases;Machine learning;Collaborative work","data privacy;learning (artificial intelligence);pattern clustering;scheduling","HACCS;Heterogeneity-Aware;accelerated federated Learning;machine learning paradigm;global model;distributed edge devices;central location;heterogeneous hardware resources;statistical heterogeneity;client selection strategies;timely convergence;model training;training process;Client Selection system;distinguishable data distributions;individual device dropout;similar data distributions;privacy-preserving methods;client distributions;federated learning system","","","",44.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"A Swap Dominated Tensor Re-Generation Strategy for Training Deep Learning Models","L. Wen; Z. Zong; L. Lin; L. Lin","School of Software, Tsinghua University; School of Software, Tsinghua University; School of Software, Tsinghua University; School of Management, Capital Normal University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","996","1006","With the growing of the depth of neural networks and the scale of data, the difficulty of network training also increases. When the GPU memory is insufficient, it is challenging to train deeper models. Recent research uses tensor swapping and recomputation techniques in a combined manner to optimize the memory usage. However, complex dependencies of the DNN graph limit the improvement of the single GPU memory optimization. Improper swap decisions even brings negative effects because the source of the recomputation may have been swapped out. In this paper, we propose a novel swap dominated tensor re-generation strategy, called STR, which combines swap and recomputation techniques to find the optimal execution plan for the DNN training when the memory is limited. We formalize our memory optimization problem with constraints which describe the dependency of the operator calculation and the bandwidth usage of swap. A host checkpoint mechanism is designed to make full use of the swapped tensors, which reduces the cost of the recomputation. We also present an approximation method based on a recursive source tracing procedure to improve the optimization efficiency. We implement a prototype of STR as a plugin on TensorFlow. The experimental result shows that STR improves up to 21.3% throughput compared with the state-of-the-art hybrid optimization strategy.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00101","National Key Research and Development Program of China(grant numbers:2019YFB1704003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820619","recomputation;swap;DNN training;GPU memory optimization","Training;Deep learning;Distributed processing;Tensors;Costs;Neural networks;Graphics processing units","approximation theory;checkpointing;cost reduction;deep learning (artificial intelligence);graph theory;graphics processing units;optimisation;tensors","deep learning;neural networks;network training;TensorFlow;approximation method;cost reduction;host checkpoint mechanism;bandwidth usage;recomputation techniques;recursive source tracing;swap dominated tensor re-generation;hybrid optimization;swapped tensors;DNN training;optimal execution plan;STR;swap decisions;single GPU memory optimization;DNN graph;memory usage;tensor swapping","",1.0,"",39.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Fast Convergence to Fairness for Reduced Long Flow Tail Latency in Datacenter Networks","J. Snyder; A. R. Lebeck","Department of Computer Science, Duke University; Department of Computer Science, Duke University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1007","1017","Many data-intensive applications, such as distributed deep learning and data analytics, require moving vast amounts of data between compute servers in a distributed system. To meet the demands of these applications, datacenters are adopting Remote Direct Memory Access (RDMA), which has higher bandwidth and lower latency than traditional kernel-based networking. To ensure high performance of RDMA networks, congestion control manages queue depth on switches, and historically focused on moderating queue depth to ensure small flows complete quickly. Unfortunately, one side-effect of many common decisions is that large flows are starved of bandwidth. This negatively impacts the flow completion time (FCT) of large, bandwidth-bound flows, which are integral to the performance of data-intensive applications. The FCT is particularly impacted at the tail, which is increasingly critical for predictable application performance. We identify the root causes of the poor performance for long flows and measure the impact. We then design mechanisms that improve long flow FCT without compromising small flow performance. Our evaluations show that these improvements reduce 99.9% tail FCT of long flows by over 2x.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00102","National Science Foundation(grant numbers:CNS-1616947); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820642","datacenter networks, congestion control, RDMA","Distributed processing;Protocols;Additives;Distributed databases;Tail;Bandwidth;Throughput","computer centres;computer network management;data analysis;deep learning (artificial intelligence);file organisation;network servers;queueing theory;telecommunication congestion control;telecommunication switching","data-intensive applications;distributed deep learning;data analytics;compute servers;distributed system;remote direct memory access;RDMA networks;moderating queue depth;flow completion time;bandwidth-bound flows;predictable application performance;flow FCT;flow performance;kernel-based networking;data centers networks;congestion control","","","",31.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Dynamic Computation Offloading for Green Things-Edge-Cloud Computing with Local Caching","X. Tian; H. Meng; Y. Li; P. Miao; P. Xu","School of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1018","1028","With the increasing popularity of the internet of things (IoT) and 5G, emerging things-edge-cloud computing (TEC) paradigm provides a flexible way for execution of delay-sensitive and computation-intensive applications running on the user equipment (UE). By offloading these workloads to the mobile edge computing (MEC) or mobile cloud computing (MCC) server, the quality of experience, e.g., the execution delay, could be greatly improved. Nevertheless, conventional battery-powered devices face the challenge of battery exhaustion for task offloading. Using renewable energy via energy harvesting (EH) technologies has become a promising way to power these devices. In this paper, we investigate a multi-user green TEC system with EH UEs, each has a task buffer with limited capacity. A joint offloading decision and resource allocation problem is formulated, which addresses the long-term average execution delay, the task dropping and the long-term average energy cost constraint. A low-complexity online algorithm is proposed leveraging Lyapunov optimization framework and matroid theory, which jointly decides the offloading decision, the MEC server CPU frequencies and the transmit power for computation offloading. A unique advantage of this algorithm is that the decisions depend only on the current system state without requiring distribution information of the arrival tasks, wireless channel state, and EH processes. The implementation of the algorithm only requires to solve a deterministic problem in each time slot. Simulation results show that our proposed algorithm makes a best trade-off between minimizing the long-term average generalized delay and satisfying the long-term average energy cost constraint. Impacts of various parameters on the delay and energy cost performance are also discussed.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00103","Natural Science Foundation of Zhejiang Province(grant numbers:LZ21F020005,LZ22F020004); National Natural Science Foundation of China(grant numbers:61772472); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820633","Thing-edge-cloud computing;energy harvesting;online offloading;Lyapunov optimization;matroid theory","Wireless communication;Costs;Simulation;Green products;Delays;Servers;Resource management","cache storage;cloud computing;energy harvesting;green computing;mobile computing;optimisation;power aware computing;resource allocation;wireless channels","energy harvesting technologies;multiuser green TEC system;EH UEs;task buffer;joint offloading decision;resource allocation;long-term average execution delay;task dropping;long-term average energy cost constraint;matroid theory;MEC server CPU frequencies;transmit power;arrival tasks;EH processes;long-term average generalized delay;energy cost performance;deterministic problem;Lyapunov optimization framework;wireless channel state;renewable energy;task offloading;battery exhaustion;mobile cloud;user equipment;computation-intensive applications;delay-sensitive;green things-edge-cloud computing;local caching;dynamic computation offloading","","","",28.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Excavating the Potential of Graph Workload on RDMA-based Far Memory Architecture","J. Wang; C. Li; T. Wang; L. Zhang; P. Wang; J. Mei; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Qi Zhi Institute, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Qi Zhi Institute, Shanghai, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1029","1039","Disaggregated architecture brings new opportunities to memory -consuming applications like graph processing. It allows one to outspread memory access pressure from local to far memory, providing an attractive alternative to disk-based processing. Although existing works on general-purpose far mem-ory platforms show great potentials for application expansion, it is unclear how graph processing applications could benefit from disaggregated architecture, and how different optimization methods influence the overall performance. In this paper, we take the first step to analyze the impact of graph processing workload on disaggregated architecture by extending the GridGraph framework on top of the RDMA-based far memory system. We design Fargraph, a far memory coordi-nation strategy for enhancing graph processing workload. Specif-ically, Fargraph reduces the overall data movement through a well-crafted, graph-aware data segment offloading mechanism. In addition, we use optimal data segment splitting and asynchronous data buffering to achieve graph iteration-friendly far memory access. We show that Fargraph achieves near-oracle performance for typical in-local-memory graph processing systems. Fargraph shows up to 8.3 x speedup compared to Fastswap, the state-of-the-art, general-purpose far memory platform.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00104","National Natural Science Foundation of China(grant numbers:61832006,61972247); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820708","far memory;RDMA;graph processing","Distributed processing;Memory architecture;Optimization methods","data handling;data structures;graph theory;memory architecture;operating system kernels;storage management","memory architecture;disaggregated architecture;memory access pressure;disk-based processing;mem-ory platforms;application expansion;graph processing applications;different optimization methods;graph processing workload;RDMA-based;memory system;memory coordi-nation strategy;Fargraph;graph-aware data segment offloading mechanism;optimal data segment splitting;graph iteration-friendly;in-local-memory graph;graph workload","",2.0,"",42.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"SpectralFly: Ramanujan Graphs as Flexible and Efficient Interconnection Networks","S. Young; S. Aksoy; J. Firoz; R. Gioiosa; T. Hagge; M. Kempton; J. Escobedo; M. Raugas","Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory; Brigham Young University; Pacific Northwest National Laboratory; Pacific Northwest National Laboratory","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1040","1050","In recent years, graph theoretic considerations have become increasingly important in the design of HPC interconnection topologies. One approach is to seek optimal or near-optimal families of graphs with respect to a particular graph theoretic property, such as diameter. In this work, we consider topologies which optimize the spectral gap. We study a novel HPC topology, SpectralFly, designed around the Ramanujan graph construction of Lubotzky, Phillips, and Sarnak (LPS). We show combinatorial properties, such as diameter, bisection bandwidth, average path length, and resilience to link failure, of SpectralFly topologies are better than, or comparable to, similarly constrained DragonFly, SlimFly, and BundleFly topologies. Additionally, we simulate the performance of SpectralFly on a representative sample of micro-benchmarks using the Structure Simulation Toolkit Macroscale Element Library simulator and study cost-minimizing layouts, demonstrating considerable benefit of the SpectralFly topology.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00105","PNNL(grant numbers:PNNL-SA-160551); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820688","Graph theory;spectral gap;spectral expansion;interconnection networks;Ramanujan graphs","Fault tolerance;Network topology;Multiprocessor interconnection;Layout;Fault tolerant systems;Bandwidth;Particle measurements","graph theory;multiprocessor interconnection networks;parallel processing;topology","cost-minimizing layout;structure simulation toolkit macroscale element library simulator;BundleFly topology;SlimFly topology;DragonFly topology;Ramanujan graph construction;graph theoretic property;HPC interconnection topologies;interconnection networks;study cost-minimizing layouts;BundleFly topologies;SpectralFly topology;average path length;combinatorial properties;spectral gap","","","",42.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Booster: An Accelerator for Gradient Boosting Decision Trees Training and Inference","M. He; M. Thottethodi; T. N. Vijaykumar","Elmore Family School of ECE, Purdue University, West Lafayette, IN, USA; Elmore Family School of ECE, Purdue University, West Lafayette, IN, USA; Elmore Family School of ECE, Purdue University, West Lafayette, IN, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1051","1062","Recent breakthroughs in machine learning (ML) have sparked hardware innovation for efficient execution of the emerging ML workloads. For instance, due to recent refine-ments and high-performance implementations, well-established gradient boosting decision tree (GBT) models (e.g., XGBoost) have demonstrated their dominance in commercially-important contexts, such as table-based datasets (e.g., relational databases and spreadsheets). Unfortunately, GBT training and inference are time-consuming (e.g., several hours of training for large datasets). Despite their importance, GBTs have not been targeted for hardware acceleration as much as neural networks. We propose Booster, a novel accelerator for GBTs based on their unique characteristics. We observe that the dominant steps of GBT training and inference (accounting for 90-98% of time) involve simple, fine-grained, independent operations on small-footprint data structures (e.g., histograms and shallow trees) - i.e., GBT is on-chip memory bandwidth-bound. Unfortunately, existing multicores and GPUs do not support massively-parallel data structure accesses that are irregular and data-dependent. By employing a scalable sea-of-small-SRAMs approach and an SRAM bandwidth-preserving mapping of data record fields to the SRAMs called group-by-field mapping, Booster achieves significantly more parallelism (e.g., 3200-way parallelism) than multicores and GPUs. In addition, Booster employs a redun-dant data representation that significantly lowers the memory bandwidth demand. Our simulations reveal that Booster achieves 11.4x and 6.4x speedups for training, and 45x and 22x (21x and 11x) speedups for offline (online) inference, over an ideal 32-core multicore and an ideal GPU, respectively. Based on ASIC synthesis of FPGA-validated RTL using 45 nm technology, we estimate a Booster chip to occupy 60 mm2 of area and dissipate 23 W when operating at 1-G Hz clock speed.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820706","Gradient boosting;accelerator","Training;Histograms;Multicore processing;Graphics processing units;Parallel processing;Boosting;Data structures","decision trees;field programmable gate arrays;inference mechanisms;multiprocessing systems;parallel processing;SRAM chips;supervised learning","gradient boosting decision trees training;machine learning;ML;GBT training;SRAM bandwidth-preserving mapping;group-by-field mapping;parallelism;data representation;Booster;sea-of-small-SRAMs;GBT inference","","","",33.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"FlashWalker: An In-Storage Accelerator for Graph Random Walks","F. Niu; J. Yue; J. Shen; X. Liao; H. Liu; H. Jin","National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, Michigan Technological University, Houghton, Michigan, USA; Department of Computer Science, Michigan Technological University, Houghton, Michigan, USA; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1063","1073","Graph random walk is widely used in the graph processing as it is a fundamental component in graph analysis, ranging from vertices ranking to the graph embedding. Different from traditional graph processing workload, random walk features massive processing parallelisms and poor graph data reuse, being limited by low I/O efficiency. Prior designs for random walk mitigate slow I/O operations. However, the state-of-the-art random walk processing systems are bounded by slow disk I/O bandwidth, which is confirmed by our experiments with real-world graphs. To address this issue, we propose FlashWalker, an in-storage accelerator for random walk that moves walk updating close to graph data stored in flash memory, by exploiting significant parallelisms inside SSD. Featuring a heterogeneous and parallel processing system, FlashWalker includes a board-level accelerator, channel-level accelerators, and chip-level accelerators. To address challenges posed by the tight resource constraints for processing large-scale graphs, we propose novel designs: storing a few popular subgraphs in accelerators, the pre-walking for dense walks, two optimizations to search the subgraph mapping table, and a subgraph scheduling algorithm. We implement FlashWalker in RTL, showing small circuit area overhead. Our evaluation shows FlashWalker reduces the execution time of random walk algorithms by up to 660.50×, compared with GraphWalker, which is the state-of-the-art system for random walk algorithms.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00107","National Natural Science Foundation of China (NSFC)(grant numbers:61825202,62072198,61732010); USA NSF(grant numbers:1745748); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820661","random walk;graph computing;in-storage processing;accelerator","Distributed processing;Scheduling algorithms;Bandwidth;Parallel processing;Distance measurement;Flash memories;Optimization","flash memories;graph theory;parallel processing","FlashWalker;in-storage accelerator;graph random walk;graph analysis;graph embedding;traditional graph processing workload;random walk features massive processing parallelisms;poor graph data reuse;state-of-the-art random walk processing systems;real-world graphs;heterogeneous processing system;parallel processing system;board-level accelerator;channel-level accelerators;chip-level accelerators;large-scale graphs;pre-walking;dense walks;random walk algorithms","","","",53.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Memory Access Granularity Aware Lossless Compression for GPUs","S. Lal; M. Renz; J. Hartmer; B. Juurlink","Technische Universität, Hamburg, Germany; Technische Universität, Berlin, Germany; Technische Universität, Berlin, Germany; Technische Universität, Berlin, Germany","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1074","1084","High-bandwidth off-chip memory has played a key role in the success of Graphics Processing Units (GPUs) as an accelerator. However, as memory bandwidth scaling continues to lag behind the computational power, it remains a key bottleneck in computing systems. While memory compression has shown immense potential to increase the effective memory bandwidth by compressed data transfers between on-chip and off-chip memory, the large memory access granularity (MAG) of off-chip memory limits compression techniques from achieving a high effective compression ratio. Unfortunately, state-of-the-art lossless memory compression techniques do not take the large MAG of off-chip memory into account. A recent study has used MAG-aware approximation to increase the effective compression ratio, however, not all applications can tolerate errors, which limits its applicability. We propose extensions and GPU-specific optimizations to adapt a lossless memory compression technique to a MAG size to increase the effective compression ratio and performance gain. Our technique is based on the well-known Base-Delta-Immediate (BDI) compression technique that compresses a memory block to a common base and multiple deltas. We leverage the key observation that deltas often contain enough leading zeros to compress a block to a multiple of MAG without any loss of information. We show that MAG-aware BDI provides, on average, 48 % higher effective compression ratio, 10% (up to 27%) higher speedup, and 16% bandwidth reduction compared to normal BDI. While BDI, FPC, and CPACK have a similar compression ratio, MAG-aware BDI outperforms FPC, CPACK, and SLC by 56%, 47%, and 33%, respectively.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820698","GPUs;compression;memory access granularity","Distributed processing;Graphics processing units;Bandwidth;Performance gain;Data transfer;Encoding;System-on-chip","approximation theory;data compression;graphics processing units;hardware accelerators;optimisation","memory access granularity aware lossless compression;GPU;high-bandwidth off-chip memory;Graphics Processing Units;memory bandwidth scaling;computational power;key bottleneck;computing systems;effective memory bandwidth;compressed data transfers;off-chip memory;high effective compression ratio;state-of-the-art lossless memory compression techniques;MAG-aware approximation;lossless memory compression technique;MAG size;Base-Delta-Immediate compression technique;memory block;common base;multiple deltas;MAG-aware BDI;higher effective compression ratio;similar compression ratio","","","",29.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Why Globally Re-shuffle? Revisiting Data Shuffling in Large Scale Deep Learning","T. T. Nguyen; F. Trahay; J. Domke; A. Drozd; E. Vatai; J. Liao; M. Wahib; B. Gerofi","National Institute of Advanced Industrial Science and Technology (AIST), Japan; Télécom SudParis, Institut Polytechnique de Paris, France; Tokyo Institute of Technology, Tokyo, Japan; Amigawa GK, Tokyo, Japan; Tokyo Institute of Technology, Tokyo, Japan; College of Computer and Information Science, Southwest University of China, China; Tokyo Institute of Technology, Tokyo, Japan; Tokyo Institute of Technology, Tokyo, Japan","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1085","1096","Stochastic gradient descent (SGD) is the most prevalent algorithm for training Deep Neural Networks (DNN). SGD iterates the input data set in each training epoch processing data samples in a random access fashion. Because this puts enormous pressure on the I/O subsystem, the most common approach to distributed SGD in HPC environments is to replicate the entire dataset to node local SSDs. However, due to rapidly growing data set sizes this approach has become increasingly infeasible. Surprisingly, the questions of why and to what extent random access is required have not received a lot of attention in the literature from an empirical standpoint. In this paper, we revisit data shuffling in DL workloads to investigate the viability of partitioning the dataset among workers and performing only a partial distributed exchange of samples in each training epoch. Through extensive experiments on up to 2,048 GPUs of ABCI and 4,096 compute nodes of Fugaku, we demonstrate that in practice validation accuracy of global shuffling can be maintained when carefully tuning the partial distributed exchange. We provide a solution implemented in PyTorch that enables users to control the proposed data exchange scheme.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820654","I/O;Distributed Deep Learning;Data Shuffling","Training;Deep learning;Distributed processing;Costs;Neural networks;Distributed databases;Stochastic processes","data handling;deep learning (artificial intelligence);gradient methods;parallel processing","distributed SGD;HPC environments;SSD;random access;data shuffling;partial distributed exchange;data exchange;large scale deep learning;stochastic gradient descent;data sample processing;deep neural network training;DNN;dataset partitioning;Fugaku;PyTorch;GPU","","","",48.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"DistrEdge: Speeding up Convolutional Neural Network Inference on Distributed Edge Devices","X. Hou; Y. Guan; T. Han; N. Zhang","New Jersey Institute of Technology, USA; New Jersey Institute of Technology, USA; New Jersey Institute of Technology, USA; Windsor University, Canada","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1097","1107","As the number of edge devices with computing resources (e.g., embedded GPUs, mobile phones, and laptops) in-creases, recent studies demonstrate that it can be beneficial to col-laboratively run convolutional neural network (CNN) inference on more than one edge device. However, these studies make strong assumptions on the devices' conditions, and their application is far from practical. In this work, we propose a general method, called DistrEdge, to provide CNN inference distribution strategies in environments with multiple IoT edge devices. By addressing heterogeneity in devices, network conditions, and nonlinear characters of CNN computation, DistrEdge is adaptive to a wide range of cases (e.g., with different network conditions, various device types) using deep reinforcement learning technology. We utilize the latest embedded AI computing devices (e.g., NVIDIA Jetson products) to construct cases of heterogeneous devices' types in the experiment. Based on our evaluations, DistrEdge can properly adjust the distribution strategy according to the devices' computing characters and the network conditions. It achieves 1.1 to 3 x speedup compared to state-of-the-art methods.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820719","distributed computing;convolutional neural net-work;edge computing;deep reinforcement learning","Distributed processing;Adaptive systems;Portable computers;Distribution strategy;Computational modeling;Laboratories;Reinforcement learning","convolutional neural nets;deep learning (artificial intelligence);distributed processing;embedded systems;groupware;inference mechanisms;Internet of Things;reinforcement learning","CNN inference;IoT edge devices;convolutional neural network inference;distributed edge devices;DistrEdge;embedded AI computing devices;deep reinforcement learning","",5.0,"",73.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Model-Architecture Co-Design for High Performance Temporal GNN Inference on FPGA","H. Zhou; B. Zhang; R. Kannan; V. Prasanna; C. Busart","University of Southern California; University of Southern California; US Army Research Lab; University of Southern California; US Army Research Lab","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1108","1117","Temporal Graph Neural Networks (TGNNs) are powerful models to capture temporal, structural, and contextual information on temporal graphs. The generated temporal node embeddings outperform other methods in many downstream tasks. Real-world applications require high performance inference on real-time streaming dynamic graphs. However, these models usually rely on complex attention mechanisms to capture relationships between temporal neighbors. In addition, maintaining vertex memory suffers from intrinsic temporal data dependency that hinders task-level parallelism, making it inefficient on general-purpose processors. In this work, we present a novel model-architecture co-design for inference in memory-based TGNNs on FPGAs. The key modeling optimizations we propose include a light-weight method to compute attention scores and a related temporal neighbor pruning strategy to further reduce computation and memory accesses. These are holistically coupled with key hardware optimizations that leverage FPGA hardware. We replace the temporal sampler with an on-chip FIFO based hardware sampler and the time encoder with a look-up-table. We train our simplified models using knowledge distillation to ensure similar accuracy vis-á-vis the original model. Taking advantage of the model optimizations, we propose a principled hardware architecture using batching, pipelining, and prefetching techniques to further improve the performance. We also propose a hardware mechanism to ensure the chronological vertex updating without sacrificing the computation parallelism. We evaluate the performance of the proposed hardware accelerator on three real-world datasets. The proposed model reduces the computation complexity by 84% and memory accesses by 67% with less than 0.33% accuracy loss. Compared with CPU/GPU, our FPGA accelerator achieves 16.4/2.3× speedup in latency and 0.27% improvement in accuracy compared with the state-of-the-art inference algorithm. To the best of our knowledge, this is the first work that performs model-architecture co-design on memory-based Temporal Graph Neural Networks.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00111","National Science Foundation (NSF)(grant numbers:OAC-1911229); Army Research Lab (ARL)(grant numbers:DIRA-ECI:DEC21-CI-037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820671","Temporal Graph Neural Network;Hardware Architecture;FPGA","Computational modeling;Inference algorithms;Graph neural networks;Real-time systems;System-on-chip;Task analysis;Computational complexity","batch processing (computers);computational complexity;field programmable gate arrays;general purpose computers;graph theory;inference mechanisms;neural nets;parallel architectures;pipeline processing;storage management","temporal node embeddings;temporal data dependency;task-level parallelism;general-purpose processors;memory accesses;FPGA hardware;temporal sampler;on-chip FIFO based hardware sampler;principled hardware architecture;hardware accelerator;computation complexity;FPGA accelerator;model architecture co-design;high performance temporal GNN inference;real-time streaming dynamic graphs;memory-based TGNN;temporal neighbor pruning strategy;memory-based temporal graph neural networks;batching technique;pipelining technique;prefetching technique","",2.0,"",21.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Preprocessing Pipeline Optimization for Scientific Deep Learning Workloads","K. Z. Ibrahim; L. Oliker","Applied Mathematics & Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA; Applied Mathematics & Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1118","1128","Newly developed machine learning technology is promising to profoundly impact high-performance computing, with the potential to significantly accelerate scientific discoveries. However, scientific machine learning performance is often constrained by data movement overheads, particularly on existing and emerging hardware-accelerated systems. In this work, we focus on optimizing the data movement across storage and memory systems, by developing domain-specific data encoder/decoders. These plugins have the dual benefit of significantly reducing communication while enabling efficient decoding on the accelerated hardware. We explore detailed performance analysis for two important scientific learning workloads from cosmology and climate analytics, CosmoFlow and DeepCAM, on the GPU-enabled Summit and Cori supercomputers. Results demonstrate that our optimizations can significantly improve overall performance by up to 10× compared with the default baseline, while preserving convergence behavior. Overall, this methodology can be applied to various machine learning domains and emerging AI technologies.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00112","Office of Science(grant numbers:DE-AC02-05CH11231); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820645","Deep Neural Network;Pytorch;TensorFlow;Preprocessing;Compression;CosmoFlow;DeepCAM","Deep learning;Graphics processing units;Systems architecture;Throughput;Supercomputers;Decoding;Computational efficiency","decoding;deep learning (artificial intelligence);natural sciences computing;parallel machines;pipeline processing","preprocessing pipeline optimization;scientific deep learning workloads;machine learning technology;high-performance computing;scientific discoveries;scientific machine learning performance;data movement overheads;hardware-accelerated systems;memory systems;dual benefit;efficient decoding;accelerated hardware;detailed performance analysis;optimizations;machine learning domains;scientific learning workloads;climate analytics;cosmology;CosmoFlow;DeepCAM;GPU-enabled summit supercomputers;Cori supercomputers;emerging AI technologies","","","",45.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Fault-tolerant Snapshot Objects in Message Passing Systems","V. K. Garg; S. Kumar; L. Tseng; X. Zheng","University of Texas at Austin, USA; Boston College, USA; Boston College, USA; Google, Inc., USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1129","1139","The atomic snapshot object (ASO) can be seen as a generalization of the atomic read/write register. ASO divides the object into $n$ segments such that each node can update its own segment, and instantaneously scan all segments of the object. ASO is a powerful data structure that has many important applications, such as update-query state machines, linearizable conflict-free replicated data types, generalized lattice agreement, and cryptocurrency as in the form of an asset transfer object. This paper studies ASO in asynchronous message passing systems and proposes a framework for implementing efficient fault-tolerant snapshot objects. Denote by $D$ the maximum message delay and $k$ the actual number of failures in an execution. Our framework derives two ASO algorithms: •A crash-tolerant ASO algorithm that achieves O(√k. D) time complexity for both update and scan operations, and achieves amortized constant time operations if there are Ω(√k) operations. •A Byzantine ASO algorithm that achieves O(k.D) time complexity for both update and scan operations, and achieves amortized constant time operations if there is no Byzantine node in a given execution. The framework can also be adapted to implement sequentially consistent snapshot objects (SSO) that complete scan operations locally without any communication, and have the same time complexlty for update onerations as in our ASO algorithms.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820691","Atomic Snapshot Object;Sequential Snapshot Object;Crash Failure;Asynchrony;Byzantine Failure","Fault tolerance;Message passing;Fault tolerant systems;Lattices;Writing;Data structures;Cryptocurrency","computational complexity;data structures;fault tolerant computing;message passing;replicated databases;software fault tolerance","fault-tolerant snapshot objects;atomic snapshot object;atomic read;ASO divides;powerful data structure;update-query state machines;linearizable conflict-free;generalized lattice agreement;asset transfer object;paper studies ASO;asynchronous message passing systems;efficient fault-tolerant;maximum message delay;ASO algorithms;crash-tolerant ASO algorithm;constant time operations;Byzantine ASO algorithm;Byzantine node;sequentially consistent snapshot objects;complete scan operations;time complexlty;update onerations","","","",42.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"A self-stabilizing 2-minimal dominating set algorithm based on loop composition in networks of girth at least 7","S. Maruyama; Y. Sudo; S. Kamei; H. Kakugawa","Fujitsu Ltd., Japan; Hosei University, Japan; Hiroshima University, Japan; Ryukoku University, Japan","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1140","1150","We propose a silent self-stabilizing asynchronous distributed algorithm to find a 2-minimal dominating set (2-MDS) in networks of girth at least 7. Given a graph $G=(V, E)$, a 2-MDS of $G$ is a minimal dominating set $D\subseteq V$ such that $D\backslash \{p_{i},p_{j}\}\cup\{p_{z}\}$ is not a dominating set for any nodes $p_{i},p_{j}\in L (p_{i}\neq p_{j})$ and $p_{z}\ /{\!\!\!\in} D$. The girth is the length of the shortest cycles in the graph. We assume that the processes have unique identifiers. The proposed algorithm constructs a 2-MDS in the networks of girth at least 7 under the weakly fair distributed daemon. The time complexity is $O(nH)$ rounds, and the space complexity is $O(\log n)$ bits per process, where $n$ is the number of processes and $H$ is the diameter of the network.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00114","JSPS(grant numbers:19K11826,19K11828,19H04085,20H04140); Japan Science and Technology Agency (JST); SICORP(grant numbers:JPMJSC1806); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820687","Dominating set;Self-stabilization;Loop composition;2-minimality;girth","Distributed processing;Network topology;Complexity theory;Topology;Time complexity;Distributed algorithms","computational complexity;distributed algorithms;graph theory;network theory (graphs);set theory","girth;silent self-stabilizing asynchronous distributed algorithm;2-MDS;self-stabilizing 2-minimal dominating set;time complexity;weakly fair distributed daemon","","","",23.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Optimal Arbitrary Pattern Formation on a Grid by Asynchronous Autonomous Robots","R. Hector; G. Sharma; R. Vaidyanathan; J. L. Trahan","Louisiana State University, Baton Rouge, LA, USA; Kent State University, Kent, OH, USA; Louisiana State University, Baton Rouge, LA, USA; Louisiana State University, Baton Rouge, LA, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1151","1161","We consider the distributed setting of $N$ autonomous mobile robots that operate in Look-Compute-Move (LCM) cycles following either the robots with lights model or the classical oblivious robots model. For the lights model, we assume obstructed visibility so that a robot cannot see another robot if a third robot is positioned between them on the straight line connecting them. In contrast, we assume unobstructed visibility in the classical model so that a robot sees all others irrespective of their positions. In addition, we consider a grid-based terrain embedded in the 2-dimensional Euclidean plane that restricts each robot's movement to one of the four neighboring grid points from its current position. This grid setting is a natural discretization of the 2-dimensional real plane and extends the robot swarm model in directions of greater applicability. The Arbitrary Pattern Formationproblem is to relocate the $N$ robots (starting at arbitrary but distinct initial positions on a grid) to form an arbitrary target pattern given as input. In this paper, we provide two asynchronous algorithms for Arbitrary Pattern Formation, one on the lights model and another on the classical model. Key measures of the algorithms' performance include the time taken and the number of moves by each robot. Both algorithms run in $O(\max\{D^{i}, D^{p}\})$ time with $O(\max\{D^{i}, D^{p}\})$ moves by each robot, where $D^{i}$ and $D^{p}$, respectively, are the diameters of the initial and pattern configurations. The algorithm for the lights model uses $O(1)$ colors. We also prove a lower bound of $\Omega(\max\{D^{i}, D^{p}\})$ for time for any Arbitrary Pattern Formationalgorithm if scaling is not allowed on the target pattern. Therefore, our algorithms are optimal w.r.t. time. Furthermore, our algorithms are also optimal w.r.t. the number of moves given the existing lower bound of $\Omega(\max\{D^{i}, D^{p}\})$ on the number of moves. In sum, our results show that having lights provides a trade-off on the unobstructed visibility requirement in the classical model for Arbitrary Pattern Formation.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820646","mobile robots;time complexity;pattern formation;grid","Pattern formation;Distributed processing;Color;Time measurement;Mobile robots;Robots;Autonomous robots","collision avoidance;computational complexity;mobile robots;multi-robot systems","optimal arbitrary pattern formation;asynchronous autonomous robots;autonomous mobile robots;lights model;classical oblivious robots model;grid-based terrain;2-dimensional Euclidean plane;neighboring grid points;grid setting;robot swarm model;arbitrary but distinct initial positions;arbitrary target pattern;asynchronous algorithms;arbitrary pattern formation algorithm","","","",19.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"The Universal Gossip Fighter","A. Gorbunova; R. Guerraoui; A. -M. Kermarrec; A. Kucherenko; R. Pinot","Ecole Polytechnique Fédérale de Lausanne; Ecole Polytechnique Fédérale de Lausanne; Ecole Polytechnique Fédérale de Lausanne; Ecole Polytechnique Fédérale de Lausanne; Ecole Polytechnique Fédérale de Lausanne","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1162","1172","The notion of adversary is a staple of distributed computing. An adversary typically models “hostile” assumptions about the underlying distributed environment, e.g., a network that can drop messages, an operating system that can delay processes or an attacker that can hack machines. So far, the goal of distributed computing researchers has mainly been to develop a distributed algorithm that can face a given adversary, the abstraction characterizing worst-case scenarios. This paper initiates the study of the somehow opposite approach. Given a distributed algorithm, the adversary is the abstraction we seek to implement. More specifically, we consider the problem of controlling the spread of messages in a large-scale system, conveying the practical motivation of limiting the dissemination of fake news or viruses. Essentially, we assume a general class of gossip protocols, called all-to-all gossip protocols, and devise a practical method to hinder the dissemination. We present the Universal Gossip Fighter (UGF). Just like classical adversaries in distributed computing, UGF can observe the status of a dissemination and decide to stop some processes or delay some messages. The originality of UGF lies in the fact that it is universal, i.e., it applies to any all-to-all gossip protocol. We show that any gossip protocol attacked by UGF ends up exhibiting a quadratic message complexity (in the total number of processes) if it achieves sublinear time of dissemination. We also show that if a gossip protocol aims to achieve a message complexity $\alpha$ times smaller than quadratic, then the time complexity rises exponentially in relation to $\alpha$. We convey the practical relevance of our theoretical findings by implementing UGF and conducting a set of empirical experiments that confirm some of our results.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820658","Distributed Computing;Gossip Protocols;Adaptive Adversaries","Protocols;Limiting;Operating systems;Delays;Complexity theory;Large-scale systems;Distributed computing","communication complexity;computational complexity;computer crime;computer network security;distributed algorithms;protocols","models hostile assumptions;distributed computing researchers;distributed algorithm;given adversary;abstraction;gossip protocol;universal gossip fighter;UGF;classical adversaries;quadratic message complexity","","","",25.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Modeling Matrix Engines for Portability and Performance","N. Tukanov; R. Srinivasaraghavan; J. E. Moreira; T. M. Low","Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA; IBM Systems, Austin, TX; IBM Research, Yorktown Heights, NY; Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1173","1183","Matrix engines, also known as matrix-multiplication accel-erators, capable of computing on 2D matrices of various data types are traditionally found only on GPUs. However, they are increasingly being introduced into CPU architectures to support AI/ML computations. Unlike traditional SIMD functional units, these accelerators require both the input and output data to be packed into a specific 2D-data layout that is often dependent on the input and output data types. Due to the large variety of supported data types and architectures, a common abstraction is required to unify these seemingly disparate accelerators and more efficiently produce high-performance code. In this paper, we show that the hardware characteristics of a vast array of different matrix engines can be unified using a single analytical model that casts matrix engines as an accumulation of multiple outer-products (also known as rank-k updates). This allows us to easily and quickly develop high-performance kernels using matrix engines for different architectures. We demonstrate our matrix engine model and its portability by applying it to two distinct architectures. Using our model, we show that high-performance computational kernels and packing routines required for high-performance dense linear algebra libraries can be easily designed. Furthermore, we show that the performance attained by our implementations is around 90–99 % (80–95 % on large problems) of the theoretical peak throughput of the matrix engines.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00117","Defense Advanced Research Projects Agency (DARPA)(grant numbers:HR00112190099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820727","MMA;VNNI;Matrix Engine;High Performance Computing;Dense Linear Algebra","Analytical models;Computational modeling;Linear algebra;Machine learning;Writing;Throughput;Libraries","linear algebra;mathematics computing;matrix algebra;matrix multiplication;parallel architectures","matrix-multiplication accelerators;2D-data layout;output data types;supported data types;high-performance code;high-performance computational kernels;packing routines;high-performance dense linear algebra libraries;matrix engine modeling;2D matrices;CPU architectures;AI;ML;hardware characteristics","","","",42.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"MLCNN: Cross-Layer Cooperative Optimization and Accelerator Architecture for Speeding Up Deep Learning Applications","B. Jiang; X. Cheng; S. Tang; X. Ma; Z. Gu; S. Fu; Q. Yang; M. Liu","University of North Texas; University of North Texas; University of North Texas; University of North Texas; University of North Texas; University of North Texas; University of North Texas; Los Alamos National Laboratory","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1184","1194","The ever-increasing number of layers, millions of parameters, and large data volume make deep learning workloads resource-intensive and power-hungry. In this paper, we develop a convolutional neural network (CNN) acceleration framework, named MLCNN, which explores algorithm-hardware co-design to achieve cross-layer cooperative optimization and acceleration. MLCNN dramatically reduces computation and on-off chip communication, improving CNN's performance. To achieve this, MLCNN reorders the position of nonlinear activation layers and pooling layers, which we prove results in a negligible accuracy loss; then the convolutional layer and pooling layer are co-optimized by means of redundant multiplication elimination, local addition reuse, and global addition reuse. To the best of our knowledge, MLCNN is the first of its kind that incorporates cooperative optimization across convolutional, activation, and pooling layers. We further customize the MLCNN accelerator to take full advantage of cross-layer CNN optimization to reduce both computation and on-off chip communication. Our analysis shows that MLCNN can significantly reduce (up to 98%) multiplications and additions. We have implemented a prototype of MLCNN and evaluated its performance on several widely used CNN models using both an accelerator-level cycle and energy model and RTL implementation. Experimental results show that MLCNN achieves 3.2x speedup and 2.9x energy efficiency compared with dense CNNs. MLCNN's optimization methods are orthogonal to other CNN acceleration techniques, such as quantization and pruning. Combined with quantization, our quantized MLCNN gains a 12.8x speedup and 11.3x energy efficiency compared with DCNN.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00118","National Science Foundation(grant numbers:CNS-2113805,CNS-1852134,OAC-2017564,ECCS-2010332,CNS-2037982,CNS-1563750,CNS-1828105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820611","Deep learning;Cross-layer optimization;Accelerators;Performance evaluation","Deep learning;Quantization (signal);Recurrent neural networks;Power demand;Statistical analysis;Computational modeling;Optimization methods","convolutional neural nets;deep learning (artificial intelligence);optimisation;power aware computing;telecommunication computing","energy model;CNN acceleration;quantized MLCNN gains;accelerator architecture;deep learning;convolutional neural network acceleration;cross-layer cooperative optimization;on-off chip communication;nonlinear activation layers;pooling layer;convolutional layer;MLCNN accelerator;cross-layer CNN optimization;accelerator-level cycle;MLCNN optimization;algorithm-hardware co-design","","","",35.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Shared-Memory Parallel Algorithms for Fully Dynamic Maintenance of 2-Connected Components","C. A. Haryan; G. Ramakrishna; K. Kothapalli; D. S. Banerjee","Indian Institute of Technology, Tirupati, India; Indian Institute of Technology, Tirupati, India; International Institute of Information Technology, Hyderabad, India; Indian Institute of Technology, Jodhpur, India","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1195","1205","Finding the biconnected components of a graph has a large number of applications in many other graph problems including planarity testing, computing the centrality metrics, finding the (weighted) vertex cover, coloring, and the like. Recent years saw the design of efficient algorithms for this problem across sequential and parallel computational models. However, current algorithms do not work in the setting where the underlying graph changes over time in a dynamic manner via the insertion or deletion of edges. Dynamic algorithms in the sequential setting that obtain the biconnected components of a graph upon insertion or deletion of a single edge are known from over two decades ago. Parallel algorithms for this problem are not heavily studied. In this paper, we design shared-memory parallel algorithms that obtain the biconnected components of a graph subsequent to the insertion or deletion of a batch of edges. Our algorithms hence will be capable of exploiting the parallelism adduced due to a batch of updates. We implement our algorithms on an AMD EPYC 7742 CPU having 128 cores. Our experiments on a collection of 10 real-world graphs from multiple classes indicate that our algorithms outperform parallel state-of-the-art static algorithms.11The implementation and an extended version of this paper is at [5].","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820712","biconnected components;cut vertices;cut edges;dynamic algorihtm;multi core;parallel","Semiconductor device modeling;Measurement;Distributed processing;Heuristic algorithms;Instruction sets;Computational modeling;Maintenance engineering","computational complexity;graph theory;microprocessor chips;parallel algorithms;shared memory systems","parallelism;real-world graphs;shared-memory parallel algorithms;fully dynamic maintenance;biconnected components;graph problems;planarity testing;centrality metrics;vertex cover;sequential models;parallel computational models;graph changes;dynamic manner;dynamic algorithms;sequential setting;single edge;design shared-memory;AMD EPYC 7742 CPU;parallel state-of-the-art static algorithms","","","",20.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Learning Intermediate Representations using Graph Neural Networks for NUMA and Prefetchers Optimization","A. TehraniJamsaz; M. Popov; A. Dutta; E. Saillard; A. Jannesari","Iowa State University, Ames, Iowa, USA; Inria, Bordeaux, France; Iowa State University, Ames, Iowa, USA; Inria, Bordeaux, France; Iowa State University, Ames, Iowa, USA","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1206","1216","There is a large space of NUMA and hardware prefetcher configurations that can significantly impact the performance of an application. Previous studies have demonstrated how a model can automatically select configurations based on the dynamic properties of the code to achieve speedups. This paper demonstrates how the static Intermediate Representation (IR) of the code can guide NUMA/prefetcher optimizations without the prohibitive cost of performance profiling. We propose a method to create a comprehensive dataset that includes a diverse set of intermediate representations along with optimum configurations. We then apply a graph neural network model in order to validate this dataset. We show that our static intermediate representation based model achieves 80 % of the performance gains provided by expensive dynamic performance profiling based strategies. We further develop a hybrid model that uses both static and dynamic information. Our hybrid model achieves the same gains as the dynamic models but at a reduced cost by only profiling 30 % of the programs.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00120","Inria; CNRS (LABRI and IMB); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820690","OpenMP;NUMA;prefetching;graph neural networks;LLVM Intermediate Representation","Costs;Codes;Prefetching;Performance gain;Dynamic scheduling;Graph neural networks;Hardware","graph theory;learning (artificial intelligence);neural nets;storage management","hardware prefetcher configurations;prohibitive cost;static intermediate representation based model;static information;dynamic information;graph neural networks;dynamic performance profiling based strategies;prefetcher optimization;NUMA optimization","",2.0,"",57.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"HDagg: Hybrid Aggregation of Loop-carried Dependence Iterations in Sparse Matrix Computations","B. Zarebavani; K. Cheshmi; B. Liu; M. M. Strout; M. M. Dehnavi","Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Toronto; Department of Computer Science, University of Arizona; Department of Computer Science, University of Toronto","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1217","1227","This paper proposes a novel aggregation algorithm, called Hybrid DAG Aggregation (HDagg), that groups iterations of sparse matrix computations with loop carried dependence to improve their parallel execution on multicore processors. Prior approaches to optimize sparse matrix computations fail to provide an efficient balance between locality, load balance, and synchronization and are primarily optimized for codes with a tree-structure data dependence. HDagg is optimized for sparse matrix computations that their data dependence graphs (DAGs) do not have a tree structure, such as incomplete matrix factorization algorithms. It uses a hybrid approach to aggregate vertices and wavefronts in the DAG of a sparse computation to create well-balanced parallel workloads with good locality. Across three sparse kernels, triangular solver, incomplete Cholesky, and incomplete LU, HDagg outperforms existing sparse libraries such as MKL with an average speedup of 3.56× and is faster than state-of-the-art inspector-executor approaches that optimize sparse computations, i.e. DAGP, LBC, wavefront parallelism techniques, and SpMP by an average speedup of 3.87×, 3.41×, 1.95×, and 1.43× respectively.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820651","Parallelism;Sparse Matrix Computations;Loop-carried Dependence","Measurement;Codes;Program processors;Multicore processing;Parallel processing;Load management;Computational efficiency","graph theory;iterative methods;matrix decomposition;multiprocessing systems;parallel processing;program control structures;resource allocation;sparse matrices;tree data structures","HDagg;Hybrid Aggregation;loop-carried dependence iterations;sparse matrix computations;novel aggregation algorithm;called Hybrid DAG Aggregation;tree-structure data dependence;data dependence graphs;incomplete matrix factorization algorithms;sparse computation","",1.0,"",46.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Alias-Chain: Improving Blockchain Scalability via Exploring Content Locality among Transactions","J. Liu; S. Wan; X. He","Huazhong University of Sci. and Tech.; Huazhong University of Sci. and Tech.; Temple University","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1228","1238","A Blockchain is a promising infrastructure but it has serious scalability problems, i.e., long block synchronization time and high storage cost. Conventional coarse-grained data deduplication schemes (block or file level) are proved to be ineffective on this problem. Based on comprehensive analysis on typical blockchain workloads, we are the first to propose two new locality concepts: economic and argument locality. To further explore these new localities, we propose a novel fine-grained data deduplication scheme (transaction level) named Alias-Chain to improve the scalability of blockchains. Specifically, Alias-Chain replaces frequently used data, e.g., smart contract arguments, with much shorter aliases to reduce the block size. During prop-agation and preservation of blocks, smaller blocks result in both shorter synchronization time and lower storage cost. Simulation results show the average transfer and SC-call transaction sizes can be reduced by up to 11.23% and 43.23% in native Ethereum, and up to 61.95 % and 77.54 % in Ethereum optimized by state-of-the-art techniques, respectively. Prototyping-based experiments are further conducted on a testbed consisting of up to 3200 miners. The results demonstrate the effectiveness and efficiency of Alias-Chain on reducing block synchronization time and storage cost under typical real-world workloads.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00122","National Natural Science Foundation of China(grant numbers:61972445); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820700","Blockchain;Scalability;Locality;Deduplication","Distributed processing;Costs;Scalability;Simulation;Smart contracts;Blockchains;Synchronization","blockchains;contracts;storage management;synchronisation","content locality;block synchronization time;economic argument locality;transaction level;smart contract arguments;block size;storage cost;SC-call transaction sizes;blockchain scalability;alias-chain;fine-grained data deduplication;coarse-grained data deduplication;blockchain workloads;economic locality;argument locality;Ethereum","","","",35.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"SFP: Service Function Chain Provision on Programmable Switches for Cloud Tenants","H. Huang; W. Wu; Y. He; Z. Guo","Tsinghua University; Peking University; Tsinghua University; Beijing Institute of Technology","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1239","1249","Recent progress in programmable switches provides opportunities for service function chains (SFCs) provision to cloud tenants, which has the advantage of flexible deployment and high performance. We devise SFP for such SFC provision in the cloud. SFP's data plane installs physical NFs and is virtualized to host logical SFCs from multiple tenants. SFP's control plane uses a relaxed integer programming model to jointly optimize the placement of physical and logical NFs, which can achieve resource efficiency and high tenant traffic processing throughput within efficient execution time. Our prototype and evaluation shows that SFP can significantly offload NFV computation from server to the switch and maximize the switch resource utilization.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00123","National Natural Science Foundation of China(grant numbers:61802225,62002019); Beijing Institute of Technology Research Fund Program for Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820721","NFV;programmable switch","Integer programming;Cloud computing;Runtime;Service function chaining;Pipelines;Prototypes;Process control","cloud computing;integer programming;resource allocation;virtualisation","switch resource utilization;service function chain provision;programmable switches;cloud tenants;service function chains provision;SFC provision;logical SFCs;relaxed integer programming model;logical NFs;traffic processing throughput;SFP control plane;SFP data plane;NFV computation","","","",49.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"An Efficient Block Validation Mechanism for UTXO-based Blockchains","X. Dai; B. Xiao; J. Xiao; H. Jin","National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1250","1260","It has been recognized that one of the bottlenecks in the UTXO-based blockchain systems is the slow block validation - the process of validating a newly-received block by a node before locally storing it and further broadcasting it. As a block contains multiple inputs, the block validation mainly involves checking the inputs against the status data, which is also known as the Unspent Transaction Outputs (UTXO) set. As time goes by, the UTXO set becomes more and more expansive, most of which can only be stored on disks. This considerably slows down the input checking and thus block validation, which can potentially compromise system security. To deal with the above problem, we disassemble the function of input checking into three parts: existence validation (EV), unspent validation (UV), and script validation (SV). Based on the disassembly, we propose EBV, an efficient block validation mechanism to speed up EV, UV, and SV individually. First, EBV changes the representation of status data, from UTXO set to a bit-vector set, which drastically reduces its size. The smaller status data can be entirely maintained in memory, thereby accelerating UV and also block validation. Second, EBV requires each transaction to carry the proof data, which enables EV and SV without accessing the disks. Furthermore, we also cope with two challenges in the design of EBV, namely transaction inflation and fake positions. To evaluate the EBV mechanism, we implement a prototype on top of Bitcoin, the most widely known UTXO-based blockchain, and conduct extensive experiments to compare EBV and Bitcoin. The experimental results demonstrate that EBV successfully reduces the memory requirement by 93.1 % and the block validation time by up to 93.5%.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00124","National Key Research and Development Program of China(grant numbers:2021Yfb2700700); National Natural Science Foundation of China(grant numbers:62072197); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820744","Blockchain;Bitcoin;UTXO;UTXO set;block validation","Distributed processing;Memory management;Prototypes;Bitcoin;Broadcasting;Blockchains;Delays","computational complexity;data integrity;data structures;transaction processing;vectors","efficient block validation mechanism;UTXO-based blockchains;UTXO-based blockchain systems;slow block validation;status data;UTXO set;input checking;unspent validation;script validation;EBV;widely known UTXO-based blockchain;block validation time","",1.0,"",30.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"DEAN: A Lightweight and Resource-efficient Blockchain Protocol for Reliable Edge Computing","A. Al-Mamun; H. Shen; D. Zhao","University of Nevada, Reno; University of Nevada, Reno; University of Nevada, Reno","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1261","1271","Edge computing draws a lot of recent research interests because of the performance improvement by offloading many workloads from the remote data center to nearby edge nodes. Nonetheless, one open challenge of this emerging paradigm lies in the potential security issues on edge nodes. This paper proposes a cooperative protocol, namely DEAN, equipped with a unique resource-efficient quorum building mechanism to adopt blockchain seamlessly in an edge computing infrastructure to prevent data manipulation and allow fair data sharing with quick recovery under resource constraints of limited storage, computing, and network capacity. Specifically, DEAN leverages a parallel mechanism equipped with three independent core components, effectively achieving low resource consumption while allowing secured parallel block processing on edge nodes. We have implemented a system prototype based on DEAN and experimentally verified its effectiveness with a comparison with four popular blockchain implementations: Ethereum, Parity, IOTA, and Hyperledger Fabric. Experimental results show that the system prototype exhibits high resilience to arbitrary failures. Performance-wise, DEAN-based blockchain implementation out-performs the state-of-the-art blockchain systems with up to 88.6 x higher throughput and 26 x lower latency.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820728","Edge computing;Blockchains;Distributed computing;Consensus protocols","Voting;Prototypes;Throughput;Fabrics;Blockchains;Safety;Security","blockchains;computer network reliability;cryptographic protocols","Hyperledger Fabric;IOTA;Parity;Ethereum;secured parallel block;low resource consumption;parallel mechanism;DEAN leverages;resource constraints;data manipulation;unique resource-efficient quorum;potential security issues;nearby edge nodes;remote data center;reliable edge computing;resource-efficient blockchain protocol;DEAN-based blockchain implementation","","","",43.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"PowerSpector: Towards Energy Efficiency with Calling-Context-Aware Profiling","X. You; H. Yang; Z. Xuan; Z. Luan; D. Qian","School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1272","1282","Energy efficiency has become one of the major concerns in high-performance computing systems towards exascale. On mainstream systems, dynamic voltage and frequency scaling (DVFS) and uncore frequency scaling (UFS) are two popular techniques to trade-off performance and power consumption to achieve better energy efficiency. However, the existing system software is oblivious to application characteristics and thus misses the opportunity for fine-grained power management. Meanwhile, manually instrumenting applications with power management codes are prohibitive due to heavy engineering efforts and thus hardly portable across platforms. In this paper, we propose Powerspector, a fine-grained code profiling and optimization tool with calling context awareness to automatically explore the opportunity for optimizing energy efficiency. The design of Powerspector consists of three phases, including significant region detection, performance profiling and power modeling, and frequency optimization. The first phase automatically identifies the profitable regions for frequency optimization. Then, the second phase guides the core/uncore frequency optimization with power models. The third phase injects frequency optimization codes targeting each significant code region across different calling contexts automatically. The experiment results demonstrate that Powerspector can achieve 1.13×(1.00×), 1.28×(1.09×), and 1.17×(1.06×) improvement on energy efficiency compared to static(region-based) tuning on Haswell, Broadwell, and Skylake platforms, respectively.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00126","National Key Research and Development Program of China(grant numbers:2020YFB1506703); National Natural Science Foundation of China(grant numbers:62072018); State Key Laboratory of Software Development Environment(grant numbers:SKLSDE-2021ZX-06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820673","performance profiling;energy efficiency;calling-context awareness;compilation optimization","Adaptation models;Codes;System dynamics;Power system management;Voltage;Switches;Energy efficiency","energy conservation;parallel processing;power aware computing;power consumption;ubiquitous computing","fine-grained power management;power management codes;fine-grained code profiling;optimization tool;performance profiling;power modeling;code region;PowerSpector;energy efficiency;calling-context-aware profiling;high-performance computing systems;uncore frequency scaling;power consumption;system software;frequency optimization codes;dynamic voltage and frequency scaling;DVFS;UFS;calling context awareness;region detection;frequency optimization;Skylake platforms;Broadwell platforms;Haswell platforms","","","",29.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"TagTree: Global Tagging Index with Efficient Querying for Time Series Databases","J. Xue; Z. Wang; T. Wang; Z. Shao","The Chinese University of Hong Kong Shatin, Hong Kong, N.T.; The Chinese University of Hong Kong Shatin, Hong Kong, N.T.; The Chinese University of Hong Kong Shatin, Hong Kong, N.T.; The Chinese University of Hong Kong Shatin, Hong Kong, N.T.","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1283","1293","Modern time series databases come with a tag-based query interface that allows users to select time series, which are essentially sequences of timestamped data values, based on a set of specific tags. A tagging index is an important component that can efficiently provide such tag-based services. However, existing methods store tag information in external databases or time-partitioned data structures, which has a negative impact on query performance. In this paper, we present a novel abstraction for efficient queries of tag information in time series databases: a hybrid tagging index that manages all tags in one place. By managing tag information globally in a single disk-based data structure, we can fundamentally relieve memory pressure and eliminate I/O overhead of duplicate metadata from existing methods. Furthermore, the tagging index is internally partitioned by time to support time range based queries and data retention which are essential to time series databases. We implement the proposed tagging index as a standalone module which can be integrated with time series storage engines. Experiments on the TSBS benchmark show our proposed method can significantly speed up queries by on average 84.0% and 87.2% compared to Prometheus (using a time-partitioned segment method) and Graphite (using an external database for tag management), respectively.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00127","The Chinese University of Hong Kong(grant numbers:4055151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820720","time series databases;tag index","Databases;Time series analysis;Graphite;Tagging;Metadata;Data structures;Throughput","data structures;database management systems;image segmentation;indexing;information retrieval;meta data;query processing;time series","query performance;efficient queries;hybrid tagging index;single disk-based data structure;time range based queries;time series storage engines;time-partitioned segment method;external database;tag management;global tagging index;efficient querying;modern time series databases;tag-based query interface;timestamped data values;specific tags;tag-based services;existing methods store tag information;time-partitioned data structures","","","",18.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"An End-to-end and Adaptive I/O Optimization Tool for Modern HPC Storage Systems","B. Yang; Y. Zou; W. Liu; W. Xue","National Supercomputing Center, Wuxi; National Supercomputing Center, Wuxi; National Supercomputing Center, Wuxi; National Supercomputing Center, Wuxi","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1294","1304","Real-world large-scale applications expose more and more pressures to storage services of modern supercomputers. Supercomputers have been introducing new storage devices and technologies to meet the performance requirements of various applications, leading to more complicated architectures. High I/O demand of applications and the complicated and shared storage architectures make the issues, such as unbalanced load, I/O interference, system parameter configuration error, and node performance degradation, more frequently observed. And it is challenging to both achieve high I/O performance on application level and efficiently utilize scarce storage resources. We propose AIOT, an end-to-end and adaptive I/O optimization tool for HPC storage systems, which introduces effective I/O performance modeling and several active tuning strategies to improve both the I/O performance of applications and the utilization of storage resources. AIOT provides a global view of the whole storage system and searches for the optimal end-to-end I/O path through flow network modeling. Moreover, AIOT tunes system parameters across multiple layers of the storage system by using the automated identified application I/O behaviors and the instant status of the workload of storage system. We verified the effectiveness of AIOT for balancing I/O load, resolving I/O interference, improving I/O performance by configuring appropriate system parameters, and avoiding I/O performance degradation caused by abnormal nodes through quite a few real-world cases. AIOT has helped to save over ten millions of core-hours during the deployment on Sunway TaihuLight since July 2021. It's worth mentioning that our proposed AIOT is capable of managing other I/O optimization methods across various storage platforms.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00128","National Key R&D Program of China(grant numbers:2017YFC1502203); National Natural Science Foundation of China(grant numbers:U1806205,61972231); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820667","I/O modeling;auto-tuning;load imbalance;resource allocation;Sunway TaihuLight","Degradation;Performance evaluation;Distributed processing;Adaptive systems;Optimization methods;Interference;Supercomputers","memory architecture;parallel machines;resource allocation;shared memory systems;storage management","large-scale applications;storage services;supercomputers;storage devices;performance requirements;shared storage architectures;system parameter configuration error;node performance degradation;storage resources;AIOT;system parameters;HPC storage systems;adaptive I/O optimization tool;I/O interference;I/O performance modeling;optimal end-to-end I/O path;flow network modeling;automated identified application I/O behaviors;I/O load balancing","",1.0,"",46.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"The Fast and Scalable MPI Application Launch of the Tianhe HPC system","Y. Dai; Y. Dong; M. Xie; K. Lu; R. Wang; M. Shao; J. Chen","College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1305","1315","Fast and scalable MPI application launch helps achieve exascale performance and is becoming a common goal in high-performance computing. However, the traditional launch technique suffers from scalability deficiencies in the global information exchange and the global barrier operation. This drawback makes it challenging to launch MPI applications quickly in large-scale systems. In this paper, we propose a fast and scalable application launch technique and details its associated hardware and software support. The optimized launch technique includes a locality-aware static address generation rule for eliminating the need for address exchange and a topology-aware global communication scheme for improving global communication efficiency. We also propose an optimized application launch sequence for supporting the above launch technique. We implement and evaluate the proposed launch technique on the Tianhe-2A supercomputer and the Tianhe Exascale Prototype Upgrade System. Experimental results show that our technique can reduce the launch time by 26.1% when launching an application with 256K processes.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00129","NSF(grant numbers:61902405); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820711","","Costs;Scalability;High performance computing;Prototypes;Supercomputers;Software;Hardware","application program interfaces;message passing;parallel processing","Tianhe HPC system;global information exchange;optimized launch technique;locality-aware static address generation rule;topology-aware global communication scheme;optimized application launch sequence;Tianhe Exascale Prototype Upgrade System;fast MPI application launch;scalable MPI application launch;high performance computing;address exchange;Tianhe-2A supercomputer","","","",42.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks","Y. Jia; G. Xu; C. W. Sung; S. Mostafa; Y. Wu","Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin, China; Department of Electrical Engineering, City University of Hong Kong, Hong Kong, SAR, China; Department of Electrical Engineering, City University of Hong Kong, Hong Kong, SAR, China; College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, UK","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1316","1326","Distributed data services usually rely on consensus protocols like Paxos and Raft to provide fault-tolerance and data consistency across global and local-distributed data centers. Erasure coding replication has appealing storage and network cost saving compared with full copy replication, which helps consensus protocols achieve low latency, high fault tolerance, and high throughput for data access. Applying erasure coding in consensus protocols directly will degrade the liveness level when the number of failure servers reaches a certain level. To address the challenge, CRaft just stores full copy replication instead of erasure coding replication when the number of failed servers reaches a certain threshold. In such situation, CRaft will be downgraded sharply to the same storage and network costs as Raft. To overcome the shortcoming of CRaft, we propose a protocol, called HRaft, which can adapt the placement of data blocks in order to always have enough blocks to recover the stored value when servers fail. By replenishing some coded blocks in healthy servers instead of full copy replication, it can avoid switching to the full replication when a certain threshold on the number of failures is reached. We designed and implemented a key-value (KV) storage prototype to validate the proposed protocol and evaluate its performance. The experimental results show HRaft can significantly reduce storage and network costs and improve write performance while keeping the liveness level compared to CRaft.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00130","National Natural Science Foundation of China(grant numbers:61971309); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820714","Erasure coding;Consensus protocol;Raft;Paxos;Fault tolerance;Network storage","Fault tolerance;Costs;System performance;Fault tolerant systems;Distributed databases;Switches;Throughput","computer centres;distributed processing;protocols;software fault tolerance;storage management","CRaft;Raft;data blocks;coded blocks;copy replication;key-value storage prototype;HRaft;liveness level;adaptive erasure coded data maintenance;distributed networks;data services;consensus protocols;data consistency;data centers;erasure coding replication;network cost saving;fault tolerance;data access;Paxos","","","",23.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"CSC: Collaborative System Configuration for I/O-Intensive Applications in Multi-Tenant Clouds","H. Huang; P. Pang; Q. Chen; J. Zhao; W. Zheng; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1327","1337","I/O-intensive applications are important workloads of public clouds. Multiple cloud applications co-run on the same physical machine in different virtual machines (VMs), and the shared resources (e.g., disk bandwidth) are often isolated for fairness. Our investigation shows that the performance of an I/O-intensive application is impacted by both disk bandwidth allocation and the page cache settings in the guest operating system. However, none of prior work considers adjusting the page cache settings for better performance, when the disk bandwidth allocation is adjusted. We therefore propose CSC, a system that collaboratively identifies the appropriate disk bandwidth allocation and page cache settings in the guest operating system of each VM. CSC aims to improve the system-wide I/O throughput of the physical machine, while also improve the I/O throughput of each individual I/O-intensive application in VMs. CSC comprises an online disk bandwidth allocator and an adaptive dirty page setting optimizer. The bandwidth allocator monitors the disk bandwidth utilization and re-allocates some bandwidth from free VMs to busy VMs periodically. After the re-allocation, the opti-mizer identifies the appropriate dirty page settings in the guest operating system of the VMs using Bayesian Optimization. The experimental results show that CSC improves the performance of I/O-intensive applications by 9.5 % on average (up to 17.29 %) when 5 VMs are co-located while fairness is guaranteed.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00131","National Natural Science Foundation of China(grant numbers:62022057,61832006,61872240); Shanghai international science and technology collaboration project(grant numbers:21510713600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820741","I/O-intensive applications;page cache;multi-tenant clouds","Cloud computing;Operating systems;Bandwidth;Channel allocation;Throughput;Virtual machining;Bayes methods","bandwidth allocation;cache storage;cloud computing;operating systems (computers);virtual machines","CSC;collaborative system configuration;multitenant clouds;public clouds;multiple cloud applications;physical machine;different virtual machines;page cache settings;guest operating system;appropriate disk bandwidth allocation;system-wide;online disk bandwidth allocator;adaptive dirty page setting optimizer;disk bandwidth utilization;appropriate dirty page settings;5 VMs","","","",32.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Archpipe: Fast and Flexible Pipelined Erasure-coded Archival Scheme for Heterogeneous Networks","B. Xu; J. Huang; X. Qin; Q. Cao; Y. Dong; W. Kong","Wuhan National Laboratory for Optoelectronics, Huazhong University of Sci.& Tech; Wuhan National Laboratory for Optoelectronics, Huazhong University of Sci.& Tech; Auburn University; Wuhan National Laboratory for Optoelectronics, Huazhong University of Sci.& Tech; Alibaba Group; Alibaba Group","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1338","1348","Erasure-coded archival converts the redundancy mechanism of low access-frequency data from replication to erasure coding for balancing access performance and storage efficiency. A variety of pipelined schemes are designed to speed up the archival operation, however they neglect such three factors as heterogeneous network, under-utilization of replica resources and tight coupling with underlying platforms which restrict or even negate the performance gains. In this paper, we propose Archpipe, a fast and flexible pipelined erasure-coded archival scheme. It exhibits three distinct features: 1) heterogeneous network awareness, for a single-pipelined construction, sufficient-bandwidth links are given high scheduling priority to avoid network congestion, while considering locality to reducing network transmissions; 2) parallel encoding, the unused replica resources are exploited to adaptively construct multiple pipelines for each stripe based on the single-pipelined algorithm, thereby enabling parity blocks to be encoded in parallel; 3) loose coupling, it does not rely on specific block placement policies and stripe construction algorithms. Experimental results indicate that, Archpipe can be seamlessly integrated with common distributed storage systems, and it improves the erasure-coded archival performance by 3.6 ∼ 4.7× and 1.3 ∼ 2.6× in on-disk and in-memory scenarios, respectively.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00132","National key research and development program of China(grant numbers:2018Y-FA0701800); NSFC(grant numbers:62172175); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820635","erasure coding;replication;heterogeneous network;pipelined archival","Couplings;Distributed processing;Systematics;Shape;Redundancy;Boosting;Heterogeneous networks","computer network reliability;encoding;information retrieval systems;pipeline processing;storage management;telecommunication congestion control;telecommunication scheduling","distributed storage systems;parity blocks;parallel encoding;network transmissions;network congestion;heterogeneous network awareness;storage efficiency;erasure coding;erasure-coded archival performance;single-pipelined algorithm;single-pipelined construction;archival operation;low access-frequency data;flexible pipelined erasure-coded archival scheme;Archpipe","","","",35.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"A Quantitative Study of the Spatiotemporal I/O Burstiness of HPC Application","W. Yang; X. Liao; D. Dong; J. Yu","College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; China Aerodynamics Research and Development Center, Computational Aerodynamics Institute, Mianyang, China","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1349","1359","Understanding the I/O characteristics of applications on supercomputers is crucial to paving the path for application optimization and system resource allocation. We collect and analyze I/O traces of applications on a production supercomputer and reconfirm that I/O bursts exist in most applications. What's more, we find that the I/O bursts not only occur in short periods of time but also originate from a minority of adjacent compute nodes allocated to the applications, which we call spatiotemporal I/O burstiness. The concentration of I/O traffic in both time and space dimension will make applications experience poor I/O performance and incur I/O inefficiency of the storage system. Although there are some solutions, such as burst buffer, can help alleviate such inefficiency, there is still no work that measures, analyzes and further predicts the application I/O characteristic in terms of spatiotemporal burstiness, which we think is vital for application-aware optimizations, including but not limited to burst buffer allocation and job scheduling. In this paper, we first propose a mathematical model to measure the spatiotemporal I/O burstiness. Then a thorough analysis on the spatiotemporal I/O characteristic of all applications on the system is elaborated. We further make use of the job's submitting path to explore the I/O characteristic similarity among jobs, based on which a machine learning classification algorithm is proposed to accurately predict the job spatiotemporal I/O burstiness in advance. With accurate job I/O characteristic at hand, some useful suggestions are put forward to hedge the impacts of the spatiotemporal I/O burstiness.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820616","Spatiotemporal Burstiness;I/O Behavior;Application-aware;Machine Learning","Distributed processing;Production;Machine learning;Supercomputers;Mathematical models;Spatiotemporal phenomena;Classification algorithms","data mining;learning (artificial intelligence);mainframes;parallel machines;parallel processing;performance evaluation;resource allocation;scheduling","storage system;burst buffer;spatiotemporal burstiness;application-aware optimizations;job spatiotemporal;HPC application;supercomputers;application optimization;production supercomputer;reconfirm;adjacent compute;space dimension","","","",28.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"DENOVA: Deduplication Extended NOVA File System","H. Kwon; Y. Cho; A. Khan; Y. Park; Y. Kim","Dept. of Computer Science and Engineering, Sogang University, Seoul, South Korea; Dept. of Computer Science and Engineering, Sogang University, Seoul, South Korea; Oak Ridge National Laboratory, TN, USA; Dept. of Computer Science and Engineering, Sogang University, Seoul, South Korea; Dept. of Computer Science and Engineering, Sogang University, Seoul, South Korea","2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","15 Jul 2022",2022,"","","1360","1371","This paper shows mathematically and experimentally that inline deduplication is not suitable for file systems on ultra-low latency Intel Optane DC PM devices in terms of performance, and proposes DeNova, an offline deduplication specially designed for log-structured NVM file systems such as NOVA. DeNova offers high-performance and low-latency I/O processing and executes deduplication in the background without interfering with foreground I/Os. DeNova employs DRAM-free persistent deduplication metadata, favoring CPU cache line, and ensures failure consistency on any system failure. We implement DeNova in the NOVA file system. Evaluation with DeNova confirms a negligible performance drop of baseline NOVA of less than 1%, while gaining high storage space savings. Extensive experiments show DeNova is failure consistent in all failure scenario cases.","1530-2075","978-1-6654-8106-9","10.1109/IPDPS53621.2022.00134","National Research Foundation of Korea(NRF)(grant numbers:NRF-2021R1A2C2014386); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9820647","Non-Volatile Memory;File System;Deduplication;Consistency","Performance evaluation;Degradation;Distributed processing;File systems;Nonvolatile memory;Metadata;Indexes","failure analysis;random-access storage","DENOVA;deduplication extended NOVA file system;inline deduplication;Intel Optane DC PM devices;DeNova;offline deduplication;log-structured NVM file systems;DRAM-free persistent deduplication metadata;system failure;CPU cache line","",2.0,"",23.0,"IEEE","15 Jul 2022","","","IEEE","IEEE Conferences"
"Guest Editorial","P. Balaji; J. Zhai; M. Si","NA; NA; NA","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2021",2021,32.0,7.0,1511,1512,"The papers in this special section present the state-of-the-art technologies and the challenges of parallel and distributed computing techniques for artificial intelligence (AI), machine learning (ML), and deep learning (DL). AI, ML, and DL have established themselves in a multitude of domains because of their ability to process and model unstructured input data.","1558-2183","","10.1109/TPDS.2020.3047357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9357353","","Special issues and sections;Parallel computing;Distributed computing;Artificial intelligence;Machine learning;Deep learning","","","","","",1.0,"IEEE","18 Feb 2021","","","IEEE","IEEE Journals"
"Biscotti: A Blockchain System for Private and Secure Federated Learning","M. Shayan; C. Fung; C. J. M. Yoon; I. Beschastnikh","University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1513,1525,"Federated Learning is the current state-of-the-art in supporting secure multi-party machine learning (ML): data is maintained on the owner's device and the updates to the model are aggregated through a secure protocol. However, this process assumes a trusted centralized infrastructure for coordination, and clients must trust that the central service does not use the byproducts of client data. In addition to this, a group of malicious clients could also harm the performance of the model by carrying out a poisoning attack. As a response, we propose Biscotti: a fully decentralized peer to peer (P2P) approach to multi-party ML, which uses blockchain and cryptographic primitives to coordinate a privacy-preserving ML process between peering clients. Our evaluation demonstrates that Biscotti is scalable, fault tolerant, and defends against known attacks. For example, Biscotti is able to both protect the privacy of an individual client's update and maintain the performance of the global model at scale when 30 percent adversaries are present in the system.","1558-2183","","10.1109/TPDS.2020.3044223","Natural Sciences and Engineering Research Council of Canada(grant numbers:2014-04870); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9292450","Distributed machine learning;blockchain;privacy;security","Peer-to-peer computing;Data models;Collaborative work;Training;Privacy;Machine learning;Training data","cryptography;data privacy;learning (artificial intelligence);peer-to-peer computing;protocols;security of data","owner;secure protocol;trusted centralized infrastructure;central service;client data;malicious clients;poisoning attack;Biscotti;fully decentralized peer;multiparty ML;privacy-preserving ML process;peering clients;individual client;global model;blockchain system;Federated Learning;secure multiparty machine learning;efficiency 30.0 percent","",41.0,"",90.0,"IEEE","11 Dec 2020","","","IEEE","IEEE Journals"
"Mutual Information Driven Federated Learning","M. P. Uddin; Y. Xiang; X. Lu; J. Yearwood; L. Gao","Deakin Blockchain Innovation Lab, School of Information Technology, Deakin University, Geelong, VIC, Australia; Deakin Blockchain Innovation Lab, School of Information Technology, Deakin University, Geelong, VIC, Australia; Deakin Blockchain Innovation Lab, School of Information Technology, Deakin University, Geelong, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia; Deakin Blockchain Innovation Lab, School of Information Technology, Deakin University, Geelong, VIC, Australia","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1526,1538,"Federated Learning (FL) is an emerging research field that yields a global trained model from different local clients without violating data privacy. Existing FL techniques often ignore the effective distinction between local models and the aggregated global model when doing the client-side weight update, as well as the distinction of local models for the server-side aggregation. In this article, we propose a novel FL approach with resorting to mutual information (MI). Specifically, in client-side, the weight update is reformulated through minimizing the MI between local and aggregated models and employing Negative Correlation Learning (NCL) strategy. In server-side, we select top effective models for aggregation based on the MI between an individual local model and its previous aggregated model. We also theoretically prove the convergence of our algorithm. Experiments conducted on MNIST, CIFAR-10, ImageNet, and the clinical MIMIC-III datasets manifest that our method outperforms the state-of-the-art techniques in terms of both communication and testing performance.","1558-2183","","10.1109/TPDS.2020.3040981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272656","Distributed learning;federated learning;parallel optimization;data parallelism;information theory;mutual information;communication bottleneck;data heterogeneity","Data models;Training;Computational modeling;Servers;Mathematical model;Convergence;Analytical models","data privacy;learning (artificial intelligence);mobile computing","emerging research field;global trained model;local clients;data privacy;FL techniques;effective distinction;local models;aggregated global model;client-side;weight update;aggregation;FL approach;MI;aggregated models;effective models;individual local model;previous aggregated model;mutual information driven federated learning;MNIST;CIFAR-10;ImageNet;MIMIC-III datasets","",8.0,"",35.0,"IEEE","26 Nov 2020","","","IEEE","IEEE Journals"
"Accelerating Federated Learning Over Reliability-Agnostic Clients in Mobile Edge Computing Systems","W. Wu; L. He; W. Lin; R. Mao","Department of Computer Science, University of Warwick, Coventry, U.K.; Department of Computer Science, University of Warwick, Coventry, U.K.; School of Computer Science and Engineering, South China University of Technology, Guangzhou, Guangdong Province, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong Province, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1539,1551,"Mobile Edge Computing (MEC), which incorporates the Cloud, edge nodes, and end devices, has shown great potential in bringing data processing closer to the data sources. Meanwhile, Federated learning (FL) has emerged as a promising privacy-preserving approach to facilitating AI applications. However, it remains a big challenge to optimize the efficiency and effectiveness of FL when it is integrated with the MEC architecture. Moreover, the unreliable nature (e.g., stragglers and intermittent drop-out) of end devices significantly slows down the FL process and affects the global model's quality in such circumstances. In this article, a multi-layer federated learning protocol called HybridFL is designed for the MEC architecture. HybridFL adopts two levels (the edge level and the cloud level) of model aggregation enacting different aggregation strategies. Moreover, in order to mitigate stragglers and end device drop-out, we introduce regional slack factors into the stage of client selection performed at the edge nodes using a probabilistic approach without identifying or probing the state of end devices (whose reliability is agnostic). We demonstrate the effectiveness of our method in modulating the proportion of clients selected and present the convergence analysis for our protocol. We have conducted extensive experiments with machine learning tasks in different scales of MEC system. The results show that HybridFL improves the FL training process significantly in terms of shortening the federated round length, speeding up the global model's convergence (by up to 12×) and reducing end device energy consumption (by up to 58 percent).","1558-2183","","10.1109/TPDS.2020.3040867","Worldwide Byte Security Information Technology Company Ltd.; Guangdong Project(grant numbers:2018B030325002); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164003); Guangzhou Science and Technology Program key projects(grant numbers:202007040002,201902010040); Guangzhou Development Zone Science and Technology(grant numbers:2018GH17); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272671","Federated learning;mobile edge computing;distributed computing;machine learning","Protocols;Data models;Training;Cloud computing;Performance evaluation;Distributed databases;Computer architecture","data privacy;learning (artificial intelligence);mobile computing;routing protocols","reliability-agnostic clients;mobile edge computing systems;edge nodes;data sources;privacy-preserving approach;AI applications;MEC architecture;FL process;multilayer federated learning protocol;HybridFL;machine learning;MEC system;FL training process;end device energy consumption;efficiency 58.0 percent","",17.0,"",25.0,"IEEE","26 Nov 2020","","","IEEE","IEEE Journals"
"An Efficiency-Boosting Client Selection Scheme for Federated Learning With Fairness Guarantee","T. Huang; W. Lin; W. Wu; L. He; K. Li; A. Y. Zomaya","School of Computer Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China; Department of Computer Science, University of Warwick, Coventry, United Kingdom; Department of Computer Science, University of Warwick, Coventry, United Kingdom; Department of Computer Science, State University of New York, New Paltz, NY, USA; School of Computer Science, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1552,1564,"The issue of potential privacy leakage during centralized AI's model training has drawn intensive concern from the public. A Parallel and Distributed Computing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new paradigm to cope with the privacy issue by allowing clients to perform model training locally, without the necessity to upload their personal sensitive data. In FL, the number of clients could be sufficiently large, but the bandwidth available for model distribution and re-upload is quite limited, making it sensible to only involve part of the volunteers to participate in the training process. The client selection policy is critical to an FL process in terms of training efficiency, the final model's quality as well as fairness. In this article, we will model the fairness guaranteed client selection as a Lyapunov optimization problem and then a C2MAB-based method is proposed for estimation of the model exchange time between each client and the server, based on which we design a fairness guaranteed algorithm termed RBCS-F for problem-solving. The regret of RBCS-F is strictly bounded by a finite constant, justifying its theoretical feasibility. Barring the theoretical results, more empirical data can be derived from our real training experiments on public datasets.","1558-2183","","10.1109/TPDS.2020.3040887","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164003); National Natural Science Foundation of China(grant numbers:62072187,61872084,61772205); Guangdong Major Project of Basic and Applied Basic Research(grant numbers:2019B030302002); Guangzhou Science and Technology Program key projects(grant numbers:202007040002,201902010040,201907010001); Fundamental Research Funds for the Central Universities(grant numbers:2019ZD26); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272649","Client selection;contextual combinatorial multi-arm bandit;fairness scheduling;federated learning;lyapunov optimization","Training;Data models;Collaborative work;Mathematical model;Servers;Optimization;Computer science","client-server systems;data privacy;learning (artificial intelligence);optimisation","efficiency-boosting client selection scheme;fairness guarantee;potential privacy leakage;federated learning;privacy issue;personal sensitive data;training process;client selection policy;FL process;fairness guaranteed client selection;Lyapunov optimization problem;RBCS-F algorithm;centralized AI","",37.0,"",27.0,"IEEE","26 Nov 2020","","","IEEE","IEEE Journals"
"FedSCR: Structure-Based Communication Reduction for Federated Learning","X. Wu; X. Yao; C. -L. Wang","Department of Computer Science, University of Hong Kong, Hong Kong, China; Department of Computer Science, University of Hong Kong, Hong Kong, China; Department of Computer Science, University of Hong Kong, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1565,1577,"Federated Learning allows edge devices to collaboratively train a shared model on their local data without leaking user privacy. The non-independent-and-identically-distributed (Non-IID) property of data distribution, which leads to severe accuracy degradation, and enormous communication overhead for aggregating parameters should be tackled in federated learning. In this article, we conduct a detailed analysis of parameter updates on the Non-IID datasets and compare the difference with the IID setting. Experimental results exhibit that parameter update matrices are structure-sparse and show that more gradients could be identified as negligible updates on the Non-IID data. As a result, we propose a structure-based communication reduction algorithm, called FedSCR, that reduces the number of parameters transported through the network while maintaining the model accuracy. FedSCR aggregates the parameter updates over channels and filters, identifies and removes the redundant updates by comparing the aggregated values with a threshold. Unlike the traditional structured pruning methods, FedSCR retains the complete model that does not require to be retrained and fine-tuned. The local loss and weight divergence on each device vary a lot because of the unbalanced data distribution. We further propose an adaptive FedSCR, that dynamically changes the bounded threshold, to enhance the model robustness on the Non-IID data. Evaluation results show that our proposed strategies achieve almost 50 percent upstream communication reduction without loss of accuracy. FedSCR can be integrated into state-of-the-art federated learning algorithms to dramatically reduce the number of parameters pushed to the global server with a tolerable accuracy reduction.","1558-2183","","10.1109/TPDS.2020.3046250","Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303442","Federated learning;communication reduction;Non-IID data","Training;Servers;Data models;Collaborative work;Adaptation models;Performance evaluation;Linear programming","data privacy;graph theory;iterative methods;learning (artificial intelligence);query processing","structure-based communication reduction algorithm;model accuracy;FedSCR aggregates;parameter updates;redundant updates;aggregated values;structured pruning methods;local loss;weight divergence;unbalanced data distribution;adaptive FedSCR;model robustness;NonIID data;federated learning;tolerable accuracy reduction;edge devices;shared model;NonIID datasets;parameter update matrices;upstream communication reduction","",9.0,"",39.0,"IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"Learning Spatiotemporal Failure Dependencies for Resilient Edge Computing Services","A. Aral; I. Brandić","Institute of Information Systems Engineering, Vienna University of Technology, Vienna, Austria; Institute of Information Systems Engineering, Vienna University of Technology, Vienna, Austria","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2021",2021,32.0,7.0,1578,1590,"Edge computing services are exposed to infrastructural failures due to geographical dispersion, ad hoc deployment, and rudimentary support systems. Two unique characteristics of the edge computing paradigm necessitate a novel failure resilience approach. First, edge servers, contrary to cloud counterparts with reliable data center networks, are typically connected via ad hoc networks. Thus, link failures need more attention to ensure truly resilient services. Second, network delay is a critical factor for the deployment of edge computing services. This restricts replication decisions to geographical proximity and necessitates joint consideration of delay and resilience. In this article, we propose a novel machine learning based mechanism that evaluates the failure resilience of a service deployed redundantly on the edge infrastructure. Our approach learns the spatiotemporal dependencies between edge server failures and combines them with the topological information to incorporate link failures. Ultimately, we infer the probability that a certain set of servers fails or disconnects concurrently during service runtime. Furthermore, we introduce Dependency- and Topology-aware Failure Resilience (DTFR), a two-stage scheduler that minimizes either failure probability or redundancy cost, while maintaining low network delay. Extensive evaluation with various real-world failure traces and workload configurations demonstrate superior performance in terms of availability, number of failures, network delay, and cost with respect to the state-of-the-art schedulers.","1558-2183","","10.1109/TPDS.2020.3046188","Rucon project; Austrian Science Fund; City of Vienna; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303420","Edge computing;failure resilience;quality of service;dependency learning;dynamic Bayesian networks","Servers;Resilience;Edge computing;Delays;Task analysis;Spatiotemporal phenomena;Reliability","ad hoc networks;cloud computing;computer centres;fault tolerant computing;Internet;learning (artificial intelligence);probability;resource allocation;scheduling;telecommunication network reliability;telecommunication network routing;telecommunication network topology","edge computing paradigm;rudimentary support systems;ad hoc deployment;infrastructural failures;resilient edge computing services;spatiotemporal failure;real-world failure;low network delay;failure probability;Topology-aware Failure Resilience;service runtime;incorporate link failures;edge server failures;spatiotemporal dependencies;edge infrastructure;necessitates joint consideration;geographical proximity;truly resilient services;ad hoc networks;reliable data center networks;edge servers;novel failure resilience approach","",12.0,"",58.0,"CCBY","22 Dec 2020","","","IEEE","IEEE Journals"
"Accelerating Gossip-Based Deep Learning in Heterogeneous Edge Computing Platforms","R. Han; S. Li; X. Wang; C. H. Liu; G. Xin; L. Y. Chen","Beijing Institute of Technology, Beijing, P. R. China; Beijing Institute of Technology, Beijing, P. R. China; Beijing Institute of Technology, Beijing, P. R. China; Beijing Institute of Technology, Beijing, P. R. China; Beijing Institute of Technology, Beijing, P. R. China; Delft University of Technology. Mekelweg 5, Delft, The Netherlands","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1591,1602,"With the exponential growth of data created at the network edge, decentralized and Gossip-based training of deep learning (DL) models on edge computing (EC) gains tremendous research momentum, owing to its capability to learn from resource-strenuous edge nodes with limited network connectivity. Today's edge devices are extremely heterogeneous, e.g., hardware and software stacks, and result in high performance variation of training time and inducing extra delay to synchronize and converge. The large body of prior art accelerates DL, being data or model parallelization, via a centralized server, e.g., parameter server scheme, which may easily turn into the system bottleneck or single point of failure. In this artice, we propose EdgeGossip, a framework specifically designed to accelerate the training process of decentralized and Gossip-based DL training for heterogeneous EC platforms. EdgeGossip features on: (i) low performance variation among multiple EC platforms during iterative training, and (ii) accuracy-aware training to fastly obtain best possible model accuracy. We implement EdgeGossip based on popular Gossip algorithms and demonstrate its effectiveness using real-world DL workloads, i.e., considerably reducing model training time by an average of 2.70 times while only incurring accuracy losses of 0.78 percent.","1558-2183","","10.1109/TPDS.2020.3046440","National Key Research and Development Plan of China(grant numbers:2018YFB1003701,2018YFB1003700); National Natural Science Foundation of China(grant numbers:61872337); Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung(grant numbers:407540_167266); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303468","Deep learning;decentralized training;gossip;edge computing","Training;Data models;Computational modeling;Peer-to-peer computing;Distributed databases;Acceleration;Servers","Internet;learning (artificial intelligence);protocols","multiple EC platforms;iterative training;accuracy-aware training;possible model accuracy;popular Gossip algorithms;model training time;2.70 times while only incurring accuracy losses;low performance variation;EdgeGossip features;heterogeneous EC platforms;training process;parameter server scheme;model parallelization;prior art accelerates DL;inducing extra delay;high performance variation;software stacks;edge devices;network connectivity;resource-strenuous edge nodes;tremendous research momentum;deep learning models;Gossip-based training;network edge;exponential growth;heterogeneous edge computing platforms;efficiency 0.78 percent","",12.0,"",49.0,"IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"Distributed Task Migration Optimization in MEC by Extending Multi-Agent Deep Reinforcement Learning Approach","C. Liu; F. Tang; Y. Hu; K. Li; Z. Tang; K. Li","National Supercomputing Center in Changsha, Changsha, Hunan, China; National Supercomputing Center in Changsha, Changsha, Hunan, China; National Supercomputing Center in Changsha, Changsha, Hunan, China; National Supercomputing Center in Changsha, Changsha, Hunan, China; National Supercomputing Center in Changsha, Changsha, Hunan, China; Department of Computer Science, State University of NY, New Paltz, NY, USA","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1603,1614,"Closer to mobile users geographically, mobile edge computing (MEC) can provide some cloud-like capabilities to users more efficiently. This enables it possible for resource-limited mobile users to offload their computation-intensive and latency-sensitive tasks to MEC nodes. For its great benefits, MEC has drawn wide attention and extensive works have been done. However, few of them address task migration problem caused by distributed user mobility, which can't be ignored with quality of service (QoS) consideration. In this article, we study task migration problem and try to minimize the average completion time of tasks under migration energy budget. There are multiple independent users and the movement of each mobile user is memoryless with a sequential decision-making process, thus reinforcement learning algorithm based on Markov chain model is applied with low computation complexity. To further facilitate cooperation among users, we devise a distributed task migration algorithm based on counterfactual multi-agent (COMA) reinforcement learning approach to solve this problem. Extensive experiments are carried out to assess the performance of this distributed task migration algorithm. Compared with no migrating (NM) and single-agent actor-critic (AC) algorithms, the proposed distributed task migration algorithm can achieve up 30-50 percent reduction about average completion time.","1558-2183","","10.1109/TPDS.2020.3046737","National Key Research and Development Program of China(grant numbers:2018YFB1701403); National Natural Science Foundation of China(grant numbers:62072165,61876061,U19A2058); Zhijiang Lab, China(grant numbers:2020KE0AB01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305956","Energy;mobile edge computing;mobility;multi-agent reinforcement learning;task migration","Task analysis;Reinforcement learning;Quality of service;Energy consumption;Optimization;Markov processes;Computational modeling","computational complexity;decision making;decision theory;learning (artificial intelligence);Markov processes;multi-agent systems;quality of service","migration energy budget;multiple independent users;mobile user;distributed task migration algorithm;counterfactual multiagent reinforcement learning approach;average completion time;distributed task migration optimization;multiagent deep reinforcement learning approach;mobile edge computing;latency-sensitive tasks;MEC nodes;task migration problem;distributed user mobility;efficiency 50.0 percent","",31.0,"",32.0,"IEEE","23 Dec 2020","","","IEEE","IEEE Journals"
"Systematically Landing Machine Learning onto Market-Scale Mobile Malware Detection","L. Gong; H. Lin; Z. Li; F. Qian; Y. Li; X. Ma; Y. Liu","School of Software and BNRist, Tsinghua University, Beijing, China; School of Software and BNRist, Tsinghua University, Beijing, China; School of Software and BNRist, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA; School of Software and BNRist, Tsinghua University, Beijing, China; MOE Key Lab for Intelligent Networks and Network Security, School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China; Global Innovation Exchange, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1615,1628,"Despite being crucial to today's mobile ecosystem, app markets have meanwhile become a natural, convenient malware delivery channel as they actually “lend credibility” to malicious apps. In the past few years, machine learning (ML) techniques have been widely explored for automated, robust malware detection, but till now we have not seen an ML-based malware detection solution applied at market scales. To systematically understand the real-world challenges, we conduct a collaborative study with T-Market, a popular Android app market that offers us large-scale ground-truth data. Our study illustrates that the key to successfully developing such systems is multifold, including feature selection and encoding, feature engineering and exposure, app analysis speed and efficacy, developer and user engagement, as well as ML model evolution. Failure in any of the above aspects could lead to the “wooden barrel effect” of the whole system. This article presents our judicious design choices and first-hand deployment experiences in building a practical ML-powered malware detection system. It has been operational at T-Market, using a single commodity server to check ~12K apps every day, and has achieved an overall precision of 98.9 percent and recall of 98.1 percent with an average per-app scan time of 0.9 minutes.","1558-2183","","10.1109/TPDS.2020.3046092","National Key R&D Program of China(grant numbers:2018YFB1004700); NSF of China(grant numbers:61902211,61972313,61822205,61632020,61632013); NSF of Tianjin(grant numbers:18JCQNJC69900); Postdoctoral Science Fund of China(grant numbers:2019M663725); BNRist; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301262","Machine learning;mobile malware detection;app market;dynamic analysis;Android emulation","Malware;Feature extraction;Encoding;Emulation;Servers;Security;Metadata","Android (operating system);feature selection;invasive software;mobile computing;smart phones","market-scale mobile malware detection;mobile ecosystem;app markets;natural malware delivery channel;malicious apps;machine learning techniques;ML-based malware detection solution;T-Market;ground-truth data;feature selection;encoding;user engagement;ML model evolution;first-hand deployment experiences;ML-powered malware detection system;Android app market","",4.0,"",61.0,"IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"A Game-Based Approach for Cost-Aware Task Assignment With QoS Constraint in Collaborative Edge and Cloud Environments","S. Long; W. Long; Z. Li; K. Li; Y. Xia; Z. Tang","Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent Network, School of Computer Science, Xiangtan University, Xiangtan, Hunan, China; Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent Network, School of Computer Science, Xiangtan University, Xiangtan, Hunan, China; Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent Network, School of Computer Science, Xiangtan University, Xiangtan, Hunan, China; National Supercomputing Center, Changsha, Hunan, China; School of Automation, Beijing Institute of Technology, Beijing, China; National Supercomputing Center, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1629,1640,"With the development of the Internet of Things, the data that needs to be processed is increasing rapidly. Therefore, the collaboration of cloud and edge emerges as the times require. Edge nodes are mainly responsible for collecting data, and decide to process the data locally or offload to cloud data centers. Cloud data centers are suitable for data analysis, model training, and managing edge nodes. In this article, we focus on the task assignment problems in collaborative edge and cloud environments and study it in a distributed, non-cooperative environment. An M/M/1 queueing model is established to characterize the task transmission. Because of the multi-core processors, we set an M/M/C queueing model to characterize the task computation. We consider the problem from the perspective of game theory and formulate it into a non-cooperative game among multi-agents (multiple edge data centers) in which each agent is informed with incomplete information (allocation strategies) of others. For each agent, we define a function of the expected cost of tasks as the disutility function, and minimize it subject to the QoS constraint. We analyze the existence of Nash equilibrium and develop a Greedy Energy-aware Algorithm (GEA) to choose active servers using the Limit Searching Algorithm (LSA) to find the ceiling utilization. Then we propose the Best Response Algorithm (BRA) to optimize the utility function. The convergence of the BRA algorithm has been discussed. Finally, the results demonstrate that the BRA algorithm can get a solution close to Nash equilibrium and reach it quickly.","1558-2183","","10.1109/TPDS.2020.3041029","National Key Research and Development Program of China(grant numbers:2018YFB1003702); National Natural Science Foundation of China(grant numbers:62032020); Hunan Science and Technology Planning Project(grant numbers:2019RS3019); Hunan Provincial Natural Science Foundation of China for Distinguished Young Scholars(grant numbers:2018JJ1025); Hunan Province Department of Education(grant numbers:18C0107); National Natural Science Foundation of China(grant numbers:61502407,62076214); Natural Science Foundation of Hunan(grant numbers:2019JJ50618); Hunan Province Science and Technology Project(grant numbers:2018TP1036); Xiangtan University(grant numbers:11kz/kz08057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272869","Data centers;game theory;mutliple agent system;QoS constraint;queueing system","Task analysis;Servers;Cloud computing;Quality of service;Energy consumption;Games;Data centers","cloud computing;computer centres;game theory;power aware computing;quality of service;queueing theory;resource allocation","Greedy Energy-aware Algorithm;game-based approach;cost-aware task assignment;QoS constraint;collaborative edge;cloud environments;cloud data centers;data analysis;model training;managing edge nodes;task assignment problems;noncooperative environment;task transmission;task computation;game theory;noncooperative game;multiple edge data centers","",27.0,"",41.0,"IEEE","27 Nov 2020","","","IEEE","IEEE Journals"
"The Case for Strong Scaling in Deep Learning: Training Large 3D CNNs With Hybrid Parallelism","Y. Oyama; N. Maruyama; N. Dryden; E. McCarthy; P. Harrington; J. Balewski; S. Matsuoka; P. Nugent; B. Van Essen","Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Tokyo Institute of Technology, Tokyo, Japan; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1641,1652,"We present scalable hybrid-parallel algorithms for training large-scale 3D convolutional neural networks. Deep learning-based emerging scientific workflows often require model training with large, high-dimensional samples, which can make training much more costly and even infeasible due to excessive memory usage. We solve these challenges by extensively applying hybrid parallelism throughout the end-to-end training pipeline, including both computations and I/O. Our hybrid-parallel algorithm extends the standard data parallelism with spatial parallelism, which partitions a single sample in the spatial domain, realizing strong scaling beyond the mini-batch dimension with a larger aggregated memory capacity. We evaluate our proposed training algorithms with two challenging 3D CNNs, CosmoFlow and 3D U-Net. Our comprehensive performance studies show that good weak and strong scaling can be achieved for both networks using up to 2K GPUs. More importantly, we enable training of CosmoFlow with much larger samples than previously possible, realizing an order-of-magnitude improvement in prediction accuracy.","1558-2183","","10.1109/TPDS.2020.3047974","JSPS KAKENHI(grant numbers:JP18J22858); Exascale Computing(grant numbers:17-SC-20-SC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311438","Deep learning;convolutional neural network;model-parallel training;hybrid-parallel training","Training;Three-dimensional displays;Computational modeling;Parallel processing;Solid modeling;Memory management;Image segmentation","convolutional neural nets;learning (artificial intelligence);parallel algorithms","hybrid-parallel algorithm;standard data parallelism;spatial parallelism;strong scaling;training algorithms;3D CNN;3D U-Net;good weak scaling;hybrid parallelism;large-scale 3D convolutional neural networks;deep learning-based;model training;high-dimensional samples;excessive memory usage;end-to-end training pipeline;aggregated memory capacity","",9.0,"",64.0,"IEEE","30 Dec 2020","","","IEEE","IEEE Journals"
"A Hybrid Fuzzy Convolutional Neural Network Based Mechanism for Photovoltaic Cell Defect Detection With Electroluminescence Images","C. Ge; Z. Liu; L. Fang; H. Ling; A. Zhang; C. Yin","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1653,1664,"In the intelligent manufacturing process of solar photovoltaic (PV) cells, the automatic defect detection system using the Industrial Internet of Things (IIoT) smart cameras and sensors cooperated in IIoT has become a promising solution. Many works have been devoted to defect detection of PV cells in a data-driven way. However, because of the subjectivity and fuzziness of human annotation, the data contains a high quantity of noise and unpredictable uncertainties, which creates great difficulties in automatic defect detection. To address this problem, we propose a novel architecture named fuzzy convolution, which integrates fuzzy logic and convolution operations at microscopic level. Combining the proposed fuzzy convolution with the regular convolution, we build a network called Hybrid Fuzzy Convolutional Neural Network (HFCNN). Compared with convolutional neural networks (CNNs), HFCNN can address the uncertainties of PV cell data to improve the accuracy with fewer parameters, making it possible to apply our method in smart cameras. Experimental results on a public dataset show the superiority of our proposed method compared with CNNs.","1558-2183","","10.1109/TPDS.2020.3046018","National Natural Science Foundation of China(grant numbers:62032025,62076125,U20B2050); National Science Foundation for Post-doctoral Scientists of China(grant numbers:2019M651826); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180421); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301240","Convolutional neural network;photovoltaic cell;defect detection;fuzzy logic;fuzzy inference","Fuzzy logic;Convolution;Feature extraction;Uncertainty;Computer architecture;Deep learning;Smart cameras","automatic optical inspection;convolutional neural nets;electroluminescence;fuzzy logic;fuzzy neural nets;fuzzy set theory;intelligent manufacturing systems;manufacturing processes;power apparatus;production engineering computing;solar cells","hybrid fuzzy convolutional neural network;photovoltaic cell defect detection;intelligent manufacturing process;solar photovoltaic cells;automatic defect detection system;IIoT;PV cells;fuzzy convolution;fuzzy logic;convolution operations;regular convolution;convolutional neural networks;PV cell data;industrial Internet of Things smart cameras;HFCNN","",6.0,"",52.0,"IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"Model Parallelism Optimization for Distributed Inference Via Decoupled CNN Structure","J. Du; X. Zhu; M. Shen; Y. Du; Y. Lu; N. Xiao; X. Liao","National Supercomputer Center in Guangzhou, Guangzhou, China; National Supercomputer Center in Guangzhou, Guangzhou, China; National Supercomputer Center in Guangzhou, Guangzhou, China; National Supercomputer Center in Guangzhou, Guangzhou, China; National Supercomputer Center in Guangzhou, Guangzhou, China; National Supercomputer Center in Guangzhou, Guangzhou, China; National Supercomputer Center in Guangzhou, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1665,1676,"It is promising to deploy CNN inference on local end-user devices for high-accuracy and time-sensitive applications. Model parallelism has the potential to provide high throughput and low latency in distributed CNN inference. However, it is non-trivial to use model parallelism as the original CNN model is inherently tightly-coupled structure. In this article, we propose DeCNN, a more effective inference approach that uses decoupled CNN structure to optimize model parallelism for distributed inference on end-user devices. DeCNN is novel consisting of three schemes. Scheme-1 is structure-level optimization. It exploits group convolution and channel shuffle to decouple the original CNN structure for model parallelism. Scheme-2 is partition-level optimization. It is based on channel group to partition the convolutional layers, and then leverages input-based method to partition the fully connected layers, further exposing high degree of parallelism. Scheme-3 is communication-level optimization. It uses inter-sample parallelism to hide communications for better performance and robustness, especially in the weak network connections. We use ImageNet classification task to evaluate the effectiveness of DeCNN on a distributed multi-ARM platform. Notably, when using the number of devices from 1 to 4, DeCNN can accelerate the inference of large-scale ResNet-50 by 3.21×, and reduce 65.3 percent memory footprint, with 1.29 percent accuracy improvement.","1558-2183","","10.1109/TPDS.2020.3041474","National Key R&D Program of China(grant numbers:2018YFB0204303); Natural Science Foundation of China(grant numbers:U1811464,62072479,61433019,61802446); Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2016ZT06D211); Guangdong Basic and Application Basic Research Teams(grant numbers:2018B030312002); Fundamental Research Funds for the Central Universities(grant numbers:19lgpy215); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9275375","Intelligent applications;distributed deep learning;distributed inference;model parallelism;decoupled CNN structure","Parallel processing;Kernel;Convolution;Computational modeling;Optimization;Performance evaluation;Task analysis","convolutional neural nets;inference mechanisms;Internet of Things;mobile computing;optimisation;parallel processing","model parallelism optimization;decoupled CNN structure;end user devices;distributed CNN inference;DeCNN;structure level optimization;communication level optimization;partition level optimization;group convolution;channel shuffle;convolutional neural network;IoT devices;mobile devices","",6.0,"",36.0,"IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"FT-CNN: Algorithm-Based Fault Tolerance for Convolutional Neural Networks","K. Zhao; S. Di; S. Li; X. Liang; Y. Zhai; J. Chen; K. Ouyang; F. Cappello; Z. Chen","Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA; Argonne National Laboratory, Mathematics and Computer Science Division, Lemont, IL, USA; Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA; Oak Ridge National Laboratory, Computer Science and Mathematics Division, Oak Ridge, TN, USA; Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA; Oak Ridge National Laboratory, Computer Science and Mathematics Division, Oak Ridge, TN, USA; Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA; Argonne National Laboratory, Mathematics and Computer Science Division, Lemont, IL, USA; Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1677,1689,"Convolutional neural networks (CNNs) are becoming more and more important for solving challenging and critical problems in many fields. CNN inference applications have been deployed in safety-critical systems, which may suffer from soft errors caused by high-energy particles, high temperature, or abnormal voltage. Of critical importance is ensuring the stability of the CNN inference process against soft errors. Traditional fault tolerance methods are not suitable for CNN inference because error-correcting code is unable to protect computational components, instruction duplication techniques incur high overhead, and existing algorithm-based fault tolerance (ABFT) techniques cannot protect all convolution implementations. In this article, we focus on how to protect the CNN inference process against soft errors as efficiently as possible, with the following three contributions. (1) We propose several systematic ABFTschemes based on checksum techniques and analyze their fault protection ability and runtime thoroughly. Unlike traditional ABFT based on matrix-matrix multiplication, our schemes support any convolution implementations. (2) We design a novel workflow integrating all the proposed schemes to obtain a high detection/correction ability with limited total runtime overhead. (3) We perform our evaluation using ImageNet with well-known CNN models including AlexNet, VGG-19, ResNet-18, and YOLOv2. Experimental results demonstrate that our implementation can handle soft errors with very limited runtime overhead (4%~8% in both error-free and error-injected situations).","1558-2183","","10.1109/TPDS.2020.3043449","Exascale Computing Project(grant numbers:17-SC-20-SC); National Nuclear Security Administration; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:CCF-1513201,CCF-1619253,OAC-2034169); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311863","Algorithm-based fault tolerance;deep learning;silent data corruption;reliability;high-performance computing","Convolution;Runtime;Kernel;Fault tolerant systems;Fault tolerance;Error correction codes;Mathematical model","checkpointing;convolutional neural nets;error correction codes;error detection;matrix multiplication;software fault tolerance","high-energy particles;CNN inference process;soft errors;error-correcting code;instruction duplication techniques;algorithm-based fault tolerance;checksum techniques;fault protection ability;error-injected situations;FT-CNN;convolutional neural networks;CNN inference applications;safety-critical systems;CNN models","",11.0,"",50.0,"IEEE","31 Dec 2020","","","IEEE","IEEE Journals"
"SmartTuning: Selecting Hyper-Parameters of a ConvNet System for Fast Training and Small Working Memory","X. Li; G. Zhang; W. Zheng","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1690,1701,"It is desirable to deploy a ConvNet system with high inference accuracy, as well as fast training and small inference memory. However, existing approaches to hyper-parameter tuning only focus on high accuracy. Although achieving high accuracy, tuning poorly can significantly increase the performance burden, and thus degrade the overall performance of a ConvNet system. In this article, we propose SmartTuning, an approach to identifying the hyper-parameters of a ConvNet system for high training speed and small working memory, with the restriction of high inference accuracy. The key idea of SmartTuning is to build a new performance model for a ConvNet system, and to integrate Bayesian Optimization to learn the relationship between the overall performance and the hyper-parameters of a ConvNet system. In this way, SmartTuning can balance inference accuracy, training speed and inference memory usage during the tuning process, and thus maximizes the overall performance of a ConvNet system. Our experiments show that SmartTuning can stably identify the hyper-parameter sets that offer very close accuracy with faster training speed (i.e., 7×-11× over MNIST and 2×-3× over CIFAR-10) and much less inference memory usage (i.e., 17×-23× over MNIST and 4×-9× over CIFAR-10), compared with existing tuning approaches.","1558-2183","","10.1109/TPDS.2020.3040723","National key R&D Program of China(grant numbers:2018YFB0203902); National Natural Science Foundation of China(grant numbers:61672315); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9271899","Convolutional neural network;high performance computing;hyper-parameter tuning;training speed;working memory","Tuning;Training;Bayes methods;Optimization;Neural networks;Performance evaluation;Memory management","Bayes methods;learning (artificial intelligence);neural nets","ConvNet system;hyper-parameters;inference memory usage;SmartTuning;hyper-parameter sets;fast training;working memory;high inference accuracy;hyper-parameter tuning;high training speed","",2.0,"",60.0,"IEEE","25 Nov 2020","","","IEEE","IEEE Journals"
"Why Dataset Properties Bound the Scalability of Parallel Machine Learning Training Algorithms","D. Cheng; S. Li; H. Zhang; F. Xia; Y. Zhang","SKL, Institute of Computing Technology, Chinese Academy of Science, Beijing, China; Department of Computer Science, ETH, Zurich, ZH, Switzerland; Algorithm Department, Beijing Wisdom Uranium Technology Co., Ltd., Beijing, China; Algorithm Department, Beijing Wisdom Uranium Technology Co., Ltd., Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Science, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2021",2021,32.0,7.0,1702,1712,"As the training dataset size and the model size of machine learning increase rapidly, more computing resources are consumed to speedup the training process. However, the scalability and performance reproducibility of parallel machine learning training, which mainly uses stochastic optimization algorithms, are limited. In this paper, we demonstrate that the sample difference in the dataset plays a prominent role in the scalability of parallel machine learning algorithms. We propose to use statistical properties of dataset to measure sample differences. These properties include the variance of sample features, sample sparsity, sample diversity, and similarity in sampling sequences. We choose four types of parallel training algorithms as our research objects: (1) the asynchronous parallel SGD algorithm (Hogwild! algorithm), (2) the parallel model average SGD algorithm (minibatch SGD algorithm), (3) the decentralization optimization algorithm, and (4) the dual coordinate optimization (DADM algorithm). Our results show that the statistical properties of training datasets determine the scalability upper bound of these parallel training algorithms.","1558-2183","","10.1109/TPDS.2020.3048836","National Natural Science Foundation of China(grant numbers:61972376,61502450,61432018,61521092); National Key Research and Development Program of China(grant numbers:2016YFB0200800,2016YFB0200803,2017YFB0202302,2017YFB0202105); State Key Laboratory of Computer Architecture Foundation(grant numbers:CARCH3504); Natural Science Foundation of Beijing Municipality(grant numbers:L182053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316159","Parallel training algorithms;training dataset;scalability;stochastic optimization methods","Training;Scalability;Machine learning;Machine learning algorithms;Stochastic processes;Task analysis;Upper bound","gradient methods;learning (artificial intelligence);optimisation;parallel processing;stochastic processes","parallel machine learning algorithms;sample difference;stochastic optimization algorithms;training process;training dataset size;parallel machine learning training;dataset properties;scalability upper bound;training datasets;statistical properties;DADM algorithm;decentralization optimization algorithm;minibatch SGD algorithm;parallel model average SGD algorithm;asynchronous parallel SGD algorithm;parallel training algorithms;sampling sequences;sample diversity;sample sparsity;sample features","",5.0,"",44.0,"IEEE","6 Jan 2021","","","IEEE","IEEE Journals"
"A Runtime and Non-Intrusive Approach to Optimize EDP by Tuning Threads and CPU Frequency for OpenMP Applications","J. Schwarzrock; C. C. de Oliveira; M. Ritt; A. F. Lorenzon; A. C. S. Beck","Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, RS, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, RS, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, RS, Brazil; Federal University of Pampa, Bagé, RS, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, RS, Brazil","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1713,1724,"Efficiently exploiting thread-level parallelism has been challenging. Many parallel applications are not sufficiently balanced or CPU-bound to take advantage of the increasing number of cores and the highest possible operating frequency. Moreover, many variables may change according to the system (input set, microarchitecture, and number of cores) or during execution, influencing each parallel region in different ways. Therefore, the task of rightly choosing the ideal configuration (number of threads and DVFS) for each parallel region to deliver the best Energy-Delay Product (EDP) is not straightforward. While the significant number of variables prevents the use of exhaustive search methods, the changing nature of the problem precludes offline strategies. Few solutions are online and synergistically consider thread throttling and DVFS. However, they lack transparency (demand changes in the original code) and/or adaptability (do not automatically adjust to applications at run-time). Our proposed Hoder covers all the characteristics above, optimizing at run-time any dynamically linked OpenMP application, without requiring any code transformation or recompilation. We show Hoder's efficiency by comparing it to two exhaustive offline and two online search approaches, three state-of-the-art techniques, and regular OpenMP execution, considering different setups (Intel 44-, 16- and 12-core; AMD 8- and 12-core).","1558-2183","","10.1109/TPDS.2020.3046537","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303387","Thread throttling;DVFS;OpenMP applications","Libraries;Runtime;Message systems;Minimization;Instruction sets;Tuning;Software","application program interfaces;cache storage;message passing;microprocessor chips;multiprocessing systems;multi-threading;optimisation;parallel programming;power aware computing;search problems;shared memory systems","regular OpenMP execution;online search approaches;exhaustive offline;Hoder's efficiency;code transformation;dynamically linked OpenMP application;original code;demand changes;thread throttling;offline strategies;changing nature;exhaustive search methods;Energy-Delay Product;DVFS;ideal configuration;parallel region;highest possible operating frequency;parallel applications;thread-level parallelism;OpenMP applications;CPU frequency;EDP;nonintrusive approach","",11.0,"",44.0,"IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"Breaking (Global) Barriers in Parallel Stochastic Optimization With Wait-Avoiding Group Averaging","S. Li; T. Ben-Nun; G. Nadiradze; S. D. Girolamo; N. Dryden; D. Alistarh; T. Hoefler","Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Zürich, Switzerland; IST Austria, Klosterneuburg, Austria; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Zürich, Switzerland; IST Austria, Klosterneuburg, Austria; Department of Computer Science, ETH Zurich, Zürich, Switzerland","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1725,1739,"Deep learning at scale is dominated by communication time. Distributing samples across nodes usually yields the best performance, but poses scaling challenges due to global information dissemination and load imbalance across uneven sample lengths. State-of-the-art decentralized optimizers mitigate the problem, but require more iterations to achieve the same accuracy as their globally-communicating counterparts. We present Wait-Avoiding Group Model Averaging (WAGMA) SGD, a wait-avoiding stochastic optimizer that reduces global communication via subgroup weight exchange. The key insight is a combination of algorithmic changes to the averaging scheme and the use of a group allreduce operation. We prove the convergence of WAGMA-SGD, and empirically show that it retains convergence rates similar to Allreduce-SGD. For evaluation, we train ResNet-50 on ImageNet; Transformer for machine translation; and deep reinforcement learning for navigation at scale. Compared with state-of-the-art decentralized SGD variants, WAGMA-SGD significantly improves training throughput (e.g., 2.1× on 1,024 GPUs for reinforcement learning), and achieves the fastest time-to-solution (e.g., the highest score using the shortest training time for Transformer).","1558-2183","","10.1109/TPDS.2020.3040606","European Research Council(grant numbers:678880); EPiGRAM-HS(grant numbers:801039); ERC Starting Grant ScaleML(grant numbers:805223); Swiss National Science Foundation(grant numbers:185778); ETH Postdoctoral; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9271898","Stochastic gradient descent;distributed deep learning;decentralized optimization","Computational modeling;Training;Convergence;Program processors;Stochastic processes;Deep learning;Task analysis","convergence;deep learning (artificial intelligence);gradient methods;stochastic programming","decentralized optimizers;wait-avoiding group model averaging SGD;wait-avoiding stochastic optimizer;subgroup weight exchange;WAGMA-SGD;Allreduce-SGD;deep reinforcement learning;training time;parallel stochastic optimization;deep learning;communication time;global information dissemination;load imbalance;wait-avoiding group averaging;decentralized SGD variants;convergence;ResNet-50;ImageNet;ImageNet;machine translation;Transformer;navigation;stochastic gradient descent","",3.0,"",75.0,"IEEE","25 Nov 2020","","","IEEE","IEEE Journals"
"iMLBench: A Machine Learning Benchmark Suite for CPU-GPU Integrated Architectures","C. Zhang; F. Zhang; X. Guo; B. He; X. Zhang; X. Du","Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; School of Computing, National University of Singapore, Singapore; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1740,1752,"Utilizing heterogeneous accelerators, especially GPUs, to accelerate machine learning tasks has shown to be a great success in recent years. GPUs bring huge performance improvements to machine learning and greatly promote the widespread adoption of machine learning. However, the discrete CPU-GPU architecture design with high PCIe transmission overhead decreases the GPU computing benefits in machine learning training tasks. To overcome such limitations, hardware vendors release CPU-GPU integrated architectures with shared unified memory. In this article, we design a benchmark suite for machine learning training on CPU-GPU integrated architectures, called iMLBench, covering a wide range of machine learning applications and kernels. We mainly explore two features on integrated architectures: 1) zero-copy, which means that the PCIe overhead has been eliminated for machine learning tasks and 2) co-running, which means that the CPU and the GPU co-run together to process a single machine learning task. Our experimental results on iMLBench show that the integrated architecture brings an average 7.1× performance improvement over the original implementations. Specifically, the zero-copy design brings 4.65× performance improvement, and co-running brings 1.78× improvement. Moreover, integrated architectures exhibit promising results from both performance-per-dollar and energy perspectives, achieving 6.50× performance-price ratio while 4.06× energy efficiency over discrete GPUs. The benchmark is open-sourced at https://github.com/ChenyangZhang-cs/iMLBench.","1558-2183","","10.1109/TPDS.2020.3046870","National Key Research and Development Program of China(grant numbers:2018YFB1004401); National Natural Science Foundation of China(grant numbers:61802412,61732014,61972403,62072459,U1911203); State Key Laboratory of Computer Architecture(grant numbers:CARCHA202007); MoE AcRF Tier 1(grant numbers:T1 251RES1824); Tier 2(grant numbers:MOE2017-T2-1-122); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305972","Machine learning;benchmark;CPU;GPU;integrated architectures","Computer architecture;Machine learning;Benchmark testing;Graphics processing units;Task analysis;Hardware;Training","graphics processing units;learning (artificial intelligence)","integrated architectures;co-running;zero-copy;iMLBench;performance improvement;single machine learning task;integrated architecture;machine learning applications;GPU computing benefits;discrete CPU-GPU architecture design;machine learning tasks;CPU-GPU integrated architectures;benchmark suite","",4.0,"",58.0,"IEEE","23 Dec 2020","","","IEEE","IEEE Journals"
"A Distributed Framework for EA-Based NAS","Q. Ye; Y. Sun; J. Zhang; J. Lv","School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China; State Key Laboratory of Hydraulics and Mountain River Engineering, School of Computer Science, Sichuan University, Chengdu, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1753,1764,"Evolutionary Algorithms (EA) are widely applied in Neural Architecture Search (NAS) and have achieved appealing results. Different EA-based NAS algorithms may utilize different encoding schemes for network representation, while they have the same workflow. Specifically, the first step is the initialization of the population with different encoding schemes, and the second step is the evaluation of the individuals by the fitness function. Then, the EA-based NAS algorithm executes evolution operations, e.g., selection, mutation, and crossover, to eliminate weak individuals and generate more competitive ones. Lastly, evolution continues until the max generation and the best neural architectures will be chosen. Because each individual needs complete training and validation on the target dataset, the EA-based NAS always consumes significant computation and time inevitably, which results in the bottleneck of this approach. To ameliorate this issue, this article proposes a distributed framework to boost the computation of the EA-based NAS algorithm. This framework is a server/worker model where the server distributes individuals requested by the computing nodes and collects the validated individuals and hosts the evolution operations. Meanwhile, the most time-consuming phase (i.e., individual evaluation) of the EA-based NAS is allocated to the computing nodes, which send requests asynchronously to the server and evaluate the fitness values of the individuals. Additionally, a new packet structure of the message delivered in the cluster is designed to encapsulate various network representations and support different EA-based NAS algorithms. We design an EA-based NAS algorithm as a case to investigate the efficiency of the proposed framework. Extensive experiments are performed on an illustrative cluster with different scales, and the results reveal that the framework can achieve a nearly linear reduction of the search time with the increase of the computational nodes. Furthermore, the length of the exchanged messages among the cluster is tiny, which benefits the framework expansion.","1558-2183","","10.1109/TPDS.2020.3046774","National Key R&D Program of China(grant numbers:YFB1002201); National Natural Science Fund for Distinguished Young Scholar(grant numbers:61625204); National Science Foundation of China(grant numbers:61836006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305984","Distributed framework;evolutionary algorithm (EA);neural architecture search (NAS);evolutionary neural networks","Computer architecture;Training;Statistics;Sociology;Neural networks;Clustering algorithms;Servers","client-server systems;computational complexity;genetic algorithms;pattern clustering;search problems","computational nodes;distributed framework;encoding schemes;computing nodes;individual evaluation;evolution operations;EA-based NAS algorithm;selection operation;mutation operation;crossover operation","",4.0,"",61.0,"IEEE","23 Dec 2020","","","IEEE","IEEE Journals"
"Parallel Blockwise Knowledge Distillation for Deep Neural Network Compression","C. Blakeney; X. Li; Y. Yan; Z. Zong","Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computer Science, Texas State University, San Marcos, TX, USA","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1765,1776,"Deep neural networks (DNNs) have been extremely successful in solving many challenging AI tasks in natural language processing, speech recognition, and computer vision nowadays. However, DNNs are typically computation intensive, memory demanding, and power hungry, which significantly limits their usage on platforms with constrained resources. Therefore, a variety of compression techniques (e.g., quantization, pruning, and knowledge distillation) have been proposed to reduce the size and power consumption of DNNs. Blockwise knowledge distillation is one of the compression techniques that can effectively reduce the size of a highly complex DNN. However, it is not widely adopted due to its long training time. In this article, we propose a novel parallel blockwise distillation algorithm to accelerate the distillation process of sophisticated DNNs. Our algorithm leverages local information to conduct independent blockwise distillation, utilizes depthwise separable layers as the efficient replacement block architecture, and properly addresses limiting factors (e.g., dependency, synchronization, and load balancing) that affect parallelism. The experimental results running on an AMD server with four Geforce RTX 2080Ti GPUs show that our algorithm can achieve 3x speedup plus 19 percent energy savings on VGG distillation, and 3.5x speedup plus 29 percent energy savings on ResNet distillation, both with negligible accuracy loss. The speedup of ResNet distillation can be further improved to 3.87 when using four RTX6000 GPUs in a distributed cluster.","1558-2183","","10.1109/TPDS.2020.3047003","National Science Foundation(grant numbers:CNS-1908658); National Science Foundation; Texas State University Research Enhancement Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305986","Deep neural networks;model compression;knowledge distillation;parallel training","Computational modeling;Training;Task analysis;Quantization (signal);Neural networks;Deep learning;Hardware","computer vision;data compression;learning (artificial intelligence);multiprocessing systems;natural language processing;neural nets;parallel algorithms;parallel programming;power aware computing;speech recognition","Geforce RTX 2080Ti GPU;ResNet distillation;VGG distillation;efficient replacement block architecture;depthwise separable layers;independent blockwise distillation;distillation process;parallel blockwise distillation algorithm;highly complex DNN;compression techniques;constrained resources;computer vision;speech recognition;natural language processing;AI tasks;deep neural network compression;parallel blockwise knowledge distillation","",8.0,"",47.0,"IEEE","23 Dec 2020","","","IEEE","IEEE Journals"
"Privacy-Preserving Computation Offloading for Parallel Deep Neural Networks Training","Y. Mao; W. Hong; H. Wang; Q. Li; S. Zhong","Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science, College of William & Mary, Williamsburg, VA, USA; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1777,1788,"Deep neural networks (DNNs) have brought significant performance improvements to various real-life applications. However, a DNN training task commonly requires intensive computing resources and a huge data collection, which makes it hard for personal devices to carry out the entire training, especially for mobile devices. The federated learning concept has eased this situation. However, it is still an open problem for individuals to train their own DNN models at an affordable price. In this article, we propose an alternative DNN training strategy for resource-limited users. With the help of an untrusted server, end users can offload their DNN training tasks to the server in a privacy-preserving manner. To this end, we study the possibility of the separation of a DNN. Then we design a differentially private activation algorithm for end users to ensure the privacy of the offloading after model separation. Furthermore, to meet the rising demand for federated learning, we extend the offloading solution to parallel DNN models training with a secure model weights aggregation scheme for the privacy concern. Experimental results prove the feasibility of computation offloading solutions for DNN models in both solo and parallel modes.","1558-2183","","10.1109/TPDS.2020.3040734","National Key R&D Program of China(grant numbers:2018YFB1004301,BK20190294,NSFC-61902176,NSFC-61872176); Fundamental Research Funds for the Central Universities(grant numbers:14380069); National Science Foundation(grant numbers:CNS-1816399); Commonwealth Cyber Initiative; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272685","Deep neural network;federated learning;computation offloading;data privacy;model parallelism","Servers;Training;Computational modeling;Privacy;Data models;Task analysis;Cryptography","data privacy;learning (artificial intelligence);mobile computing;neural nets","privacy-preserving computation offloading;parallel deep neural networks training;real-life applications;DNN training task;intensive computing resources;huge data collection;personal devices;entire training;mobile devices;federated learning concept;DNN models;alternative DNN training strategy;resource-limited users;end users;privacy-preserving manner;model separation;offloading solution;privacy concern;computation offloading solutions","",10.0,"",33.0,"IEEE","26 Nov 2020","","","IEEE","IEEE Journals"
"Fine-Grained Powercap Allocation for Power-Constrained Systems Based on Multi-Objective Machine Learning","M. Hao; W. Zhang; Y. Wang; G. Lu; F. Wang; A. V. Vasilakos","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Peng Cheng Laboratory, Cyberspace Security Research Center, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Department of Computer Science, Electrical and Space Engineering, Lulea University of Technology, Lulea, Sweden","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1789,1801,"Power capping is an important solution to keep the system within a fixed power constraint. However, for the over-provisioned and power-constrained systems, especially the future exascale supercomputers, powercap needs to be reasonably allocated according to the workloads of compute nodes to achieve trade-offs among performance, energy and powercap. Thus it is necessary to model performance and energy and to predict the optimal powercap allocation strategies. Existing power allocation approaches have insufficient granularity within nodes. Modeling approaches usually model performance and energy separately, ignoring the correlation between objectives, and do not expose the Pareto-optimal powercap configurations. Therefore, this article combines the powercap with uncore frequency scaling and proposes an approach to predict the Pareto-optimal powercap configurations on the power-constrained system for input MPI and OpenMP parallel applications. Our approach first uses the elaborately designed micro-benchmarks and a small number of existing benchmarks to build the training set, and then applies a multi-objective machine learning algorithm which combines the stacked single-target method with extreme gradient boosting to build multi-objective models of performance and energy. The models can be used to predict the optimal processor and memory powercap settings, helping compute nodes perform fine-grained powercap allocation. When the optimal powercap configuration is determined, the uncore frequency scaling is used to further optimize the energy consumption. Compared with the reference powercap configuration, the predicted optimal configurations predicted by our method can achieve an average powercap reduction of 31.35 percent, an average energy reduction of 12.32 percent, and average performance degradation of only 2.43 percent.","1558-2183","","10.1109/TPDS.2020.3045983","National Key Research and Development Program of China(grant numbers:2017YFB0202901); Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010136001); National Natural Science Foundation of China(grant numbers:61672186); Shenzhen Science and Technology Research and Development Foundation(grant numbers:JCYJ20190806143418198); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301369","Power capping;performance and energy modeling;pareto front;multi-objective machine learning","Resource management;Random access memory;Mathematical model;Computational modeling;Energy consumption;Analytical models;Benchmark testing","application program interfaces;energy consumption;learning (artificial intelligence);message passing;microprocessor chips;multiprocessing systems;parallel machines;Pareto optimisation;power aware computing","power constraint;power-constrained systems;future exascale supercomputers;optimal powercap allocation strategies;power allocation approaches;Pareto-optimal powercap configurations;power-constrained system;multiobjective machine learning algorithm;fine-grained powercap allocation;energy consumption;reference powercap configuration;predicted optimal configurations;energy reduction;power capping;powercap reduction;OpenMP parallel application;MPI parallel application;efficiency 31.35 percent;efficiency 12.32 percent;efficiency 2.43 percent","",7.0,"",41.0,"IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"Accelerating End-to-End Deep Learning Workflow With Codesign of Data Preprocessing and Scheduling","Y. Cheng; D. Li; Z. Guo; B. Jiang; J. Geng; W. Bai; J. Wu; Y. Xiong","Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Microsoft Research, Beijing, China; Shanghai Jiao Tong University, Shanghai, China; Tsinghua University, Beijing, China; Microsoft Research, Beijing, China; Tsinghua University, Beijing, China; Microsoft Research, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1802,1814,"In this article, we investigate the performance bottleneck of existing deep learning (DL) systems and propose DLBooster to improve the running efficiency of deploying DL applications on GPU clusters. At its core, DLBooster leverages two-level optimizations to boost the end-to-end DL workflow. On the one hand, DLBooster selectively offloads some key decoding workloads to FPGAs to provide high-performance online data preprocessing services to the computing engine. On the other hand, DLBooster reorganizes the computational workloads of training neural networks with the backpropagation algorithm and schedules them according to their dependencies to improve the utilization of GPUs at runtime. Based on our experiments, we demonstrate that compared with baselines, DLBooster can improve the image processing throughput by 1.4× - 2.5× and reduce the processing latency by 1/3 in several real-world DL applications and datasets. Moreover, DLBooster consumes less than 1 CPU core to manage FPGA devices at runtime, which is at least 90 percent less than the baselines in some cases. DLBooster shows its potential to accelerate DL workflows in the cloud.","1558-2183","","10.1109/TPDS.2020.3047966","National Key Research and Development Program of China(grant numbers:2018YFB1800500); Key Areas of Guangdong Province(grant numbers:2018B010113001); National Natural Science Foundation of China(grant numbers:61772305); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9310194","Deep learning;data preprocessing;workload offloading;computation scheduling;FPGAs","Data preprocessing;Training;Field programmable gate arrays;Task analysis;Graphics processing units;Artificial neural networks;Hardware","backpropagation;data handling;deep learning (artificial intelligence);field programmable gate arrays;image processing;microprocessor chips;neural nets;scheduling","end-to-end DL workflow;key decoding workloads;computational workloads;data preprocessing;end-to-end deep learning workflow;FPGA;CPU core;backpropagation algorithm;neural networks;image processing;DLBooster;two-level optimizations","",3.0,"",62.0,"IEEE","29 Dec 2020","","","IEEE","IEEE Journals"
"A Probabilistic Machine Learning Approach to Scheduling Parallel Loops With Bayesian Optimization","K. -R. Kim; Y. Kim; S. Park","Department of Electronics Engineering, Sogang University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1815,1827,"This article proposes Bayesian optimization augmented factoring self-scheduling (BO FSS), a new parallel loop scheduling strategy. BO FSS is an automatic tuning variant of the factoring self-scheduling (FSS) algorithm and is based on Bayesian optimization (BO), a black-box optimization algorithm. Its core idea is to automatically tune the internal parameter of FSS by solving an optimization problem using BO. The tuning procedure only requires online execution time measurement of the target loop. In order to apply BO, we model the execution time using two Gaussian process (GP) probabilistic machine learning models. Notably, we propose a locality-aware GP model, which assumes that the temporal locality effect resembles an exponentially decreasing function. By accurately modeling the temporal locality effect, our locality-aware GP model accelerates the convergence of BO. We implemented BO FSS on the GCC implementation of the OpenMP standard and evaluated its performance against other scheduling algorithms. Also, to quantify our method's performance variation on different workloads, or workload-robustness in our terms, we measure the minimax regret. According to the minimax regret, BO FSS shows more consistent performance than other algorithms. Within the considered workloads, BO FSS improves the execution time of FSS by as much as 22% and 5% on average.","1558-2183","","10.1109/TPDS.2020.3046461","National Research Foundation of Korea(grant numbers:2017M3C4A7080245); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303443","Parallel loop scheduling;Bayesian optimization;parallel computing;OpenMP","Frequency selective surfaces;Task analysis;Heuristic algorithms;Dynamic scheduling;Optimization;Scheduling algorithms;Copper","Bayes methods;Gaussian processes;learning (artificial intelligence);minimax techniques;optimisation;processor scheduling;program control structures","black-box optimization algorithm;optimization problem;execution time;Gaussian process probabilistic machine learning models;locality-aware GP model;temporal locality effect;BO FSS;scheduling parallel loops;Bayesian optimization;parallel loop scheduling strategy;automatic tuning variant;Bayesian optimization augmented factoring self-scheduling;minimax regret","",6.0,"",58.0,"IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"SGD$\_$_Tucker: A Novel Stochastic Optimization Strategy for Parallel Sparse Tucker Decomposition","H. Li; Z. Li; K. Li; J. S. Rellermeyer; L. Chen; K. Li","TU Delft, Delft, Netherlands; National Supercomputing Center, Changsha, Hunan, China; National Supercomputing Center, Changsha, Hunan, China; TU Delft, Delft, Netherlands; TU Delft, Delft, Netherlands; Department of Computer Science, State University of New York, New Paltz, NY, USA","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1828,1841,"Sparse Tucker Decomposition (STD) algorithms learn a core tensor and a group of factor matrices to obtain an optimal low-rank representation feature for the High-Order, High-Dimension, and Sparse Tensor (HOHDST). However, existing STD algorithms face the problem of intermediate variables explosion which results from the fact that the formation of those variables, i.e., matrices Khatri-Rao product, Kronecker product, and matrix-matrix multiplication, follows the whole elements in sparse tensor. The above problems prevent deep fusion of efficient computation and big data platforms. To overcome the bottleneck, a novel stochastic optimization strategy (SGD Tucker) is proposed for STD which can automatically divide the high-dimension intermediate variables into small batches of intermediate matrices. Specifically, SGD Tucker only follows the randomly selected small samples rather than the whole elements, while maintaining the overall accuracy and convergence rate. In practice, SGD Tucker features the two distinct advancements over the state of the art. First, SGD Tucker can prune the communication overhead for the core tensor in distributed settings. Second, the low data-dependence of SGD Tucker enables fine-grained parallelization, which makes SGD Tucker obtaining lower computational overheads with the same accuracy. Experimental results show that SGD Tucker runs at least 2X faster than the state of the art.","1558-2183","","10.1109/TPDS.2020.3047460","Swiss National Science Foundation(grant numbers:407540_167266); China Scholarship Council(grant numbers:CSC201906130109); National Natural Science Foundation of China(grant numbers:61751204); National Natural Science Foundation of China(grant numbers:61625202); National Natural Science Foundation of China(grant numbers:61860206011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309187","High-order, high-dimension and sparse tensor;low-rank representation learning;machine learning algorithm;sparse tucker decomposition;stochastic optimization;parallel strategy","Tensors;Sparse matrices;Optimization;Stochastic processes;Matrix decomposition;Indexes;Data models","Big Data;image coding;learning (artificial intelligence);matrix algebra;matrix decomposition;matrix multiplication;optimisation;sparse matrices;stochastic processes;tensors","SGD Tucker;core tensor;novel stochastic optimization strategy;parallel sparse tucker decomposition;Sparse Tucker Decomposition algorithms;sparse tensor;high-dimension intermediate variables","",3.0,"",55.0,"IEEE","25 Dec 2020","","","IEEE","IEEE Journals"
"Adaptive SpMV/SpMSpV on GPUs for Input Vectors of Varied Sparsity","M. Li; Y. Ao; C. Yang","University of Chinese Academy of Sciences, Beijing, China; School of Mathematical Sciences, Peking University, Beijing, China; Peng Cheng Laboratory, Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1842,1853,"Despite numerous efforts for optimizing the performance of Sparse Matrix and Vector Multiplication (SpMV) on modern hardware architectures, few works are done to its sparse counterpart, Sparse Matrix and Sparse Vector Multiplication (SpMSpV), not to mention dealing with input vectors of varied sparsity. The key challenge is that depending on the sparsity levels, distribution of data, and compute platform, the optimal choice of SpMV/SpMSpV kernel can vary, and a static choice does not suffice. In this article, we propose an adaptive SpMV/SpMSpV framework, which can automatically select the appropriate SpMV/SpMSpV kernel on GPUs for any sparse matrix and vector at the runtime. Based on systematic analysis on key factors such as computing pattern, workload distribution and write-back strategy, eight candidate SpMV/SpMSpV kernels are encapsulated into the framework to achieve high performance in a seamless manner. A comprehensive study on machine learning-based kernel selector is performed to choose the kernel and adapt with the varieties of both the input and hardware from both accuracy and overhead perspectives. Experiments demonstrate that the adaptive framework can substantially outperform the previous state-of-the-art in real-world applications on NVIDIA Tesla K40m, P100, and V100 GPUs.","1558-2183","","10.1109/TPDS.2020.3040150","National Key R&D Plan of China(grant numbers:2016YFB0200603); Guangdong Key-Area R&D Program(grant numbers:2019B121204008); Beijing Natural Science Foundation(grant numbers:JQ18001); Beijing Academy of Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268964","Sparse matrix and vector multiplication (SpMV);sparse matrix and sparse vector multiplication (SpMSpV);GPU computing;adaptive performance optimization;machine learning","Kernel;Sparse matrices;Hardware;Machine learning;Computational modeling;Adaptation models;Runtime","graphics processing units;learning (artificial intelligence);matrix multiplication;sparse matrices;vectors","computing pattern;workload distribution;machine learning-based kernel selector;adaptive framework;GPUs;hardware architectures;sparsity levels;optimal choice;write-back strategy;SpMV-SpMSpV kernel;sparse matrix-and-sparse vector multiplication;sparse matrix-and-vector multiplication;distribution-of-data","","","",49.0,"IEEE","24 Nov 2020","","","IEEE","IEEE Journals"
"Improving HW/SW Adaptability for Accelerating CNNs on FPGAs Through A Dynamic/Static Co-Reconfiguration Approach","L. Gong; C. Wang; X. Li; X. Zhou","School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1854,1865,"With the continuous evolution of Convolutional Neural Networks (CNNs) and the improvement of the computing capability of FPGAs, the deployment of CNN accelerator based on FPGA has become more and more popular in various computing scenarios. The key element of implementing these accelerators is to take full advantage of underlying hardware characteristics to adapt to the computational features of the software-level CNN model. To achieve this goal, however, previous designs mainly focus on the static hardware reconfiguration pattern, which is not flexible enough and can hardly make the accelerator architecture and the CNN features fully fit, resulting in inefficient computations and data communications. By leveraging the dynamic partial reconfiguration technology equipped in the modern FPGA devices, in this article, we propose a new accelerator architecture for implementing CNNs on FPGAs in which static and dynamic reconfigurabilities of the hardware are cooperatively utilized to maximize the acceleration efficiency. Based on this architecture, we further present a systematic design and optimization methodology for implementing the specific CNN model in the particular computing scenario, in which a static design space exploration method and a reinforcement learning-based decision method are proposed to obtain the optimal static hardware configuration and run-time reconfiguration strategy respectively. We evaluate our proposal by implementing three widely used CNN models, AlexNet, VGG16C, and ResNet34, on the Xilinx ZCU102 FPGA platform. Experimental results show that our implementations on average can achieve 683 GOPS under 16-bit fixed data type and 1.37 TOPS under 8-bit fixed data type for three targeted CNN models, and improve the computational density from 1.1× to 1.91× compared with previous implementations on the same type of FPGA platform.","1558-2183","","10.1109/TPDS.2020.3046762","National Science Foundation of China(grant numbers:61976200,61772482); Jiangsu Provincial Natural Science Foundation(grant numbers:BK20181193); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2017497); Fundamental Research Funds for the Central Universities(grant numbers:WK2150110003); 2019 Youth Innovation Fund of University of Science and Technology of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305957","Convolutional neural networks;FPGA;hardware accelerator;computing adapativity;dynamic partial reconfiguration","Hardware;Field programmable gate arrays;Computational modeling;Acceleration;Mathematical model;Convolution;Accelerator architectures","convolutional neural nets;field programmable gate arrays;hardware-software codesign;learning (artificial intelligence);reconfigurable architectures","acceleration efficiency;systematic design;optimization methodology;specific CNN model;particular computing scenario;static design space exploration method;reinforcement learning-based decision method;optimal static hardware configuration;run-time reconfiguration strategy;Xilinx ZCU102 FPGA platform;16-bit fixed data type;8-bit fixed data type;targeted CNN models;computational density;continuous evolution;convolutional neural networks;computing capability;CNN accelerator;computing scenarios;hardware characteristics;computational features;software-level CNN model;static hardware reconfiguration pattern;accelerator architecture;CNN features;inefficient computations;data communications;dynamic partial reconfiguration technology;modern FPGA devices;static reconfigurabilities;dynamic reconfigurabilities","",4.0,"",22.0,"IEEE","23 Dec 2020","","","IEEE","IEEE Journals"
"Efficient Methods for Mapping Neural Machine Translator on FPGAs","Q. Li; X. Zhang; J. Xiong; W. -M. Hwu; D. Chen","Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Champaign, IL, USA; Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Champaign, IL, USA; IBM T.J. Watson Research Center, NY, USA; Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Champaign, IL, USA; Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Champaign, IL, USA","IEEE Transactions on Parallel and Distributed Systems","18 Feb 2021",2021,32.0,7.0,1866,1877,"Neural machine translation (NMT) is one of the most critical applications in natural language processing (NLP) with the main idea of converting text in one language to another using deep neural networks. In recent year, we have seen continuous development of NMT by integrating more emerging technologies, such as bidirectional gated recurrent units (GRU), attention mechanisms, and beam-search algorithms, for improved translation quality. However, with the increasing problem size, the real-life NMT models have become much more complicated and difficult to implement on hardware for acceleration opportunities. In this article, we aim to exploit the capability of FPGAs to deliver highly efficient implementations for real-life NMT applications. We map the inference of a large-scale NMT model with total computation of 172 GFLOP to a highly optimized high-level synthesis (HLS) IP and integrate the IP into Xilinx VCU118 FPGA platform. The model has widely used key features for NMTs, including the bidirectional GRU layer, attention mechanism, and beam search. We quantize the model to mixed-precision representation in which parameters and portions of calculations are in 16-bit half precision, and others remain as 32-bit floating-point. Compared to the float NMT implementation on FPGA, we achieve 13.1× speedup with an end-to-end performance of 22.0 GFLOPS without any accuracy degradation. Based on our knowledge, this is the first work that successfully implements a real-life end-to-end NMT model to an FPGA on board.","1558-2183","","10.1109/TPDS.2020.3047371","IBM-Illinois Center for Cognitive Computing System Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309170","Hardware-efficient inference;neural machine translation;FPGA;high level synthesis","Computational modeling;Field programmable gate arrays;Decoding;Task analysis;Hardware;IP networks;Dictionaries","deep learning (artificial intelligence);field programmable gate arrays;floating point arithmetic;high level synthesis;language translation;natural language processing;recurrent neural nets;search problems","beam search;mixed-precision representation;16-bit half precision;32-bit floating-point;float NMT implementation;neural machine translation;natural language processing;NLP;deep neural networks;bidirectional gated recurrent units;attention mechanism;translation quality;acceleration opportunities;GFLOP;Xilinx VCU118 FPGA platform;bidirectional GRU layer;high-level synthesis IP;large-scale NMT;real-life NMT;neural machine translator","",5.0,"",35.0,"CCBY","25 Dec 2020","","","IEEE","IEEE Journals"
"Accelerating Binarized Neural Networks via Bit-Tensor-Cores in Turing GPUs","A. Li; S. Su","High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, WA, USA; U.S. Army Research Laboratory (ARL), DoD Supercomputing Resource Center, Aberdeen Proving Ground, MD, USA","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1878,1891,"Despite foreseeing tremendous speedups over conventional deep neural networks, the performance advantage of binarized neural networks (BNNs) has merely been showcased on general-purpose processors such as CPUs and GPUs. In fact, due to being unable to leverage bit-level-parallelism with a word-based architecture, GPUs have been criticized for extremely low utilization (1 percent) when executing BNNs. Consequently, the latest tensorcores in NVIDIA Turing GPUs start to experimentally support bit computation. In this article, we look into this brand new bit computation capability and characterize its unique features. We show that the stride of memory access can significantly affect performance delivery and a data-format co-design is highly desired to support the tensorcores for achieving superior performance than existing software solutions without tensorcores. We realize the tensorcore-accelerated BNN design, particularly the major functions for fully-connect and convolution layers - bit matrix multiplication and bit convolution. Evaluations on two NVIDIA Turing GPUs show that, with ResNet-18, our BTC-BNN design can process ImageNet at a rate of 5.6K images per second, 77 percent faster than state-of-the-art. Our BNN approach is released on https://github.com/pnnl/TCBNN.","1558-2183","","10.1109/TPDS.2020.3045828","PNNL's DMC-CFA; DS-HPC LDRD projects; U.S. DOE SC, ASCR(grant numbers:66150); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303370","","Graphics processing units;Hardware;Convolution;Synchronization;Tensors;Libraries;Field programmable gate arrays","AI chips;convolution;deep learning (artificial intelligence);graphics processing units;hardware accelerators;matrix multiplication;parallel algorithms;parallel architectures;performance evaluation;tensors","bit-level-parallelism;ImageNet;BTC-BNN design;ResNet-18;bit convolution;bit matrix multiplication;memory access;NVIDIA Turing GPUs;tensorcore-accelerated BNN design;data-format co-design;performance delivery;bit computation;word-based architecture;deep neural networks;bit-tensor-cores;binarized neural networks","",12.0,"",75.0,"IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"EDGES: An Efficient Distributed Graph Embedding System on GPU Clusters","D. Yang; J. Liu; J. Lai","NVIDIA, Beijing, China; NVIDIA, Beijing, China; NVIDIA, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","19 Feb 2021",2021,32.0,7.0,1892,1902,"Graph embedding training models access parameters sparsely in a “one-hot” manner. Currently, the distributed graph embedding neural network is learned by data parallel with the parameter server, which suffers significant performance and scalability problems. In this article, we analyze the problems and characteristics of training this kind of models on distributed GPU clusters for the first time, and find that fixed model parameters scattered among different machine nodes are a major limiting factor for efficiency. Based on our observation, we develop an efficient distributed graph embedding system called EDGES, which can utilize GPU clusters to train large graph models with billions of nodes and trillions of edges using data and model parallelism. Within the system, we propose a novel dynamic partition architecture for training these models, achieving at least one half of communication reduction compared to existing training systems. According to our evaluations on real-world networks, our system delivers a competitive accuracy for the trained embeddings, and significantly accelerates the training process of the graph node embedding neural network, achieving a speedup of 7.23x and 18.6x over the existing fastest training system on single node and multi-node, respectively. As for the scalability, our experiments show that EDGES obtains a nearly linear speedup.","1558-2183","","10.1109/TPDS.2020.3041219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272876","Large-scale distributed training;graph node embedding;GPU clusters;parallel algorithm;scalability","Training;Data models;Servers;Graphics processing units;Computational modeling;Neural networks;Training data","embedded systems;graph theory;graphics processing units;learning (artificial intelligence);neural nets;parallel processing","efficient distributed graph embedding system;graph embedding training models access parameters;neural network;parameter server;distributed GPU clusters;fixed model parameters;graph models;model parallelism;trained embeddings;training process;graph node;fastest training system;EDGES","",3.0,"",34.0,"IEEE","27 Nov 2020","","","IEEE","IEEE Journals"
"A Thread Level SLO-Aware I/O Framework for Embedded Virtualization","X. Gong; D. Cao; Y. Li; X. Liu; Y. Li; J. Zhang; T. Li","Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China; Tianjin Key Laboratory of Network and Data Security Technology, Tianjin, China","IEEE Transactions on Parallel and Distributed Systems","9 Oct 2020",2021,32.0,3.0,500,513,"With the development of virtualization technology, it is practical and necessary to integrate virtual machine software into embedded systems. I/O scheduling is important for embedded systems, because embedded systems always face different situations and their requests have more diversity on the requirement of real-time and importance. However, the semantic information associated with the I/O data is completely lost when crossing the virtualized I/O software stack. Here, we present an I/O scheduling framework to connect the semantic gap between the application threads in virtual machines and hardware schedulers in the host machine. Therefore, the details for the I/O request can be passed through the layers of the software stack and each layer can get the specific information about the device environment. Also, various scheduling points have been provided to implement different I/O strategies. Our framework was implemented based on Linux operating system, KVM, QEMU and virtio protocol. A prototype scheduler, Orthrus, was implemented to evaluate the effectiveness of the framework. Comprehensive experiments were conducted and the results show that our framework can guarantee the real-time requirements, and reserve more system resources for critical tasks, with negligible memory consumption and throughput overhead.","1558-2183","","10.1109/TPDS.2020.3026042","National Key Research and Development Program of China(grant numbers:2018YFB1003405); National Natural Science Foundation of China(grant numbers:61702286); Natural Science Foundation of Tianjin, China(grant numbers:18JCYBJC15600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204842","Embedded system;I/O virtualization;I/O scheduling;SLO-Aware;semantic gap","Virtualization;Semantics;Real-time systems;Virtual machining;Hardware;Embedded systems","embedded systems;Linux;multiprocessing systems;scheduling;virtual machines;virtualisation","Orthrus;virtio protocol;KVM;QEMU;I-O data;embedded virtualization technology;thread level SLO-aware I-O framework;semantic information;embedded systems;virtual machine software;system resources;prototype scheduler;Linux operating system;scheduling points;host machine;hardware schedulers;virtual machines;application threads;I-O scheduling framework;virtualized I-O software stack","",1.0,"",36.0,"IEEE","23 Sep 2020","","","IEEE","IEEE Journals"
"ADRL: A Hybrid Anomaly-Aware Deep Reinforcement Learning-Based Resource Scaling in Clouds","S. Kardani-Moghaddam; R. Buyya; K. Ramamohanarao","Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia","IEEE Transactions on Parallel and Distributed Systems","8 Oct 2020",2021,32.0,3.0,514,526,"The virtualization concept and elasticity feature of cloud computing enable users to request resources on-demand and in the pay-as-you-go model. However, the high flexibility of the model makes the on-time resource scaling problem more complex. A variety of techniques such as threshold-based rules, time series analysis, or control theory are utilized to increase the efficiency of dynamic scaling of resources. However, the inherent dynamicity of cloud-hosted applications requires autonomic and adaptable systems that learn from the environment in real-time. Reinforcement Learning (RL) is a paradigm that requires some agents to monitor the surroundings and regularly perform an action based on the observed states. RL has a weakness to handle high dimensional state space problems. Deep-RL models are a recent breakthrough for modeling and learning in complex state space problems. In this article, we propose a Hybrid Anomaly-aware Deep Reinforcement Learning-based Resource Scaling (ADRL) for dynamic scaling of resources in the cloud. ADRL takes advantage of anomaly detection techniques to increase the stability of decision-makers by triggering actions in response to the identified anomalous states in the system. Two levels of global and local decision-makers are introduced to handle the required scaling actions. An extensive set of experiments for different types of anomaly problems shows that ADRL can significantly improve the quality of service with less number of actions and increased stability of the system.","1558-2183","","10.1109/TPDS.2020.3025914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203981","Cloud computing;anomaly detection;deep reinforcement learning;performance management;vertical scaling","Cloud computing;Adaptation models;Resource management;Computational modeling;Quality of service;Dynamic scheduling;Decision making","cloud computing;learning (artificial intelligence);time series;virtualisation","cloud computing;resource scaling;Hybrid Anomaly-aware Deep Reinforcement Learning;identified anomalous states;anomaly detection techniques;ADRL;complex state space problems;modeling learning;Deep-RL models;high dimensional state space problems;observed states;cloud-hosted applications;time series analysis;threshold-based rules;on-time resource scaling problem;resources on-demand;elasticity feature;virtualization concept","",17.0,"",25.0,"IEEE","22 Sep 2020","","","IEEE","IEEE Journals"
"Optimistic Causal Consistency for Geo-Replicated Key-Value Stores","K. Spirovska; D. Didona; W. Zwaenepoel","École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; IBM Research Europe, Rüschlikon, Switzerland; University of Sydney, Camperdown, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","8 Oct 2020",2021,32.0,3.0,527,542,"Causal consistency (CC) is an attractive consistency model for geo-replicated data stores because it hits a sweet spot in the ease-of-programming versus performance trade-off. We present a new approach for implementing CC in geo-replicated data stores, which we call Optimistic Causal Consistency (OCC). OCC's main design goal is to maximize data freshness. The optimism in our approach lies in the fact that the updates replicated to a remote data center are made visible immediately, without checking if their causal dependencies have been received. Servers perform the dependency check needed to enforce CC only upon serving a client operation, rather than on receipt of a replicated data item as in existing systems. OCC offers a significant gain in data freshness, which is of crucial importance for various types of applications, such as real-time systems. OCC's potentially blocking behavior makes it vulnerable to network partitions. We therefore propose a recovery mechanism that allows an OCC system to fall back on a pessimistic protocol to continue operating during network partitions. We implement POCC, the first causally consistent geo-replicated multi-master key-value data store designed to maximize data freshness. We show that POCC improves data freshness, while offering comparable or better performance than its pessimistic counterparts.","1558-2183","","10.1109/TPDS.2020.3026778","The Swiss National Science Foundation(grant numbers:166306); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206094","Optimistic causal consistency;causal consistency;geo-replication;key-value data stores;read-only transactions;data freshness","Protocols;Data integrity;Servers;Data centers;Data models;Clocks;Distributed databases","protocols;storage management","pessimistic protocol;blocking behavior;dependency check;performance trade-off;ease-of-programming;causally consistent geo-replicated multimaster key-value data;OCC system;replicated data item;causal dependencies;remote data center;data freshness;Optimistic Causal Consistency;geo-replicated data stores;attractive consistency model;geo-replicated key-value stores;Optimistic causal consistency","",5.0,"",59.0,"IEEE","25 Sep 2020","","","IEEE","IEEE Journals"
"Bi-Objective Optimization of Data-Parallel Applications on Heterogeneous HPC Platforms for Performance and Energy Through Workload Distribution","H. Khaleghzadeh; M. Fahad; A. Shahid; R. R. Manumachu; A. Lastovetsky","School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland; School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland; School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland; School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland; School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland","IEEE Transactions on Parallel and Distributed Systems","8 Oct 2020",2021,32.0,3.0,543,560,"Performance and energy are the two most important objectives for optimization on modern parallel platforms. In this article, we show that moving from single-objective optimization for performance or energy to their bi-objective optimization on heterogeneous processors results in a tremendous increase in the number of optimal solutions (workload distributions) even for the simple case of linear performance and energy profiles. We then study full performance and energy profiles of two real-life data-parallel applications and find that they exhibit shapes that are non-linear and complex enough to prevent good approximation of them as analytical functions for input to exact algorithms or optimization software for determining the Pareto front. We, therefore, propose a solution method solving the bi-objective optimization problem on heterogeneous processors. The method's novel component is an efficient and exact global optimization algorithm that takes as an input performance and energy profiles as arbitrary discrete functions of workload size, which accurately and realistically take into account resource contention and NUMA inherent in modern parallel platforms, and returns the Pareto-optimal solutions (generally speaking, load imbalanced). To construct the input discrete energy functions, the method employs a methodology that accurately models the energy consumption by a hybrid data-parallel application executing on a heterogeneous HPC platform containing different computing devices using system-level power measurements provided by power meters. We experimentally analyse the proposed solution method using three data-parallel applications, matrix multiplication, 2D fast Fourier transform (2D-FFT), and gene sequencing, on two connected heterogeneous servers consisting of multicore CPUs, GPUs, and Intel Xeon Phi. We show that it determines a superior Pareto front containing the best load balanced solutions and all the load imbalanced solutions that are ignored by load balancing methods.","1558-2183","","10.1109/TPDS.2020.3027338","Science Foundation Ireland(grant numbers:14/IA/2474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207974","Heterogeneous platforms;data-parallel applications;workload partitioning;performance optimization;energy optimization;bi-objective optimization;workload distribution;multicore CPU;GPU;Intel Xeon Phi","Optimization;Program processors;Energy consumption;Heuristic algorithms;Multicore processing;Software algorithms;Sequential analysis","approximation theory;energy consumption;fast Fourier transforms;matrix multiplication;multiprocessing systems;parallel processing;Pareto optimisation;power aware computing;resource allocation","data-parallel applications;matrix multiplication;2D fast Fourier transform;2D-FFT;gene sequencing;multicore CPUs;Intel Xeon Phi;load balancing methods;system-level power measurements;NUMA;Pareto front;optimization software;analytical functions;input performance;exact global optimization algorithm;efficient optimization algorithm;bi-objective optimization problem;real-life data-parallel applications;energy profiles;linear performance;heterogeneous processors;single-objective optimization;workload distribution;connected heterogeneous servers;heterogeneous HPC platform;hybrid data-parallel application;energy consumption;input discrete energy functions;Pareto-optimal solutions;modern parallel platforms;workload size;arbitrary discrete functions","",13.0,"",62.0,"IEEE","28 Sep 2020","","","IEEE","IEEE Journals"
"Privacy-Preserving Multi-Keyword Searchable Encryption for Distributed Systems","X. Liu; G. Yang; W. Susilo; J. Tonien; X. Liu; J. Shen","Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia; Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia; Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia; Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","16 Oct 2020",2021,32.0,3.0,561,574,"As cloud storage has been widely adopted in various applications, how to protect data privacy while allowing efficient data search and retrieval in a distributed environment remains a challenging research problem. Existing searchable encryption schemes are still inadequate on desired functionality and security/privacy perspectives. Specifically, supporting multi-keyword search under the multi-user setting, hiding search pattern and access pattern, and resisting keyword guessing attacks (KGA) are the most challenging tasks. In this article, we present a new searchable encryption scheme that addresses the above problems simultaneously, which makes it practical to be adopted in distributed systems. It not only enables multi-keyword search over encrypted data under a multi-writer/multi-reader setting but also guarantees the data and search pattern privacy. To prevent KGA, our scheme adopts a multi-server architecture, which accelerates search response, shares the workload, and lowers the key leakage risk by allowing only authorized servers to jointly test whether a search token matches a stored ciphertext. A novel subset decision mechanism is also designed as the core technique underlying our scheme and can be further used in applications other than keyword search. Finally, we prove the security and evaluate the computational and communication efficiency of our scheme to demonstrate its practicality.","1558-2183","","10.1109/TPDS.2020.3027003","National Natural Science Foundation of China(grant numbers:U1804263,62072109); National Natural Science Foundation of China(grant numbers:61922045,U1836115,61672295); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207857","Searchable encryption;multi-keyword search;multi-user access;search pattern;access pattern","Encryption;Servers;Cloud computing;Keyword search;Public key;Data privacy","cloud computing;cryptography;data privacy","privacy-preserving multikeyword;distributed systems;keyword search;search token;search response;multiserver architecture;search pattern privacy;encrypted data;keyword guessing attacks;access pattern;multiuser setting;multikeyword search;searchable encryption scheme;distributed environment;data search;data privacy","",25.0,"",30.0,"IEEE","28 Sep 2020","","","IEEE","IEEE Journals"
"PQC Acceleration Using GPUs: FrodoKEM, NewHope, and Kyber","N. Gupta; A. Jati; A. K. Chauhan; A. Chattopadhyay","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Physical and Mathematical Sciences, NTU, Singapore; Department of Computer Science and Engineering, IIT, Ropar, Rupnagar, Punjab, India; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Parallel and Distributed Systems","13 Oct 2020",2021,32.0,3.0,575,586,"In this article, we present the first GPU implementation for FrodoKEM-976, NewHope-1024, and Kyber-1024. These algorithms belong to three different classes of post-quantum algorithms: Learning with errors (LWE), Ring-LWE, and Module-LWE. We show the practical applicability of the algorithms in different scenarios using two different implementation approaches. Moreover, we achieve highly efficient realization of computationally expensive operations such as NTT (Number Theoretic Transform), matrix multiplication, and Keccak. Since, these are the most common operations in lattice-based cryptographic algorithms, the techniques presented in this article will likely benefit other similar algorithms. Using a NVIDIA QUADRO GV100 graphics card, we undertook a detailed experimental study. For NewHope and Kyber we were able to perform approximately 504K and 473K key exchanges per second, demonstrating a speedup of almost 53.1× and 51.05× compared to the reference C implementation. Compared to the optimized AVX2 versions we obtain speedups of 25.7× and 14.6×, respectively. Further, implementation of FrodoKEM resulted in a speedup of 50.6×, 44.2×, and 36.9× for KeyGen, Encaps and Decaps operations. Compared to its AVX2 counterpart, we achieved a speedup of about 7.3×, 4.7× and 4.9×, respectively. We also show that using multiple streams resulted in further speedup of about 28-38 percent.","1558-2183","","10.1109/TPDS.2020.3025691","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9201530","Cryptography;post-quantum;key exchange;PQC;NewHope;Kyber;FrodoKEM;GPU;CUDA;accelerator;NTT;SHAKE","Graphics processing units;Lattices;Encapsulation;Acceleration;Cryptography;Zinc;Quantum computing","graphics processing units;learning (artificial intelligence);matrix multiplication;number theory;quantum cryptography","module-LWE;504K key exchanges;Keccak;NTT;ring-LWE;learning with errors;NVIDIA QUADRO GV100 graphics card;lattice-based cryptographic algorithms;matrix multiplication;number theoretic transform;post-quantum algorithms;Kyber-1024;NewHope-1024;FrodoKEM-976;GPU implementation;PQC acceleration;optimized AVX2 versions;473K key exchanges","",22.0,"",18.0,"IEEE","21 Sep 2020","","","IEEE","IEEE Journals"
"Investigating the Adoption of Hybrid Encrypted Cloud Data Deduplication With Game Theory","X. Liang; Z. Yan; R. H. Deng; Q. Zheng","Department of Communications and Networking, Aalto University, 02150, Finland; Department of Communications and Networking, Aalto University, 02150, Finland; School of Information Systems, Singapore Management University, Singapore; School of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, China","IEEE Transactions on Parallel and Distributed Systems","14 Oct 2020",2021,32.0,3.0,587,600,"Encrypted data deduplication, along with different preferences in data access control, brings the birth of hybrid encrypted cloud data deduplication (H-DEDU for short). However, whether H-DEDU can be successfully deployed in practice has not been seriously investigated. Obviously, the adoption of H-DEDU depends on whether it can bring economic benefits to all stakeholders. But existing economic models of cloud storage fail to support H-DEDU due to complicated interactions among stakeholders. In this article, we establish a formal economic model of H-DEDU by formulating the utilities of all involved stakeholders, i.e., data holders, data owners, and Cloud Storage Providers (CSPs). Then, we construct a multi-stage Stackelberg game, which consists of Holder Participation Game, Owner Online Game, and CSP Pricing Game, to capture the interactions among all system stakeholders. We further analyze the conditions of the existence of a sub-game perfect Nash Equilibrium and propose a gradient-based algorithm to help the stakeholders choose near-optimal strategies. Extensive experiments show the feasibility of the proposed algorithm in achieving the Nash Equilibrium of the Stackelberg game. Additionally, we investigate the effects of parameters related to CSP, data owners and data holders on H-DEDU adoption. Our study advises all stakeholders the best strategies to adopt H-DEDU.","1558-2183","","10.1109/TPDS.2020.3028685","National Natural Science Foundation of China(grant numbers:61672410,62072351,61802293); Academy of Finland(grant numbers:308087,314203,335262); Shaanxi Innovation Team Project(grant numbers:2018TD-007); Higher Education Discipline Innovation Project(grant numbers:B16037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214531","Cloud computing;deduplication;gradient-based algorithm;multi-stage stackelberg game","Games;Stakeholders;Economics;Biological system modeling;Cloud computing;Cryptography;Game theory","cloud computing;cryptography;game theory;pricing;storage management","gradient-based algorithm;CSP pricing game;owner online game;holder participation game;H-DEDU adoption;sub-game perfect Nash equilibrium;multistage Stackelberg game;cloud storage providers;data owners;data holders;formal economic model;stakeholders;hybrid encrypted cloud data deduplication;data access control;game theory","",5.0,"",42.0,"IEEE","6 Oct 2020","","","IEEE","IEEE Journals"
"Failure-Atomic Byte-Addressable R-tree for Persistent Memory","S. Cho; W. Kim; S. Oh; C. Kim; K. Koh; B. Nam","Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea; ETRI, Daejeon, South Korea; Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea; Data Centric Computing Systems, ETRI, Daejeon, South Korea; SW Fundamental Research, ETRI, Daejeon, South Korea; Sungkyunkwan University, Seoul","IEEE Transactions on Parallel and Distributed Systems","14 Oct 2020",2021,32.0,3.0,601,614,"In this article, we propose Failure-atomic Byte-addressable R-tree (FBR-tree) that leverages the byte-addressability, persistence, and high performance of persistent memory while guaranteeing the crash consistency. We carefully control the order of store and cacheline flush instructions and prevent any single store instruction from making an FBR-tree inconsistent and unrecoverable. We also develop a non-blocking lock-free range query algorithm for FBR-tree. Since FBR-tree allows read transactions to detect and ignore any transient inconsistent states, multiple read transactions can concurrently access tree nodes without using shared locks while other write transactions are making changes to them. Our performance study shows that FBR-tree successfully reduces the legacy logging overhead and the lock-free range query algorithm shows up to 2.6x higher query processing throughput than the shared lock-based crabbing concurrency protocol.","1558-2183","","10.1109/TPDS.2020.3028699","National Research Foundation of Korea(grant numbers:NRF-2016M3C4A7952587,NRF-2018R1A2B3006681); Institute for Information and Communications Technology Promotion(grant numbers:2018-0-00549); Electronics and Telecommunications Research Institute(grant numbers:20ZS1310); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214450","R-tree;persistent memory;failure-atomicity;multidimensional indexing structure","Data structures;Metadata;Indexing;Computer crashes;Transient analysis;Concurrent computing","cache storage;concurrency (computers);protocols;query processing;transaction processing;trees (mathematics)","persistent memory;lock-free range query algorithm;failure-atomic byte-addressable R-tree;FBR-tree;concurrency protocol","",4.0,"",60.0,"IEEE","6 Oct 2020","","","IEEE","IEEE Journals"
"Modeling and Optimization of Performance and Cost of Serverless Applications","C. Lin; H. Khazaei","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada","IEEE Transactions on Parallel and Distributed Systems","14 Oct 2020",2021,32.0,3.0,615,632,"Function-as-a-Service (FaaS) and serverless applications have proliferated significantly in recent years because of their high scalability, ease of resource management, and pay-as-you-go pricing model. However, cloud users are facing practical problems when they migrate their applications to the serverless pattern, which are the lack of analytical performance and billing model and the trade-off between limited budget and the desired quality of service of serverless applications. In this article, we fill this gap by proposing and answering two research questions regarding the prediction and optimization of performance and cost of serverless applications. We propose a new construct to formally define a serverless application workflow, and then implement analytical models to predict the average end-to-end response time and the cost of the workflow. Consequently, we propose a heuristic algorithm named Probability Refined Critical Path Greedy algorithm (PRCP) with four greedy strategies to answer two fundamental optimization questions regarding the performance and the cost. We extensively evaluate the proposed models by conducting experimentation on AWS Lambda and Step Functions. Our analytical models can predict the performance and cost of serverless applications with more than 98 percent accuracy. The PRCP algorithms can achieve the optimal configurations of serverless applications with 97 percent accuracy on average.","1558-2183","","10.1109/TPDS.2020.3028841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214428","Cloud serverless computing;performance modeling;performance optimization;cost modeling;cost optimization","FAA;Cloud computing;Optimization;Time factors;Analytical models;Computational modeling","cloud computing;greedy algorithms;optimisation;pricing;probability;quality of service;Web services","Amazon Web services;cloud serverless computing;PRCP algorithms;step functions;AWS lambda;probability refined critical path greedy algorithm;heuristic algorithm;average end-to-end response time;quality of service;analytical performance;billing model;pay-as-you-go pricing model;resource management;FaaS;function-as-a-service;cost optimization;performance optimization;cost modelling;performance modelling;analytical models;serverless application workflow;efficiency 98.0 percent;efficiency 97.0 percent","",37.0,"",51.0,"IEEE","6 Oct 2020","","","IEEE","IEEE Journals"
"Energy-Efficient Hardware-Accelerated Synchronization for Shared-L1-Memory Multiprocessor Clusters","F. Glaser; G. Tagliavini; D. Rossi; G. Haugou; Q. Huang; L. Benini","Department of Information Technology and Electrical Engineering (D-ITET), ETH, Zürich, Switzerland; Department of Electrical, Electronic, and Information Engineering, University of Bologna, Italy; Department of Electrical, Electronic, and Information Engineering, University of Bologna, Italy; Department of Information Technology and Electrical Engineering (D-ITET), ETH, Zürich, Switzerland; Department of Information Technology and Electrical Engineering (D-ITET), ETH, Zürich, Switzerland; Department of Electrical, Electronic, and Information Engineering, University of Bologna, Italy","IEEE Transactions on Parallel and Distributed Systems","16 Oct 2020",2021,32.0,3.0,633,648,"The steeply growing performance demands for highly power- and energy-constrained processing systems such as end-nodes of the Internet-of-Things (IoT) have led to parallel near-threshold computing (NTC), joining the energy-efficiency benefits of low-voltage operation with the performance typical of parallel systems. Shared-L1-memory multiprocessor clusters are a promising architecture, delivering performance in the order of GOPS and over 100 GOPS/W of energy-efficiency. However, this level of computational efficiency can only be reached by maximizing the effective utilization of the processing elements (PEs) available in the clusters. Along with this effort, the optimization of PE-to-PE synchronization and communication is a critical factor for performance. In this article, we describe a light-weight hardware-accelerated synchronization and communication unit (SCU) for tightly-coupled clusters of processors. We detail the architecture, which enables fine-grain per-PE power management, and its integration into an eight-core cluster of RISC-V processors. To validate the effectiveness of the proposed solution, we implemented the eight-core cluster in advanced 22 nm FDX technology and evaluated performance and energy-efficiency with tunable microbenchmarks and a set of real-life applications and kernels. The proposed solution allows synchronization-free regions as small as 42 cycles, over 41× smaller than the baseline implementation based on fast test-and-set access to L1 memory when constraining the microbenchmarks to 10 percent synchronization overhead. When evaluated on the real-life DSP-applications, the proposed SCU improves performance by up to 92 and 23 percent on average and energy efficiency by up to 98 and 39 percent on average.","1558-2183","","10.1109/TPDS.2020.3028691","EU Horizon 2020 Projects OPRECOMP(grant numbers:732631); Eurostars(grant numbers:10691); WiPLASH(grant numbers:863337); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214436","Energy-efficient embedded parallel computing;fine-grain parallelism;tightly memory-coupled multiprocessors","Synchronization;Hardware;Complexity theory;Parallel processing;Kernel;Benchmark testing;Computer architecture","digital signal processing chips;microprocessor chips;multiprocessing systems;parallel processing;performance evaluation;power aware computing;shared memory systems;synchronisation","RISC-V processor;lightweight hardware-accelerated synchronization;synchronization-free regions;performance evaluation;eight-core cluster;per-PE power management;parallel systems;energy-constrained processing systems;shared-L1-memory multiprocessor clusters;energy-efficient hardware-accelerated synchronization","",5.0,"",46.0,"IEEE","6 Oct 2020","","","IEEE","IEEE Journals"
"Lewat: A Lightweight, Efficient, and Wear-Aware Transactional Persistent Memory System","K. Huang; S. Li; L. Huang; K. -L. Tan; H. Mei","School of Computing, Shanghai Jiao Tong University, Shanghai, China; Computer and Science, Shanghai Jiao Tong University, Shanghai, China; Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computing, National University of Singapore, Singapore; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","16 Oct 2020",2021,32.0,3.0,649,664,"Emerging non-volatile memory (also termed as persistent memory, PM) technologies promise persistence, byte-addressability, and DRAM-like read/write latency. A proliferation of persistent memory systems have been proposed to leverage PM for fast data persistence and expose malloc-like persistent APIs. By eliminating disk I/Os, these systems gain low-latency and high-throughput access performance for persistent data. However, there still exist non-negligible limitations in these systems, such as frequent context switches, inefficient allocation, heavy logging overhead, and lack of wear-leveling techniques. To solve these problems, we develop Lewat, a lightweight, efficient, and wear-aware transactional persistent memory system. Lewat is built in user-layer to avoid kernel/user layer context switches and enables lightweight persistent data access. We decouple the data space into slot zone and page zone. Based on this, we design different allocators in these two zones to achieve efficient allocation performance for both small-sized data and large-sized data. To minimize logging overhead, we propose an efficient adaptive logging framework. The main idea is to utilize different logging techniques for different workloads. We also propose a suite of system-coupled wear-leveling techniques that contain wear-aware allocation, wear-aware update, and write reduction. We evaluate Lewat on a real non-volatile memory platform and the experimental results show that compared with state-of-the-art persistent memory systems, Lewat has much lower latency and higher throughput.","1558-2183","","10.1109/TPDS.2020.3028385","National Key Research & Development Program of China(grant numbers:2018YFB1003302); China Scholarship Council(grant numbers:201906230180); SJTU-Huawei Innovation Research Lab Funding(grant numbers:FA2018091021-202004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211773","Non-volatile memory;memory management;persistence;consistency;crash recovery;wear-leveling","Resource management;Computer crashes;Nonvolatile memory;Memory management;Micromechanical devices;Random access memory;Libraries","application program interfaces;cache storage;DRAM chips;transaction processing;wear","lightweight wear-aware transactional persistent memory system;lightweight persistent data access;data space;slot zone;page zone;allocation performance;small-sized data;large-sized data;adaptive logging framework;system-coupled wear-leveling techniques;wear-aware allocation;wear-aware update;nonvolatile memory platform;persistent APIs;heavy logging overhead;frequent context switches;nonvolatile memory","",3.0,"",71.0,"IEEE","2 Oct 2020","","","IEEE","IEEE Journals"
"Constructing Completely Independent Spanning Trees in Data Center Network Based on Augmented Cube","G. Chen; B. Cheng; D. Wang","School of Computer Science and Technology, Soochow University, Suzhou, China; Provincial Key Laboratory for Computer Information Processing Technology, Soochow University, Suzhou, China; Department of Computer Science, Montclair State University, Upper Montclair, NJ, USA","IEEE Transactions on Parallel and Distributed Systems","19 Oct 2020",2021,32.0,3.0,665,673,"A set of spanning trees T1; T2;. .. ; Tk in a network G are Completely Independent Spanning Trees (CISTs) if for any two nodes u and v in V (G), the paths between u and v in any two trees have no common edges and no common internal nodes. CISTs have important applications in data center networks, such as fault-tolerant multi-node broadcasting, fault-tolerant one-to-all broadcasting, reliable broadcasting, secure message distribution, and so on. The augmented cube AQn is a prominent variant of the well-known hypercube Qn, and having the important property of scalability, and both Qn and AQn have been proposed as the underlying structure for a data center network. The data center network based on AQn is denoted by AQDNn, and the logic graph of AQDNn is denoted by L-AQDNn. In this article, we study how to construct n - 1 CISTs in L-AQDNn. The constructed n - 1 CISTs are optimal in the sense that n - 1 is the maximally allowed CISTs in L-AQDNn. The correctness of our construction algorithm is proved. It is the first time a direct relationship is established between the dimension of a hypercube-family network and the number of CISTs it can host.","1558-2183","","10.1109/TPDS.2020.3029654","National Natural Science Foundation of China(grant numbers:U1905211); National Natural Science Foundation of China(grant numbers:61572337); Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:18KJA520009); China Postdoctoral Science Foundation(grant numbers:2015M581858); Jiangsu Planned Projects for Postdoctoral Research Funds(grant numbers:1501089B); Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217977","Augmented cube;hamiltonian cycle;edge-disjoint hamiltonian paths;data center network;completely independent spanning trees","Data centers;Hypercubes;Broadcasting;Servers;Scalability;Fault tolerance;Fault tolerant systems","computer centres;fault tolerant computing;hypercube networks;trees (mathematics)","augmented cube AQn;fault-tolerant multinode broadcasting;common internal nodes;Completely Independent Spanning Trees;hypercube-family network;CISTs;L-AQDNn;data center network","",19.0,"",34.0,"IEEE","8 Oct 2020","","","IEEE","IEEE Journals"
"Cryptomining Detection in Container Clouds Using System Calls and Explainable Machine Learning","R. R. Karn; P. Kudva; H. Huang; S. Suneja; I. M. Elfadel","Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE","IEEE Transactions on Parallel and Distributed Systems","26 Oct 2020",2021,32.0,3.0,674,691,"The use of containers in cloud computing has been steadily increasing. With the emergence of Kubernetes, the management of applications inside containers (or pods) is simplified. Kubernetes allows automated actions like self-healing, scaling, rolling back, and updates for the application management. At the same time, security threats have also evolved with attacks on pods to perform malicious actions. Out of several recent malware types, cryptomining has emerged as one of the most serious threats with its hijacking of server resources for cryptocurrency mining. During application deployment and execution in the pod, a cryptomining process, started by a hidden malware executable can be run in the background, and a method to detect malicious cryptomining software running inside Kubernetes pods is needed. One feasible strategy is to use machine learning (ML) to identify and classify pods based on whether or not they contain a running process of cryptomining. In addition to such detection, the system administrator will need an explanation as to the reason(s) of the ML's classification outcome. The explanation will justify and support disruptive administrative decisions such as pod removal or its restart with a new image. In this article, we describe the design and implementation of an ML-based detection system of anomalous pods in a Kubernetes cluster by monitoring Linux-kernel system calls (syscalls). Several types of cryptominers images are used as containers within an anomalous pod, and several ML models are built to detect such pods in the presence of numerous healthy cloud workloads. Explainability is provided using SHAP, LIME, and a novel auto-encoding-based scheme for LSTM models. Seven evaluation metrics are used to compare and contrast the explainable models of the proposed ML cryptomining detection engine.","1558-2183","","10.1109/TPDS.2020.3029088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9215018","Cryptomining;docker;kubernetes;containers;machine learning;explainability;pod;anomaly","Containers;Cloud computing;Malware;Machine learning;Cryptocurrency;Data mining","cloud computing;cryptography;data mining;distributed databases;invasive software;learning (artificial intelligence);Linux;operating system kernels;public domain software;system recovery","Kubernetes pods;ML-based detection system;Kubernetes cluster;cryptominers images;ML cryptomining detection engine;container clouds;explainable machine learning;cloud computing;security threats;cryptomining process;healthy cloud workloads;malicious cryptomining software detection;Linux-kernel system call monitoring;SHAP;LIME;autoencoding;LSTM","",19.0,"",93.0,"CCBY","6 Oct 2020","","","IEEE","IEEE Journals"
"Hierarchical Multi-Agent Optimization for Resource Allocation in Cloud Computing","X. Gao; R. Liu; A. Kaushik","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic and Electrical Engineering, University College London (UCL), London, U.K.","IEEE Transactions on Parallel and Distributed Systems","22 Oct 2020",2021,32.0,3.0,692,707,"In cloud computing, an important concern is to allocate the available resources of service nodes to the requested tasks on demand and to make the objective function optimum, i.e., maximizing resource utilization, payoffs, and available bandwidth. This article proposes a hierarchical multi-agent optimization (HMAO) algorithm in order to maximize the resource utilization and make the bandwidth cost minimum for cloud computing. The proposed HMAO algorithm is a combination of the genetic algorithm (GA) and the multi-agent optimization (MAO) algorithm. With maximizing the resource utilization, an improved GA is implemented to find a set of service nodes that are used to deploy the requested tasks. A decentralized-based MAO algorithm is presented to minimize the bandwidth cost. We study the effect of key parameters of the HMAO algorithm by the Taguchi method and evaluate the performance results. The results demonstrate that the HMAO algorithm is more effective than two baseline algorithms of genetic algorithm (GA) and fast elitist non-dominated sorting genetic algorithm (NSGA-II) in solving the large-scale optimization problem of resource allocation. Furthermore, we provide the performance comparison of the HMAO algorithm with two heuristic Greedy and Viterbi algorithms in on-line resource allocation.","1558-2183","","10.1109/TPDS.2020.3030920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224163","Cloud computing;resource allocation;resource utilization;bandwidth cost;genetic algorithm;multi-agent optimization","Resource management;Task analysis;Optimization;Bandwidth;Cloud computing;Genetic algorithms;Signal processing algorithms","cloud computing;genetic algorithms;multi-agent systems;resource allocation;sorting;Taguchi methods","Taguchi method;NSGA-II;non-dominated sorting genetic algorithm;hierarchical multiagent optimization;resource allocation;large-scale optimization problem;genetic algorithm;baseline algorithms;decentralized-based MAO algorithm;GA;HMAO algorithm;cloud computing;resource utilization","",19.0,"",40.0,"IEEE","14 Oct 2020","","","IEEE","IEEE Journals"
"The Deep Learning Compiler: A Comprehensive Survey","M. Li; Y. Liu; X. Liu; Q. Sun; X. You; H. Yang; Z. Luan; L. Gan; G. Yang; D. Qian","School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","30 Oct 2020",2021,32.0,3.0,708,727,"The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this article, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. We present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey article focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.","1558-2183","","10.1109/TPDS.2020.3030548","National Key R&D Program of China(grant numbers:2020YFB150001); National Natural Science Foundation of China(grant numbers:62072018,61502019,61732002); SenseTime Research Fund for Young Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222299","Neural networks;deep learning;compiler;intermediate representation;optimization","Optimization;Hardware;Computational modeling;Libraries;Computer architecture;Deep learning;Integrated circuit modeling","learning (artificial intelligence);neural nets;optimisation;program compilers;software architecture","deep learning compiler;deep learning models;diverse DL hardware;DL compiler;DL oriented multilevel IR;DL frameworks;frontend-backend optimizations","",40.0,"",75.0,"IEEE","13 Oct 2020","","","IEEE","IEEE Journals"
"Achieving Fine-Grained Flow Management Through Hybrid Rule Placement in SDNs","G. Zhao; H. Xu; J. Fan; L. Huang; C. Qiao","Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, Jiangsu, China; Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, Jiangsu, China; Department of Computer Science & Engineering, University at Buffalo, Buffalo, NY, USA; Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, Jiangsu, China; Department of Computer Science & Engineering, University at Buffalo, Buffalo, NY, USA","IEEE Transactions on Parallel and Distributed Systems","27 Oct 2020",2021,32.0,3.0,728,742,"Fine-grained flow management is useful in many practical applications, e.g., resource allocation, anomaly detection and traffic engineering. However, it is difficult to provide fine-grained management for a large number of flows in SDNs due to switches' limited flow table capacity. While using wildcard rules can reduce the number of flow entries needed, it cannot fully ensure fine-grained management for all the flows without degrading application performance. In this article, we design and implement hybrid rule placement for fine-grained flow management (to be referred to as HiFi here after). HiFi achieves fine-grained management with a minimal number of flow entries through taking a two-step approach: wildcard entry installment and application-specific exact-match entry installment. How to optimally install wildcard and exact-match flow entries, however, is intractable. Therefore, we design approximation algorithms with bounded factors to solve these problems. We consider how to achieve network-wide load balancing via fine-grained flow management as a case study. Both experiment on a testbed built with open virtual switches and extensive simulation show that HiFi can reduce the number of required flow entries by about 45-69 percent and reduce the control overhead by about 28-50 percent compared with the state-of-the-art approaches for achieving fine-grained flow management.","1558-2183","","10.1109/TPDS.2020.3030630","National Science Foundation of China(grant numbers:61822210,61936015,U1709217); Anhui Initiative in Quantum Information Technologies(grant numbers:AHY150300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222349","Software defined networks;fine-grained management;wildcard entry;exact-match entry;approximation","Control systems;Software;Approximation algorithms;Resource management;Monitoring;Mice;Erbium","resource allocation;software defined networking;telecommunication network management","fine-grained flow management;hybrid rule placement;exact-match flow entries;wildcard entry installment;application-specific exact-match entry installment;SDNs;HiFi;network-wide load balancing;open virtual switches","",10.0,"",55.0,"IEEE","13 Oct 2020","","","IEEE","IEEE Journals"
"Guest Editorial: Special Section on SC19 Student Cluster Competition","M. Parashar","","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2606,2606,"Reproducibility is foundational to solid scientific and technical research, and the ability to repeat the research that produced is a key approach for confirming the validity of a new scientific discovery. The IEEE Transactions on Parallel and Distributed Systems (TPDS) is committed to enabling reproducible research through transparency and the availability and potential reuse of code. As part of its Reproducibility Initiative, TPDS is exploring post-publication peer review of code associated with articles published in TPDS. Authors who have published in TPDS can make their published article more reproducible and earn a reproducibility badge by submitting their associated code for post-publication peer review. However, the nature of the parallel and distributed systems research covered by TPDS makes it challenging to evaluate code and data challenging for reproducibility. This is because such an evaluation may require access to specific hardware, system architectures and scales, OS configurations, and so on, which may not be feasible or practical. Consequently, TPDS is exploring an alternate approach where members of the community can submit short, supplemental ‘critique’ papers that present their experiences in reproducing published results using the artifacts and/or evaluations or experiences with published artifacts. These supplemental paper submissions are reviewed and, if accepted, are linked to the original publication and are citable, serving to validate the reproducibility of the original publication. This special section, consisting of a primary paper and 6 critique papers that reproduce the results of the primary paper, is the initial pilot of this approach. The special section builds on the efforts of the SC19 Student Cluster Competition, which was part of the SC19 conference (https://sc19.supercomputing.org/) and represents a step forward in enabling reproducible research publications in parallel and distributed systems.","1558-2183","","10.1109/TPDS.2021.3053641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431239","","Special issues and sections;Reproducibility of results;Research and development","","","",1.0,"",3.0,"IEEE","14 May 2021","","","IEEE","IEEE Journals"
"Transparency and Reproducibility Practice in Large-Scale Computational Science: A Preface to the Special Section","B. Plale; S. L. Harrell","Intelligent Systems Engineering Department, Indiana University Bloomington, Bloomington, IN, USA; Texas Advanced Computing Center, University of Texas at Austin, Austin, TX, USA","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2607,2608,"With this special section we bring you a practice and experience effort in transparency and reproducibility for large-scale computational science. A unique section, it consists of a research work plus six critques, each by a student team that reproduced the work. The original research work has been expanded in its science and also in its contribution to open science with a discussion of the student effort. Our letter contemplates implications as well.","1558-2183","","10.1109/TPDS.2021.3058393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431328","Open science;computational science;reproducibility;practice and experience","Special issues and sections;Reproducibility of results;Research and development","","","",2.0,"",1.0,"IEEE","14 May 2021","","","IEEE","IEEE Journals"
"Planetary Normal Mode Computation: Parallel Algorithms, Performance, and Reproducibility","J. Shi; R. Li; Y. Xi; Y. Saad; M. V. de Hoop","Shell International Exploration and Production, Inc., Houston, TX, USA; Center for Applied Scientific Computing, Lawrence Livermore National Laboratory, Livermore, CA, USA; Department of Mathematics, Emory University, Atlanta, GA, USA; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA; Department of Computational and Applied Mathematics, Rice University, Houston, TX, USA","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2609,2622,"This article is an extension of work entitled “Computing planetary interior normal modes with a highly parallel polynomial filtering eigensolver.” by Shi et al., [1] originally presented at the SC18 conference. A highly parallel polynomial filtered eigensolver was developed and exploited to calculate the planetary normal modes. The proposed method is ideally suited for computing interior eigenpairs for large-scale eigenvalue problems as it greatly enhances memory and computational efficiency. In this article, the second-order finite element method is used to further improve the accuracy as only the first-order finite element method was deployed in the previous work. The parallel algorithm, its parallel performance up to 20k processors, and the great computational accuracy are illustrated. The reproducibility of the previous work was successfully performed on the Student Cluster Competition at the SC19 conference by several participant teams using a completely different Mars-model dataset on different clusters. Both weak and strong scaling performances of the reproducibility by the participant teams were impressive and encouraging. The analysis and reflection of their results are demonstrated and future direction is discussed.","1558-2183","","10.1109/TPDS.2021.3050448","Simons Foundation; National Science Foundation(grant numbers:DMS-1815143); Rice University; XSEDE Research Allocation(grant numbers:TG-EAR170019); Abel and Cooperative Institute for Dynamic Earth Research(grant numbers:NSF EAR-1135452); U.S. Department of Energy(grant numbers:DE-AC52-07NA27344 (LLNL-JRNL-814373)); NSF-OAC 2003720; NSF-CCF 1812695; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319555","Eigenvalues and eigenvectors;numerical linear algebra;earth and planetary sciences;reproducibility","Computational modeling;Eigenvalues and eigenfunctions;Planets;Surfaces;Mars;Moon;Standards","eigenvalues and eigenfunctions;finite element analysis;matrix algebra;parallel algorithms","planetary normal mode computation;parallel algorithm;planetary interior normal modes;planetary normal modes;computing interior eigenpairs;large-scale eigenvalue problems;computational efficiency;second-order finite element method;first-order finite element method;parallel performance;weak scaling performances;strong scaling performances","",2.0,"",86.0,"IEEE","11 Jan 2021","","","IEEE","IEEE Journals"
"Critique of “Planetary Normal Mode Computation: Parallel Algorithms, Performance, and Reproducibility” by SCC Team From National Tsing Hua University","W. -F. Sun; H. -H. Chen; S. -F. Lin; Y. -C. Lin; J. -W. Wu; E. -T. Lin; J. Chou","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2623,2626,"As a special activity of the Student Cluster Competition at SC19 conference, we made an attempt to reproduce the scalability evaluations of a highly paralleled polynomial filtering eigensolver for computing planetary interior normal modes. Our experiments were conducted on a Mars dataset using a small scale 4-node cluster with Intel Skylake CPU architecture, while the original article's were conducted on a Moon dataset using a large scale 256-node supercomputer with Intel CPU Skylake and KNL architectures. This article shares our experiences and observations from our reproducibility activity and discusses our findings on three main sections: the weak scalability, the strong scalability, and the relationships between variables. The results of weak scalability and strong scalability were successfully reproduced. But due to the differences on the problem scale, input dataset, and system architecture, different behaviors regarding the polynomial degree were observed.","1558-2183","","10.1109/TPDS.2021.3051725","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324965","Reproducible computation;polynomial filtering eigensolver;planetary interior normal modes;scalability","Scalability;Computational modeling;Eigenvalues and eigenfunctions;Mathematical model;Computer architecture;Mars;Market research","eigenvalues and eigenfunctions;parallel algorithms;parallel machines;pattern clustering;polynomials","polynomial degree;planetary normal mode computation;parallel algorithms;SCC team;National Tsing Hua University;Student Cluster Competition;scalability evaluations;scale 256-node supercomputer;Moon dataset;Intel Skylake CPU architecture;Mars dataset;planetary interior normal modes;highly paralleled polynomial","",1.0,"",14.0,"IEEE","14 Jan 2021","","","IEEE","IEEE Journals"
"Critique of “Planetary Normal Mode Computation: Parallel Algorithms, Performance, and Reproducibility” by SCC Team From ETH Zurich","M. Burger; J. Kleine","Department of Computer Science, ETH Zürich, Zrich, Switzerland; Department of Computer Science, ETH Zürich, Zrich, Switzerland","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2627,2630,"This report analyzes the reproducibility of the article “Computing Planetary Interior Normal Modes with A Highly Parallel Polynomial Filtering Eigensolver” by Jia Shi et al. (Shi, 2018). To reproduce the results we perform different weak and strong scaling studies using a series of Mars models. All experimental runs were performed during the SC19 Student Cluster Competition on a four node Intel Skylake cluster. We show that the findings of the original article can be reproduced in a different environment.","1558-2183","","10.1109/TPDS.2021.3050500","National Supercomputing Centre; SPCL Group; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321175","Reproducibility;normal modes;student cluster competition","Analytical models;Runtime;Data models;Software;Hardware;Filtering;Data visualization","eigenvalues and eigenfunctions;geophysical fluid dynamics;parallel algorithms;pattern clustering","planetary normal mode computation;parallel algorithm;SCC team;ETH zurich;highly parallel polynomial filtering eigensolver;strong scaling studies;SC19 Student Cluster Competition;Intel Skylake;weak scaling studies;computing planetary interior normal modes","",1.0,"",2.0,"IEEE","12 Jan 2021","","","IEEE","IEEE Journals"
"Critique of “Planetary Normal Mode Computation: Parallel Algorithms, Performance, and Reproducibility” by SCC Team From Tsinghua University","C. Zhang; C. Zhao; J. He; S. Chen; L. Zheng; K. Huang; W. Han; J. Zhai","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2631,2634,"In this article we present our results from the SC19 Student Cluster Competition Reproducibility Challenge. The challenge entails reproducing the article entitled “Computing Planetary Interior Normal Modes with A Highly Parallel Polynomial Filtering Eigensolver” presented at SC'18, which proposes a parallel polynomial filtered Lanczos algorithm to directly calculate the planetary normal modes of heterogeneous planets. The proposed algorithm showed excellent performance with relatively low memory consumption and high parallel efficiency. In this work, we reproduce the scaling tests in that article on a cluster using Intel Cascade Lake architecture and use the proposed algorithm to illustrate specific normal modes of Mars. We compare the results obtained on our cluster with those in the original article. We also design a new metric to better analyze the results. In addition, we use the profiling tool Intel VTune Amplifier to explain our discoveries. Our results demonstrate that the given models show great scalability, which is similar to the original article. The required normal modes of Mars are also successfully calculated and visualized.","1558-2183","","10.1109/TPDS.2020.3049025","National Key Research and Development Program of China(grant numbers:2016YFB0200100); Tsinghua University Initiative Scientific Research Program(grant numbers:20191080594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314242","Reproducibility;scalability;geophysics;eigenvalues;eigenfunctions;profiling;student cluster competition","Computational modeling;Eigenvalues and eigenfunctions;Earth;Solid modeling;Libraries;Kernel;Clustering algorithms","application program interfaces;astronomy computing;eigenvalues and eigenfunctions;multiprocessing systems;parallel algorithms","planetary normal mode computation;parallel algorithms;SCC team;Tsinghua University;Computing Planetary Interior Normal Modes;highly parallel polynomial filtering eigensolver;Lanczos algorithm;planetary normal modes;heterogeneous planets;memory consumption;Intel Cascade Lake architecture;Intel VTune Amplifier","",1.0,"",8.0,"IEEE","5 Jan 2021","","","IEEE","IEEE Journals"
"Critique of “Planetary Normal Mode Computation: Parallel Algorithms, Performance, and Reproducibility” by SCC Team From University of Warsaw","M. Masiak; I. Kotlarska; U. Kondraciuk; M. Szpindler","Interdisciplinary Centre for Mathematical and Computational Modelling, University of Warsaw, Warszawa, Poland; Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, Warszawa, Poland; Faculty of Mathematics, Informatics and Mechanics, University of Warsaw, Warszawa, Poland; Interdisciplinary Centre for Mathematical and Computational Modelling, University of Warsaw, Warszawa, Poland","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2635,2638,"The Supercomputing conference holds a student cluster competition - A competition that aims to introduce undergraduate students to the world of High Performance Computing. One of the tasks at the SC19 conference was to reproduce the scaling results obtained in the article titled Computing Planetary Interior Normal Modes with a Highly Parallel Polynomial Filtering Eigensolver by Jia Shi et al. They introduced a new approach to the problem of calculating planetary normal modes. The developed method is a highly parallel algorithm that approximates the results via the mixed finite element method on unstructured tetrahedral meshes. The cluster used consisted of five 40-core Intel Xeon Cascade Lake nodes, equipped with 384 GB of RAM each. The original study was demonstrated on Stampede2 system on partitions ranging from 2 to 256 nodes with 48-core Intel Xeon Skylake and 192 GB of RAM each. Our cluster was equipped with Infiniband interconnect while Stampede2 used Intel Omni-Path network. We decided to work with HPC-X MPI library in place of Intel MPI used the reference study. The design - sizes and schedule - of experimental runs was the most challenging part of our study. We run both strong and weak scalability experiments in a one set of runs in order to fit in limited computing capacity and very tight time schedule. Due to limitations of our hardware, we had to split the weak scaling study into two separate smaller studies. We didn't manage to reproduce the exact results, however, we achieved similar scalability trend.","1558-2183","","10.1109/TPDS.2021.3050997","Interdyscyplinarne Centrum Modelowania Matematycznego i Komputerowego UW; Polish Ministry of Science and Higher Education(grant numbers:Najlepsi z najlepszych. 4.0); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321106","Reproducible computation;student cluster competition","Scalability;Computational modeling;Task analysis;Runtime;Hardware;Visualization;Standards","application program interfaces;finite element analysis;message passing;multiprocessing systems;parallel algorithms;parallel machines;polynomials","planetary normal mode computation;Parallel algorithms;SCC team;student cluster competition;undergraduate students;high performance computing;Computing Planetary Interior Normal Modes;highly parallel polynomial filtering eigensolver;planetary normal modes;highly parallel algorithm;mixed finite element method;unstructured tetrahedral meshes;Intel Xeon Cascade Lake;RAM;Stampede2 system;Intel Xeon Skylake;Intel Omni-Path network;HPC-X MPI library;Intel MPI;computing capacity;Infiniband interconnect;memory size 192.0 GByte;memory size 384.0 GByte","","","",6.0,"IEEE","12 Jan 2021","","","IEEE","IEEE Journals"
"Critique of “Planetary Normal Mode Computation: Parallel Algorithms, Performance, and Reproducibility” by SCC Team From University of Washington","D. Liu; M. Cinnamon; T. Garvin; A. Karavanov; S. Park; D. Strobeck; A. Lumsdaine","Department of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Department of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Department of Computer Science & Engineering, University of Washington, Seattle, WA, USA; Department of Applied Mathematics, University of Washington, Seattle, WA, USA; Department of Electrical & Computer Engineering, University of Washington, Seattle, WA, USA; Department of Political Science, University of Washington, Seattle, WA, USA; Department of Computer Science & Engineering, University of Washington, Seattle, WA, USA","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2639,2642,"One of the tasks for the SC19 Student Cluster Competition is to reproduce the results in the reproducibility challenge article “Computing Planetary Interior Normal Modes with a Highly Parallel Polynomial Filtering Eigensolver”, by J. Shi et al., which describes a highly parallel algorithm for computing planetary normal modes. In running experiments from the article, we study the weak and strong scalability of the algorithm, as well as the relationship between model size, degree of polynomial filter, and execution time. We investigate these findings on a two-node, 64-core Intel Skylake-based Xeon cluster. Unfortunately, we are able to confirm some, but not all, of the original findings, with discrepancies possibly due to a low number of experimental runs due to competition time limits as well as nonuniform scaling of compute resources.","1558-2183","","10.1109/TPDS.2021.3051943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325093","Reproducible computation;student cluster competition","Scalability;Computational modeling;Runtime;Mars;Eigenvalues and eigenfunctions;Software;Solids","eigenvalues and eigenfunctions;flow instability;geophysical fluid dynamics;multiprocessing systems;parallel algorithms;parallel architectures;pattern clustering","Highly Parallel Polynomial Filtering Eigensolver;highly parallel algorithm;planetary normal modes;weak scalability;strong scalability;polynomial filter;64-core Intel Skylake-based Xeon cluster;competition time;critique;Planetary Normal mode computation;Parallel algorithms;SCC team;SC19 Student Cluster Competition;reproducibility challenge article;Computing Planetary Interior Normal Modes","",1.0,"",2.0,"IEEE","14 Jan 2021","","","IEEE","IEEE Journals"
"Critique of “Planetary Normal Mode Computation: Parallel Algorithms, Performance, and Reproducibility” by SCC Team From Peking University","Y. Cheng; Z. Fan; J. Mai; Y. Wu; P. Xu; Y. Yan; Z. Fu; Y. Liang","Yuanpei College, Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China; Yuanpei College, Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China; School of Physics, Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China; School of Electronics and Computer Science (EECS), Peking University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2643,2645,"Shi et al. (2018) proposed a highly parallel polynomial filtering eigensolver for the computation of planetary normal modes. As a challenge at the Student Cluster Competition in The International Conference for High Performance Computing, Networking, Storage and Analysis (SC19), we reproduce the computational efficiency of the polynomial filtering eigensolver on our Intel Xeon machine. We present the weak scalability, scaling of runtime with model size (in a fixed interval) and the strong scalability results in this report.","1558-2183","","10.1109/TPDS.2020.3049050","Inspur; Nvidia; SitonHoly; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314073","Reproducible computation;student cluster competition","Runtime;Scalability;Computational modeling;Libraries;Hardware;Solid modeling;Solids","eigenvalues and eigenfunctions;flow instability;multiprocessing systems;parallel algorithms;parallel processing;pattern clustering","Intel Xeon machine;polynomial filtering eigensolver;Peking University;SCC team;reproducibility;parallel algorithms;planetary normal mode computation;computational efficiency;high performance computing;International Conference;Student Cluster Competition;planetary normal modes;highly parallel polynomial","",1.0,"",1.0,"IEEE","5 Jan 2021","","","IEEE","IEEE Journals"
"Trust: Triangle Counting Reloaded on GPUs","S. Pandey; Z. Wang; S. Zhong; C. Tian; B. Zheng; X. Li; L. Li; A. Hoisie; C. Ding; D. Li; H. Liu","Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Huazhong University of Science and Technology, Wuhan, Hubei, China; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Brookhaven National Laboratory, Upton, NY, USA; Brookhaven National Laboratory, Upton, NY, USA; Department of Computer Science & Engineering, University of Connecticut, Storrs, CT, USA; Department of Electrical Engineering and Computer Science, University of California, Merced, CA, USA; Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2646,2660,"Triangle counting is a building block for a wide range of graph applications. Traditional wisdom suggests that i) hashing is not suitable for triangle counting, ii) edge-centric triangle counting beats vertex-centric design, and iii) communication-free and workload balanced graph partitioning is a grand challenge for triangle counting. On the contrary, we advocate that i) hashing can help the key operations for scalable triangle counting on Graphics Processing Units (GPUs), i.e., list intersection and graph partitioning, ii) vertex-centric design reduces both hash table construction cost and memory consumption, which is limited on GPUs. In addition, iii) we exploit graph and workload collaborative, and hashing-based 2D partitioning to scale vertex-centric triangle counting over 1000 GPUs with sustained scalability. In this article, we present Trust which performs triangle counting with the hash operation and vertex-centric mechanism at the core. To the best of our knowledge, Trust is the first work that achieves over one trillion Traversed Edges Per Second (TEPS) rate for triangle counting.","1558-2183","","10.1109/TPDS.2021.3064892","National Science Foundation CRII(grant numbers:2000722,2046102); Exascale Computing Project(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; National Key Research and Development Program of China(grant numbers:2018YFB1003505); National Natural Science Foundation of China(grant numbers:61772265,61802172,62072228); NSFC-61872176; Leading-edge Technology Program of Jiangsu Natural Science Foundation(grant numbers:BK20202001); National Key Research and Development Program of China(grant numbers:2020YFB1005900); U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); U.S. Department of Energy; Brookhaven Science Associates(grant numbers:DE-SC0012704); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373989","GPGPU;triangle counting;graph algorithms;parallel processing","Graphics processing units;Two dimensional displays;Partitioning algorithms;Message systems;Instruction sets;Arrays;Time complexity","graph theory;graphics processing units","triangle counting reloaded;scalable triangle;vertex-centric triangle;GPUs;vertex-centric design;graphics processing unit","",9.0,"",92.0,"IEEE","9 Mar 2021","","","IEEE","IEEE Journals"
"Coflow Scheduling in Data Centers: Routing and Bandwidth Allocation","L. Shi; Y. Liu; J. Zhang; T. Robertazzi","Snap Inc., Santa Monica, CA, USA; Google Inc., Mountain View, CA, USA; Microsoft, Redmond, WA, USA; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2661,2675,"In distributed computing frameworks like MapReduce, Spark, and Dyrad, a coflow is a set of flows transferring data between two stages of a job. The job cannot start its next stage unless all flows in the coflow finish. To improve the execution performance of such a job, it is crucial to reduce the completion time of a coflow, as it can contribute more than 50 percent of the job completion time. While several coflow schedulers have been proposed, we observe that routing, as a factor greatly impacting the Coflow Completion Time (CCT), has not been well considered. In this article, we focus on the coflow scheduling problem and jointly consider routing and bandwidth allocation. We begin by providing an analytical solution to the problem of optimal bandwidth allocation with pre-determined routes. In the following, we formulate the problem of scheduling a single coflow as a Non-linear Mixed Integer Programming problem and present its relaxed convex optimization problem. We further propose two algorithms, CoRBA and its simplified version: CoRBA-fast that solve the single coflow scheduling problem with a joint consideration of routing and bandwidth allocation. Lastly, to address multiple coflows in online scheduling, we propose an online scheduler named OnCoRBA. By comparing with the start-of-the-art algorithms and schedulers via simulations, we demonstrate that CoRBA and CoRBA-fast reduce the CCT by 30-400 percent and the OnCoRBA scheduler reduces the average online CCT by 20-230 percent. In addition, CoRBA-fast can be hundreds times faster than CoRBA with around 8 percent performance degradation compared to CoRBA, which makes the use of CoRBA-fast very appropriate in practice.","1558-2183","","10.1109/TPDS.2021.3068424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384326","Coflow scheduling;distributed computing;non-linear programming;convex optimization","Routing;Bandwidth;Schedules;Channel allocation;Data centers;Job shop scheduling;Optimal scheduling","bandwidth allocation;computer centres;computer networks;convex programming;integer programming;nonlinear programming;telecommunication network routing;telecommunication scheduling","nonlinear mixed integer programming problem;relaxed convex optimization problem;CoRBA-fast;single coflow scheduling problem;multiple coflows;online scheduling;OnCoRBA scheduler;data centers;distributed computing frameworks;job completion time;Coflow Completion Time;pre-determined routes;MapReduce;Spark;Dyrad;CCT;analytical solution;optimal bandwidth allocation problem;performance degradation;joint routing-bandwidth allocation;efficiency 8.0 percent;efficiency 50.0 percent;efficiency 400.0 percent;efficiency 230.0 percent","",3.0,"",27.0,"IEEE","23 Mar 2021","","","IEEE","IEEE Journals"
"Efficient Forwarding Anomaly Detection in Software-Defined Networks","Q. Li; Y. Liu; Z. Liu; P. Zhang; C. Pang","Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; University of Illinois, Urbana-Champaign, IL, USA; School of Computer Science, Xi'an Jiaotong University, Xi'an, China; Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2676,2690,"Data centers, the critical infrastructure underpinning Cloud computing, often employ Software-Defined Networks (SDN) to manage cluster, wide-area and enterprise networks. As the network forwarding in SDN is dynamically programmed by controllers, it is crucial to ensure that the controller intent is correctly translated into underlying forwarding rules. Therefore, detecting and locating forwarding anomalies in SDN is a fundamental problem in production networks. Existing research proposals, roughly categorized into probing-based, packet piggybacking-based, and flow statistics analysis-based, either impose significant overhead or do not provide sufficient coverage for certain forwarding anomalies. In this article, we propose ${\sf FADE}$FADE, a controllable and passive measuring scheme to simultaneously deliver detection efficiency and accuracy. ${\sf FADE}$FADE first analyzes the entire network topology and flow rules, and then computes a minimal set of flows that can cover all forwarding rules. For each selected network flow, ${\sf FADE}$FADE decides the optimal number of monitoring positions on its path (much less than total number of hops), and installs dedicated rules to collect flow statistics. ${\sf FADE}$FADE controls the installation and expiration of these rules, along with unique flow labels, to guarantee the accuracy of collected statistics, based on which ${\sf FADE}$FADE algorithmically decides whether a forwarding anomaly is detected, and if so it further locates the anomaly. On top of ${\sf FADE}$FADE, we propose ${\sf iFADE}$iFADE (a more scalable version of ${\sf FADE}$FADE) to further optimize the usage and deployment of dedicated measurement rules. ${\sf iFADE}$iFADE achieves over 40 percent rule reduction compared with ${\sf FADE}$FADE . We implement a prototype of both ${\sf FADE}$FADE and ${\sf iFADE}$iFADE in about 12000 lines of code and evaluate the prototype extensively. The experiment results demonstrate ${\sf (i)}$(i) ${\sf FADE}$FADE and ${\sf iFADE}$iFADE are accurate, e.g., they achieve over 95 percent true positive rate and 99 percent true negative rate in anomaly detection; ${\sf (ii)}$(ii) ${\sf FADE}$FADE and ${\sf iFADE}$iFADE are lightweight, e.g., they reduce the overhead of control messages compared with state-of-the-art by about 50 and 90 percent, respectively.","1558-2183","","10.1109/TPDS.2021.3068135","National Key Research and Development Program of China(grant numbers:2018YFB1800304); National Natural Science Foundation of China(grant numbers:61572278,61772412); BNRist(grant numbers:BNR2020RC01013); K. C. Wong Education Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387572","Software defined networking;cross-plane consistency check;forwarding anomaly","Anomaly detection;Probes;Network topology;Data centers;Prototypes;Production;Hardware","business communication;cloud computing;computer centres;computer network management;computer network security;critical infrastructures;software defined networking;statistical analysis;telecommunication network topology;telecommunication traffic;wide area networks","efficient forwarding anomaly detection;critical infrastructure;software-defined networks;SDN;wide-area network;enterprise networks;forwarding rules;production networks;packet piggybacking;FADE;detection efficiency;network topology;flow rules;network flow;unique flow labels;iFADE;rule reduction;controllable measuring scheme;passive measuring scheme;cloud computing;data centers;cluster network management;flow statistics analysis;measurement rules","",6.0,"",49.0,"CCBY","26 Mar 2021","","","IEEE","IEEE Journals"
"Timestamped State Sharing for Stream Analytics","Y. Zhao; Z. Liu; Y. Wu; G. Jiang; J. Cheng; K. Liu; X. Yan","Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2691,2704,"State access in existing distributed stream processing systems is restricted locally within each operator. However, in advanced stream analytics such as online learning and dynamic graph analytics, enabling state sharing across different operators makes application development easier and stream processing more efficient. In addition, when stream records are timestamped, proper time semantics should be defined for both state updates and fetches. We propose a new state abstraction to address the limitations of existing systems and develop a distributed stream processing system, Nova, with native support for timestamped state sharing. We validate the expressiveness and efficiency of Nova with extensive experiments.","1558-2183","","10.1109/TPDS.2021.3073253","GRF(grant numbers:14208318); HKSAR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404871","State sharing;distributed stream processing;online learning;dynamic graph analytics","Semantics;Pattern matching;Throughput;Sparks;Real-time systems;Industries","data analysis;distributed processing;graph theory;learning (artificial intelligence);query processing","timestamped state sharing;state access;distributed stream processing system;advanced stream analytics;dynamic graph analytics;stream records;state updates;fetches;state abstraction","",2.0,"",36.0,"IEEE","14 Apr 2021","","","IEEE","IEEE Journals"
"MCFsyn: A Multi-Party Set Reconciliation Protocol With the Marked Cuckoo Filter","L. Luo; D. Guo; Y. Zhao; O. Rottenstreich; R. T. B. Ma; X. Luo","Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; China Electronic Equipment System Engineering Company, Beijing, China; Israel Institute of Technology and ORBS Research, Haifa, Israel; School of Computing, National University of Singapore, Singapore; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","14 May 2021",2021,32.0,11.0,2705,2718,"Multi-party set reconciliation is a key component in distributed and networking systems. It naturally contains two dimensions, i.e., set representation and reconciliation protocol. However, existing sketch data structures are insufficient to satisfy the new needs brought by the multi-party scenario simultaneously, including space-efficiency, mergeability, and completeness. The current reconciliation protocols, on the other hand, fail to achieve the global optimization of communication cost. To this end, in this article, we propose the marked cuckoo filter (MCF), a data structure for representing set members. Grounded on MCF, we implement the MCFsyn protocol to reconcile multiple sets. MCFsyn aggregates and distributes sets information represented by MCFs along with an underlying minimum spanning tree among the participants. The participants then identify the different elements by traversing the overall MCF which contains the information of all elements in the union set. For the identified missing elements, MCFsyn helps the participants to choose the optimal senders to fetch with the minimum communication cost. Comprehensive evaluations indicate that MCFsyn significantly outperforms existing alternatives in terms of both reconciliation accuracy and communication cost.","1558-2183","","10.1109/TPDS.2021.3074440","National Key Research and Development Program of China(grant numbers:2018YFB1800203); National Natural Science Foundation of China(grant numbers:62002378); Research Funding of NUDT(grant numbers:ZK20-3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9409946","Set reconciliation;minimum spanning tree;marked cuckoo filter;accuracy;communication cost","Protocols;Data structures;Cloud computing;Aggregates;Servers;Hash functions;Relays","data structures;optimisation;protocols;set theory;trees (mathematics)","global optimization;minimum spanning tree;networking systems;sketch data structures;set members;space-efficiency;multiparty scenario;set representation;marked cuckoo filter;multiparty set reconciliation protocol;reconciliation accuracy;minimum communication cost;union set;MCF;sets information;MCFsyn aggregates;multiple sets;MCFsyn protocol","",4.0,"",51.0,"CCBY","21 Apr 2021","","","IEEE","IEEE Journals"
"High Performance Multivariate Geospatial Statistics on Manycore Systems","M. L. O. Salvaña; S. Abdulah; H. Huang; H. Ltaief; Y. Sun; M. G. Genton; D. E. Keyes","Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia","IEEE Transactions on Parallel and Distributed Systems","14 Jun 2021",2021,32.0,11.0,2719,2733,"Modeling and inferring spatial relationships and predicting missing values of environmental data are some of the main tasks of geospatial statisticians. These routine tasks are accomplished using multivariate geospatial models and the cokriging technique. The latter requires the evaluation of the expensive Gaussian log-likelihood function, which has impeded the adoption of multivariate geospatial models for large multivariate spatial datasets. However, this large-scale cokriging challenge provides a fertile ground for supercomputing implementations for the geospatial statistics community as it is paramount to scale computational capability to match the growth in environmental data coming from the widespread use of different data collection technologies. In this article, we develop and deploy large-scale multivariate spatial modeling and inference on parallel hardware architectures. To tackle the increasing complexity in matrix operations and the massive concurrency in parallel systems, we leverage low-rank matrix approximation techniques with task-based programming models and schedule the asynchronous computational tasks using a dynamic runtime system. The proposed framework provides both the dense and the approximated computations of the Gaussian log-likelihood function. It demonstrates accuracy robustness and performance scalability on a variety of computer systems. Using both synthetic and real datasets, the low-rank matrix approximation shows better performance compared to exact computation, while preserving the application requirements in both parameter estimation and prediction accuracy. We also propose a novel algorithm to assess the prediction accuracy after the online parameter estimation. The algorithm quantifies prediction performance and provides a benchmark for measuring the efficiency and accuracy of several approximation techniques in multivariate spatial modeling.","1558-2183","","10.1109/TPDS.2021.3071423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397281","Gaussian log-likelihood;geospatial statistics;high-performance computing;large multivariate spatial data;low-rank approximation;multivariate modeling/prediction","Geospatial analysis;Computational modeling;Meteorology;Predictive models;Numerical models;Mathematical model;Graphics processing units","approximation theory;concurrency (computers);data analysis;environmental science computing;Gaussian processes;geographic information systems;matrix algebra;multiprocessing systems;parallel processing;statistical analysis","high performance multivariate geospatial statistics;manycore systems;environmental data;multivariate geospatial models;cokriging technique;Gaussian log-likelihood function;geospatial statistics community;data collection;parallel systems;low-rank matrix approximation;task-based programming;asynchronous computational tasks;dynamic runtime system;large-scale multivariate spatial datasets;massive concurrency;online parameter estimation;supercomputing","",5.0,"",79.0,"IEEE","6 Apr 2021","","","IEEE","IEEE Journals"
"Memory-Side Prefetching Scheme Incorporating Dynamic Page Mode in 3D-Stacked DRAM","M. M. Rafique; Z. Zhu","Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA","IEEE Transactions on Parallel and Distributed Systems","27 May 2021",2021,32.0,11.0,2734,2747,"Modern multiprocessor systems running multiple applications concurrently exhibit irregular memory access pattern during different phases of execution. The principle of locality is hard to exploit in the presence of such irregular memory requests and may result in additional delays due to resource conflicts throughout memory hierarchy. Prefetching is a promising technique to reduce the memory access latency where data is speculatively fetched ahead of time and stored in a faster memory structure like cache or dedicated prefetch buffer. The emergence of 3D-stacked DRAM provides huge internal bandwidth that makes memory-side prefetching an effective approach to improving system performance. Leveraging the unique architecture of 3D-stacked DRAM, we introduce a memory-side prefetching scheme that works in conjunction with dynamic page mode to reduce memory access latency. We introduce a novel prefetch buffer management scheme that makes intelligent replacement decision based on the utilization and recency of the prefetched data, which also serves as a guidance for future prefetching. Simulation results indicate that our approach improves performance by 21.8 percent on average, compared to a baseline scheme that prefetches a whole row on consecutive hits and implements static open page policy. Our scheme also outperforms an existing memory-side prefetching scheme by 13.2 percent on average, which dynamically adjusts the prefetch degree based on the usefulness of prefetched data.","1558-2183","","10.1109/TPDS.2020.3044856","National Science Foundation(grant numbers:CCF-1513899); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9293400","3D-stacked DRAM;memory bandwidth;memory-side prefetching;page mode","Prefetching;Random access memory;Memory management;Bandwidth;Program processors;Through-silicon vias;Three-dimensional displays","cache storage;DRAM chips;microprocessor chips;multiprocessing systems;storage management","3D-stacked DRAM;multiprocessor systems;irregular memory access pattern;irregular memory requests;memory hierarchy;memory structure;dynamic page mode;prefetched data;future prefetching;baseline scheme;prefetch degree;static open page policy;prefetch buffer management","","","",57.0,"IEEE","14 Dec 2020","","","IEEE","IEEE Journals"
"WindFlow: High-Speed Continuous Stream Processing With Parallel Building Blocks","G. Mencagli; M. Torquati; A. Cardaci; A. Fais; L. Rinaldi; M. Danelutto","Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy","IEEE Transactions on Parallel and Distributed Systems","26 May 2021",2021,32.0,11.0,2748,2763,"Nowadays, we are witnessing the diffusion of Stream Processing Systems (SPSs) able to analyze data streams in near realtime. Traditional SPSs like Storm and Flink target distributed clusters and adopt the continuous streaming model, where inputs are processed as soon as they are available while outputs are continuously emitted. Recently, there has been a great focus on SPSs for scale-up machines. Some of them (e.g., BriskStream) still use the continuous model to achieve low latency. Others optimize throughput with batching approaches that are, however, often inadequate to minimize latency for live-streaming applications. Our contribution is to show a novel software engineering approach to design the runtime system of SPSs targeting multicores, with the aim of providing a uniform solution able to optimize throughput and latency. The approach has a formal nature based on the assembly of components called building blocks, whose composition allows optimizations to be easily expressed in a compositional manner. We use this methodology to build a new SPS called WindFlow. Our evaluation showcases the benefits of WindFlow: it provides lower latency than SPSs for continuous streaming, and can be configured to optimize throughput, to perform similarly and even better than batch-based scale-up SPSs.","1558-2183","","10.1109/TPDS.2021.3073970","European H2020 Project TEACHING(grant numbers:871385); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408386","Data stream processing;multicore programming;parallel computing","Runtime;Throughput;Libraries;Multicore processing;Algebra;Semantics","","","",6.0,"",35.0,"IEEE","19 Apr 2021","","","IEEE","IEEE Journals"
"Improved MPC Algorithms for Edit Distance and Ulam Distance","M. Boroujeni; M. Ghodsi; S. Seddighin","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; School of Computer Science, Institute for Research in Fundamental Sciences, Tehran, Iran; Toyota Technological Institute at Chicago, Chicago, IL, USA","IEEE Transactions on Parallel and Distributed Systems","24 May 2021",2021,32.0,11.0,2764,2776,"Edit distance is one of the most fundamental problems in combinatorial optimization to measure the similarity between strings. Ulam distance is a special case of edit distance where no character is allowed to appear more than once in a string. Recent developments have been very fruitful for obtaining fast and parallel algorithms for both edit distance and Ulam distance. In this work, we present an almost optimal MPC (massively parallel computation) algorithm for Ulam distance and improve MPC algorithms for edit distance. Our algorithm for Ulam distance is almost optimal in the sense that (1) the approximation factor of our algorithm is 1+ε1+ε, (2) the round complexity of our algorithm is constant, (3) the total memory of our algorithm is almost linear (~Oε (n)Õε(n)), and (4) the overall running time of our algorithm is almost linear which is the best known for Ulam distance. We also improve the work of Hajiaghayi et al. for edit distance in terms of total memory. The best previously known MPC algorithm for edit distance requires ~O(n2x)Õ(n2x) machines when the memory of each machine is bounded by ~O(n1-x)Õ(n1-x). In this work, we improve the number of machines to ~O(n(9/5)x)Õ(n(9/5)x) while keeping the memory limit intact. Moreover, the round complexity of our algorithm is constant and the total running time of our algorithm is truly subquadratic. However, our improvement comes at the expense of a constant factor in the approximation guarantee of the algorithm. This improvement is inspired by the recent techniques of Boroujeni et al. and Chakraborty et al. for obtaining truly subquadratic time algorithms for edit distance.","1558-2183","","10.1109/TPDS.2021.3076534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419758","MapReduce;parallel algorithms;approximation algorithms;ulam distance;edit distance","Approximation algorithms;Optimized production technology;Computer science;Computational modeling;Complexity theory;Transforms;Distributed databases","approximation theory;computational complexity;graph theory;optimisation","combinatorial optimization;optimal MPC algorithm;Ulam distance;edit distance","",1.0,"",31.0,"IEEE","29 Apr 2021","","","IEEE","IEEE Journals"
"Offloading Tasks With Dependency and Service Caching in Mobile Edge Computing","G. Zhao; H. Xu; Y. Zhao; C. Qiao; L. Huang","Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, Jiangsu, China; Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, Jiangsu, China; Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, Jiangsu, China; Department of Computer Science & Engineering, University at Buffalo, Buffalo, NY, USA; Suzhou Institute for Advanced Study, University of Science and Technology of China, Suzhou, Jiangsu, China","IEEE Transactions on Parallel and Distributed Systems","25 May 2021",2021,32.0,11.0,2777,2792,"In Mobile Edge Computing (MEC), many tasks require specific service support for execution and in addition, have a dependent order of execution among the tasks. However, previous works often ignore the impact of having limited services cached at the edge nodes on (dependent) task offloading, thus may lead to an infeasible offloading decision or a longer completion time. To bridge the gap, this article studies how to efficiently offload dependent tasks to edge nodes with limited (and predetermined) service caching. We formally define the problem of offloading dependent tasks with service caching (ODT-SC), and prove that there exists no algorithm with constant approximation for this hard problem. Then, we design an efficient convex programming based algorithm (CP) to solve this problem. Moreover, we study a special case with a homogeneous MEC and propose a favorite successor based algorithm (FS) to solve this special case with a competitive ratio of O(1)O(1)<; inline-graphic xlink:href=""zhao-ieq1-3076687.gif""/>. Extensive simulation results using Google data traces show that our proposed algorithms can significantly reduce applications' completion time by about 21-47 percent compared with other alternatives.","1558-2183","","10.1109/TPDS.2021.3076687","National Natural Science Foundation of China(grant numbers:61822210,61936015,U1709217); Anhui Initiative in Quantum Information Technologies(grant numbers:AHY150300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419755","Mobile edge computing;task offloading;service caching;dependency;approximation","Task analysis;Approximation algorithms;Face recognition;Edge computing;Feature extraction;Optimization;Mobile handsets","cache storage;cloud computing;convex programming;mobile computing;scheduling","service caching;mobile edge computing;specific service support;dependent order;edge nodes;task offloading;longer completion time;efficiently offload dependent tasks;MEC;infeasible offloading decision tasks;ODT-SC;constant approximation;convex programming based algorithm;efficiency 47.0 percent","",28.0,"",48.0,"IEEE","29 Apr 2021","","","IEEE","IEEE Journals"
"Efficient Virtual Network Embedding of Cloud-Based Data Center Networks into Optical Networks","W. Fan; F. Xiao; X. Chen; L. Cui; S. Yu","Jiangsu High Technology Research Key Laboratory, Wireless Sensor Networks, Nanjing, Jiangsu, China; Jiangsu High Technology Research Key Laboratory, Wireless Sensor Networks, Nanjing, Jiangsu, China; Jiangsu High Technology Research Key Laboratory, Wireless Sensor Networks, Nanjing, Jiangsu, China; School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia; School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","24 May 2021",2021,32.0,11.0,2793,2808,"The demand for data center bandwidth has exploded due to the continuous development of cloud computing, causing the use of network resources close to saturation. Optical network has become an encouraging technology for many burgeoning networks and parallel/distributed computing applications because of its huge bandwidth. This article focuses on efficient embedding of data centers into optical networks, which aims to reduce complexity of the network topology by using the parallel transmission characteristics of optical fiber. We first present a novel virtual network embedding (VNE) mathematical model used for optical data center networks. Then we derive a priority of location VNE algorithm according to node proximity sensing and path comprehensive evaluation. Furthermore, we propose routing and wavelength assignment for DCNs into optical networks, and identify the lower bound of the required number of wavelengths. Extensive evaluations show that the proposed embedding algorithm can reduce the average waiting time of virtual network requests by 20 percent, increase the request acceptance rate and revenue-overhead ratio by 13 percent, as compared to the latest VNE algorithm.","1558-2183","","10.1109/TPDS.2021.3075296","National Natural Science Foundation of China(grant numbers:61932013); Research Foundation of Jiangsu(grant numbers:BRA2020065); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200753); NUPTSF(grant numbers:NY219151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415134","Data center network;network virtualization;optical network;embedding;performance evaluation","Optical fiber networks;Bandwidth;Heuristic algorithms;Optical sensors;Approximation algorithms;Multicast algorithms;Data centers","cloud computing;computer centres;computer networks;optical fibre networks;resource allocation;telecommunication network routing;telecommunication network topology;virtualisation","efficient virtual network embedding;cloud-based data center networks;optical network;data center bandwidth;network resources;burgeoning networks;data centers;network topology;novel virtual network;optical data center networks;virtual network requests;efficiency 20.0 percent;efficiency 13.0 percent","",11.0,"",42.0,"IEEE","23 Apr 2021","","","IEEE","IEEE Journals"
"A Split Execution Model for SpTRSV","N. Ahmad; B. Yilmaz; D. Unat","Department of Computer Science and Engineering, Koç University, Istanbul, Turkey; Department of Computer Science, Istinye University, Istanbul, Turkey; Department of Computer Science and Engineering, Koç University, Istanbul, Turkey","IEEE Transactions on Parallel and Distributed Systems","24 May 2021",2021,32.0,11.0,2809,2822,"Sparse Triangular Solve (SpTRSV) is an important and extensively used kernel in scientific computing. Parallelism within SpTRSV depends upon matrix sparsity pattern and, in many cases, is non-uniform from one computational step to the next. In cases where the SpTRSV computational steps have contrasting parallelism characteristics- some steps are more parallel, others more sequential in nature, the performance of an SpTRSV algorithm may be limited by the contrasting parallelism characteristics. In this work, we propose a split-execution model for SpTRSV to automatically divide SpTRSV computation into two sub-SpTRSV systems and an SpMV, such that one of the sub-SpTRSVs has more parallelism than the other. Each sub-SpTRSV is then computed using different SpTRSV algorithms, which are possibly executed on different platforms (CPU or GPU). By analyzing the SpTRSV Directed Acyclic Graph (DAG) and matrix sparsity features, we use a heuristics-based approach to (i) automatically determine the suitability of an SpTRSV for split-execution, (ii) find the appropriate split-point, and (iii) execute SpTRSV in a split fashion using two SpTRSV algorithms while managing any required inter-platform communication. Experimental evaluation of the execution model on two CPU-GPU machines with a matrix dataset of 327 matrices from the SuiteSparse Matrix Collection shows that our approach correctly selects the fastest SpTRSV method (split or unsplit) for 88 percent of matrices on the Intel Xeon Gold (6148) + NVIDIA Tesla V100 and 83 percent on the Intel Core I7 + NVIDIA G1080 Ti platform achieving speedups up to 10x and 6.36x respectively.","1558-2183","","10.1109/TPDS.2021.3074501","Aramco Overseas Company; Saudi Aramco; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9409717","Sparse triangular solve;CPU-GPU computing;heterogeneous computing;sparse linear systems;SpTRSV;SpTS","Sparse matrices;Parallel algorithms;Computational modeling;Kernel;Graphics processing units;Fats;Phased arrays","directed graphs;graphics processing units;multiprocessing systems;parallel processing;sparse matrices","fastest SpTRSV method;CPU-GPU machines;DAG;directed acyclic graph;sub SpTRSV systems;scientific computing;matrix sparsity features;different SpTRSV algorithms;SpTRSV computation;contrasting parallelism characteristics;SpTRSV algorithm;parallelism characteristics;SpTRSV computational steps;computational step;split execution model","","","",45.0,"IEEE","20 Apr 2021","","","IEEE","IEEE Journals"
"An Incremental Iterative Acceleration Architecture in Distributed Heterogeneous Environments With GPUs for Deep Learning","X. Zhang; Z. Tang; L. Du; L. Yang","College of Computer Science and Electronic Engineering, National Supercomputing Center in Changsha, Hunan University, Changsha, China; Science and Technology on Parallel and Distributed Processing Laboratory, National University of Defense Technology, Changsha, China; College of Computer Science and Electronic Engineering, National Supercomputing Center in Changsha, Hunan University, Changsha, China; College of Computer and Communication Engineering, Changsha University of Science and Technology, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","26 May 2021",2021,32.0,11.0,2823,2837,"The parallel computing capabilities of GPUs have a significant impact on computationally intensive iterative tasks. Offloading part or all of the deep learning tasks from the CPU to the GPU for execution is mainstream. However, a large number of redundant iterative calculations exist in the iterative process of computing tasks. Therefore, we propose a GPU-based distributed incremental iterative computing architecture that can make full use of distributed parallel computing and GPU memory structure. The architecture supports deep learning and other computationally intensive iterative applications by optimizing data placement and reducing redundant iterative calculations. To support block-based data partitioning and coalesced memory access on GPUs, we propose GDataSet, an abstract data set. The GPU incremental iteration manager called GTracker is designed to be responsible for GDataSet cache management on the GPU. In order to solve the limitation of on-chip memory size, we propose a variable sliding window mechanism. It improves the hit rate of cache access and the speed of data access by realizing the best block arrangement between on-chip memory and off-chip memory. Besides, a communication channel based on an incremental iterative model is designed to support data transmission and task communication in cluster computing. Finally, we implement the proposed architecture based on Spark 2.4.1 and CUDA 10.0. Comparative experiments with widely used computationally intensive iterative applications (K-means, LSTM, etc.) show that the incremental iterative acceleration architecture can significantly improve the efficiency of iterative computing.","1558-2183","","10.1109/TPDS.2021.3078254","National Key Research and Development Program of China(grant numbers:2018YFB1701400,2018YFB0203804,2017YFB0202201); National Natural Science Foundation of China(grant numbers:92055213,61873090,L1924056,62002114); China Knowledge Centre for Engineering Sciences and Technology(grant numbers:CKCEST-2020-2-5); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426412","Deep learning;distributed computing;incremental iteration;GPU","Graphics processing units;Computational modeling;Deep learning;Training;Task analysis;Sparks;Parallel processing","cache storage;computer graphic equipment;deep learning (artificial intelligence);graphics processing units;iterative methods;parallel architectures","redundant iterative calculations;cluster computing;task communication;data transmission;incremental iterative model;off-chip memory;data access;on-chip memory size;GPU incremental iteration manager;abstract data set;memory access;block-based data partitioning;data placement;GPU memory structure;distributed parallel computing;incremental iterative computing architecture;GPU-based;iterative process;deep learning tasks;intensive iterative tasks;parallel computing capabilities;distributed heterogeneous environments;incremental iterative acceleration architecture","",1.0,"",36.0,"IEEE","7 May 2021","","","IEEE","IEEE Journals"
"Multi-Queue Request Scheduling for Profit Maximization in IaaS Clouds","S. Wang; X. Li; Q. Z. Sheng; R. Ruiz; J. Zhang; A. Beheshti","Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, China; Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, China; Department of Computing, Macquarie University, Sydney, NSW, Australia; Grupo de Sistemas de Optimización Aplicada, Universitat Politècnica de València, Camino de Vera s/n, València, Spain; School of Computer Science and Engineering, Southeast University, Nanjing, China; Department of Computing, Macquarie University, Sydney, NSW, Australia","IEEE Transactions on Parallel and Distributed Systems","26 May 2021",2021,32.0,11.0,2838,2851,"In cloud computing, service providers rent heterogeneous servers from cloud providers, i.e., Infrastructure as a Service (IaaS), to meet requests of consumers. The heterogeneity of servers and impatience of consumers pose great challenges to service providers for profit maximization. In this article, we transform this problem into a multi-queue model where the optimal expected response time of each queue is theoretically analyzed. A multi-queue request scheduling algorithm framework is proposed to maximize the total profit of service providers, which consists of three components: request stream splitting, requests allocation, and server assignment. A request stream splitting algorithm is designed to split the arriving requests to minimize the response time in the multi-queue system. An allocation algorithm, which adopts a one-step improvement strategy, is developed to further optimize the response time of the requests. Furthermore, an algorithm is developed to determine the appropriate number of required servers of each queue. After statistically calibrating parameters and algorithm components over a comprehensive set of random instances, the proposed algorithms are compared with the state-of-the-art over both simulated and real-world instances. The results indicate that the proposed multi-queue request scheduling algorithm outperforms the other algorithms with acceptable computational time.","1558-2183","","10.1109/TPDS.2021.3075254","National Key Research and Development Program of China(grant numbers:2017YFB1400800); National Natural Science Foundation of China(grant numbers:61872077,61832004); Collaborative Innovation Center of Wireless Communications Technology; Australian Research Council Future Fellowship(grant numbers:FT140101247); Discovery Project(grant numbers:DP180102378); Spanish Ministry of Science, Innovation; OPTEP-Port Terminal Operations Optimization(grant numbers:RTI2018-094940-B-I00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415144","Profit maximization;consumer impatience;queue;scheduling;cloud computing","Servers;Time factors;Cloud computing;Task analysis;Queueing analysis;Resource management;Scheduling algorithms","cloud computing;optimisation;queueing theory;resource allocation;scheduling","profit maximization;IaaS clouds;cloud computing;service providers rent heterogeneous servers;cloud providers;multiqueue model;optimal expected response time;multiqueue request scheduling algorithm framework;requests allocation;server assignment;request stream splitting algorithm;arriving requests;multiqueue system;allocation algorithm;algorithm components;acceptable computational time","",3.0,"",43.0,"IEEE","23 Apr 2021","","","IEEE","IEEE Journals"
"MG-WFBP: Merging Gradients Wisely for Efficient Communication in Distributed Deep Learning","S. Shi; X. Chu; B. Li","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong, China; Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","22 Feb 2021",2021,32.0,8.0,1903,1917,"Distributed synchronous stochastic gradient descent has been widely used to train deep neural networks (DNNs) on computer clusters. With the increase of computational power, network communications generally limit the system scalability. Wait-free backpropagation (WFBP) is a popular solution to overlap communications with computations during the training process. In this article, we observe that many DNNs have a large number of layers with only a small amount of data to be communicated at each layer in distributed training, which could make WFBP inefficient. Based on the fact that merging some short communication tasks into a single one can reduce the overall communication time, we formulate an optimization problem to minimize the training time in pipelining communications and computations. We derive an optimal solution that can be solved efficiently without affecting the training performance. We then apply the solution to propose a distributed training algorithm named merged-gradient WFBP (MG-WFBP) and implement it in two platforms Caffe and PyTorch. Extensive experiments in three GPU clusters are conducted to verify the effectiveness of MG-WFBP. We further exploit trace-based simulations of 4 to 2048 GPUs to explore the potential scaling efficiency of MG-WFBP. Experimental results show that MG-WFBP achieves much better scaling performance than existing methods.","1558-2183","","10.1109/TPDS.2021.3052862","Hong Kong RGC GRF(grant numbers:HKBU 12200418,HKUST 16206417,16207818); RGC CRF(grant numbers:C7036-15G); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328614","Deep learning;GPU;distributed stochastic gradient descent;gradient communication;merged-gradient","Training;Backpropagation;Hardware;Graphics processing units;Tensors;Neural networks;Data models","backpropagation;deep learning (artificial intelligence);gradient methods;graphics processing units;optimisation;stochastic processes","GPU clusters;PyTorch platform;Caffe platform;optimal solution;optimization problem;wait-free backpropagation;pipelining communications;communication time;short communication tasks;network communications;computational power;computer clusters;DNNs;deep neural networks;distributed synchronous stochastic gradient descent;distributed deep learning;MG-WFBP;merged-gradient WFBP;distributed training algorithm","",9.0,"",46.0,"IEEE","19 Jan 2021","","","IEEE","IEEE Journals"
"Burst Load Evacuation Based on Dispatching and Scheduling In Distributed Edge Networks","S. Deng; C. Zhang; C. Li; J. Yin; S. Dustdar; A. Y. Zomaya","College of Computer Science, Zhejiang University, Hangzhou, PR China; College of Computer Science, Zhejiang University, Hangzhou, PR China; College of Computer Science, Zhejiang University, Hangzhou, PR China; College of Computer Science, Zhejiang University, Hangzhou, PR China; Distributed Systems Group, TU Wien, Vienna, Austria; School of Computer Science, The University of Sydney, Sydney, Australia","IEEE Transactions on Parallel and Distributed Systems","22 Feb 2021",2021,32.0,8.0,1918,1932,"Edge computing, a fast evolving computing paradigm, has spawned a variety of new system architectures and computing methods discussed in both academia and industry. Edge servers are directly deployed near users’ equipment or devices owned by telecommunications companies. This allows for offloading computing tasks of various devices nearby to edge servers. Due to the shortage of computing resources in edge computing networks, they are often not as sufficient as the computing resources in a cloud computing center. This leads to the problem of service load imbalance once the load in the edge computing network increases suddenly. To solve the problem of “load evacuation” in edge environments, we introduce a strategy when the number of service requests for mobile devices or IoT devices increases rapidly within a short period of time. Therefore, to prevent poor QoS in edge computing, service load should be migrated to other edge servers to reduce the overall delay of these service requests. In this article, we have introduced a strategy with two stages during the burst load evacuation. Based on an optimal routing search at the dispatching stage, tasks will be migrated from the server in which the burst load occurs to other servers as soon as possible. Subsequently, with the assistance of the remote server and edge servers, these tasks are processed with the highest efficiency through the proposed parallel structure at the scheduling stage. Finally, we conduct numerical experiments to clarify the superiority of our algorithm in an edge environment simulation.","1558-2183","","10.1109/TPDS.2021.3052236","National Natural Science Foundation of China(grant numbers:U20A20173,61772461); Natural Science Foundation of Zhejiang Province(grant numbers:LR18F020003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328221","Edge computing;routing search;online scheduling","Job shop scheduling;Uncertainty;Processor scheduling;Dispatching;Servers;Task analysis;Edge computing","cloud computing;Internet of Things;mobile computing","edge environment simulation;burst load evacuation;distributed edge networks;computing methods;edge servers;computing resources;edge computing networks;cloud computing center;service load imbalance;edge environments;service requests;telecommunications companies;offloading computing tasks;mobile devices;IoT devices;optimal routing search","",21.0,"",44.0,"IEEE","18 Jan 2021","","","IEEE","IEEE Journals"
"An Optimized Weighted Average Makespan in Fault-Tolerant Heterogeneous MPSoCs","H. Youness; A. Omar; M. Moness","Department of Computers and Systems Engineering, Minia University, Minia, CO, Egypt; Department of Computers and Systems Engineering, Minia University, Minia, CO, Egypt; Department of Computers and Systems Engineering, Minia University, Minia, CO, Egypt","IEEE Transactions on Parallel and Distributed Systems","22 Feb 2021",2021,32.0,8.0,1933,1946,"The multiprocessor system on chips (MPSoCs) are considered today the core of most modern systems. Most of the applications of these heterogeneous MPSoCs include critical systems and hence terms of fault tolerance and reliability have become essential. Task replication is a technique to carry out fault tolerance and can help for reducing the schedule length by increasing locality. It introduces an upper and lower bound for the makespan of each schedule while each task is replicated more than once. If a fault occurs during execution, the expected makespan will be some value between the upper bound and the lower bound based on when and where the fault has occurred. In this research a new performance parameter namely the weighted average makespan is introduced. It is calculated as the average of the lower and upper bounds of makespan using the probability of occurrence of each. Two scheduling algorithms are presented for fault tolerant scheduling based on directed acyclic graphs. These algorithms are the list scheduling algorithm and the optimizing of the weighted average makespan based on simulated annealing method. The simulation results show that the techniques can improve the schedule length and increase the system reliability without compromising the performance.","1558-2183","","10.1109/TPDS.2021.3053150","Hassan Youness; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9329106","Bi-criteria scheduling;DAGs;fault tolerance;heterogeneous MPSoC;reliability;scheduling;simulated annealing","Fault tolerant systems;Reliability;Task analysis;Schedules;Scheduling algorithms;Simulated annealing;Redundancy","directed graphs;fault tolerant computing;multiprocessing systems;probability;processor scheduling;simulated annealing;system-on-chip","simulated annealing method;directed acyclic graphs;occurrence probability;fault-tolerant heterogeneous MPSoCs;optimized weighted average makespan;system reliability;list scheduling algorithm;fault tolerant scheduling;schedule length;task replication;critical systems;multiprocessor system on chips","",2.0,"",100.0,"IEEE","20 Jan 2021","","","IEEE","IEEE Journals"
"DL2: A Deep Learning-Driven Scheduler for Deep Learning Clusters","Y. Peng; Y. Bao; Y. Chen; C. Wu; C. Meng; W. Lin","University of Hong Kong, Hong Kong, China; University of Hong Kong, Hong Kong, China; University of Hong Kong, Hong Kong, China; University of Hong Kong, Hong Kong, China; NAOC, Beijing, China; Alibaba Inc., Hanzhou, Zhejiang, China","IEEE Transactions on Parallel and Distributed Systems","22 Feb 2021",2021,32.0,8.0,1947,1960,"Efficient resource scheduling is essential for maximal utilization of expensive deep learning (DL) clusters. Existing cluster schedulers either are agnostic to machine learning (ML) workload characteristics, or use scheduling heuristics based on operators' understanding of particular ML framework and workload, which are less efficient or not general enough. In this article, we show that DL techniques can be adopted to design a generic and efficient scheduler. Specifically, we propose DL2, a DL-driven scheduler for DL clusters, targeting global training job expedition by dynamically resizing resources allocated to jobs. DL2 advocates a joint supervised learning and reinforcement learning approach: a neural network is warmed up via offline supervised learning based on job traces produced by the existing cluster scheduler; then the neural network is plugged into the live DL cluster, fine-tuned by reinforcement learning carried out throughout the training progress of the DL jobs, and used for deciding job resource allocation in an online fashion. We implement DL2 on Kubernetes and enable dynamic resource scaling in DL jobs on MXNet. Extensive evaluation shows that DL2 outperforms fairness scheduler (i.e., DRF) by 44.1 percent and expert heuristic scheduler (i.e., Optimus) by 17.5 percent in terms of average job completion time.","1558-2183","","10.1109/TPDS.2021.3052895","Alibaba Innovative Research; Hong Kong RGC(grant numbers:HKU 17204619,17208920); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328612","Deep learning;resource allocation;distributed training","Training;Resource management;Graphics processing units;Adaptation models;Supervised learning;Servers;Reinforcement learning","deep learning (artificial intelligence);pattern clustering;resource allocation;scheduling","cluster schedulers;machine learning workload characteristics;ML;DL2;DL-driven scheduler;DL clusters;joint supervised learning;reinforcement learning;neural network;job traces;DL jobs;job resource allocation;dynamic resource scaling;expert heuristic scheduler;job completion time;deep learning-driven scheduler;maximal utilization;deep learning clusters;job expedition","",15.0,"",67.0,"IEEE","19 Jan 2021","","","IEEE","IEEE Journals"
"e-PoS: Making Proof-of-Stake Decentralized and Fair","M. Saad; Z. Qin; K. Ren; D. Nyang; D. Mohaisen","University of Central Florida, Orlando, FL, USA; Zhejiang University, Zhejiang, China; Zhejiang University, Zhejiang, China; Ewha Womans University, Seoul, South Korea; University of Central Florida, Orlando, FL, USA","IEEE Transactions on Parallel and Distributed Systems","22 Feb 2021",2021,32.0,8.0,1961,1973,"Blockchain applications that rely on the Proof-of-Work (PoW) have increasingly become energy inefficient with a staggering carbon footprint. In contrast, energy efficient alternative consensus protocols such as Proof-of-Stake (PoS) may cause centralization and unfairness in the blockchain system. To address these challenges, we propose a modular version of PoS-based blockchain systems called e-PoS that resists the centralization of network resources by extending mining opportunities to a wider set of stakeholders. Moreover, e-PoS leverages the in-built system operations to promote fair mining practices by penalizing malicious entities. We validate e-PoS 's achievable objectives through theoretical analysis and simulations. Our results show that e-PoS ensures fairness and decentralization, and can be applied to existing blockchain applications.","1558-2183","","10.1109/TPDS.2020.3048853","NRF(grant numbers:NRF-2016K1A1A2912757); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9312484","Blockchains;consensus protocols;bitcoin","Bitcoin;Cryptography;Smart contracts;Stakeholders;Data mining;Consensus protocol;Complexity theory","blockchains;cryptographic protocols;data mining;energy conservation;sustainable development","proof-of-stake;blockchain applications;Proof-of-Work;staggering carbon footprint;energy efficient alternative consensus protocols;PoS-based blockchain systems;in-built system operations;fair mining practices;e-PoS","",19.0,"",51.0,"IEEE","1 Jan 2021","","","IEEE","IEEE Journals"
"True Load Balancing for Matricized Tensor Times Khatri-Rao Product","N. Abubaker; S. Acer; C. Aykanat","Department of Computer Engineering, Bilkent University, Ankara, Turkey; Sandia National Labs, Albuquerque, NM, USA; Department of Computer Engineering, Bilkent University, Ankara, Turkey","IEEE Transactions on Parallel and Distributed Systems","22 Feb 2021",2021,32.0,8.0,1974,1986,"MTTKRP is the bottleneck operation in algorithms used to compute the CP tensor decomposition. For sparse tensors, utilizing the compressed sparse fibers (CSF) storage format and the CSF-oriented MTTKRP algorithms is important for both memory and computational efficiency on distributed-memory architectures. Existing intelligent tensor partitioning models assume the computational cost of MTTKRP to be proportional to the total number of nonzeros in the tensor. However, this is not the case for the CSF-oriented MTTKRP on distributed-memory architectures. We outline two deficiencies of nonzero-based intelligent partitioning models when CSF-oriented MTTKRP operations are performed locally: failure to encode processors' computational loads and increase in total computation due to fiber fragmentation. We focus on existing fine-grain hypergraph model and propose a novel vertex weighting scheme that enables this model encode correct computational loads of processors. We also propose to augment the fine-grain model by fiber nets for reducing the increase in total computational load via minimizing fiber fragmentation. In this way, the proposed model encodes minimizing the load of the bottleneck processor. Parallel experiments with real-world sparse tensors on up to 1024 processors prove the validity of the outlined deficiencies and demonstrate the merit of our proposed improvements in terms of parallel runtimes.","1558-2183","","10.1109/TPDS.2021.3053836","Türkiye Bilimsel ve Teknolojik Araştirma Kurumu(grant numbers:EEEAG-116E043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9334414","Load balancing;sparse tensors;MTTKRP;CP decomposition;fine-grain hypergraph partitioning","Tensors;Computational modeling;Program processors;Load modeling;Sparse matrices;Partitioning algorithms;Computational efficiency","distributed memory systems;graph theory;matrix algebra;matrix decomposition;matrix multiplication;memory architecture;parallel algorithms;resource allocation;sparse matrices;tensors","load balancing;matricized tensor times khatri-rao product;bottleneck operation;CP tensor decomposition;compressed sparse fibers storage format;CSF-oriented MTTKRP algorithms;computational efficiency;distributed-memory architectures;intelligent tensor partitioning models;computational cost;nonzero-based intelligent partitioning models;CSF-oriented MTTKRP operations;fiber fragmentation;fine-grain hypergraph model;fiber nets;total computational load;bottleneck processor;real-world sparse tensors","",1.0,"",37.0,"IEEE","22 Jan 2021","","","IEEE","IEEE Journals"
"A GPU Acceleration Framework for Motif and Discord Based Pattern Mining","B. Zhu; Y. Jiang; M. Gu; Y. Deng","School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","22 Feb 2021",2021,32.0,8.0,1987,2004,"With the fast digitalization of our society, mining patterns from large time series data is increasingly becoming a critical problem for a wide range of big data applications. Motif and discord discovery algorithms, which offer effective solutions to identify repeatedly appearing and abnormal patterns, respectively, are fundamental building blocks for time series processing. Both approaches, however, can be time extremely consuming when handling large time series due to the subsequence-based computations of distance similarity metrics. In this article, we show that the highly involved subsequence-based computations can actually be decomposed into a few fine-grained computing patterns for efficient data parallel computing. By developing highly efficient GPU algorithms for such basic patterns and effectively composing such patterns, we are able to solve both motif and discord discovery problems under euclidean and DTW distance metrics in a unified GPU acceleration framework. Extensive experiments prove that the proposed framework outperforms pruned CPU algorithms by up to three orders of magnitude. Our work paves the foundation of building GPU acceleration frameworks for large-scale time series datasets.","1558-2183","","10.1109/TPDS.2021.3055765","National Key Research and Development Program of China(grant numbers:2018YFB1702600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343677","Time series;pattern mining;motif;discord;dynamic time warping (DTW);euclidean distance;GPU;acceleration","Lenses;Time series analysis;Graphics processing units;Euclidean distance;Data mining;Acceleration;Force","Big Data;data mining;graphics processing units;parallel processing;time series","time series data;big data applications;fundamental building blocks;distance similarity metrics;fine-grained computing patterns;highly efficient GPU algorithms;Motif;discord discovery problems;Euclidean distance metrics;unified GPU acceleration framework;pruned CPU algorithms;large-scale time series datasets;discord based pattern mining;subsequence-based computations;DTW distance metrics;efficient data parallel computing","",4.0,"",33.0,"IEEE","1 Feb 2021","","","IEEE","IEEE Journals"
"Pebbles: Leveraging Sketches for Processing Voluminous, High Velocity Data Streams","T. Buddhika; S. L. Pallickara; S. Pallickara","Department of Computer Science, Colorado State University, Fort Collins, CO, USA; Department of Computer Science, Colorado State University, Fort Collins, CO, USA; Department of Computer Science, Colorado State University, Fort Collins, CO, USA","IEEE Transactions on Parallel and Distributed Systems","24 Feb 2021",2021,32.0,8.0,2005,2020,"Voluminous, time-series data streams originating in continuous sensing environments pose data ingestion and processing challenges. We present a holistic methodology centered around data sketching to address both challenges. We introduce an order-preserving sketching algorithm that we have designed for space-efficient representation of multi-feature streams with native support for stream processing related operations. Observational streams are preprocessed at the edges of the network generating sketched streams to reduce data transfer costs and energy consumption. Ingested sketched streams are then processed using sketch-aware extensions to existing stream processing APIs delivering improved performance. Our benchmarks with real-world datasets show up to a ~8× reduction in data volumes transferred and a ~27× improvement in throughput.","1558-2183","","10.1109/TPDS.2021.3055265","National Science Foundation(grant numbers:OAC-1931363,ACI-1553685); National Institute of Food and Agriculture(grant numbers:COL0-FACT-2019); Cochran Family Professorship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9339879","Data sketches;stream processing systems;edge computing;Internet-of-Things","Sensors;Cloud computing;Data transfer;Monitoring;Logic gates;Distributed databases;Throughput","application program interfaces;data analysis;time series","data volumes;APIs;sketch-aware extensions;data transfer costs;observational streams;stream processing related operations;multifeature streams;space-efficient representation;order-preserving sketching algorithm;data ingestion;continuous sensing environments;time series data streams;pebbles","",1.0,"",69.0,"IEEE","28 Jan 2021","","","IEEE","IEEE Journals"
"Hone: Mitigating Stragglers in Distributed Stream Processing With Tuple Scheduling","W. Li; D. Liu; K. Chen; K. Li; H. Qi","iSING Laboratory, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; iSING Laboratory, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; iSING Laboratory, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China","IEEE Transactions on Parallel and Distributed Systems","22 Feb 2021",2021,32.0,8.0,2021,2034,"Low latency stream processing on large clusters consisting of hundreds to thousands of servers is an increasingly important challenge. A crucial barrier to tackling this challenge is stragglers, i.e., tasks that are significantly straggling behind others in processing the stream data. However, prior straggler mitigation solutions have significant limitations. They balance streaming workloads among tasks but may incur imbalanced backlogs when the workloads exhibit variance, causing stragglers as well. Fortunately, we observe that carefully scheduling the outgoing tuples of different tasks can yield benefits for balancing backlogs, and thus avoids stragglers. To this end, we present Hone, a tuple scheduler that aims to minimize the maximum queue backlog of all tasks over time. Hone leverages an online Largest-Backlog-First (LBF) algorithm with a provable good competitive ratio to perform efficient tuple scheduling. We have implemented Hone based on Apache Storm and evaluated it extensively via both simulations and testbed experiments. Our results show that under the same workload balancing strategy-shuffle grouping, Hone outperforms the original Storm significantly, with the end-to-end tuple processing latency reduced by 78.7 percent on average.","1558-2183","","10.1109/TPDS.2021.3051059","Hong Kong RGC TRS(grant numbers:T41-603/20-R,GRF-16215119); National Natural Science Foundation of China(grant numbers:62002259,62032017,61772251,61772112); Science Innovation Foundation of Dalian(grant numbers:2019J12GX037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320519","Distributed stream processing;tuple scheduling;straggler task;backlog balancing","Task analysis;Storms;Scheduling;Instruction sets;Schedules;Computer science;Technological innovation","distributed processing;query processing;resource allocation;scheduling","competitive ratio;tuple scheduler;balancing backlogs;outgoing tuples;balance streaming workloads;prior straggler mitigation solutions;stream data;distributed stream processing;mitigating stragglers;end-to-end tuple processing;workload balancing strategy-shuffle grouping;efficient tuple scheduling;online Largest-Backlog-First;Hone;maximum queue backlog;efficiency 78.7 percent","",2.0,"",55.0,"IEEE","12 Jan 2021","","","IEEE","IEEE Journals"
"Hardware Accelerator Integration Tradeoffs for High-Performance Computing: A Case Study of GEMM Acceleration in N-Body Methods","M. Asri; D. Malhotra; J. Wang; G. Biros; L. K. John; A. Gerstlauer","Electrical and Computer Engineering Department, The University of Texas at Austin, Austin, TX, USA; Flatiron Institute, New York, USA; Electrical and Computer Engineering Department, The University of Texas at Austin, Austin, TX, USA; Institute for Computational Engineering and Sciences, The University of Texas at Austin, Austin, TX, USA; Electrical and Computer Engineering Department, The University of Texas at Austin, Austin, TX, USA; Electrical and Computer Engineering Department, The University of Texas at Austin, Austin, TX, USA","IEEE Transactions on Parallel and Distributed Systems","22 Feb 2021",2021,32.0,8.0,2035,2048,"In this article, we study performance and energy saving benefits of hardware acceleration under different hardware configurations and usage scenarios for a state-of-the-art Fast Multipole Method (FMM), which is a popular N-body method. We use a dedicated Application Specific Integrated Circuit (ASIC) to accelerate General Matrix-Matrix Multiply (GEMM) operations. FMM is widely used in applications and is representative example of the workload for many HPC applications. We compare architectures that integrate the GEMM ASIC next to, in or near main memory with an on-chip coupling aimed at minimizing or avoiding repeated round-trip transfers through DRAM for communication between accelerator and CPU. We study tradeoffs using detailed and accurately calibrated x86 CPU, accelerator and DRAM simulations. Our results show that simply moving accelerators closer to the chip does not necessarily lead to performance/energy gains. We demonstrate that, while careful software blocking and on-chip placement optimizations can reduce DRAM accesses by 2X over a naive on-chip integration, these dramatic savings in DRAM traffic do not automatically translate into significant total energy or runtime savings. This is chiefly due to the application characteristics, the high idle power and effective hiding of memory latencies in modern systems. Only when more aggressive co-optimizations such as software pipelining and overlapping are applied, additional performance and energy savings can be unlocked by 37 and 35 percent respectively over baseline acceleration. When similar optimizations (pipelining and overlapping) are applied with an off-chip integration, on-chip integration delivers up to 20 percent better performance and 17 percent less total energy consumption than off-chip integration.","1558-2183","","10.1109/TPDS.2021.3056045","National Science Foundation(grant numbers:CCF-1817048,CCF-1725743,CCF-1337393); DOE(grant numbers:DE-SC0019393,DE-NA0003969); Air Force Office of Scientific Research(grant numbers:FA9550-17-1-0190); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343690","","System-on-chip;Acceleration;Random access memory;Optimization;Couplings;Computer architecture;Software","application specific integrated circuits;circuit optimisation;DRAM chips;energy conservation;integrated circuit design;matrix multiplication;parallel processing;power aware computing","on-chip integration;total energy consumption;off-chip integration;hardware accelerator integration tradeoffs;high-performance computing;GEMM acceleration;N-body methods;hardware acceleration;hardware configurations;fast multipole method;FMM;popular N-body method;application specific integrated circuit;general matrix-matrix multiply operations;HPC applications;GEMM ASIC;on-chip coupling;careful software blocking;on-chip placement optimizations;DRAM traffic;energy savings;x86 CPU;DRAM simulations;software blocking;memory latencies;aggressive co-optimizations","",2.0,"",54.0,"IEEE","1 Feb 2021","","","IEEE","IEEE Journals"
"Silhouette: Efficient Cloud Configuration Exploration for Large-Scale Analytics","Y. Chen; L. Lin; B. Li; Q. Wang; Q. Zhang","College of Electrical Engineering, Zhejiang University, Hangzhou, Zhejiang, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","2 Mar 2021",2021,32.0,8.0,2049,2061,"Choosing the best cloud configuration for large-scale data analytics jobs deployed in the cloud can substantially improve their performance and reduce costs. However, current cloud providers offer a wide variety of instance types and customized cluster sizes, making it both time-consuming and costly to pinpoint the optimal cloud configuration. This article presents the design, implementation, and evaluation of Silhouette, a cloud configuration selection framework based on performance models for various large-scale analytics jobs with minimal training overhead. The essence of Silhouette is to build performance prediction models with carefully selected small-scale experiments on small subsets of input data to estimate the performance with entire input data on larger cluster sizes. To reduce the training time and cost, Silhouette incorporates new statistical techniques to select those experiments that yield the best possible information for performance prediction. Moreover, we develop a novel model transformer to convert a prediction model built on one instance type to a different instance type with only one extra experiment, which significantly reduces the training overhead. We evaluate Silhouette with an extensive array of large-scale data analytics jobs on Amazon EC2. Our experimental results have shown convincing evidence that Silhouette is effective in optimizing cloud configuration while saving both training time and costs compared with existing solutions.","1558-2183","","10.1109/TPDS.2021.3058165","National Natural Science Foundation of China(grant numbers:61972296); Wuhan Advanced Application Project(grant numbers:2019010701011419); National Key Research and Development Program of China(grant numbers:2020AAA0107700); National Natural Science Foundation of China(grant numbers:61822207,U20B2049); Fundamental Research Funds for the Central Universities(grant numbers:2042019kf0210); RGC(grant numbers:CERG 16204418,16203719,R8015); Natural Science Foundation of Guangdong Province(grant numbers:2017A030312008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351648","Cloud configuration;large-scale data analytics;performance prediction;training overhead","Cloud computing;Predictive models;Computational modeling;Training;Data models;Analytical models;Runtime","cloud computing;data analysis;pattern clustering","Silhouette;data analytics jobs;cluster sizes;optimal cloud configuration;cloud configuration selection framework;performance prediction models;cloud providers;Amazon EC2","",6.0,"",51.0,"IEEE","9 Feb 2021","","","IEEE","IEEE Journals"
"A Fault-Tolerant Distributed Framework for Asynchronous Iterative Computations","T. Zhou; L. Gao; X. Guan","Department of Electrical and Computer Engineering, University of Massachusetts Amhers, Amherst, MA, USA; Department of Electrical and Computer Engineering, University of Massachusetts Amhers, Amherst, MA, USA; Systems Engineering Instituteg, Xi'an Jiaotong University, Xi'an, China","IEEE Transactions on Parallel and Distributed Systems","1 Mar 2021",2021,32.0,8.0,2062,2073,"Asynchronous iterative computations (AIC) are common in machine learning and data mining systems. However, the lack of synchronization barriers in asynchronous processing brings challenges for continuous processing while workers might fail. There is no global synchronization point that all workers can roll back to. In this article, we propose a fault-tolerant framework for asynchronous iterative computations (FAIC). Our framework takes a virtual snapshot of the AIC system without halting the computation of any worker. We prove that the virtual snapshot capture by FAIC can recover the AIC system correctly. We evaluate our FAIC framework on two existing AIC systems, Maiter and NOMAD. Our experiment result shows that the checkpoint overhead of FAIC is more than 50 percent shorter than the synchronous checkpoint method. FAIC is around 10 percent faster than other asynchronous snapshot algorithms, such as the Chandy-Lamport algorithm. Our experiments on a large cluster demonstrate that FAIC scales with the number of workers.","1558-2183","","10.1109/TPDS.2021.3059420","National Science Foundation(grant numbers:CNS-1815412,CNS-1908536); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354533","Fault-tolerance;cloud computing;asynchronous iterative computation;asynchronous snapshot","Computational modeling;Synchronization;Servers;Transient analysis;Fault tolerant systems;Fault tolerance;Data models","checkpointing;data mining;distributed processing;fault tolerant computing;iterative methods;learning (artificial intelligence);synchronisation","NOMAD;Maiter;Chandy-Lamport algorithm;synchronous checkpoint method;virtual snapshot;asynchronous processing;data mining systems;machine learning;fault-tolerant distributed framework;asynchronous snapshot algorithms;FAIC framework;asynchronous iterative computations;global synchronization point","","","",26.0,"IEEE","15 Feb 2021","","","IEEE","IEEE Journals"
"Proof of Federated Learning: A Novel Energy-Recycling Consensus Algorithm","X. Qu; S. Wang; Q. Hu; X. Cheng","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; Department of Computer and Information Science, Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA; School of Computer Science and Technology, Shandong University, Jinan, China","IEEE Transactions on Parallel and Distributed Systems","1 Mar 2021",2021,32.0,8.0,2074,2085,"Proof of work (PoW), the most popular consensus mechanism for blockchain, requires ridiculously large amounts of energy but without any useful outcome beyond determining accounting rights among miners. To tackle the drawback of PoW, we propose a novel energy-recycling consensus algorithm, namely proof of federated learning (PoFL), where the energy originally wasted to solve difficult but meaningless puzzles in PoW is reinvested to federated learning. Federated learning and pooled-mining, a trend of PoW, have a natural fit in terms of organization structure. However, the separation between the data usufruct and ownership in blockchain lead to data privacy leakage in model training and verification, deviating from the original intention of federal learning. To address the challenge, a reverse game-based data trading mechanism and a privacy-preserving model verification mechanism are proposed. The former can guard against training data leakage while the latter verifies the accuracy of a trained model with privacy preservation of the task requester's test data as well as the pool's submitted model. To the best of our knowledge, our article is the first work to employ federal learning as the proof of work for blockchain. Extensive simulations based on synthetic and real-world data demonstrate the effectiveness and efficiency of our proposed mechanisms.","1558-2183","","10.1109/TPDS.2021.3056773","National Key Research and Development Program of China(grant numbers:2019YFB2102600); National Natural Science Foundation of China(grant numbers:61772080,62072044); Ministry of Education of the People's Republic of China(grant numbers:2020KJ010301); Engineering Research Center of Intelligent Technology and Educational Application; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9347812","Blockchain;federated learning;consensus algorithm;incentive mechanism","Data models;Blockchain;Collaborative work;Training;Computational modeling;Training data;Task analysis","data mining;data privacy;learning (artificial intelligence);recycling","energy-recycling consensus algorithm;PoW;consensus mechanism;reverse game-based data trading mechanism;privacy-preserving model verification mechanism;proof of work;proof of federated learning;data privacy leakage;task requester's test data","",25.0,"",36.0,"IEEE","4 Feb 2021","","","IEEE","IEEE Journals"
"Joint Task Scheduling and Containerizing for Efficient Edge Computing","J. Zhang; X. Zhou; T. Ge; X. Wang; T. Hwang","UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China; UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China; UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China; UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China; Yonsei University, Seoul, South Korea","IEEE Transactions on Parallel and Distributed Systems","1 Mar 2021",2021,32.0,8.0,2086,2100,"Container-based operation system (OS) level virtualization has been adopted by many edge-computing platforms. However, for an edge server, inter-container communications, and container management consume significant CPU resources. Given an application composed of interdependent tasks, the number of such operations is closely related to the dependency between the scheduled tasks. Thus, to improve the execution efficiency of an application in an edge server, task scheduling and task containerizing need to be considered together. To this end, a joint task scheduling and containerizing (JTSC) scheme is developed in this article. Experiments are first carried out to quantify the resource utilization of container operations. System models are then built to capture the features of task execution in containers in an edge server with multiple processors. With these models, joint task scheduling and containerizing is conducted as follows. First, tasks are scheduled without considering containerization, which results in initial schedules. Second, based on system models and guidelines gained from the initial schedules, several containerization algorithms are designed to map tasks to containers. Third, task execution durations are updated by adding the time for inter-container communications, and then the task schedules are updated accordingly. The JTSC scheme is evaluated through extensive simulations. The results show that it reduces inefficient container operations and enhances the execution efficiency of applications by 60 percent.","1558-2183","","10.1109/TPDS.2021.3059447","National Natural Science Foundation of China(grant numbers:61771312); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355003","Container;task scheduling;resource consumption;execution efficiency;edge computing","Containers;Task analysis;Servers;Schedules;Image edge detection;Edge computing;Virtualization","distributed processing;operating systems (computers);scheduling;virtualisation","JTSC scheme;joint task scheduling and containerizing scheme;inefficient container operations;task schedules;task execution durations;map tasks;containerization algorithms;task containerizing need;execution efficiency;scheduled tasks;interdependent tasks;significant CPU resources;container management;inter-container communications;edge server;edge-computing platforms;container-based operation system level virtualization;efficient edge computing;efficiency 60.0 percent","",15.0,"",56.0,"IEEE","16 Feb 2021","","","IEEE","IEEE Journals"
"High-Performance Computing Implementations of Agent-Based Economic Models for Realizing 1:1 Scale Simulations of Large Economies","A. Gill; M. Lalith; S. Poledna; M. Hori; K. Fujita; T. Ichimura","Department of Civil Engineering, The University of Tokyo, Bunkyo City, Tokyo, Japan; Department of Civil Engineering and the Earthquake Research Institute, The University of Tokyo, Bunkyo City, Tokyo, Japan; Institute for Advanced Studies (IHS), Vienna, Austria; Japan Agency for Marine-Earth Science and Technology, Research Institute for Value-Added-Information Generation, Yokohama, Kanagawa, Japan; Department of Civil Engineering and the Earthquake Research Institute, The University of Tokyo, Bunkyo City, Tokyo, Japan; Department of Civil Engineering and the Earthquake Research Institute, The University of Tokyo, Bunkyo City, Tokyo, Japan","IEEE Transactions on Parallel and Distributed Systems","9 Mar 2021",2021,32.0,8.0,2101,2114,"We present a scalable high-performance computing implementation of an agent-based economic model using distributed + shared-memory hybrid parallelization paradigms, capable of simulating 1:1 scale models of large economies like the eurozone. Agent-based economic models consist of millions of agents interacting over several graphs, which are either centralized or scale-free in nature. While most of the interactions are bi-directional, the interaction graphs are dense and random and keep evolving as the simulation progresses. These characteristics cause a very large and unknown number of random communications among MPI processes, posing challenges to developing scalable parallel extensions. Further, random access to large volume of data makes the algorithms highly memory-bound, severely degrading computational performance. Adopting various strategies inspired by the real-world functioning of economies, we reduce the large unknown number of communications to a known handful number. Memory-intensive algorithms are improved to make these cache-efficient, and advanced MPI functions are used to minimize communication overhead, thereby attaining higher performance and scalability. Further, an MPI + OpenMP hybrid model is developed to best utilize modern many-core computing nodes with low per-core memory capacity. It is demonstrated that our implementation can simulate a full fledged economic model with 331 million agents within 108 seconds using 128 CPU cores attaining 70 percent strong scalability.","1558-2183","","10.1109/TPDS.2021.3060462","Japan Society for the Promotion of Science(grant numbers:18H01675); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359510","Agent-based economic models;high-performance computing;one-to-one scale simulations;large economies;scale-free graphs;message passing interface;OpenMP","Biological system modeling;Economics;Computational modeling;Government;Finance;Scalability;Distributed databases","application program interfaces;message passing;parallel processing;shared memory systems","high-performance computing implementations;agent-based economic model;scalable high-performance computing implementation;shared-memory hybrid parallelization paradigms;unknown number;algorithms highly memory-bound;computational performance;MPI + OpenMP hybrid model;fledged economic model;MPI processes;memory-intensive algorithms;many-core computing nodes;CPU cores;time 108.0 s;efficiency 70.0 percent","",4.0,"",15.0,"IEEE","19 Feb 2021","","","IEEE","IEEE Journals"
"Online Scheduling Technique To Handle Data Velocity Changes in Stream Workflows","M. Barika; S. Garg; A. Y. Zomaya; R. Ranjan","Discipline of ICT — School of Technology, Environments and Design, University of Tasmania, Hobart, TAS, Australia; Discipline of ICT — School of Technology, Environments and Design, University of Tasmania, Hobart, TAS, Australia; School of IT, University of Sydney, New South Wales, Sydney, NSW, Australia; School of Computing, Newcastle University, Newcastle Upon Tyne, U.K","IEEE Transactions on Parallel and Distributed Systems","16 Mar 2021",2021,32.0,8.0,2115,2130,"Many IoT applications and services such as smart parking and smart traffic control contain a network of different analytical components, which are composed in the form of a workflow to make better decisions. These workflows are also known as stream workflows. The focus of existing research works is on the streaming operator graph, which differs from stream workflow application as it involves heterogeneity, multiple data sources and multiple outputs. Considering the complexity and dynamism of stream workflow, meeting real-time data analysis requirements at deployment time is not the whole story as the velocity of data changes over time. This change is the most dynamic form of stream workflow that occurs frequently during the execution of this application. In this article, we propose a new dynamic scheduling technique that manages cloud resources over time to handle data velocity changes in stream workflow while maintaining user-defined real-time data analysis requirements and minimising execution cost. The efficiency of the proposed technique is evaluated, and experimental results showed that this technique outperformed its competitors and is close to the lower bound.","1558-2183","","10.1109/TPDS.2021.3059480","Australian Government Research Training Program; UK Research and Innovation(grant numbers:SUPER (EP/T021985/1),,PACE (EP/R033293/1)); Centre for Digital Citizens(grant numbers:EP/T022582/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354934","IoT;Stream workflow;dynamic scheduling;alpha-beta pruning;GA with random immigrants;cloud environments","Real-time systems;Big Data;Processor scheduling;Dynamic scheduling;Runtime;Computational modeling;Data models","cloud computing;data analysis;Internet of Things;resource allocation;scheduling","streaming operator graph;minimising execution cost;user-defined real-time data analysis requirements;data velocity changes;data changes;multiple data sources;stream workflow application","",3.0,"",31.0,"IEEE","16 Feb 2021","","","IEEE","IEEE Journals"
"Joint SFC Deployment and Resource Management in Heterogeneous Edge for Latency Minimization","Y. Liu; X. Shang; Y. Yang","Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA","IEEE Transactions on Parallel and Distributed Systems","15 Mar 2021",2021,32.0,8.0,2131,2143,"With the advancement of edge computing and network function virtualization, it is promising to provide flexible and low-latency network services at the network edge. However, due to resource limitation and heterogeneity of servers at the edge, it is unlikely to achieve an efficient service function chain deployment without considering the resource management of edge servers jointly. In this article, we consider the Joint Service function chain Deployment and Resource Management problem (JSDRM) in heterogeneous edge environments with the goal of minimizing the total system latency. We prove the NP-hardness of JSDRM and propose a scheme called JOint service function chain deployment and resource management Scheme (JOS) based on a game-theoretic approach to deploy service function chains and manage resources. We prove that JOS has a constant approximation ratio of 2.62 Extensive simulation results show that our scheme performs comparably to the optimal solution and much better than the baselines. The simulation results also show that the proposed scheme is time-efficient.","1558-2183","","10.1109/TPDS.2021.3062341","National Science Foundation(grant numbers:CCF-1717731); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363567","Network function virtualization;edge computing;latency minimization","Servers;Resource management;Service function chaining;Routing;Minimization;Bandwidth;Simulation","computational complexity;computer network management;game theory;optimisation;virtualisation","NP-hardness;joint service function chain deployment;joint SFC deployment;service function chains;resource management scheme;heterogeneous edge environments;edge servers;network edge;low-latency network services;network function virtualization;edge computing;latency minimization","",10.0,"",30.0,"IEEE","25 Feb 2021","","","IEEE","IEEE Journals"
"Learning-Driven Interference-Aware Workload Parallelization for Streaming Applications in Heterogeneous Cluster","H. Zhang; X. Geng; H. Ma","Beijing Key Lab of Intelligent Telecomm. Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Lab of Intelligent Telecomm. Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Lab of Intelligent Telecomm. Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","29 Jul 2020",2021,32.0,1.0,1,15,"In the past few years, with the rapid development of CPU-GPU heterogeneous computing, the issue of task scheduling in the heterogeneous cluster has attracted a great deal of attention. This problem becomes more challenging with the need for efficient co-execution of tasks on the GPUs. However, the uncertainty of heterogeneous cluster and the interference caused by resource contention among co-executing tasks can lead to the unbalanced use of computing resource and further cause the degradation in performance of computing platform. In this article, we propose a two-stage task scheduling approach for streaming applications based on deep reinforcement learning and neural collaborative filtering, which considers fine-grained task division and task interference on the GPU. Specifically, the Learning-Driven Workload Parallelization (LDWP) method selects an appropriate execution node for the mutually independent tasks. By using the deep Q-network, the cluster-level scheduling model is online learned to perform the current optimal scheduling actions according to the runtime status of cluster environments and characteristics of tasks. The Interference-Aware Workload Parallelization (IAWP) method assigns subtasks with dependencies to the appropriate computing units, taking into account the interference of subtasks on the GPU by using neural collaborative filtering. For making the learning of neural network more efficient, we use pre-training in the two-stage scheduler. Besides, we use transfer learning technology to efficiently rebuild task scheduling model referring to the existing model. We evaluate our learning-driven and interference-aware task scheduling approach on a prototype platform with other widely used methods. The experimental results show that the proposed strategy can averagely improve the throughout for distributed computing system by 26.9 percent and improve the GPU resource utilization by around 14.7 percent.","1558-2183","","10.1109/TPDS.2020.3008725","National Natural Science Foundation of China(grant numbers:61720106007,61921003); Higher Education Discipline Innovation Project(grant numbers:B18008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139412","Parallel computing;heterogeneous computing;task scheduling;deep reinforcement learning;neural collaborative filtering;interference aware","Task analysis;Graphics processing units;Processor scheduling;Interference;Optimal scheduling;Collaboration;Throughput","distributed processing;electronic engineering computing;graphics processing units;learning (artificial intelligence);resource allocation","transfer learning technology;task scheduling model;interference-aware task;distributed computing system;GPU resource utilization;heterogeneous cluster;CPU-GPU heterogeneous computing;resource contention;computing resource;computing platform;two-stage task scheduling approach;deep reinforcement learning;neural collaborative filtering;fine-grained task division;task interference;appropriate execution node;mutually independent tasks;cluster-level scheduling model;optimal scheduling actions;cluster environments;appropriate computing units;interference-aware workload parallelization method;learning-driven workload parallelization method;learning-driven interference-aware workload parallelization;streaming applications;deep Q-network;neural network","",7.0,"",40.0,"IEEE","13 Jul 2020","","","IEEE","IEEE Journals"
"Design and Evaluation of a Risk-Aware Failure Identification Scheme for Improved RAS in Erasure-Coded Data Centers","W. Huang; J. Fang; S. Wan; C. Xie; X. He","Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Department of Computer and Information Sciences, Temple University, Philadelphia, USA","IEEE Transactions on Parallel and Distributed Systems","29 Jul 2020",2021,32.0,1.0,16,30,"Data reliability and availability, and serviceability (RAS) of erasure-coded data centers are highly affected by data repair induced by node failures. In a traditional failure identification scheme, all chunks share the same identification time threshold, thus losing opportunities to further improve the RAS. To solve this problem, we propose RAFI, a novel risk-aware failure identification scheme. In RAFI, chunk failures in stripes experiencing different numbers of failed chunks are identified using different time thresholds. For those chunks in a high-risk stripe, a shorter identification time is adopted, thus improving the overall data reliability and availability. For those chunks in a low-risk stripe, a longer identification time is adopted, thus reducing the repair network traffic. Therefore, RAS can be improved simultaneously. We also propose three optimization techniques to reduce the additional overhead that RAFI imposes on management nodes and to ensure that RAFI can work properly under large-scale clusters. We use simulation, emulation, and prototyping implementation to evaluate RAFI from multiple aspects. Simulation and prototype results prove the effectiveness and correctness of RAFI, and the performance improvement of the optimization techniques on RAFI is demonstrated by running the emulator.","1558-2183","","10.1109/TPDS.2020.3010048","National Natural Science Foundation of China(grant numbers:61972445,61300046); National Science Foundation(grant numbers:CCF-1717660,CNS-1702474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143488","Distributed storage system;erasure coding;failure identification","Maintenance engineering;Reliability;Data centers;Heart beat;Silicon;Optimization;Encoding","computer centres;data handling;pattern clustering;software reliability;storage management;telecommunication traffic","identification time threshold;RAFI;chunk failures;high-risk stripe;data reliability;low-risk stripe;repair network traffic;performance improvement;erasure-coded data centers;data repair;node failures;traditional failure identification scheme;risk-aware failure identification scheme;data availability;large-scale clusters;optimization techniques","",2.0,"",49.0,"IEEE","17 Jul 2020","","","IEEE","IEEE Journals"
"Cost-Effective App Data Distribution in Edge Computing","X. Xia; F. Chen; Q. He; J. C. Grundy; M. Abdelrazek; H. Jin","School of Information Technology, Deakin University, Geelong, Australia; School of Information Technology, Deakin University, Geelong, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia; School of Information Technology, Deakin University, Geelong, Australia; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technolgoy, HuaZhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","31 Jul 2020",2021,32.0,1.0,31,44,"Edge computing, as an extension of cloud computing, distributes computing and storage resources from centralized cloud to distributed edge servers, to power a variety of applications demanding low latency, e.g., IoT services, virtual reality, real-time navigation, etc. From an app vendor's perspective, app data needs to be transferred from the cloud to specific edge servers in an area to serve the app users in the area. However, according to the pay-as-you-go business model, distributing a large amount of data from the cloud to edge servers can be expensive. The optimal data distribution strategy must minimize the cost incurred, which includes two major components, the cost of data transmission between the cloud to edge servers and the cost of data transmission between edge servers. In the meantime, the delay constraint must be fulfilled - the data distribution must not take too long. In this article, we make the first attempt to formulate this Edge Data Distribution (EDD) problem as a constrained optimization problem from the app vendor's perspective and prove its NP-hardness. We propose an optimal approach named EDD-IP to solve this problem exactly with the Integer Programming technique. Then, we propose an O(k)-approximation algorithm named EDD-A for finding approximate solutions to largescale EDD problems efficiently. EDD-IP and EDD-A are evaluated on a real-world dataset and the results demonstrate that they significantly outperform three representative approaches.","1558-2183","","10.1109/TPDS.2020.3010521","Australian Research Council(grant numbers:DP180100212,DP200102491); Laureate Fellowship(grant numbers:FL190100035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145634","Edge computing;optimization;data distribution;cost-effectiveness;edge server network","Servers;Cloud computing;Data communication;Videos;Facebook;Distributed databases;Edge computing","cloud computing;computational complexity;integer programming;mobile computing","cloud computing;storage resources;centralized cloud;distributed edge servers;app vendor;app data needs;app users;optimal data distribution strategy;data transmission;Edge Data Distribution problem;EDD-IP;cost-effective app data distribution;edge computing","",86.0,"",41.0,"IEEE","21 Jul 2020","","","IEEE","IEEE Journals"
"PredCom: A Predictive Approach to Collecting Approximated Communication Traces","S. Miwa; I. Laguna; M. Schulz","Department of Computer and Network Engineering, University of Electro-Communications, Tokyo, Japan; Lawrence Livermore National Laboratory, Livermore, USA; Technical University of Munich, München, Germany","IEEE Transactions on Parallel and Distributed Systems","31 Jul 2020",2021,32.0,1.0,45,58,"Communication traces collected from MPI applications are an important source of information for performance optimization as they can help analysts determine communication patterns and identify inefficiencies. However, their collection, especially at scale, is time consuming, since it usually requires running the complete target application on a large number of nodes. In this work, we present PredCom, a tool-chain to generate a predictive communication proxy based on information gathered from a few small scale runs, which allows us to extract approximate communication traces with an accuracy high enough for most analysis goals. For this, we combine LLVM passes on the original source code (to capture static program structure) with parameter prediction (to capture dynamic and scaling behavior). This approach drastically reduces the time needed for collecting the communication traces, even for traces on large numbers of MPI processes. We demonstrate that PredCom generates communication traces of various applications up to 1612x faster with an accuracy loss of 0.11 on average compared to the original large-scale traces, and we show that the generated traces can be used to optimize process placement.","1558-2183","","10.1109/TPDS.2020.3011121","Japan Science and Technology Agency; Kayamori Foundation of Informational Science Advancement(grant numbers:K30-XXIII-524); Japan Society for the Promotion of Science(grant numbers:JP20H04193); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146385","Communication traces;MPI;LLVM","Predictive models;Optimization;Instruments;Transforms;Libraries;Static analysis;Reactive power","message passing;parallel processing;program compilers;source code (software);virtual machines","process placement;static program structure;LLVM;source code;performance optimization;approximated communication traces;large-scale traces;scaling behavior;dynamic behavior;parameter prediction;predictive communication proxy;PredCom;communication patterns;MPI applications;predictive approach","",1.0,"",45.0,"IEEE","22 Jul 2020","","","IEEE","IEEE Journals"
"Self-Balancing Federated Learning With Global Imbalanced Data in Mobile Systems","M. Duan; D. Liu; X. Chen; R. Liu; Y. Tan; L. Liang","College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China","IEEE Transactions on Parallel and Distributed Systems","4 Aug 2020",2021,32.0,1.0,59,71,"Federated learning (FL) is a distributed deep learning method that enables multiple participants, such as mobile and IoT devices, to contribute a neural network while their private training data remains in local devices. This distributed approach is promising in the mobile systems where have a large corpus of decentralized data and require high privacy. However, unlike the common datasets, the data distribution of the mobile systems is imbalanced which will increase the bias of model. In this article, we demonstrate that the imbalanced distributed training data will cause an accuracy degradation of FL applications. To counter this problem, we build a self-balancing FL framework named Astraea, which alleviates the imbalances by 1) Z-score-based data augmentation, and 2) Mediator-based multi-client rescheduling. The proposed framework relieves global imbalance by adaptive data augmentation and downsampling, and for averaging the local imbalance, it creates the mediator to reschedule the training of clients based on Kullback-Leibler divergence (KLD) of their data distribution. Compared with FedAvg, the vanilla FL algorithm, Astraea shows +4.39 and +6.51 percent improvement of top-1 accuracy on the imbalanced EMNIST and imbalanced CINIC-10 datasets, respectively. Meanwhile, the communication traffic of Astraea is reduced by 75 percent compared to FedAvg.","1558-2183","","10.1109/TPDS.2020.3009406","National Natural Science Foundation of China(grant numbers:61672116,61601067,61802038,61672115); Chongqing High-Tech Research Key Program(grant numbers:cstc2019jscx-mbdx0063); Fundamental Research Funds for the Central Universities(grant numbers:0214005207005,2019CDJGFJSJ001); Chongqing Youth Talent Support Program; China Postdoctoral Science Foundation(grant numbers:2017M620412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9141436","Federated learning;distributed machine learning;neural networks","Distributed databases;Training;Machine learning;Mobile handsets;Data models;Servers;Neural networks","data handling;learning (artificial intelligence);mobile computing;neural nets;pattern classification","vanilla FL algorithm;Astraea;self-balancing federated learning;global imbalanced data;mobile systems;distributed deep learning method;multiple participants;mobile devices;IoT devices;neural network;private training data;local devices;decentralized data;data distribution;imbalanced distributed training data;FL applications;self-balancing FL framework;Z-score-based data augmentation;adaptive data augmentation;downsampling;local imbalance;imbalanced EMNIST dataset;imbalanced CINIC-10 dataset;efficiency 6.51 percent;efficiency 75.0 percent","",92.0,"",49.0,"IEEE","15 Jul 2020","","","IEEE","IEEE Journals"
"GPU Tensor Cores for Fast Arithmetic Reductions","C. A. Navarro; R. Carrasco; R. J. Barrientos; J. A. Riquelme; R. Vega","Institute of Informatics, Universidad Austral de Chile, Valdivia, Chile; Institute of Informatics, Universidad Austral de Chile, Valdivia, Chile; Laboratory of Technological Research in Pattern Recognition (LITRP), Department of DCI, Faculty of Engineering Science, Universidad Católica del Maule, San Miguel, Chile; Laboratory of Technological Research in Pattern Recognition (LITRP), Department of DCI, Faculty of Engineering Science, Universidad Católica del Maule, San Miguel, Chile; Institute of Informatics, Universidad Austral de Chile, Valdivia, Chile","IEEE Transactions on Parallel and Distributed Systems","6 Aug 2020",2021,32.0,1.0,72,84,"This article proposes a parallel algorithm for computing the arithmetic reduction of n numbers as a set of matrix-multiply accumulate (MMA) operations that are executed simultaneously by GPU tensor cores. The analysis, assuming tensors of size m x m, shows that the proposed algorithm has a parallel running time of T(n) = 5logm2n and a speedup of S = 45log2m2 over a canonical parallel reduction. Experimental performance results on a Tesla V100 GPU show that the tensor-core based approach is energy efficient and runs up to ~ 3:2× and 2× faster than a standard GPU-based reduction and Nvidia's CUB library, respectively, while keeping the numerical error below 1 percent with respect to a double precision CPU reduction. The chained design of the algorithm allows a flexible configuration of GPU thread-blocks and the optimal values found through experimentation agree with the theoretical ones. The results obtained in this work show that GPU tensor cores are relevant not only for Deep Learning or Linear Algebra computations, but also for applications that require the acceleration of large summations.","1558-2183","","10.1109/TPDS.2020.3011893","FONDECYT(grant numbers:11180881); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9147055","Arithmetic reduction;GPU computing;tensor cores;matrix multiply accumulate;parallel reduction","Graphics processing units;Tensile stress;Programming;Machine learning;Computational modeling;Acceleration;Instruction sets","graphics processing units;linear algebra;parallel algorithms;tensors","double precision CPU reduction;standard GPU-based reduction;tensor-core based approach;Tesla V100 GPU;canonical parallel reduction;parallel running time;matrix-multiply accumulate operations;arithmetic reduction;parallel algorithm;GPU tensor cores;GPU thread-blocks","",11.0,"",41.0,"IEEE","24 Jul 2020","","","IEEE","IEEE Journals"
"K-Athena: A Performance Portable Structured Grid Finite Volume Magnetohydrodynamics Code","P. Grete; F. W. Glines; B. W. O'Shea","Department of Computational Mathematics, Science, and Engineering, Michigan State University, East Lansing, USA; Department of Computational Mathematics, Science, and Engineering, Michigan State University, East Lansing, USA; National Superconducting Cyclotron Laboratory, Michigan State University, East Lansing, USA","IEEE Transactions on Parallel and Distributed Systems","6 Aug 2020",2021,32.0,1.0,85,97,"Large scale simulations are a key pillar of modern research and require ever-increasing computational resources. Different novel manycore architectures have emerged in recent years on the way towards the exascale era. Performance portability is required to prevent repeated non-trivial refactoring of a code for different architectures. We combine ATHENA++, an existing magnetohydrodynamics (MHD) CPU code, with KOKKOS, a performance portable on-node parallel programming paradigm, into K-ATHENA to allow efficient simulations on multiple architectures using a single codebase. We present profiling and scaling results for different platforms including Intel Skylake CPUs, Intel Xeon Phis, and NVIDIA GPUs. K-ATHENA achieves > 108 cell-updates/s on a single V100 GPU for second-order double precision MHD calculations, and a speedup of 30 on up to 24 576 GPUs on Summit (compared to 172,032 CPU cores), reaching 1:94 × 1012 total cell-updates/s at 76 percent parallel efficiency. Using a roofline analysis we demonstrate that the overall performance is currently limited by DRAM bandwidth and calculate a performance portability metric of 62.8 percent. Finally, we present the implementation strategies used and the challenges encountered in maximizing performance. This will provide other research groups with a straightforward approach to prepare their own codes for the exascale era. K-ATHENA is available at https://gitlab.com/pgrete/kathena.","1558-2183","","10.1109/TPDS.2020.3010016","NASA Astrophysics Theory(grant numbers:#NNX15AP39G); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143480","D.2.8.b performance measures;D.3.2.d concurrent, distributed and parallel languages;I.6.8.h parallel;J.2.i physics;J.2.c astronomy","Magnetohydrodynamics;Computer architecture;Graphics processing units;Message systems;Programming;C++ languages;Kernel","DRAM chips;graphics processing units;magnetohydrodynamics;multiprocessing systems;parallel programming;power aware computing","CPU cores;performance portability metric;K-ATHENA;performance portable structured grid finite volume magnetohydrodynamics code;computational resources;manycore architectures;nontrivial refactoring;multiple architectures;single codebase;Intel Skylake CPUs;Intel Xeon Phis;single V100 GPU;second-order double precision MHD calculations;performance portable on-node parallel programming paradigm;DRAM bandwidth","",15.0,"",35.0,"IEEE","17 Jul 2020","","","IEEE","IEEE Journals"
"Elastic Scheduling for Microservice Applications in Clouds","S. Wang; Z. Ding; C. Jiang","Department of Computer Science and Technology, Tongji University, Shanghai, China; Department of Computer Science and Technology, Tongji University, Shanghai, China; Department of Computer Science and Technology, Tongji University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","6 Aug 2020",2021,32.0,1.0,98,115,"Microservices are widely used for flexible software development. Recently, containers have become the preferred deployment technology for microservices because of fast start-up and low overhead. However, the container layer complicates task scheduling and auto-scaling in clouds. Existing algorithms do not adapt to the two-layer structure composed of virtual machines and containers, and they often ignore streaming workloads. To this end, this article proposes an Elastic Scheduling for Microservices (ESMS) that integrates task scheduling with auto-scaling. ESMS aims to minimize the cost of virtual machines while meeting deadline constraints. Specifically, we define the task scheduling problem of microservices as a cost optimization problem with deadline constraints and propose a statistics-based strategy to determine the configuration of containers under a streaming workload. Then, we propose an urgency-based workflow scheduling algorithm that assigns tasks and determines the type and quantity of instances for scale-up. Finally, we model the mapping of new containers to virtual machines as a variable-sized bin-packing problem and solve it to achieve integrated scaling of the virtual machines and containers. Via simulation-based experiments with well-known workflow applications, the ability of ESMS to improve the success ratio of meeting deadlines and reduce the cost is verified through comparison with existing algorithms.","1558-2183","","10.1109/TPDS.2020.3011979","National Basic Research Program of China (973 Program)(grant numbers:2019YFB1704102); National Natural Science Foundation of China(grant numbers:61672381); Fundamental Research Funds for the Central Universities(grant numbers:22120180508); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149819","Auto-scaling;cloud computing;containers;microservice;task scheduling","Task analysis;Scheduling;Containers;Cloud computing;Scheduling algorithms;Virtual machining","bin packing;cloud computing;scheduling;statistics;virtual machines","elastic scheduling for microservices;workflow applications;variable-sized bin-packing problem;urgency-based workflow scheduling algorithm;statistics-based strategy;cost optimization problem;task scheduling problem;deadline constraints;ESMS;virtual machines;flexible software development;microservice applications","",38.0,"",53.0,"IEEE","27 Jul 2020","","","IEEE","IEEE Journals"
"Sova: A Software-Defined Autonomic Framework for Virtual Network Allocations","Z. Ye; Y. Wang; S. He; C. Xu; X. -H. Sun","Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of IoT for Smart City, Faculty of Science and Technology, University of Macau, Macau, China; Department of Computer Science, Illinois Institute of Technology, Chicago, USA","IEEE Transactions on Parallel and Distributed Systems","6 Aug 2020",2021,32.0,1.0,116,130,"With the rise of network virtualization, the workloads deployed on data center are dramatically changed to support diverse service-oriented applications, which are in general characterized by the time-bounded service response that in turn puts great burden on the data-center networks. Although there have been numerous techniques proposed to optimize the virtual network allocation in data center, the research on coordinating them in a flexible and effective way to autonomically adapt to the workloads for service time reduction is few and far between. To address these issues, in this article we propose Sova, an autonomic framework that can combine the virtual dynamic SR-IOV (DSR-IOV) and the virtual machine live migration (VLM) for virtual network allocations in data centers. DSR-IOV is a SR-IOV-based virtual network allocation technology, but its operation scope is very limited to a single physical machine, which could lead to the local hotspot issue in the course of computation and communication, likely increasing the service response time. In contrast, VLM is an often-used virtualization technique to optimize global network traffic via VM migration. Sova exploits the software-defined approach to combine these two technologies with reducing the service response time as a goal. To realize the autonomic coordination, the architecture of Sova is designed based on the MAPE-K loop in autonomic computing. With this design, Sova can adaptively optimize the network allocation between different services by coordinating DSR-IOV and VLM in autonomic way, depending on the resource usages of physical servers and the network characteristics of VMs. To this end, Sova needs to monitor the network traffic as well as the workload characteristics in the cluster, whereby the network properties are derived on the fly to direct the coordination between these two technologies. Our experiments show that Sova can exploit the advantages of both techniques to match and even beat the better performance of each individual technology by adapting to the VM workload changes.","1558-2183","","10.1109/TPDS.2020.3012146","National Key R&D Program of China(grant numbers:2018YFB1004804); National Natural Science Foundation of China(grant numbers:61672513); Science and Technology Planning Project of Guangdong Province(grant numbers:2019B010137002); Shenzhen Oversea High-Caliber Personnel Innovation Funds(grant numbers:KQCX20170331161854); Shenzhen Basic Research Program(grant numbers:JCYJ20170818153016513); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9151358","Virtual machine migration;dynamic SR-IOV;software-defined approach;autonomic computing;MAPE-K loop;network allocation","Resource management;Bandwidth;Data centers;Servers;Virtualization;Time factors;Quality of service","computer centres;software defined networking;virtual machines;virtualisation","SR-IOV-based virtual network allocation technology;service response time;virtualization technique;global network traffic;Sova;autonomic coordination;network properties;software-defined autonomic framework;network virtualization;diverse service-oriented applications;time-bounded service response;data-center networks;service time reduction;virtual dynamic SR-IOV;DSR-IOV;data center;virtual machine live migration","",4.0,"",54.0,"IEEE","28 Jul 2020","","","IEEE","IEEE Journals"
"CASpMV: A Customized and Accelerative SpMV Framework for the Sunway TaihuLight","G. Xiao; K. Li; Y. Chen; W. He; A. Y. Zomaya; T. Li","David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada; National Supercomputing Center in Changsha, Changsha, China; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada; State Key Laboratory of Mathematic Engineering and Advance Computing, Jiangnan Institute of Computing Technology, Wuxi, China; School of Information Technologies, University of Sydney, Sidney, Australia; Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA","IEEE Transactions on Parallel and Distributed Systems","1 Sep 2020",2021,32.0,1.0,131,146,"The Sunway TaihuLight, equipped with 10 million cores, is currently the world's third fastest supercomputer. SpMV is one of core algorithms in many high-performance computing applications. This paper implements a fine-grained design for generic parallel SpMV based on the special Sunway architecture and finds three main performance limitations, i.e., storage limitation, load imbalance, and huge overhead of irregular memory accesses. To address these problems, this paper introduces a customized and accelerative framework for SpMV (CASpMV) on the Sunway. The CASpMV customizes an auto-tuning four-way partition scheme for SpMV based on the proposed statistical model, which describes the sparse matrix structure characteristics, to make it better fit in with the computing architecture and memory hierarchy of the Sunway. Moreover, the CASpMV provides an accelerative method and customized optimizations to avoid irregular memory accesses and further improve its performance on the Sunway. Our CASpMV achieves a performance improvement that ranges from 588.05 to 2118.62 percent over the generic parallel SpMV on a CG (which corresponds to an MPI process) of the Sunway on average and has good scalability on multiple CGs. The performance comparisons of the CASpMV with state-of-the-art methods on the Sunway indicate that the sparsity and irregularity of data structures have less impact on CASpMV.","1558-2183","","10.1109/TPDS.2019.2907537","National Key R&D Program of China(grant numbers:2018YFB0203800,2016YFB0200201); National Natural Science Foundation of China(grant numbers:61625202); National Natural Science Foundation of China(grant numbers:61432005); National Natural Science Foundation of China(grant numbers:61806077); Hunan Provincial Innovation Foundation for Postgraduate(grant numbers:CX2018B230); China Postdoctoral Council(grant numbers:OCPC2017032); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8674603","Heterogeneous many-core processor;matrix partition;optimization;parallelism;SpMV;Sunway TaihuLight supercomputer","Sparse matrices;Computer architecture;Parallel processing;Acceleration;Supercomputers;Kernel;Graphics processing units","application program interfaces;data structures;message passing;optimisation;parallel architectures;parallel machines;sparse matrices;statistical analysis","MPI process;data structures;CG;supercomputer;sparse matrix structure characteristics;statistical model;customized optimizations;customized and accelerative framework for SpMV;auto-tuning four-way partition scheme;computing architecture;special Sunway architecture;generic parallel SpMV;fine-grained design;high-performance computing applications;core algorithms;Sunway TaihuLight;CASpMV","",45.0,"",32.0,"IEEE","26 Mar 2019","","","IEEE","IEEE Journals"
"Partitioning Models for General Medium-Grain Parallel Sparse Tensor Decomposition","M. O. Karsavuran; S. Acer; C. Aykanat","Computer Engineering Department, Bilkent University, Turkey; Center for Computing Research, Sandia National Laboratories, Albuquerque, USA; Computer Engineering Department, Bilkent University, Turkey","IEEE Transactions on Parallel and Distributed Systems","13 Aug 2020",2021,32.0,1.0,147,159,"The focus of this article is efficient parallelization of the canonical polyadic decomposition algorithm utilizing the alternating least squares method for sparse tensors on distributed-memory architectures. We propose a hypergraph model for general medium-grain partitioning which does not enforce any topological constraint on the partitioning. The proposed model is based on splitting the given tensor into nonzero-disjoint component tensors. Then a mode-dependent coarse-grain hypergraph is constructed for each component tensor. A net amalgamation operation is proposed to form a composite medium-grain hypergraph from these mode-dependent coarse-grain hypergraphs to correctly encapsulate the minimization of the communication volume. We propose a heuristic which splits the nonzeros of dense slices to obtain sparse slices in component tensors. So we partially attain slice coherency at (sub)slice level since partitioning is performed on (sub)slices instead of individual nonzeros. We also utilize the well-known recursive-bipartitioning framework to improve the quality of the splitting heuristic. Finally, we propose a medium-grain tripartite graph model with the aim of a faster partitioning at the expense of increasing the total communication volume. Parallel experiments conducted on 10 real-world tensors on up to 1024 processors confirm the validity of the proposed hypergraph and graph models.","1558-2183","","10.1109/TPDS.2020.3012624","Türkiye Bilimsel ve Teknolojik Araştirma Kurumu(grant numbers:EEEAG-116E043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158556","sparse tensor;tensor decomposition;canonical polyadic decomposition;communication cost;communication volume,medium-grain partitioning,recursive bipartitioning;hypergraph partitioning,graph partitioning","Tensile stress;Program processors;Partitioning algorithms;Computational modeling;Sparse matrices;Load modeling;Minimization","distributed memory systems;graph theory;matrix decomposition;matrix multiplication;parallel algorithms;sparse matrices;tensors","hypergraph model;distributed-memory architectures;sparse tensors;alternating least squares method;canonical polyadic decomposition algorithm;general medium-grain parallel sparse tensor decomposition;partitioning models;graph models;real-world tensors;parallel experiments;faster partitioning;medium-grain tripartite graph model;slice coherency;sparse slices;dense slices;composite medium-grain hypergraph;component tensor;mode-dependent coarse-grain hypergraph;nonzero-disjoint component tensors;general medium-grain partitioning","",6.0,"",33.0,"IEEE","4 Aug 2020","","","IEEE","IEEE Journals"
"Feluca: A Two-Stage Graph Coloring Algorithm With Color-Centric Paradigm on GPU","Z. Zheng; X. Shi; L. He; H. Jin; S. Wei; H. Dai; X. Peng","National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, University of Warwick, Coventry, United Kingdom; National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","13 Aug 2020",2021,32.0,1.0,160,173,"There are great challenges in performing graph coloring on GPU in general. First, the long-tail problem exists in the recursion algorithm because the conflict (i.e., different threads assign the adjacent nodes to the same color) becomes more likely to occur as the number of iterations increases. Second, it is hard to parallelize the sequential spread algorithm because the color allocation depends on the adjoining iteration. Third, the atomic operation is widely used on GPU to maintain the color list, which can greatly reduce the efficiency of GPU threads. In this article, we propose a two-stage high-performance graph coloring algorithm, called Feluca, aiming to address the above challenges. Feluca combines the recursion-based method with the sequential spread-based method. In the first stage, Feluca uses a recursive routine to color a majority of vertices in the graph. Then, it switches to the sequential spread method to color the remaining vertices in order to avoid the conflicts of the recursive algorithm. Moreover, the following techniques are proposed to further improve the graph coloring performance. i) A new method is proposed to eliminate the cycles in the graph; ii) a top-down scheme is developed to avoid the atomic operation originally required for color selection; and iii) a novel color-centric coloring paradigm is designed to improve the degree of parallelism for the sequential spread part. All these newly developed techniques, together with further GPU-specific optimizations such as coalesced memory access, comprise an efficient parallel graph coloring solution in Feluca. We have conducted extensive experiments on NVIDIA GPU. The results show that Feluca can achieve 1.19 - 8.39× speedup over the state-of-the-art algorithms.","1558-2183","","10.1109/TPDS.2020.3014173","National Key R&D Program of China(grant numbers:2017YFC0803700); National Natural Science Foundation of China(grant numbers:61772218); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162529","Graph coloring;GPGPU;parallelism;color-centric paradigm;pipeline","Color;Graphics processing units;Image color analysis;Task analysis;Parallel processing;Computational modeling;Synchronization","graph colouring;graph theory;graphics processing units;optimisation;parallel algorithms","two-stage graph coloring algorithm;color-centric paradigm;long-tail problem;recursion algorithm;sequential spread algorithm;color allocation;adjoining iteration;atomic operation;color list;GPU threads;two-stage high-performance graph coloring algorithm;recursion-based method;sequential spread-based method;recursive routine;recursive algorithm;graph coloring performance;color selection;GPU-specific optimizations;efficient parallel graph coloring solution;NVIDIA GPU;Feluca;color-centric coloring paradigm;sequential spread","","","",54.0,"IEEE","7 Aug 2020","","","IEEE","IEEE Journals"
"Blockchain at the Edge: Performance of Resource-Constrained IoT Networks","S. Misra; A. Mukherjee; A. Roy; N. Saurabh; Y. Rahulamathavan; M. Rajarajan","Department of Computer Science, Engineering at Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science, Engineering at Indian Institute of Technology Kharagpur, Kharagpur, India; Advanced technology Development Center, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Electronics and Communication Engineering, National Institute of Technology Patna, Patna, India; Institute for Digital Technologies, Loughborough University London, London, UK; Information Security Group, School of Engineering and Mathematical Sciences, City University London, London, UK","IEEE Transactions on Parallel and Distributed Systems","18 Aug 2020",2021,32.0,1.0,174,183,"The proliferation of IoT in various technological realms has resulted in the massive spurt of unsecured data. The use of complex security mechanisms for securing these data is highly restricted owing to the low-power and low-resource nature of most of the IoT devices, especially at the Edge. In this article, we propose to use blockchains for extending security to such IoT implementations. We deploy a Ethereum blockchain consisting of both regular and constrained devices connecting to the blockchain through wired and wireless heterogeneous networks. We additionally implement a secure and encrypted networked clock mechanism to synchronize the non-real-time IoT Edge nodes within the blockchain. Further, we experimentally study the feasibility of such a deployment and the bottlenecks associated with it by running necessary cryptographic operations for blockchains in IoT devices. We study the effects of network latency, increase in constrained blockchain nodes, data size, Ether, and blockchain node mobility during transaction and mining of data within our deployed blockchain. This study serves as a guideline for designing secured solutions for IoT implementations under various operating conditions such as those encountered for static IoT nodes and mobile IoT devices.","1558-2183","","10.1109/TPDS.2020.3013892","University Grants Commission; UK-India Education and Research Initiative; Joint Research Programme(grant numbers:184-17/2017(IC)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158540","Internet of Things;blockchain;edge nodes;ethereum;constrained-networks","Ecosystems;Synchronization;Logic gates;Data privacy;Reliability","computer network security;cryptocurrencies;cryptography;data mining;distributed databases;Internet of Things;radio networks;synchronisation","constrained blockchain nodes;blockchain node mobility;static IoT nodes;mobile IoT devices;resource-constrained IoT networks;complex security mechanisms;Ethereum blockchain;wired networks;wireless heterogeneous networks;secure encrypted networked clock mechanism;nonreal-time IoT Edge nodes","",41.0,"",23.0,"IEEE","4 Aug 2020","","","IEEE","IEEE Journals"
"Rusty: Runtime Interference-Aware Predictive Monitoring for Modern Multi-Tenant Systems","D. Masouros; S. Xydis; D. Soudris","Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece","IEEE Transactions on Parallel and Distributed Systems","18 Aug 2020",2021,32.0,1.0,184,198,"Modern micro-service and container-based cloud-native applications have leveraged multi-tenancy as a first class system design concern. The increasing number of co-located services/workloads into server facilities stresses resource availability and system capability in an unconventional and unpredictable manner. To efficiently manage resources in such dynamic environments, run-time observability and forecasting are required to capture workload sensitivities under differing interference effects, according to applied co-location scenarios. While several research efforts have emerged on interference-aware performance modelling, they are usually applied at a very coarse-grained manner e.g., estimating the overall performance degradation of an application, thus failing to effectively quantify, predict or provide educated insights on the impact of continuous runtime interference on per-resource allocations. In this paper, we present Rusty, a predictive monitoring system that leverages the power of Long Short-Term Memory networks to enable fast and accurate runtime forecasting of key performance metrics and resource stresses of cloud-native applications under interference. We evaluate Rusty under a diverse set of interference scenarios for a plethora of representative cloud workloads, showing that Rusty i) achieves extremely high prediction accuracy, average R2 value of 0.98, ii) enables very deep prediction horizons retaining high accuracy, e.g., R2 of around 0.99 for a horizon of 1 sec ahead and around 0.94 for an horizon of 5 sec ahead, while iii) satisfying, at the same time, the strict latency constraints required to make Rusty practical for continuous predictive monitoring at runtime.","1558-2183","","10.1109/TPDS.2020.3013948","European Union's Horizon 2020 Research and Innovation programme(grant numbers:825061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158547","predictive monitoring;system predictability;LSTM networks;interference aware;multi-tenant systems","Monitoring;Interference;Runtime;Resource management;Measurement;Degradation;Servers","cloud computing;recurrent neural nets;resource allocation","run-time observability;container-based cloud-native applications;modern multitenant systems;runtime interference-aware predictive monitoring;continuous predictive monitoring;representative cloud workloads;interference scenarios;key performance metrics;long short-term memory networks;Rusty;resource allocations;continuous runtime interference;interference-aware performance modelling","",15.0,"",88.0,"IEEE","4 Aug 2020","","","IEEE","IEEE Journals"
"O3BNN-R: An Out-of-Order Architecture for High-Performance and Regularized BNN Inference","T. Geng; A. Li; T. Wang; C. Wu; Y. Li; R. Shi; W. Wu; M. Herbordt","Department of Electrical and Computer Engineering, Boston University, Boston, USA; PCSD, Pacific Northwest National Laboratory, Richland, USA; Department of Electrical and Computer Engineering, Boston University, Boston, USA; Department of Electrical and Computer Engineering, Boston University, Boston, USA; College of Information Science and Electronic Engineering, Hangzhou, China; Department of Electrical and Electronic Engineering, Hong Kong University, Hong Kong; Program Model Team, Los Alamos National Laboratory (LANL), Los Alamos, USA; Department of Electrical and Computer Engineering, Boston University, Boston, USA","IEEE Transactions on Parallel and Distributed Systems","18 Aug 2020",2021,32.0,1.0,199,213,"Binarized Neural Networks (BNN), which significantly reduce computational complexity and memory demand, have shown potential in cost- and power-restricted domains, such as IoT and smart edge-devices, where reaching certain accuracy bars is sufficient and real-time is highly desired. In this article, we demonstrate that the highly-condensed BNN model can be shrunk significantly by dynamically pruning irregular redundant edges. Based on two new observations on BNN-specific properties, an out-of-order (OoO) architecture, O3BNN-R, which can curtail edge evaluation in cases where the binary output of a neuron can be determined early at runtime during inference, is proposed. Similar to instruction level parallelism (ILP), fine-grained, irregular, and runtime pruning opportunities are traditionally presumed to be difficult to exploit. To further enhance the pruning opportunities, we conduct an algorithm/architecture co-design approach where we augment the loss function during the training stage with specialized regularization terms favoring edge pruning. We evaluate our design on an embedded FPGA using networks that include VGG-16, AlexNet for ImageNet, and a VGG-like network for Cifar-10. Results show that O3BNN-R without regularization can prune, on average, 30 percent of the operations, without any accuracy loss, bringing 2.2× inference-speedup, and on average 34× energy-efficiency improvement over state-of-the-art BNN implementations on FPGA/GPU/CPU. With regularization at training, the performance is further improved, on average, by 15 percent.","1558-2183","","10.1109/TPDS.2020.3013637","National Science Foundation(grant numbers:CNS-1405695,CCF-1618303/7960); National Institutes of Health(grant numbers:1R41GM128533); Microsoft; Red Hat; Xilinx; Intel Corporation; Pacific Northwest National Laboratory; U.S. DOE Office of Science, Office of Advanced Scientific Computing Research(grant numbers:66150); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9154597","Machine learning;BNN;high-performance computing;neural network pruning;out-of-order architecture","Neurons;Training;Out of order;Computer architecture;Convolution;Integrated circuits;Computational complexity","computational complexity;field programmable gate arrays;logic design;neural chips;neural net architecture","O3BNN-R;out-of-order architecture;regularized BNN inference;Binarized Neural Networks;power-restricted domains;smart edge-devices;irregular redundant edges;BNN-specific properties;edge evaluation;instruction level parallelism;specialized regularization terms;edge pruning;high-performance inference;computational complexity;IoT;condensed BNN model;runtime pruning opportunities;architecture co-design approach;loss function;embedded FPGA;VGG-16;AlexNet;ImageNet;VGG-like network;Cifar-10;energy-efficiency improvement;GPU;CPU;efficiency 30.0 percent;efficiency 15.0 percent","",19.0,"",38.0,"IEEE","3 Aug 2020","","","IEEE","IEEE Journals"
"Improving the Performance of Deduplication-Based Storage Cache via Content-Driven Cache Management Methods","Y. Tan; C. Xu; J. Xie; Z. Yan; H. Jiang; W. Srisa-an; X. Chen; D. Liu","College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; HewlettPackard Enterprise, San Jose, USA; University of Texas Arlington, Arlington, USA; University of Nebraska Lincoln, Lincoln, USA; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China","IEEE Transactions on Parallel and Distributed Systems","18 Aug 2020",2021,32.0,1.0,214,228,"Data deduplication, as a proven technology for effective data reduction in backup and archiving storage systems, is also showing promises in increasing the logical space capacity for storage caches by removing redundant data. However, our in-depth evaluation of the existing deduplication-aware caching algorithms reveals that they only work well when the cached block size is set to 4 KB. Unfortunately, modern storage systems often set the block size to be much larger than 4 KB, and in this scenario, the overall performance of these caching schemes drops below that of the conventional replacement algorithms without any deduplication. There are several reasons for this performance degradation. The first reason is the deduplication overhead, which is the time spent on generating the data fingerprints and their use to identify duplicate data. Such overhead offsets the benefits of deduplication. The second reason is the extremely low cache space utilization caused by read and write alignment. The third reason is that existing algorithms only exploit access locality to identify block replacement. There is a lost opportunity to effectively leverage the content usage patterns such as intensity of content redundancy and sharing in deduplication-based storage caches to further improve performance. We propose CDAC, a Content-driven Deduplication-Aware Cache, to address this problem. CDAC focuses on exploiting the content redundancy in blocks and intensity of content sharing among source addresses in cache management strategies. We have implemented CDAC based on LRU and ARC algorithms, called CDAC-LRU and CDAC-ARC respectively. Our extensive experimental results show that CDAC-LRU and CDAC-ARC outperform the state-of-the-art deduplication-aware caching algorithms, D-LRU, and D-ARC, by up to 23.83X in read cache hit ratio, with an average of 3.23X, and up to 53.3 percent in IOPS, with an average of 49.8 percent, under a real-world mixed workload when the cache size ranges from 20 to 50 percent of the workload size and the block size ranges from 4KB to 32 KB.","1558-2183","","10.1109/TPDS.2020.3012704","Wuhan National Laboratory for Optoelectronics(grant numbers:2019WNLOKF009); Fundamental Research Funds for the Central Universities(grant numbers:2019CDJGFJSJ001); National Natural Science Foundation of China(grant numbers:61402061,61672116,61802038); Chongqing High-Tech Research Program(grant numbers:cstc2016jcyjA0274,cstc2016jcyjA0332); China Postdoctoral Science Foundation(grant numbers:2017M620412); Chongqing Postdoctoral Special Science Foundation(grant numbers:XmT2018003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152152","Data deduplication;storage cache;content sharing","Metadata;Cache storage;Redundancy;Performance evaluation;Distributed databases;Degradation;Indexes","cache storage;data reduction","Deduplication-based storage Cache;Content-driven Cache management methods;data deduplication;effective data reduction;backup storage systems;archiving storage systems;redundant data;cached block size;modern storage systems;caching schemes;deduplication overhead;data fingerprints;duplicate data;extremely low cache space utilization;content redundancy;deduplication-based storage caches;cache management strategies;CDAC-ARC;read cache;cache size;block size ranges;deduplication-aware caching algorithms;memory size 4.0 KByte;efficiency 49.8 percent;efficiency 50.0 percent;memory size 4.0 KByte to 32.0 KByte","",7.0,"",39.0,"IEEE","29 Jul 2020","","","IEEE","IEEE Journals"
"Multi-GPU Parallelization of the NAS Multi-Zone Parallel Benchmarks","M. González; E. Morancho","Computer Architecture Department, Universitat Politècnica de Catalunya - Barcelona Tech, Barcelona, Spain; Computer Architecture Department, Universitat Politècnica de Catalunya - Barcelona Tech, Barcelona, Spain","IEEE Transactions on Parallel and Distributed Systems","18 Aug 2020",2021,32.0,1.0,229,241,"GPU-based computing systems have become a widely accepted solution for the high-performance-computing (HPC) domain. GPUs have shown highly competitive performance-per-watt ratios and can exploit an astonishing level of parallelism. However, exploiting the peak performance of such devices is a challenge, mainly due to the combination of two essential aspects of multi-GPU execution. On one hand, the workload should be distributed evenly among the GPUs. On the other hand, communications between GPU devices are costly and should be minimized. Therefore, a trade-of between work-distribution schemes and communication overheads will condition the overall performance of parallel applications run on multi-GPU systems. In this article we present a multi-GPU implementation of NAS Multi-Zone Parallel Benchmarks (which execution alternate communication and computational phases). We propose several work-distribution strategies that try to evenly distribute the workload among the GPUs. Our evaluations show that performance is highly sensitive to this distribution strategy, as the the communication phases of the applications are heavily affected by the work-distribution schemes applied in computational phases. In particular, we consider Static, Dynamic, and Guided schedulers to find a trade-off between both phases to maximize the overall performance. In addition, we compare those schedulers with an optimal scheduler computed offline using IBM CPLEX. On an evaluation environment composed of 2 x IBM Power9 8335-GTH and 4 x GPU NVIDIA V100 (Volta), our multi-GPU parallelization outperforms single-GPU execution from 1.48x to 1.86x (2 GPUs) and from 1.75x to 3.54x (4 GPUs). This article analyses these improvements in terms of the relationship between the computational and communication phases of the applications as the number of GPUs is increased. We prove that Guided schedulers perform at similar level as optimal schedulers.","1558-2183","","10.1109/TPDS.2020.3015148","Ministerio de Ciencia y Tecnología(grant numbers:TIN2015-65316-P); Generalitat de Catalunya(grant numbers:2014-SGR-1051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162505","Multi-GPU parallelization;load balancing;static;dynamic;guided schedulings","Benchmark testing;Graphics processing units;Parallel processing;Dynamic scheduling;Performance evaluation;Optimal scheduling;Load management","benchmark testing;graphics processing units;optimisation;performance evaluation;processor scheduling","MultiGPU parallelization;NAS MultiZone Parallel Benchmarks;GPU-based computing systems;high-performance-computing domain;performance-per-watt ratios;peak performance;multiGPU execution;GPU devices;work-distribution schemes;communication overheads;multiGPU systems;multiGPU implementation;computational phases;work-distribution strategies;distribution strategy;optimal scheduler computed offline;4 x GPU NVIDIA V100;multiGPU parallelization;single-GPU execution;computational communication phases","","","",27.0,"IEEE","7 Aug 2020","","","IEEE","IEEE Journals"
"Fast Adaptive Task Offloading in Edge Computing Based on Meta Reinforcement Learning","J. Wang; J. Hu; G. Min; A. Y. Zomaya; N. Georgalas","Department of Computer Science, University of Exeter, Exeter, United Kingdom; Department of Computer Science, University of Exeter, Exeter, United Kingdom; Department of Computer Science, University of Exeter, Exeter, United Kingdom; School of Information Technologies, The University of Sydney, Sydney, Australia; Applied Research Department, British Telecom, Edinburgh, United Kingdom","IEEE Transactions on Parallel and Distributed Systems","24 Aug 2020",2021,32.0,1.0,242,253,"Multi-access edge computing (MEC) aims to extend cloud service to the network edge to reduce network traffic and service latency. A fundamental problem in MEC is how to efficiently offload heterogeneous tasks of mobile applications from user equipment (UE) to MEC hosts. Recently, many deep reinforcement learning (DRL)-based methods have been proposed to learn offloading policies through interacting with the MEC environment that consists of UE, wireless channels, and MEC hosts. However, these methods have weak adaptability to new environments because they have low sample efficiency and need full retraining to learn updated policies for new environments. To overcome this weakness, we propose a task offloading method based on meta reinforcement learning, which can adapt fast to new environments with a small number of gradient updates and samples. We model mobile applications as Directed Acyclic Graphs (DAGs) and the offloading policy by a custom sequence-to-sequence (seq2seq) neural network. To efficiently train the seq2seq network, we propose a method that synergizes the first order approximation and clipped surrogate objective. The experimental results demonstrate that this new offloading method can reduce the latency by up to 25 percent compared to three baselines while being able to adapt fast to new environments.","1558-2183","","10.1109/TPDS.2020.3014896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9161406","Multi-access edge computing;task offloading;meta reinforcement learning;deep learning","Task analysis;Training;Neural networks;Heuristic algorithms;Mobile applications;Learning (artificial intelligence);Edge computing","approximation theory;directed graphs;distributed processing;learning (artificial intelligence);mobile radio;multi-access systems;neural nets;telecommunication computing;telecommunication traffic;wireless channels","weak adaptability;low sample efficiency;meta reinforcement learning;gradient updates;mobile applications;offloading policy;sequence-to-sequence neural network;seq2seq network;adaptive task offloading;multiaccess edge computing;cloud service;network edge;network traffic;heterogeneous tasks;user equipment;MEC hosts;MEC environment;deep reinforcement learning;service latency;wireless channels;directed acyclic graphs;first order approximation;clipped surrogate objective","",100.0,"",40.0,"IEEE","6 Aug 2020","","","IEEE","IEEE Journals"
"Architectural Adaptation and Performance-Energy Optimization for CFD Application on AMD EPYC Rome","L. Szustak; R. Wyrzykowski; L. Kuczynski; T. Olas","Department of Computer Science, Czestochowa University of Technology, Czestochowa, Poland; Department of Computer Science, Czestochowa University of Technology, Czestochowa, Poland; Department of Computer Science, Czestochowa University of Technology, Czestochowa, Poland; Department of Computer Science, Czestochowa University of Technology, Czestochowa, Poland","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2021",2021,32.0,12.0,2852,2866,"The advantages of the second-generation AMD EPYC Rome processors can be successfully used in the race to Exascale. However, the novel architecture's complexity makes it challenging to adapt demanding scientific codes - like stencil ones - to platforms with Rome CPUs. This article tackles this challenge by exploring the adaptation of the stencil-based CFD (computational fluid dynamics) application called MPDATA to these processors' influential features. We show that the previously proposed parametric adaptation methodology can be profitably applied to extend the performance portability of the memory-bound MPDATA on the AMD EPYC architecture. The extension of the parametric adaptation on the novel architecture requires careful consideration of two relevant aspects that reflect splitting the Rome architecture into multiple dies - features of the cache hierarchy and partitioning cores into work teams. The article also investigates the correlation between the performance optimizations and energy efficiency for a ccNUMA platform powered by top-of-the-line 64-core AMD Rome 7742 CPUs, comparing the results against two servers with Intel Xeon Scalable processors of different generations. Even without appealing to prices, the achieved performance and energy efficiency results are a solid argument confirming the competitiveness of AMD Rome processors against Intel Xeon CPUs in scientific applications.","1558-2183","","10.1109/TPDS.2021.3078153","National Science Center Poland(grant numbers:UMO-2017/26/D/ST6/00687); Polish Minister of Science and Higher Education; Regional Initiative of Excellence(grant numbers:020/RID/2018/19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426456","CFD;MPDATA;AMD EPYC Rome;shared-memory programming;performance portability;energy efficiency","Program processors;Computer architecture;Optimization;Servers;Memory management;Energy effficiency","cache storage;computational fluid dynamics;memory architecture;microprocessor chips;multiprocessing systems;optimisation;power aware computing","architectural adaptation;performance-energy optimization;second-generation AMD EPYC Rome processors;stencil-based CFD application;computational fluid dynamics;parametric adaptation methodology;performance portability;memory-bound MPDATA;AMD EPYC architecture;cache hierarchy;performance optimizations;ccNUMA platform;Rome 7742 CPUs;Intel Xeon scalable processors;AMD Rome processors;Intel Xeon CPUs;scientific applications","",3.0,"",47.0,"CCBY","7 May 2021","","","IEEE","IEEE Journals"
"Tardiness Bounds for Sporadic Gang Tasks Under Preemptive Global EDF Scheduling","Z. Dong; K. Yang; N. Fisher; C. Liu","Department of Computer Science, Wayne State University, Detroit, MI, USA; Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computer Science, Wayne State University, Detroit, MI, USA; Department of Computer Science, The University of Texas at Dallas, Dallas, TX, USA","IEEE Transactions on Parallel and Distributed Systems","2 Jun 2021",2021,32.0,12.0,2867,2879,"Following the trend of increasing autonomy in cyber-physical systems, parallel embedded architectures have enabled devices to better handle the large streams of data and intensive computation required by such autonomous systems. However, while the explosion of highly-parallel platforms has seen a proportional growth in the number of applications/devices that utilize these platforms, the embedded systems community's understanding of how to build time-predictable, safety-critical systems with parallel platforms has not kept pace. As a well-motivated but challenging parallel scheduling model, gang scheduling requires all parallel threads of each parallel task to simultaneously execute in unison, which is in contrast to traditional, multi-threaded parallel scheduling, where a parallel task may spawn multiple threads, and each thread will be scheduled independently of other threads of the same task. While increasing research efforts on hard real-time (HRT) gang scheduling have recently been seen, the problem of gang scheduling in the context of soft real-time (SRT) systems, where provably bounded deadline tardiness can be tolerated, has hardly been studied yet. In this article, we derive and prove the first tardiness bounds for sporadic gang task systems under preemptive GEDF scheduling. A total utilization bound for SRT-schedulability is required for ensuring such tardiness bounds but it is shown to be tight with respect to the platform capacity and maximum parallelism-induced idleness. Furthermore, we also empirically evaluate the effects of different degrees of task parallelism upon the SRT-schedulability.","1558-2183","","10.1109/TPDS.2021.3081019","National Science Foundation(grant numbers:CNS-2038727,CNS-1750263,CNS-1618185,IIS-1724227); Texas State University; Wayne State University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432744","Real-time scheduling;gang tasks;schedulability test;tardiness bound;Global-Earliest-Deadline","Task analysis;Real-time systems;Parallel processing;Message systems;Scheduling;Multicore processing;Instruction sets","embedded systems;multiprocessing systems;multi-threading;processor scheduling;real-time systems","tardiness bounds;sporadic gang tasks;preemptive global EDF scheduling;cyber-physical systems;parallel embedded architectures;intensive computation;autonomous systems;highly-parallel platforms;embedded systems community;safety-critical systems;parallel threads;parallel task;multithreaded parallel scheduling;real-time gang scheduling;real-time systems;provably bounded deadline tardiness;sporadic gang task systems;preemptive GEDF scheduling;SRT-schedulability;platform capacity;parallelism-induced idleness;task parallelism","",4.0,"",20.0,"IEEE","17 May 2021","","","IEEE","IEEE Journals"
"ARENA: Asynchronous Reconfigurable Accelerator Ring to Enable Data-Centric Parallel Computing","C. Tan; C. Xie; T. Geng; A. Marquez; A. Tumeo; K. Barker; A. Li","Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA","IEEE Transactions on Parallel and Distributed Systems","3 Jun 2021",2021,32.0,12.0,2880,2892,"The next generation HPC and data centers are likely to be reconfigurable and data-centric due to the trend of hardware specialization and the emergence of data-driven applications. In this article, we propose ARENA - an asynchronous reconfigurable accelerator ring architecture as a potential scenario on how the future HPC and data centers will be like. Despite using the coarse-grained reconfigurable arrays (CGRAs) as the substrate platform, our key contribution is not only the CGRA-cluster design itself, but also the ensemble of a new architecture and programming model that enables asynchronous tasking across a cluster of reconfigurable nodes, so as to bring specialized computation to the data rather than the reverse. We presume distributed data storage without asserting any prior knowledge on the data distribution. Hardware specialization occurs at runtime when a task finds the majority of data it requires are available at the present node. In other words, we dynamically generate specialized CGRA accelerators where the data reside. The asynchronous tasking for bringing computation to data is achieved by circulating the task token, which describes the dataflow graphs to be executed for a task, among the CGRA cluster connected by a fast ring network. Evaluations on a set of HPC and data-driven applications across different domains show that ARENA can provide better parallel scalability with reduced data movement (53.9 percent). Compared with contemporary compute-centric parallel models, ARENA can bring on average 4.37× speedup. The synthesized CGRAs and their task-dispatchers only occupy 2.93mm$^2$2<; inline-graphic xlink:href=""tan-ieq1-3081074.gif""/> chip area under 45nm process technology and can run at 800MHz with on average 759.8mW power consumption. ARENA also supports the concurrent execution of multi-applications, offering ideal architectural support for future high-performance parallel computing and data analytics systems.","1558-2183","","10.1109/TPDS.2021.3081074","Compute-Flow-Architecture; U.S. Department of Energy(grant numbers:66150); U.S. Department of Energy(grant numbers:DE-AC05-76RL01830); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9436051","Compute-flow-architecture;runtime reconfiguration;asynchronous parallel execution;abstract machine model","Task analysis;Computational modeling;Data models;Runtime;Programming;Computer architecture;Data centers","asynchronous circuits;data flow graphs;parallel processing;reconfigurable architectures;storage management","distributed data storage;hardware specialization;CGRA accelerators;asynchronous tasking;task token;fast ring network;data-driven applications;ARENA;reduced data movement;contemporary compute-centric;task-dispatchers;future high-performance parallel computing;data-centric parallel computing;next generation HPC;data centers;asynchronous reconfigurable accelerator ring architecture;coarse-grained reconfigurable arrays;CGRA-cluster design;programming model;reconfigurable nodes;dataflow graphs;parallel scalability;contemporary compute-centric parallel models;substrate platform;size 45.0 nm;frequency 800.0 MHz","",9.0,"",79.0,"IEEE","19 May 2021","","","IEEE","IEEE Journals"
"A Quantum Approach Towards the Adaptive Prediction of Cloud Workloads","A. K. Singh; D. Saxena; J. Kumar; V. Gupta","Department of Computer Applications, NIT Kurukshetra, Haryana, India; Department of Computer Applications, NIT Kurukshetra, Haryana, India; Department of Computer Applications, NIT Tiruchirappalli, Tamilnadu, India; Department of Electronics & Communication Engineering, NIT Kurukshetra, Haryana, India","IEEE Transactions on Parallel and Distributed Systems","3 Jun 2021",2021,32.0,12.0,2893,2905,"This work presents a novel Evolutionary Quantum Neural Network (EQNN) based workload prediction model for Cloud datacenter. It exploits the computational efficiency of quantum computing by encoding workload information into qubits and propagating this information through the network to estimate the workload or resource demands with enhanced accuracy proactively. The rotation and reverse rotation effects of the Controlled-NOT (C-NOT) gate serve activation function at the hidden and output layers to adjust the qubit weights. In addition, a Self Balanced Adaptive Differential Evolution (SB-ADE) algorithm is developed to optimize qubit network weights. The accuracy of the EQNN prediction model is extensively evaluated and compared with seven state-of-the-art methods using eight real world benchmark datasets of three different categories. Experimental results reveal that the use of the quantum approach to evolutionary neural network substantially improves the prediction accuracy up to 91.6 percent over the existing approaches.","1558-2183","","10.1109/TPDS.2021.3079341","National Institute of Technology Kurukshetra; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428529","Cloud computing;differential evolution;quantum neural network;workload forecasting","Qubit;Predictive models;Neural networks;Hidden Markov models;Prediction algorithms;Logic gates;Cloud computing","cloud computing;computer centres;evolutionary computation;neural nets;power aware computing;quantum computing","rotation effects;qubit weights;qubit network weights;EQNN prediction model;quantum approach;evolutionary neural network;prediction accuracy;adaptive prediction;cloud workloads;cloud datacenter;computational efficiency;quantum computing;workload information;evolutionary quantum neural network based workload prediction model;self balanced adaptive differential evolution algorithm","",17.0,"",46.0,"IEEE","11 May 2021","","","IEEE","IEEE Journals"
"RENDA: Resource and Network Aware Data Placement Algorithm for Periodic Workloads in Cloud","H. K. Thakkar; P. K. Sahoo; B. Veeravalli","Department of Computer Science and Engineering, SRM University, Andhra Pradesh, India; Department of Neurology, Chang Gung Memorial Hospital, Linkou, Taiwan; Department of Electrical and Computer Engineering, National University of Singapore, Singapore","IEEE Transactions on Parallel and Distributed Systems","3 Jun 2021",2021,32.0,12.0,2906,2920,"The Hadoop enabled cloud platforms are gradually becoming preferred computational environment to execute scientific big data workloads in a periodic manner. However, it is observed that the default data placement approach of such cloud platforms is not the efficient one and often ends up with significant data transfer overhead leading to degradation of the overall job completion time. In this article, a Resource and Network-aware Data Placement Algorithm (RENDA) is proposed to reduce the non-local executions and thereby reduce the overall job completion time for periodic workloads in the cloud environment. The entire job execution is modeled as a two-stage execution characterized as data distribution and data processing. The RENDA reduces the time of the stages as mentioned above by estimating the heterogeneous performance of the nodes on a real-time basis followed by careful allocation of data in several installments to participating nodes. The experimental results show that the proposed RENDA algorithm consistently outperforms over the recent state-of-the-art alternatives with as much as 28 percent reduction in data transfer overhead leading to 16 percent reduction in average job completion time with 27 percent average speedup on average job execution.","1558-2183","","10.1109/TPDS.2021.3080582","Ministry of Science and Technology, Taiwan(grant numbers:109-2221-E-182-014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431731","MapReduce;cloud computing;data placement;periodic workloads","Cloud computing;Data models;Distributed databases;Data centers;Data transfer;Switches","Big Data;cloud computing;parallel processing;scheduling","data transfer overhead;nonlocal executions;cloud environment;two-stage execution;data distribution;data processing;RENDA algorithm;average job completion time;average job execution;RENDA;resource and network aware data placement algorithm;cloud periodic workloads;scientific Big Data workloads","",10.0,"",31.0,"IEEE","14 May 2021","","","IEEE","IEEE Journals"
"GML: Efficiently Auto-Tuning Flink's Configurations Via Guided Machine Learning","Y. Guo; H. Shan; S. Huang; K. Hwang; J. Fan; Z. Yu","Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Science (CAS), Shenzhen, China; JD.com American Technologies Corporation, Mountain View, CA, USA; Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Science (CAS), Shenzhen, China; Computer Science and Engineering, Chinese University of Hong Kong (CUHK), Shenzhen, China; Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Science (CAS), Shenzhen, China; Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Science (CAS), Shenzhen, China","IEEE Transactions on Parallel and Distributed Systems","2 Jun 2021",2021,32.0,12.0,2921,2935,"The increasingly popular fused batch-streaming big data framework, Apache Flink, has many performance-critical as well as untamed configuration parameters. However, how to tune them for optimal performance has not yet been explored. Machine learning (ML) has been chosen to tune the configurations for other big data frameworks (e.g., Apache Spark), showing significant performance improvements. However, it needs a long time to collect a large amount of training data by nature. In this article, we propose a guided machine learning (GML) approach to tune the configurations of Flink with significantly shorter time for collecting training data compared to traditional ML approaches. GML innovates two techniques. First, it leverages generative adversarial networks (GANs) to generate a part of training data, reducing the time needed for training data collection. Second, GML guides a ML algorithm to select configurations that the corresponding performance is higher than the average performance of random configurations. We evaluate GML on a lab cluster with 4 servers and a real production cluster in an internet company. The results show that GML significantly outperforms the state-of-the-art, DAC (Datasize-Aware-Configuration) (Z. Yu et al. 2018) for tuning the configurations of Spark, with 2.4× of reduced data collection time but with 30 percent reduced 99th percentile latency. When GML is used in the internet company, it reduces the latency by up to 57.8× compared to the configurations made by the company.","1558-2183","","10.1109/TPDS.2021.3081600","Key R&D Program of Guangdong Province(grant numbers:2019B010155003); National Natural Science Foundation of China(grant numbers:61672511,61702495,61802384); Shenzhen Institute of Artificial Intelligence and Robotics for Society; Chinese University of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9435010","Big data systems;batch-stream fused processing;flink;configuration optimization;generative adversarial networks (GAN)","Training data;Machine learning;Generative adversarial networks;Big Data;Optimization;Tuning","Big Data;Internet;learning (artificial intelligence);storage management","GML;training data collection;random configurations;reduced data collection time;Apache Flink;performance-critical;untamed configuration parameters;optimal performance;big data frameworks;Apache Spark;guided machine learning approach;fused batch-streaming big data framework;Flink's configurations;datasize-aware-configuration","",5.0,"",43.0,"IEEE","18 May 2021","","","IEEE","IEEE Journals"
"Identifying Degree and Sources of Non-Determinism in MPI Applications Via Graph Kernels","D. Chapp; N. Tan; S. Bhowmick; M. Taufer","University of Tennessee at Knoxville, Knoxville, TN, USA; University of Tennessee at Knoxville, Knoxville, TN, USA; University of North Texas, Denton, TX, USA; University of Tennessee at Knoxville, Knoxville, TN, USA","IEEE Transactions on Parallel and Distributed Systems","3 Jun 2021",2021,32.0,12.0,2936,2952,"As the scientific community prepares to deploy an increasingly complex and diverse set of applications on exascale platforms, the need to assess reproducibility of simulations and identify the root causes of reproducibility failures increases correspondingly. One of the greatest challenges facing reproducibility issues at exascale is the inherent non-determinism at the level of inter-process communication. The use of non-deterministic communication constructs is necessary to boost performance, but communication non-determinism can also hamper software correctness and result reproducibility. To address this challenge, we propose a software framework for identifying the percentage and sources of communication non-determinism. We model parallel executions as directed graphs and leverage graph kernels to characterize run-to-run variations in inter-process communication. We demonstrate the effectiveness of graph kernel similarity as a proxy for non-determinism, by showing that these kernels can quantify the type and degree of non-determinism present in communication patterns. To demonstrate our framework's ability to link and quantify runtime non-determinism to root sources, demonstrate with present for an adaptive mesh refinement application, where our framework automatically quantifies the impact of function calls on non-determinism, and a Monte Carlo application, where our framework automatically quantifies the impact of parameter configurations on non-determinism.","1558-2183","","10.1109/TPDS.2021.3081530","National Science Foundation(grant numbers:1900888,1900765); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9435018","Non-determinism;reproducibility;debugging;trace analysis;graph similarity","Reproducibility of results;Software engineering;Program processors;Computer bugs;Runtime","application program interfaces;directed graphs;message passing;Monte Carlo methods;parallel programming","software correctness;communication nondeterminism;inter-process communication;graph kernel similarity;communication patterns;runtime nondeterminism;reproducibility failures;nondeterministic communication;MPI;scientific community;exascale platforms;parallel executions;directed graphs;run-to-run variations;adaptive mesh refinement;function calls;Monte Carlo application;parameter configurations","",3.0,"",46.0,"CCBYNCND","18 May 2021","","","IEEE","IEEE Journals"
"Towards Efficient Distributed Subgraph Enumeration Via Backtracking-Based Framework","Z. Wang; W. Hu; G. Chen; C. Yuan; R. Gu; Y. Huang","Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","3 Jun 2021",2021,32.0,12.0,2953,2969,"Finding or monitoring subgraph instances that are isomorphic to a given pattern graph in a data graph is a fundamental query operation in many graph analytic applications, such as network motif mining and fraud detection. Existing distributed methods are inefficient in communication. They have to shuffle partial matching results during the distributed multiway join. The partial matching results may be much larger than the data graph itself. To overcome the drawback, we develop the Batch-BENU framework for distributed subgraph enumeration on static data graphs. Batch-BENU executes a group of local search tasks in parallel. Each task enumerates subgraphs around a vertex in the data graph, guided by a backtracking-based execution plan. To handle large-scale data graphs that may exceed the memory capacity of a single machine, Batch-BENU stores the data graph in a distributed database. Each task queries adjacency sets of the data graph on demand, shuffling the data graph instead of partial matching results. To support incremental subgraph enumeration on dynamic data graphs, we propose the Streaming-BENU framework. Streaming-BENU turns the problem of enumerating incremental matching results into enumerating all matching results of incremental pattern graphs at each time step. We implement Batch-BENU and Streaming-BENU with the local database cache and the load balance optimization to improve their efficiency. Extensive experiments show that Batch-BENU and Streaming-BENU can scale to big graphs and complex pattern graphs. They outperform the state-of-the-art distributed methods by up to one and two orders of magnitude, respectively.","1558-2183","","10.1109/TPDS.2021.3076246","National Key Research and Development Program of China(grant numbers:2019YFC1711000); National Natural Science Foundation of China(grant numbers:U1811461,62072230); Jiangsu Province Science and Technology Research Program(grant numbers:BE2017155); Collaborative Innovation Center of Novel Software Technology and Industrialization, Jiangsu; Alibaba Innovative Resarch Project; Nanjing University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9417730","Backtracking-based framework;distributed graph querying;incremental subgraph matching;subgraph isomorphism;subgraph matching","Distributed databases;Pattern matching;Graph theory;Big Data;Software algorithms;Optimization;Distributed computing","backtracking;computational complexity;data mining;distributed databases;graph theory;optimisation;query processing","incremental pattern graphs;Batch-BENU;Streaming-BENU;complex pattern graphs;towards efficient distributed subgraph enumeration;given pattern graph;data graph;graph analytic applications;partial matching results;static data graphs;large-scale data graphs;incremental subgraph enumeration;dynamic data graphs","","","",39.0,"IEEE","28 Apr 2021","","","IEEE","IEEE Journals"
"Scalable Energy Games Solvers on GPUs","A. Formisano; R. Gentilini; F. Vella","Università di Udine, Udine, Italy; Università di Perugia, Perugia, Italy; Free University of Bozen, Bolzano, Italy","IEEE Transactions on Parallel and Distributed Systems","7 Jun 2021",2021,32.0,12.0,2970,2982,"Modeling the consumption of limited resources, e.g., time or energy, plays a central role on the design of reactive systems such as embedded controllers. To this aim, quantitative objectives are defined on game arenas that can be easily modeled as weighted graphs. Instances of these games, called energy games, can be solved in ${\mathcal {O}(\vert {E}\vert {\cdot }\vert {V}\vert {\cdot }W)}$   O ( | E | · | V | · W )    where $W$  W   is the maximum weight. Recent work has demonstrated that sequential implementations hardly solve practical instances due to their size and the number of interactions required to converge to a solution. Recent work has demonstrated that sequential implementations hardly solve practical instances. Furthermore, emerging approaches, that have investigated the parallelism of CPUs multi-core and GPU for solving the initial credit problem for energy games, still perform poorly due to the non-trivial characteristics of these graphs. In this article we first describe a revised version of the algorithm on multi-core CPU that obtains a faster convergence time on real-world graphs with up to 30x against the serial implementation by showing good scalability overall. Second, we provide a new GPU-based parallel implementation based on warp-level primitives that allows to reduce the time-to-solution on several instances with up to 3.6x of speed-up against traditional parallel vertex-based approaches. We also discuss a methodology to build synthetic energy games to validate the scalability of parallel algorithms on two totally different settings.","1558-2183","","10.1109/TPDS.2021.3080925","Libera Universit di Bolzano(grant numbers:CRC2019-IN2091); Istituto Nazionale di Alta Matematica Francesco Severi(grant numbers:Project INdAM-GNCS-2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432723","Graph analytics;GPU computing;formal verification;energy games","Formal verification;Robots;Energy states;Parallel architectures;Legged locomotion;Game theory;Economics","game theory;graph theory;graphics processing units;mathematics computing;multiprocessing systems;parallel algorithms;power aware computing","multicore CPU;real-world graphs;GPU-based parallel implementation;warp-level primitives;scalable energy games solvers","",3.0,"",29.0,"IEEE","17 May 2021","","","IEEE","IEEE Journals"
"Fast, Accurate Processor Evaluation Through Heterogeneous, Sample-Based Benchmarking","P. Prieto; P. Abad; J. A. Gregorio; V. Puente","Computer Engineering Group, University of Cantabria, Santander, Spain; Computer Engineering Group, University of Cantabria, Santander, Spain; Computer Engineering Group, University of Cantabria, Santander, Spain; Computer Engineering Group, University of Cantabria, Santander, Spain","IEEE Transactions on Parallel and Distributed Systems","10 Jun 2021",2021,32.0,12.0,2983,2995,"Performance evaluation is a key task in computing and communication systems. Benchmarking is one of the most common techniques for evaluation purposes, where the performance of a set of representative applications is used to infer system responsiveness in a general usage scenario. Unfortunately, most benchmarking suites are limited to a reduced number of applications, and in some cases, rigid execution configurations. This makes it hard to extrapolate performance metrics for a general-purpose architecture, supposed to have a multi-year lifecycle, running dissimilar applications concurrently. The main culprit of this situation is that current benchmark-derived metrics lack generality, statistical soundness and fail to represent general-purpose environments. Previous attempts to overcome these limitations through random app mixes significantly increase computational cost (workload population shoots up), making the evaluation process barely affordable. To circumvent this problem, in this article we present a more elaborate performance evaluation methodology named BenchCast. Our proposal provides more representative performance metrics, but with a drastic reduction of computational cost, limiting app execution to a small and representative fraction marked through code annotation. Thanks to this labeling and making use of synchronization techniques, we generate heterogeneous workloads where every app runs simultaneously inside its Region Of Interest, making a few execution seconds highly representative of full application execution.","1558-2183","","10.1109/TPDS.2021.3080702","Agencia Estatal de Investigacion(grant numbers:PID2019-110051GB-I00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432746","C.1 Processor Architectures;C.4 Performance of Systems;C.4.c Measurement techniques;C.4.g Measurement;evaluation;modeling;simulation of multiple-processor systems","Benchmark testing;Simulation;Performance evaluation;Microarchitecture;Computational efficiency;Program processors;Task analysis","benchmark testing;computer architecture;mobile computing;software metrics;software performance evaluation;statistical analysis;synchronisation","general-purpose environments;random app;computational cost;workload population;performance evaluation methodology;representative performance metrics;app execution;representative fraction;synchronization techniques;heterogeneous workloads;execution seconds;application execution;accurate processor evaluation;sample-based benchmarking;computing communication systems;system responsiveness;general usage scenario;benchmarking suites;rigid execution configurations;general-purpose architecture;multiyear lifecycle;statistical soundness;benchmark-derived metrics;BenchCast;region of interest","",1.0,"",39.0,"IEEE","17 May 2021","","","IEEE","IEEE Journals"
"ReliableBox: Secure and Verifiable Cloud Storage With Location-Aware Backup","T. Jiang; W. Meng; X. Yuan; L. Wang; J. Ge; J. Ma","Guangxi Key Laboratory of Cryptography and Information Security, Guilin, China; College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, China; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China","IEEE Transactions on Parallel and Distributed Systems","10 Jun 2021",2021,32.0,12.0,2996,3010,"While the prevalent cloud storage platforms are offering convenient services in support of diverse data-driven applications for clients, various security concerns raise in terms of data confidentiality, availability, and retrievability. Among them, servers' dishonesty on the location-specific data backup becomes a serious concern when the data stands out clients' control, considering the strict regulations imposed by many governments and organizations on data storage location. This article studies location-aware data backup verification for the data stored in clouds and aims to design a secure framework, named as ReliableBox, enabling the clients to verify if their data have been backed up on the remote servers with specific geolocation. In the design of ReliableBox, we leverage the prominent proof-of-storage techniques for data possession proof, and take advantage of multilateration geolocation and Intel SGX for the precise communication delay measurement and trust computing delay measurement, respectively. In ReliableBox, a client first computes integrity tags for the files and then outsources both the files and tags to the cloud storage server. In the later attestation, with the precise network delay and distance measurement from location-known verifiers, the client verifies that the outsourced files are intact and backed-up to hosts at the specific geolocation. With the customized design, ReliableBox can support the security needs in terms of both data integrity and backup location verification for clients, even when there exists potential dishonest cloud service providers who may manipulate the network delays or forge verification proofs. We provide security analysis to show the security property of ReliableBox in terms of data access, confidentiality, and verifications. In the end, we implement the system prototype and deploy it into several prevalent and commercial cloud platforms for performance evaluation. The experimental results demonstrate that ReliableBox is secure in support of data integrity checking and location-aware backup auditing, while it is robust to the data possession and location spoofing attacks.","1558-2183","","10.1109/TPDS.2021.3080594","National Key Research and Development Program of China(grant numbers:2018YFB0804103); National Natural Science Foundation of China(grant numbers:61702402,U1736216,61902291); China Postdoctoral Science Foundation(grant numbers:2017M613079); Fundamental Research Funds for the Central Universities(grant numbers:XJS211502,XJS191501); University Innovation Platform(grant numbers:2019921815KYPT009JC011); Guangxi Key Laboratory of Cryptography and Information Security(grant numbers:GCIS201716); Louisiana Board of Regents(grant numbers:LEQSF(2018-21)-RD-A-24); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431668","Cloud storage;data possession;data backup;geolocation verification","Geology;Cloud computing;Servers;Reliability engineering;Security;Data integrity","cloud computing;data integrity;mobile computing;outsourcing;storage management;telecommunication security;trusted computing","ReliableBox;data confidentiality;location-specific data backup;clients control;data storage location;secure framework;remote servers;geolocation;proof-of-storage techniques;data possession proof;precise communication delay measurement;trust computing delay measurement;cloud storage server;network delay;distance measurement;backup location verification;verification proofs;security analysis;security property;data access;commercial cloud platforms;data integrity checking;location-aware backup auditing;location spoofing attacks;location-aware data backup verification;cloud service providers","",2.0,"",60.0,"IEEE","14 May 2021","","","IEEE","IEEE Journals"
"Self-Stabilizing Population Protocols With Global Knowledge","Y. Sudo; M. Shibata; J. Nakamura; Y. Kim; T. Masuzawa","Hosei University, Tokyo, Japan; Kyushu Institute of Technology, Kitakyushu, Japan; Toyohashi University of Technology, Toyohashi, Japan; Nagoya Institute of Technology, Nagoya, Japan; Osaka University, Osaka, Japan","IEEE Transactions on Parallel and Distributed Systems","10 Jun 2021",2021,32.0,12.0,3011,3023,"In the population protocol model, many problems cannot be solved in a self-stabilizing manner. However, global knowledge, such as the number of nodes in a network, sometimes enables the design of a self-stabilizing protocol for such problems. For example, it is known that we can solve the self-stabilizing leader election in complete graphs if and only if every node knows the exact number of nodes. In this article, we investigate the effect of global knowledge on the possibility of self-stabilizing population protocols in arbitrary graphs. Specifically, we clarify the solvability of the leader election problem, the ranking problem, the degree recognition problem, and the neighbor recognition problem by self-stabilizing population protocols with knowledge of the number of nodes and/or the number of edges in a network.","1558-2183","","10.1109/TPDS.2021.3076769","JSPS KAKENHI(grant numbers:17K19977,18K18000,18K18029,18K18031,19H04085,20H04140,20KK0232); JST SICORP(grant numbers:JPMJSC1606); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420769","Population protocols;leader election;self-stabilization","Social factors;Protocols;Statistics;Globalization;Network systems","graph theory;protocols;radio networks;stability","self-stabilizing population protocols;global knowledge;population protocol model;leader election problem;ranking problem;degree recognition problem;neighbor recognition problem;self-stabilizing manner;self-stabilizing leader election;nodes exact number;arbitrary graphs;network edges number","",1.0,"",27.0,"CCBY","3 May 2021","","","IEEE","IEEE Journals"
"A Unified Framework for Flexible Playback Latency Control in Live Video Streaming","G. Zhang; J. Y. B. Lee; K. Liu; H. Hu; V. Aggarwal","Centre for Advances in Reliability and Safety (CAiRS), Pak Shek Kok, NT, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, NT, Hong Kong; University of Chinese Academy of Sciences (UCAS), Beijing, China; PolyU Shenzhen Research Institute, Kowloon, Hong Kong; Computer Engineering, Purdue University, West Lafayette, IN, USA","IEEE Transactions on Parallel and Distributed Systems","14 Jun 2021",2021,32.0,12.0,3024,3037,"Live video streaming has seen tremendous growth in the past decade. An important fact in live streaming is that the demand for low playback-latency inherently conflicts with the desire for high QoE. This requires different types of live services to seek different latency-QoE tradeoffs according to their service-requirements. However, our investigations revealed that it is fundamentally difficult for existing streaming algorithms to keep consistent latency in changing network conditions, let alone achieve the service-desired latency-QoE tradeoff. To tackle the challenge, this article develops a novel framework called Flexible Latency Aware Streaming (FLAS) that not only can achieve consistent low latency, but also control the latency-QoE tradeoff flexibly. Specifically, FLAS generates a set of adaptation logics offline, each optimized for a candidate tradeoff point, then selects the most appropriate one to run online. We first show how FLAS can be applied to optimizing the existing algorithms, then developed a novel Genetic Programming approach to fully exploit FLAS's potential. Extensive evaluations show that FLAS can precisely control latency all the way down to 1s and achieve substantially higher QoE than state-of-the-arts. FLAS can be readily implemented into real streaming platforms, offering a practical and reliable solution for live-streaming services.","1558-2183","","10.1109/TPDS.2021.3083202","Centre for Advances in Reliability and Safety Limited; National Natural Science Foundation of China(grant numbers:62072439); National Key Research and Development Program of China(grant numbers:2016YFB1000200); Natural Science Foundation of Shandong Province(grant numbers:ZR2019LZH004); Beijing Municipal Natural Science Foundation(grant numbers:4212028); State Key Laboratory of Computer Architecture Innovation Fund(grant numbers:carch4503); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439873","Video streaming;genetic programming;quality-of-experience;video reliability","Streaming media;Quality of experience;Throughput;Bit rate;3G mobile communication;Video recording","genetic algorithms;video streaming","Flexible playback Latency control;live video streaming;live streaming;low playback-latency inherently conflicts;high QoE;live services;service-requirements;streaming algorithms;service-desired latency-QoE tradeoff;FLAS;consistent low latency;candidate tradeoff point;substantially higher QoE;streaming platforms;flexible latency aware streaming","",1.0,"",43.0,"IEEE","24 May 2021","","","IEEE","IEEE Journals"
"Logically Parallel Communication for Fast MPI+Threads Applications","R. Zambre; D. Sahasrabudhe; H. Zhou; M. Berzins; A. Chandramowlishwaran; P. Balaji","Department of Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA; Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT, USA; Department of Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA","IEEE Transactions on Parallel and Distributed Systems","21 Jun 2021",2021,32.0,12.0,3038,3052,"Supercomputing applications are increasingly adopting the MPI+threads programming model over the traditional “MPI everywhere” approach to better handle the disproportionate increase in the number of cores compared with other on-node resources. In practice, however, most applications observe a slower performance with MPI+threads primarily because of poor communication performance. Recent research efforts on MPI libraries address this bottleneck by mapping logically parallel communication, that is, operations that are not subject to MPI's ordering constraints to the underlying network parallelism. Domain scientists, however, typically do not expose such communication independence information because the existing MPI-3.1 standard's semantics can be limiting. Researchers had initially proposed user-visible endpoints to combat this issue, but such a solution requires intrusive changes to the standard (new APIs). The upcoming MPI-4.0 standard, on the other hand, allows applications to relax unneeded semantics and provides them with many opportunities to express logical communication parallelism. In this article, we show how MPI+threads applications can achieve high performance with logically parallel communication. Through application case studies, we compare the capabilities of the new MPI-4.0 standard with those of the existing one and user-visible endpoints (upper bound). Logical communication parallelism can boost the overall performance of an application by over 2×.","1558-2183","","10.1109/TPDS.2021.3075157","U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:1750549); University of Utah; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9411740","MPI+threads;MPI+OpenMP;exascale MPI;MPI_THREAD_MULTIPLE;MPI endpoints;Uintah;HYPRE;wombat;Legion","Parallel processing;Supercomputing;Programming;Semantics;Upper bound","application program interfaces;message passing;multi-threading;parallel processing","supercomputing applications;MPI libraries;MPI ordering constraints;network parallelism;communication independence information;user-visible endpoints;application case studies;MPI+threads programming model;logically parallel communication mapping;MPI-3.1 standard semantics;MPI-4.0 standard","",2.0,"",66.0,"IEEE","22 Apr 2021","","","IEEE","IEEE Journals"
"Parallel Fine-Grained Comparison of Long DNA Sequences in Homogeneous and Heterogeneous GPU Platforms With Pruning","M. Figueiredo; J. P. Navarro; E. F. O. Sandes; G. Teodoro; A. C. M. A. Melo","University of Brasilia, Brasilia, Brazil; NVidia, Sao Paulo, Brazil; University of Brasilia, Brasilia, Brazil; Universidade Federal de Minas Gerais, Belo Horizonte, Brazil; University of Brasilia, Brasilia, Brazil","IEEE Transactions on Parallel and Distributed Systems","21 Jun 2021",2021,32.0,12.0,3053,3065,"The parallelization of Smith-Waterman (SW) sequence comparison tools for long DNA sequences has been a big challenge over the years, requesting the use of several devices and sophisticated optimizations. Pruning is one of these optimizations, which can reduce considerably the amount of computation. This article proposes MultiBP, a sequence comparison solution in multiple GPUs with block pruning. Two MultiBP strategies are proposed. In static score-sharing, workload is statically distributed to the GPUs, and the best score is sent to neighbor GPUs to simulate a global view. In the dynamic strategy, execution is divided into cycles and workload is dynamically assigned, according to the GPUs processing rate. MultiBP was integrated to MASA-CUDAlign and tested in homogeneous and heterogeneous platforms, with different NVidia GPU architectures. The best results in our homogeneous and heterogeneous platforms were mostly obtained by the static and dynamic approaches, respectively. We also show that our decision module is able to select the best strategy in most cases. Finally, the comparison of the human and chimpanzee chromosomes 1 in a cluster with 512 V100 NVidia GPUs took 11 minutes and obtained the impressive rate of 82,822 GCUPS (Billions of Cells Updated per Second) which is, to our knowledge, the best performance for SW tools in GPUs.","1558-2183","","10.1109/TPDS.2021.3084069","CNPq/Brazil(grant numbers:305196/2018-9); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442294","GPU;sequence comparison;pruning;workload assignment;heterogeneous platforms","Graphics processing units;Sequential analysis;Heuristic algorithms;Dynamic programming;Optimization;Evolution (biology)","bioinformatics;DNA;graphics processing units;optimisation;parallel processing","static approaches;dynamic approaches;parallel fine-grained comparison;long DNA sequences;heterogeneous GPU platforms;parallelization;Smith-Waterman sequence comparison tools;optimizations;block pruning;NVidia GPU architectures;homogeneous GPU platforms;MultiBP;MASA-CUDAlign;biological sequence comparison;bioinformatics","",1.0,"",41.0,"IEEE","26 May 2021","","","IEEE","IEEE Journals"
"An Elastic Task Scheduling Scheme on Coarse-Grained Reconfigurable Architectures","L. Chen; J. Zhu; Y. Deng; Z. Li; J. Chen; X. Jiang; S. Yin; S. Wei; L. Liu","School of Integrated Circuits, Tsinghua University, Beijing, China; School of Integrated Circuits, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Integrated Circuits, Tsinghua University, Beijing, China; Alibaba Group, Sunnyvale, CA, USA; Alibaba Group, Sunnyvale, CA, USA; School of Integrated Circuits, Tsinghua University, Beijing, China; School of Integrated Circuits, Tsinghua University, Beijing, China; School of Integrated Circuits, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","1 Jul 2021",2021,32.0,12.0,3066,3080,"Coarse-grained reconfigurable architectures (CGRAs) are increasingly employed as domain-specific accelerators due to their efficiency and flexibility. A CGRA typically relies on compilers to perform task scheduling. The longstanding problem of static scheduling is that it suffers from insufficient parallelism in handling irregularities due to over-serialization and workload imbalance, which leads to severe resource underutilization and performance loss. To counteract the limitations of static scheduling in CGRAs, it is essential to exploit dynamic parallelism automatically and manage hardware resources adaptively. However, existing dynamic scheduling mechanisms, e.g., work stealing, often reschedule aggressively for instant performance but sacrifice efficiency, which is unfavorable to CGRAs that emphasize efficiency and fewer reconfigurations. This article proposes an elastic task scheduling scheme that enables lightweight dynamic scheduling in CGRAs. Tasks are rescheduled at runtime according to the classic tagged-token dataflow paradigm to enable dynamic task-level parallelism. Meanwhile, tasks are dynamically resized according to run-time throughputs via duplication, combination, and substitution operators for balanced multitask execution. We implement the elastic task scheduling scheme on a well-known reconfigurable architecture - triggered instruction architecture (TIA). Evaluation on the MachSuite benchmarks shows that the proposed scheme is effective in improving performance and energy efficiency. The average speedup is 2× over the baseline. Also, our design attains a 57 percent improvement in the area-normalized performance and a 49 percent better energy efficiency. Compared with a state-of-the-art dynamic scheduling method, our scheme achieves 1.6× speedup and 1.6× energy efficiency than work-stealing mechanism on the same substrate.","1558-2183","","10.1109/TPDS.2021.3084804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444171","Task schedule;elastic task schedule;reconfigurable architectures;dynamic issue;dynamic mapping","Task analysis;Dynamic scheduling;Parallel processing;Reconfigurable architectures;Processor scheduling;Computer architecture;Job shop scheduling","benchmark testing;dynamic scheduling;energy conservation;multiprogramming;parallel processing;reconfigurable architectures;resource allocation","elastic task scheduling scheme;coarse-grained reconfigurable architectures;CGRAs;static scheduling;severe resource underutilization;lightweight dynamic scheduling;dynamic task-level parallelism;energy efficiency;domain-specific accelerators;insufficient parallelism;over-serialization;triggered instruction architecture;dynamic scheduling mechanisms;TIA;area-normalized performance;work-stealing mechanism;MachSuite benchmarks;tagged-token dataflow paradigm","",1.0,"",65.0,"IEEE","28 May 2021","","","IEEE","IEEE Journals"
"Modeling and Analyzing Waiting Policies for Cloud-Enabled Schedulers","P. Ambati; N. Bashir; D. Irwin; P. Shenoy","University of Massachusetts Amherst, Amherst, MA, USA; University of Massachusetts Amherst, Amherst, MA, USA; University of Massachusetts Amherst, Amherst, MA, USA; University of Massachusetts Amherst, Amherst, MA, USA","IEEE Transactions on Parallel and Distributed Systems","30 Jun 2021",2021,32.0,12.0,3081,3100,"Cloud platforms have popularized the Infrastructure-as-a-Service (IaaS) purchasing model, which enables users to rent computing resources on demand to execute their jobs. However, buying fixed resources is still much cheaper than renting if their resource utilization is high. Thus, to optimize cost, users must decide how many fixed resources to provision versus rent “on demand” based on their workload. In this article, we introduce the concept of a waiting policy for cloud-enabled schedulers and show that the optimal cost depends on it. The waiting policy explicitly controls how long jobs wait for resources, as jobs never need to wait, since cloud platforms provide the illusion of infinite scalability. A waiting policy is the dual of a scheduling policy: while a scheduling policy determines which jobs should run when fixed resources are available, a waiting policy determines which jobs should wait when fixed resources are not available. We define multiple waiting policies and develop simple and general analytical models to reveal their tradeoff between fixed resource provisioning, cost, and job waiting time. We evaluate the impact of different waiting policies on a real year-long batch workload consisting of 14M jobs run on a 14.3k-core cluster. We show that a compound waiting policy, which forces jobs with long running times or short waiting times to wait for fixed resources, offers the best tradeoff. The policy decreases both the cost (by 5 percent) and mean job waiting time (by 7×) compared to the current cluster, and also decreases the cost (by 43 percent) compared to renting on-demand resources for a modest increase in mean job waiting time (at 1.74 hours).","1558-2183","","10.1109/TPDS.2021.3086270","National Science Foundation(grant numbers:CNS-1802523,CNS-1908536); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446629","Cloud computing;job scheduling;batch scheduling;waiting policy","Analytical models;Cloud computing;Pricing;Computational modeling;Queueing analysis;Dynamic scheduling;Resource management","cloud computing;resource allocation;scheduling","resource utilization;fixed resources;cloud-enabled schedulers;long jobs;cloud platforms;scheduling policy;multiple waiting policies;fixed resource provisioning;compound waiting policy;short waiting times;mean job waiting time;on-demand resources;computing resources;infrastructure-as-a-service purchasing model;waiting policies","",1.0,"",35.0,"IEEE","3 Jun 2021","","","IEEE","IEEE Journals"
"Overlapping Communication With Computation in Parameter Server for Scalable DL Training","S. Wang; A. Pi; X. Zhou; J. Wang; C. -Z. Xu","Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, USA; Faculty of Science and Technology, University of Macau, Taipa, Macau, China","IEEE Transactions on Parallel and Distributed Systems","17 Mar 2021",2021,32.0,9.0,2144,2159,"Scalability of distributed deep learning (DL) training with parameter server (PS) architecture is often communication constrained in large clusters. There are recent efforts that use a layer by layer strategy to overlap gradient communication with backward computation so as to reduce the impact of communication constraint on the scalability. However, the approaches could bring significant overhead in gradient communication. Meanwhile, they cannot be effectively applied to the overlap between parameter communication and forward computation. In this article, we propose and develop iPart, a novel approach that partitions communication and computation in various partition sizes to overlap gradient communication with backward computation and parameter communication with forward computation. iPart formulates the partitioning decision as an optimization problem and solves it based on a greedy algorithm to derive communication and computation partitions. We implement iPart in the open-source DL framework BigDL and perform evaluations with various DL workloads. Experimental results show that iPart improves the scalability of a cluster of 72 nodes by up to 94 percent over the default PS and 52 percent over the layer by layer strategy.","1558-2183","","10.1109/TPDS.2021.3062721","National Science Foundation(grant numbers:CNS-1422119,SHF-1816850); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366342","Parameter server;parameter communication;forward computation;gradient communication;backward computation","Servers;Training;Computer architecture;Computational modeling;Neural networks;Synchronization;Scalability","client-server systems;deep learning (artificial intelligence);file organisation;greedy algorithms;optimisation","greedy algorithm;open-source DL framework BigDL;layer by layer strategy;computation partitions;partitions communication;iPart;forward computation;parameter communication;communication constraint;backward computation;overlap gradient communication;parameter server architecture;distributed deep learning training;scalable DL training;overlapping communication","",7.0,"",59.0,"IEEE","1 Mar 2021","","","IEEE","IEEE Journals"
"Retargeting Tensor Accelerators for Epistasis Detection","R. Nobre; A. Ilic; S. Santander-Jiménez; L. Sousa","INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisboa, Portugal; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisboa, Portugal; Department of Computer and Communications Technologies, University of Extremadura, Badajoz, Spain; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisboa, Portugal","IEEE Transactions on Parallel and Distributed Systems","18 Mar 2021",2021,32.0,9.0,2160,2174,"The substitution of nucleotides at specific positions in the genome of a population, known as single-nucleotide polymorphisms (SNPs), has been correlated with a number of important diseases. Complex conditions such as Alzheimer's disease or Crohn's disease are significantly linked to genetics when the impact of multiple SNPs is considered. SNPs often interact in an epistatic manner, where the joint effect of multiple SNPs may not be simply mapped to a linear additive combination of individual effects. Genome-wide association studies considering epistasis are computationally challenging, especially when performing triplet searches is required. Some contemporary computer architectures support fused XOR and population count as the highest throughput operations as part of tensor operations. This article presents a new approach for efficiently repurposing this capability to accelerate 2-way (pairs) and 3-way (triplets) epistasis detection searches. Experimental evaluation targeting the Turing GPU architecture resulted in previously unattainable levels of performance, with the proposal being able to evaluate up to 108.1 and 54.5 tera unique sets of SNPs per second, scaled to the sample size, in 2-way and 3-way searches, respectively.","1558-2183","","10.1109/TPDS.2021.3060322","Fundação para a Ciência e a Tecnologia; European Regional Development Fund(grant numbers:UIDB/50021/2020,LISBOA-01-0145-FEDER-031901,PTDC/CCI-COM/31901/2017, HiPErBio); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9357942","GWAS;two- and three-way epistasis;performance evaluation;parallel architectures","Statistics;Sociology;Tensors;Proposals;Hardware;Throughput;Diseases","biology computing;diseases;genetics;genomics;molecular biophysics;polymorphism","triplet searches;single-nucleotide polymorphisms;specific positions;tensor accelerators;Turing GPU architecture;3-way epistasis detection searches;tensor operations;highest throughput operations;contemporary computer architectures;genome-wide association studies;linear additive combination;joint effect;epistatic manner;SNP;Crohn's disease;Alzheimer's disease;complex conditions;important diseases","",5.0,"",46.0,"CCBY","18 Feb 2021","","","IEEE","IEEE Journals"
"DeepSlicing: Collaborative and Adaptive CNN Inference With Low Latency","S. Zhang; S. Zhang; Z. Qian; J. Wu; Y. Jin; S. Lu","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Center for Networked Computing, Temple University, Philadelphia, PA, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","17 Mar 2021",2021,32.0,9.0,2175,2187,"The booming of Convolutional Neural Networks (CNNs) has empowered lots of computer-vision applications. Due to its stringent requirement for computing resources, substantial research has been conducted on how to optimize its deployment and execution on resource-constrained devices. However, previous works have several weaknesses, including limited support for various CNN structures, fixed scheduling strategies, overlapped computations, high synchronization overheads, etc. In this article, we present DeepSlicing, a collaborative and adaptive inference system that adapts to various CNNs and supports customized flexible fine-grained scheduling. As a built-in functionality, DeepSlicing has supported typical CNNs including GoogLeNet, ResNet, etc. By partitioning both model and data, we also design an efficient scheduler, Proportional Synchronized Scheduler (PSS), which achieves the trade-off between computation and synchronization. Based on PyTorch, we have implemented DeepSlicing on the testbed with real-world edge settings that consists of 8 heterogeneous Raspberry Pi's. The results indicate that DeepSlicing with PSS outperforms the existing systems dramatically, e.g., the inference latency and memory footprint are reduced up to 5.79× and 14.72×, respectively.","1558-2183","","10.1109/TPDS.2021.3058532","National Key Research and Development Program of China(grant numbers:2017YFB1001801); National Natural Science Foundation of China(grant numbers:61872175,61832008); Natural Science Foundation of Jiangsu Province(grant numbers:BK20181252); Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9353250","CNN inference;edge computing;scheduling;synchronization","Feature extraction;Task analysis;Synchronization;Computational modeling;Processor scheduling;Performance evaluation;Scheduling","computer vision;convolutional neural nets;inference mechanisms;microcontrollers;query processing;scheduling;synchronisation","Raspberry Pi;proportional synchronized scheduler;PSS;efficient scheduler;fine-grained scheduling;adaptive inference system;collaborative inference system;high synchronization overheads;overlapped computations;fixed scheduling strategies;CNN structures;resource-constrained devices;substantial research;computing resources;stringent requirement;computer-vision applications;convolutional neural networks;adaptive CNN inference;DeepSlicing","",17.0,"",36.0,"IEEE","11 Feb 2021","","","IEEE","IEEE Journals"
"Optimizing Resource Allocation for Data-Parallel Jobs Via GCN-Based Prediction","Z. Hu; D. Li; D. Zhang; Y. Zhang; B. Peng","National University of Defense Technology, Changsha, P.R. China; National University of Defense Technology, Changsha, P.R. China; Zhejiang University, Hangzhou, P.R. China; National University of Defense Technology, Changsha, P.R. China; National University of Defense Technology, Changsha, P.R. China","IEEE Transactions on Parallel and Distributed Systems","17 Mar 2021",2021,32.0,9.0,2188,2201,"Under-allocating or over-allocating computation resources (e.g., CPU cores) can prolong the completion time of data-parallel jobs in a distributed system. We present a predictor, ReLocag, to find the near-optimal number of CPU cores to minimize job completion time (JCT). ReLocag includes a graph convolutional network (GCN) and a fully-connected network (FCNN). The GCN learns the dependency between operations from the workflow of a job, and then the FCNN takes the workflow dependency together with other features (e.g., the input size, the number of CPU cores, the amount of memory, and the number of computation tasks) as input for JCT prediction. The prediction result can guide the user to determine the near-optimal number of CPU cores. Besides, we propose two effective strategies to overcome the time-consuming issue of training sample collection in big data applications. First, we develop an adaptive sampling method to collect essential samples judiciously. Second, we further design a cross-application transfer learning model to exploit the training samples collected from other applications. We conduct extensive experiments in a Spark cluster for 7 types of exemplary Spark applications. Results show that ReLocag improves the JCT prediction accuracy by 4-14 percent. Moreover, the CPU core consumption decreases by 58.2 percent.","1558-2183","","10.1109/TPDS.2021.3055019","National Key Research and Development Program of China(grant numbers:2018YFB2101100); National Natural Science Foundation of China(grant numbers:61932001,61872376); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337194","Data-parallel job;resource allocation;performance prediction;sampling overhead","Sparks;Resource management;Predictive models;Training;Task analysis;Transfer learning;Adaptation models","Big Data;convolutional neural nets;graph theory;learning (artificial intelligence);microprocessor chips;optimisation;parallel processing;resource allocation;sampling methods","CPU core consumption;JCT prediction accuracy;cross-application transfer learning model;big data applications;time-consuming issue;workflow dependency;FCNN;fully-connected network;graph convolutional network;job completion time;near-optimal number;ReLocag;CPU cores;over-allocating computation resources;under-allocating;GCN-based prediction;data-parallel jobs;resource allocation","","","",39.0,"IEEE","27 Jan 2021","","","IEEE","IEEE Journals"
"Octans: Optimal Placement of Service Function Chains in Many-Core Systems","H. Yu; Z. Zheng; J. Shen; C. Miao; C. Sun; H. Hu; J. Bi; J. Wu; J. Wang","Peng Cheng Laboratory, Shenzhen, Guangdong, China; Alibaba Group, Hangzhou, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Alibaba Group, Hangzhou, China; Department of Computer Science and Engineering, University at Buffalo, the State University of New York, Buffalo, NY, USA; Beijing National Research Center for Information Science and Technology, Beijing, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China","IEEE Transactions on Parallel and Distributed Systems","17 Mar 2021",2021,32.0,9.0,2202,2215,"Network Function Virtualization (NFV) offers service delivery flexibility and reduces overall costs by running service function chains (SFCs) on commodity servers with many cores. Existing solutions for placing SFCs in one server treat all CPU cores as equal and allocate isolated CPU cores to network functions (NFs). However, advanced servers often adopt Non-Uniform Memory Access (NUMA) architecture to improve the scalability of many-core systems. CPU cores are grouped into nodes, incurring performance degradation due to cross-node memory access and intra-node resource contention. Our evaluation shows that randomly selecting cores to place NFs in an SFC could suffer from 39.2 percent lower throughput comparing to an optimal placement solution. In this article, we propose Octans, an NFV orchestrator to achieve maximum aggregate throughput of all SFCs in many-core systems. Octans first formulates the optimization problem as a Non-Linear Integer Programming (NLIP) Model. Then we identify the key factor for problem solving as evaluating the throughput drop of an NF caused by other NFs in the same SFC or different SFCs, i.e., performance drop index, and propose a formal and accurate prediction model based on system level performance metrics. Finally, we propose two online algorithms to quickly find near-optimal placement solutions for one-time and incremental deployment. Extensive evaluation on a prototype implementation shows that Octans significantly improves the aggregate throughput comparing to two state-of-the-art placement solutions by 27.1 ~ 45.2 percent for one-time deployment and by 20.9 ~ 38.1 percent for incremental deployment, with very low prediction errors. Moreover, Octans could quickly find a near-optimal placement solution with tiny optimality gap.","1558-2183","","10.1109/TPDS.2021.3063613","National Key Research and Development Program of China(grant numbers:2020YFE0200500); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369135","Many-core system;network function virtualization;service function chain;optimal placement","Servers;Throughput;Noise measurement;Indexes;Aggregates;Service function chaining;Optimization","integer programming;multiprocessing systems;nonlinear programming;resource allocation;virtualisation","NonUniform Memory Access architecture;state-of-the-art placement solutions;near-optimal placement solution;system level performance metrics;different SFCs;NonLinear Integer Programming Model;optimization problem;maximum aggregate throughput;intra-node resource contention;cross-node memory access;nonuniform memory access architecture;advanced servers;network functions;CPU cores;server treat;commodity servers;service delivery flexibility;network function virtualization;many-core systems;service function chains;Octans;efficiency 39.2 percent;efficiency 45.2 percent;efficiency 38.1 percent","",6.0,"",45.0,"IEEE","3 Mar 2021","","","IEEE","IEEE Journals"
"A Survey of System Architectures and Techniques for FPGA Virtualization","M. H. Quraishi; E. B. Tavakoli; F. Ren","School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA; School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA; School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA","IEEE Transactions on Parallel and Distributed Systems","29 Mar 2021",2021,32.0,9.0,2216,2230,"FPGA accelerators are gaining increasing attention in both cloud and edge computing because of their hardware flexibility, high computational throughput, and low power consumption. However, the design flow of FPGAs often requires specific knowledge of the underlying hardware, which hinders the wide adoption of FPGAs by application developers. Therefore, the virtualization of FPGAs becomes extremely important to create a useful abstraction of the hardware suitable for application developers. Such abstraction also enables the sharing of FPGA resources among multiple users and accelerator applications, which is important because, traditionally, FPGAs have been mostly used in single-user, single-embedded-application scenarios. There are many works in the field of FPGA virtualization covering different aspects and targeting different application areas. In this article, we review the system architectures used in the literature for FPGA virtualization. In addition, we identify the primary objectives of FPGA virtualization, based on which we summarize the techniques for realizing FPGA virtualization. This article helps researchers to efficiently learn about FPGA virtualization research by providing a comprehensive review of the existing literature.","1558-2183","","10.1109/TPDS.2021.3063670","Cisco Research Center(grant numbers:CG#1490376); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369140","FPGA;virtualization;architecture;accelerator;reconfiguration","Field programmable gate arrays;Virtualization;Hardware;Cloud computing;Computer architecture;Edge computing;Systems architecture","field programmable gate arrays;hardware accelerators;logic design;virtualisation","cloud computing;low power consumption;hardware flexibility;edge computing;FPGA accelerators;FPGA virtualization;single-embedded-application scenarios","",11.0,"",84.0,"IEEE","3 Mar 2021","","","IEEE","IEEE Journals"
"Accurate Differentially Private Deep Learning on the Edge","R. Han; D. Li; J. Ouyang; C. H. Liu; G. Wang; D. Wu; L. Y. Chen","Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; University of Florida, Gainesville, FL, USA; TU Delft, Delft, The Netherlands","IEEE Transactions on Parallel and Distributed Systems","29 Mar 2021",2021,32.0,9.0,2231,2247,"Deep learning (DL) models are increasingly built on federated edge participants holding local data. To enable insight extractions without the risk of information leakage, DL training is usually combined with differential privacy (DP). The core theme is to tradeoff learning accuracy by adding statistically calibrated noises, particularly to local gradients of edge learners, during model training. However, this privacy guarantee unfortunately degrades model accuracy due to edge learners' local noises, and the global noise aggregated at the central server. Existing DP frameworks for edge focus on local noise calibration via gradient clipping techniques, overlooking the heterogeneity and dynamic changes of local gradients, and their aggregated impact on accuracy. In this article, we present a systematical analysis that unveils the influential factors capable of mitigating local and aggregated noises, and design PrivateDL to leverage these factors in noise calibration so as to improve model accuracy while fulfilling privacy guarantee. PrivateDL features on: (i) sampling-based sensitivity estimation for local noise calibration and (ii) combining large batch sizes and critical data identification in global training. We implement PrivateDL on the popular Laplace/Gaussian DP mechanisms and demonstrate its effectiveness using Intel BigDL workloads, i.e., considerably improving model accuracy by up to 5X when comparing against existing DP frameworks.","1558-2183","","10.1109/TPDS.2021.3064345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372811","Deep learning;differential privacy;federated learning;model accuracy","Training;Privacy;Data models;Sensitivity;Differential privacy;Biological system modeling;Servers","data privacy;deep learning (artificial intelligence);gradient methods;statistical analysis","PrivateDL;DP frameworks;accurate differentially private deep learning;model training;statistically calibrated noises;differential privacy;federated edge participants;global training;privacy guarantee;aggregated noises;aggregated impact;local gradients;gradient clipping techniques;local noise calibration;edge focus;global noise;edge learners;model accuracy","",3.0,"",65.0,"IEEE","8 Mar 2021","","","IEEE","IEEE Journals"
"Structured Allocation-Based Consistent Hashing With Improved Balancing for Cloud Infrastructure","Y. Nakatani","NTT Network Service Systems Laboratories, Tokyo, Japan","IEEE Transactions on Parallel and Distributed Systems","29 Mar 2021",2021,32.0,9.0,2248,2261,"Consistent hashing has played an indispensable role in cloud infrastructure, although its load balancing performance is not necessarily perfect. Consistent hashing has long remained the most widely used method despite many methods being proposed to improve load balancing because these methods trade off load balancing against consistency, memory usage, lookup performance, and/or fault-tolerance. This article presents Structured Allocation-based Consistent Hashing (SACH), a cloud-optimized consistent hashing algorithm that overcomes the trade-offs by taking advantage of the characteristics of cloud environments: scaling management and auto-healing. Since scaling can be distinguished from failures, SACH applies two different algorithms to update hashing functions: a fast-update algorithm for unmanaged backend failures to satisfy fault-tolerance with quick response and a slow-update algorithm for managed scaling. Hashing functions are initialized or slow-updated considering the characteristics of the fast-update algorithm to satisfy load balancing and the other properties as far as the number of failed backends is kept small by auto-healing. The experimental results show that SACH outperforms existing algorithms in each aspect. SACH will improve the load balancing of cloud infrastructure components, where the trade-offs have prevented the renewal of hashing functions.","1558-2183","","10.1109/TPDS.2021.3058963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354010","Consistent hashing;load balancing;network load balancer;distributed key-value store;cloud infrastructure","Load management;Resource management;Fault tolerant systems;Fault tolerance;Cloud computing;Memory management;Hash functions","cloud computing;file organisation;resource allocation","structured allocation-based consistent hashing;fault-tolerance algorithm;load balancing performance;cloud infrastructure components;slow-update algorithm;fast-update algorithm;hashing functions;cloud environments;cloud-optimized consistent hashing algorithm;SACH","",2.0,"",27.0,"CCBY","12 Feb 2021","","","IEEE","IEEE Journals"
"An Efficient Parallel Secure Machine Learning Framework on GPUs","F. Zhang; Z. Chen; C. Zhang; A. C. Zhou; J. Zhai; X. Du","Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China; Guangdong Province Engineering Center of China-Made High Performance Data Computing System, Shenzhen University, Shenzhen, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","26 Mar 2021",2021,32.0,9.0,2262,2276,"Machine learning is widely used in our daily lives. Large amounts of data have been continuously produced and transmitted to the cloud for model training and data processing, which raises a problem: how to preserve the security of the data. Recently, a secure machine learning system named SecureML has been proposed to solve this issue using two-party computation. However, due to the excessive computation expenses of two-party computation, the secure machine learning is about 2× slower than the original machine learning methods. Previous work on secure machine learning mostly focused on novel protocols or improving accuracy, while the performance metric has been ignored. In this article, we propose a GPU-based framework ParSecureML to improve the performance of secure machine learning algorithms based on two-party computation. The main challenges of developing ParSecureML lie in the complex computation patterns, frequent intra-node data transmission between CPU and GPU, and complicated inter-node data dependence. To handle these challenges, we propose a series of novel solutions, including profiling-guided adaptive GPU utilization, fine-grained double pipeline for intra-node CPU-GPU cooperation, and compressed transmission for inter-node communication. Moreover, we integrate architecture specific optimizations, such as Tensor Cores, into ParSecureML. As far as we know, this is the first GPU-based secure machine learning framework. Compared to the state-of-the-art framework, ParSecureML achieves an average of 33.8× speedup. ParSecureML can also be applied to inferences, which achieves 31.7× speedup on average.","1558-2183","","10.1109/TPDS.2021.3059108","National R&D Program of China(grant numbers:2020AAA0105200); National Natural Science Foundation of China(grant numbers:U20A20226,61802412,61802260,61972403,61732014); Beijing Natural Science Foundation(grant numbers:4202031,L192027); Beijing Academy of Artificial Intelligence (BAAI); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program; Shenzhen Science and Technology Foundation(grant numbers:JCYJ20180305125737520); Tencent “Rhinoceros Birds” project of Scientific Research Foundation for Young Teachers of Shenzhen University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354058","Two-party computation;GPU acceleration;secure training;secure inference;machine learning","Machine learning;Servers;Graphics processing units;Acceleration;Machine learning algorithms;Optimization;Task analysis","graphics processing units;learning (artificial intelligence);parallel processing;pattern classification;power aware computing","machine learning;inter-node communication;parallel secure machine learning framework;GPU-based secure machine learning framework;intra-node CPU-GPU cooperation;complicated inter-node data dependence;frequent intra-node data transmission;complex computation patterns;GPU-based framework ParSecureML;excessive computation expenses;two-party computation;secure machine learning system;data processing","",18.0,"",73.0,"IEEE","12 Feb 2021","","","IEEE","IEEE Journals"
"PISTIS: An Event-Triggered Real-Time Byzantine-Resilient Protocol Suite","D. Kozhaya; J. Decouchant; V. Rahli; P. Esteves-Verissimo","ABB Research, Baden, Switzerland; TU Delft, Delft, CD, Netherlands; University of Birmingham, Birmingham, U.K.; King Abdullah University of Science and Technology - RC3, Thuwal, Saudi Arabia","IEEE Transactions on Parallel and Distributed Systems","26 Mar 2021",2021,32.0,9.0,2277,2290,"The accelerated digitalisation of society along with technological evolution have extended the geographical span of cyber-physical systems. Two main threats have made the reliable and real-time control of these systems challenging: (i) uncertainty in the communication infrastructure induced by scale, and heterogeneity of the environment and devices; and (ii) targeted attacks maliciously worsening the impact of the above-mentioned communication uncertainties, disrupting the correctness of real-time applications. This article addresses those challenges by showing how to build distributed protocols that provide both real-time with practical performance, and scalability in the presence of network faults and attacks, in probabilistic synchronous environments. We provide a suite of real-time Byzantine protocols, which we prove correct, starting from a reliable broadcast protocol, called PISTIS, up to atomic broadcast and consensus. This suite simplifies the construction of powerful distributed and decentralized monitoring and control applications, including state-machine replication. Extensive empirical simulations showcase PISTIS's robustness, latency, and scalability. For example, PISTIS can withstand message loss (and delay) rates up to 50 percent in systems with 49 nodes and provides bounded delivery latencies in the order of a few milliseconds.","1558-2183","","10.1109/TPDS.2021.3056718","National Cyber Security Centre (NCSC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9347806","Real-time distributed systems;probabilistic losses;consensus;atomic broadcast;Byzantine resilience;intrusion tolerance","Protocols;Uncertainty;Scalability;Process control;Bandwidth;Real-time systems;Monitoring","broadcast communication;fault tolerant computing;protocols","called PISTIS;reliable broadcast protocol;real-time Byzantine protocols;probabilistic synchronous environments;network faults;distributed protocols;real-time applications;above-mentioned communication uncertainties;targeted attacks;communication infrastructure;real-time control;reliable time control;main threats;cyber-physical systems;geographical span;technological evolution;real-time Byzantine-resilient protocol suite;PISTIS's robustness;control applications;decentralized monitoring;atomic broadcast","",4.0,"",37.0,"IEEE","4 Feb 2021","","","IEEE","IEEE Journals"
"BALS: Blocked Alternating Least Squares for Parallel Sparse Matrix Factorization on GPUs","J. Chen; J. Fang; W. Liu; C. Yang","College of Computer, National University of Defense Technology, Changsha, Hunan, China; College of Computer, National University of Defense Technology, Changsha, Hunan, China; Department of Computer Science and Technology, Super Scientific Software Laboratory, China University of Petroleum, Beijing, China; College of Computer, National University of Defense Technology, Changsha, Hunan, China","IEEE Transactions on Parallel and Distributed Systems","31 Mar 2021",2021,32.0,9.0,2291,2302,"Matrix factorization on sparse matrices has been proven to be an effective approach for data mining and machine learning. However, the prior parallel implementations for matrix factorization fail to capture the internal social property embedded in real-world use cases. This article presents an efficient implementation of the alternative least squares (ALS) algorithm called BALSbuilt on top of a new sparse matrix format for parallel matrix factorization. The BALS storage format organizes the sparse matrix into 2D tiles to avoid repeated data loads and improve data reuses. We further propose a data reordering technique to sort sparse matrices according to nonzeros. The experimental results show that BALS can yield a superior performance than state-of-the-art implementations, i.e., our BALS generally runs faster than Gates’ implementation over different latent feature sizes, with a speedup of up to 2.08× on K20C, 3.72× on TITAN X and 3.13× on TITAN RTX. When compared with alternative matrix factorization algorithms, our BALS consistently outperforms CDMF, cuMF_CCD, and cuMF_SGD over various latent feature sizes and datasets. The reordering technique can provide an extra improvement of up to 23.68 percent on K20C, 19.87 percent on TITAN X and 20.38 percent on TITAN RTX.","1558-2183","","10.1109/TPDS.2021.3064942","National Key R&D Program of China(grant numbers:2018YFB0204301); National Natural Science Foundation of China(grant numbers:61972408,61972415); Science Challenge Project(grant numbers:TZZT2016002); Science Foundation of China University of Petroleum, Beijing(grant numbers:2462019YJRC004,2462020XKJS03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373912","Matrix factorization;alternating least squares;data reuse;data reordering;performance evaluation;GPGPUs","Sparse matrices;Motion pictures;Two dimensional displays;Graphics processing units;Artificial neural networks;Matrix decomposition;Logic gates","data mining;graphics processing units;learning (artificial intelligence);least squares approximations;matrix decomposition;sparse matrices","GPUs;alternative matrix factorization algorithms;Gates' implementation;data reordering technique;data reuses;repeated data loads;BALS storage;alternative least squares algorithm;internal social property;machine learning;data mining;parallel sparse matrix factorization;blocked alternating least squares","",2.0,"",35.0,"IEEE","9 Mar 2021","","","IEEE","IEEE Journals"
"Fine-Grained Multi-Query Stream Processing on Integrated Architectures","F. Zhang; C. Zhang; L. Yang; S. Zhang; B. He; W. Lu; X. Du","Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Information Systems Technology, and Design Pillar, Singapore University of Technology, and Design, Singapore; School of Computing, National University of Singapore, Singapore; Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","6 Apr 2021",2021,32.0,9.0,2303,2320,"Exploring the sharing opportunities among multiple stream queries is crucial for high-performance stream processing. Modern stream processing necessitates accelerating multiple queries by utilizing heterogeneous coprocessors, such as GPUs, and this has shown to be an effective method. Emerging CPU-GPU integrated architectures 6integrate CPU and GPU on the same chip and eliminate PCI-e bandwidth bottleneck. Such a novel architecture provides new opportunities for improving multi-query performance in stream processing but has not been fully explored by existing systems. We introduce a stream processing engine, called FineStream, for efficient multi-query window-based stream processing on CPU-GPU integrated architectures. FineStream's key contribution is a novel fine-grained workload scheduling mechanism between CPU and GPU to take advantage of both architectures. Particularly, FineStream is able to efficiently handle multiple queries in both static and dynamic streams. Our experimental results show that 1) on integrated architectures, FineStream achieves an average 52 percent throughput improvement and 36 percent lower latency over the state-of-the-art stream processing engine; 2) compared to the coarse-grained strategy of applying different devices for multiple queries, FineStream achieves 32 percent throughput improvement; 3) compared to the stream processing engine on the discrete architecture, FineStream on the integrated architecture achieves 10.4× price-throughput ratio, 1.8× energy efficiency, and can enjoy lower latency benefits.","1558-2183","","10.1109/TPDS.2021.3066407","National Natural Science Foundation of China(grant numbers:61802412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380479","Fine-grained;multi-query;stream processing;CPU;GPU;integrated architectures","Computer architecture;Graphics processing units;Structured Query Language;Performance evaluation;Throughput;Engines;Bandwidth","computer architecture;coprocessors;graphics processing units;multiprocessing systems;power aware computing;processor scheduling;query processing","high-performance stream processing;multiple stream queries;sharing opportunities;fine-grained multiquery stream processing;discrete architecture;state-of-the-art stream processing engine;integrated architecture;dynamic streams;static streams;novel fine-grained workload scheduling mechanism;FineStream key contribution;efficient multiquery window-based stream processing;multiquery performance;CPU-GPU integrated architectures;multiple queries;modern stream processing necessitates;efficiency 36.0 percent;efficiency 32.0 percent;efficiency 52.0 percent","",5.0,"",68.0,"IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"YuenyeungSpTRSV: A Thread-Level and Warp-Level Fusion Synchronization-Free Sparse Triangular Solve","F. Zhang; J. Su; W. Liu; B. He; R. Wu; X. Du; R. Wang","Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Computer Science Department, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science and Technology, China University of Petroleum, Beijing, China; School of Computing, National University of Singapore, Singapore; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Computer Science Department, Illinois Institute of Technology, Chicago, IL, USA","IEEE Transactions on Parallel and Distributed Systems","6 Apr 2021",2021,32.0,9.0,2321,2337,"Sparse triangular solves (SpTRSVs) are widely used in linear algebra domains, and several GPU-based SpTRSV algorithms have been developed. Synchronization-free SpTRSVs, due to their short preprocessing time and high performance, are currently the most popular SpTRSV algorithms. However, we observe that the performance of those SpTRSV algorithms on different matrices can vary greatly by 845 times. Our further studies show that when the average number of components per level is high and the average number of nonzero elements per row is low, those SpTRSVs exhibit extremely low performance. The reason is that, they use a warp on the GPU to process a row in sparse matrices, and such warp-level designs have severe underutilization of the GPU. To solve this problem, we propose YuenyeungSpTRSV, a thread-level and wrap-level fusion synchronization-free SpTRSV algorithm, which handles the rows with a large number of nonzero elements at warp-level while the rows with a low number of nonzero elements at thread-level. Particularly, YuenyeungSpTRSV has three novel features. First, unlike the previous studies, YuenyeungSpTRSV does not need long preprocessing time to calculate levels. Second, YuenyeungSpTRSV exhibits high performance on matrices that previous SpTRSVs cannot handle efficiently. Third, YuenyeungSpTRSV's optimization does not rely on the specific sparse matrix storage format. Instead, it can achieve very good performance on the most popular sparse matrix storage, compressed sparse row (CSR) format, and thus users do not need to conduct format conversion. We evaluate YuenyeungSpTRSV with 245 matrices from the Florida Sparse Matrix Collection on four GPU platforms, and experiments show that our YuenyeungSpTRSV exhibits 7.14 GFLOPS/s, which is 5.98x speedup over the state-of-the-art synchronization-free SpTRSV algorithm, and 4.83x speedup over the SpTRSV in cuSPARSE.","1558-2183","","10.1109/TPDS.2021.3066635","National Natural Science Foundation of China(grant numbers:61802412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380961","Thread-level;warp-level;synchronization-free;SpTRSV;GPU","Sparse matrices;Graphics processing units;Synchronization;Optimization;Linear algebra;Instruction sets;Arrays","graphics processing units;matrix multiplication;sparse matrices","YuenyeungSpTRSV;wrap-level fusion synchronization-free SpTRSV algorithm;warp-level designs;sparse matrices;nonzero elements;linear algebra domains;warp-level fusion synchronization-free Sparse triangular solve;thread-level;GPU platforms;Florida Sparse Matrix Collection;compressed sparse row format;specific sparse matrix storage format;YuenyeungSpTRSV's optimization","",1.0,"",41.0,"IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"OWebSync: Seamless Synchronization of Distributed Web Clients","K. Jannes; B. Lagaisse; W. Joosen","imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium","IEEE Transactions on Parallel and Distributed Systems","8 Apr 2021",2021,32.0,9.0,2338,2351,"Many enterprise software services are adopting a fully web-based architecture for both internal line-of-business applications and for online customer-facing applications. Although wireless connections are becoming more ubiquitous and faster, mobile employees and customers are often offline due to expected or unexpected network disruptions. Nevertheless, continuous operation of the software is expected. This article presents OWebSync: a web-based middleware for data synchronization in interactive groupware with fast resynchronization of offline clients and continuous, interactive synchronization of online clients. To automatically resolve conflicts, OWebSync implements a fine-grained data synchronization model and leverages state-based Conflict-free Replicated Data Types. This middleware uses Merkle-trees embedded in the tree-structured data and virtual Merkle-tree levels to achieve the required interactive performance. Our comparative evaluation with available operation-based and delta-state-based middleware solutions shows that OWebSync is especially better in operating in and recovering from offline settings and network disruptions. In addition, OWebSync scales more efficiently over time, as it does not store version vectors or other meta-data for all past clients.","1558-2183","","10.1109/TPDS.2021.3066276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380486","CRDTs;groupware;web browsers;eventual consistency","Synchronization;Servers;Metadata;Collaborative software;Middleware;Companies;Clocks","business data processing;data handling;groupware;interactive systems;Internet;middleware;mobile computing;synchronisation;tree data structures","OWebSync;state-based conflict-free replicated data types;continuous interactive synchronization;resynchronization;interactive groupware;web-based middleware;network disruptions;mobile employees;wireless connections;online customer-facing applications;line-of-business applications;fully web-based architecture;enterprise software services;distributed web clients;meta-data;virtual Merkle-tree levels;tree-structured data;fine-grained data synchronization model","",5.0,"",60.0,"IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"Accelerating the Bron-Kerbosch Algorithm for Maximal Clique Enumeration Using GPUs","Y. -W. Wei; W. -M. Chen; H. -H. Tsai","Department of Electronic Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electronic Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Electronic Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","IEEE Transactions on Parallel and Distributed Systems","6 Apr 2021",2021,32.0,9.0,2352,2366,"Maximal clique enumeration (MCE) is a classic problem in graph theory to identify all complete subgraphs in a graph. In prior MCE work, the Bron-Kerbosch algorithm is one of the most popular solutions, and there are several improved algorithms proposed on CPU platforms. However, while few studies have focused on the related issue of parallel implementation, recently, there have been numerous explorations of the acceleration of general purpose applications using a graphics processing unit (GPU) to reduce the computing power consumption. In this article, we develop a GPU-based Bron-Kerbosch algorithm that efficiently solves the MCE problem in parallel by optimizing the process of subproblem decomposition and computing resource usage. To speed up the computations, we use coalesced memory accesses and warp reductions to increase bandwidth and reduce memory latency. Our experimental results show that the proposed algorithm can fully exploit the resources of GPU architectures, allowing for the vast acceleration of operations to solve the MCE problem.","1558-2183","","10.1109/TPDS.2021.3067053","Ministry of Science Technology(grant numbers:MOST 107-2221-E-011-015-MY2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381690","Maximal clique;parallel computing;GPU","Graphics processing units;Computer architecture;Time complexity;Task analysis;Central Processing Unit;Acceleration;Upper bound","graph theory;graphics processing units;optimisation","GPU architectures;memory latency reduction;warp reductions;coalesced memory accesses;subproblem decomposition process optimization;CPU platforms;GPU-based Bron-Kerbosch algorithm;power consumption;graphics processing unit;general purpose applications;complete subgraphs;graph theory;maximal clique enumeration;MCE problem","",4.0,"",28.0,"IEEE","18 Mar 2021","","","IEEE","IEEE Journals"
"Optimizing the LINPACK Algorithm for Large-Scale PCIe-Based CPU-GPU Heterogeneous Systems","G. Tan; C. Shui; Y. Wang; X. Yu; Y. Yan","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","6 Apr 2021",2021,32.0,9.0,2367,2380,"There is a widening gap between GPU and other components (CPU, PCIe bus and communication network) in heterogeneous parallel system. The gap forces us to orchestrate cooperative execution among these components much more carefully than ever before. By taking the LINPACK benchmark as a case study, this article proposes a fine-grained pipelining algorithm on large-scale CPU-GPU heterogeneous cluster systems. First, we build an algorithmic model that reveals a new approach to GPU-centric and fine-grained pipelining algorithm design. Then, we present four model-driven pipelining algorithms that incrementally squeeze bubbles in the pipeline so that it is occupied by more useful floating-point calculations. The algorithms are implemented on both the AMD and NVIDIA GPU platforms. The finally optimized LINPACK program achieves 107 PFlops on 25, 600 GPUs (70 percent floating-point efficiency). Several insights have been drawn to suggest tradeoff of algorithm design, programming support, and architecture design.","1558-2183","","10.1109/TPDS.2021.3067731","The National Key Research and Development Program of China(grant numbers:2018YFB0204400,XDC01030000,XDC05010100); National Natural Science Foundation of China(grant numbers:62032023,61972377,61702483); CAS(grant numbers:QYZDJ-SSW-JSC035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382911","LINPACK algorithm;software pipeline;performance model;heterogeneous computing;cluster","Pipeline processing;Graphics processing units;Computer architecture;Supercomputers;Clustering algorithms;Programming;Optimization","floating point arithmetic;graphics processing units;parallel processing;pipeline processing","architecture design;programming support;NVIDIA GPU platforms;AMD;floating-point calculations;model-driven pipelining algorithms;fine-grained pipelining algorithm design;large-scale CPU-GPU heterogeneous cluster systems;LINPACK benchmark;heterogeneous parallel system;communication network;large-scale PCIe-based CPU-GPU heterogeneous systems;LINPACK algorithm;efficiency 70.0 percent","",4.0,"",50.0,"IEEE","22 Mar 2021","","","IEEE","IEEE Journals"
"Middleware to Manage Fault Tolerance Using Semi-Coordinated Checkpoints","A. Wong; E. Heymann; D. Rexachs; E. Luque","Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain; Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain; Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain; Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain","IEEE Transactions on Parallel and Distributed Systems","21 Aug 2020",2021,32.0,2.0,254,268,"Compute node failures are becoming a normal event for many long-running and scalable MPI applications. Keeping within the MPI standards and applying some of the methods developed so far in terms of fault tolerance, we developed a methodology that allows applications to tolerate failures through the creation of semi-coordinated checkpoints within the RADIC architecture. To do this, we developed the ULSC2-RADIC middleware that divides the application into independent MPI worlds where each MPI world would correspond to a compute node and make use of the DMTCP checkpoint library in a semi-coordinated environment. We performed experimental results using scientific applications and the NAS Parallel Benchmarks to assess the overhead and also the functionality in case of a node failure. We evaluated the computational cost of the semi-coordinated checkpoints compared with the coordinated checkpoints.","1558-2183","","10.1109/TPDS.2020.3015615","Agencia Estatal de Investigación(grant numbers:TIN2017-84875-P); European Regional Development Fund; Fundación Escuelas Universitarias Gimbernat; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165191","Fault tolerance;checkpoint-restart libraries;MPI;checkpoint scalability","Libraries;Middleware;Computer architecture;Standards;Computational efficiency;Fault tolerance;Fault tolerant systems","application program interfaces;checkpointing;fault tolerant computing;message passing;middleware;parallel processing","semicoordinated checkpoints;compute node failures;scalable MPI applications;MPI standards;ULSC2-RADIC middleware;independent MPI worlds;DMTCP checkpoint library;scientific applications;coordinated checkpoints;fault tolerance management;RADIC architecture;NAS parallel benchmarks","","","",41.0,"IEEE","11 Aug 2020","","","IEEE","IEEE Journals"
"A Two-Phase Dynamic Throughput Optimization Model for Big Data Transfers","M. S. Q. Z. Nine; T. Kosar","Department of Computer Science and Engineering, University at Buffalo, Buffalo, USA; Department of Computer Science and Engineering, University at Buffalo, Buffalo, USA","IEEE Transactions on Parallel and Distributed Systems","24 Aug 2020",2021,32.0,2.0,269,280,"The amount of data transferred over dedicated and non-dedicated network links has been increasing much faster than the increase in the network capacity. On the other hand, the current data transfer solutions fail to guarantee even the promised achievable transfer throughput. In this article, we propose a novel two-phase dynamic throughput optimization model based on mathematical modeling with offline knowledge discovery/analysis and adaptive online decision making. In the offline analysis, we mine historical transfer logs to perform knowledge discovery about the transfer characteristics. The online phase uses the discovered knowledge from the offline analysis along with the real-time investigation of the network condition to optimize the protocol parameters. As the real-time investigation is expensive and provides partial knowledge about the current network status, our model uses historical knowledge about the network and data characteristics to reduce the real-time investigation overhead while ensuring near-optimal throughput for each transfer. Our novel approach is tested over different networks with different datasets, and it has outperformed its closest competitor by 1.7x and the default case by 5x. It also achieved up to 93 percent accuracy compared to the optimal achievable throughput possible on those networks.","1558-2183","","10.1109/TPDS.2020.3012929","National Science Foundation(grant numbers:OAC-1724898); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153004","Throughput optimization;big data transfers;offline analysis;dynamic learning;protocol tuning","Throughput;Protocols;Data transfer;Data models;Bandwidth;Optimization;Real-time systems","data handling;data mining;decision making;Internet;optimisation;protocols","promised achievable transfer throughput;current data transfer solutions;network capacity;nondedicated network links;big data transfers;two-phase dynamic;optimal achievable throughput;near-optimal throughput;real-time investigation overhead;historical knowledge;current network status;partial knowledge;network condition;discovered knowledge;online phase;transfer characteristics;historical transfer;offline analysis;adaptive online decision making;mathematical modeling;optimization model","",3.0,"",38.0,"IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"Online Collaborative Data Caching in Edge Computing","X. Xia; F. Chen; Q. He; J. Grundy; M. Abdelrazek; H. Jin","School of Information Technology, Deakin University, Geelong, Australia; School of Information Technology, Deakin University, Geelong, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia; School of Information Technology, Deakin University, Geelong, Australia; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technolgoy, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","21 Aug 2020",2021,32.0,2.0,281,294,"In the edge computing (EC) environment, edge servers are deployed at base stations to offer highly accessible computing and storage resources to nearby app users. From the app vendor's perspective, caching data on edge servers can ensure low latency in app users' retrieval of app data. However, an edge server normally owns limited resources due to its limited size. In this article, we investigate the collaborative caching problem in the EC environment with the aim to minimize the system cost including data caching cost, data migration cost, and quality-of-service (QoS) penalty. We model this collaborative edge data caching problem (CEDC) as a constrained optimization problem and prove that it is NP-complete. We propose an online algorithm, called CEDC-O, to solve this CEDC problem during all time slots. CEDC-O is developed based on Lyapunov optimization, works online without requiring future information, and achieves provable close-to-optimal performance. CEDC-O is evaluated on a real-world data set, and the results demonstrate that it significantly outperforms four representative approaches.","1558-2183","","10.1109/TPDS.2020.3016344","Australian Research Council(grant numbers:DP180100212,DP200102491); Laureate Fellowship(grant numbers:FL190100035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9166756","Edge computing;data caching;online algorithm","Servers;Mobile handsets;Cloud computing;Distributed databases;Collaboration;Edge computing;Data models","computational complexity;cost reduction;distributed processing;minimisation;quality of service","NP-complete problem;QoS;quality-of-service;Lyapunov optimization;CEDC-O problem;data caching costing;data migration costing;online collaborative edge data caching problem;collaborative caching problem;edge server;edge computing environment;CEDC problem;constrained optimization problem","",82.0,"",47.0,"IEEE","13 Aug 2020","","","IEEE","IEEE Journals"
"Recent Advances of Resource Allocation in Network Function Virtualization","S. Yang; F. Li; S. Trajanovski; R. Yahyapour; X. Fu","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Microsoft, London, United Kingdom; Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG) and Institute of Computer Science, University of Göttingen, Göttingen, Germany; Institute of Computer Science, University of Göttingen, Göttingen, Germany","IEEE Transactions on Parallel and Distributed Systems","25 Aug 2020",2021,32.0,2.0,295,314,"Network Function Virtualization (NFV) has been emerging as an appealing solution that transforms complex network functions from dedicated hardware implementations to software instances running in a virtualized environment. Due to the numerous advantages such as flexibility, efficiency, scalability, short deployment cycles, and service upgrade, NFV has been widely recognized as the next-generation network service provisioning paradigm. In NFV, the requested service is implemented by a sequence of Virtual Network Functions (VNF) that can run on generic servers by leveraging the virtualization technology. These VNFs are pitched with a predefined order through which data flows traverse, and it is also known as the Service Function Chaining (SFC). In this article, we provide an overview of recent advances of resource allocation in NFV. We generalize and analyze four representative resource allocation problems, namely, (1) the VNF Placement and Traffic Routing problem, (2) VNF Placement problem, (3) Traffic Routing problem in NFV, and (4) the VNF Redeployment and Consolidation problem. After that, we study the delay calculation models and VNF protection (availability) models in NFV resource allocation, which are two important Quality of Service (QoS) parameters. Subsequently, we classify and summarize the representative work for solving the generalized problems by considering various QoS parameters (e.g., cost, delay, reliability, and energy) and different scenarios (e.g., edge cloud, online provisioning, and distributed provisioning). Finally, we conclude our article with a short discussion on the state-of-the-art and emerging topics in the related fields, and highlight areas where we expect high potential for future research.","1558-2183","","10.1109/TPDS.2020.3017001","National Natural Science Foundation of China(grant numbers:61802018); Beijing Institute of Technology Research Fund Program for Young Scholars; National Natural Science Foundation of China(grant numbers:61772077); Natural Science Foundation of Beijing Municipality(grant numbers:4192051); EU H2020 RISE COSAFE project(grant numbers:824019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9169857","Network function virtualization;service function chaining;resource allocation;QoS;placement;routing","Resource management;Quality of service;Routing;Delays;Virtualization;Network function virtualization;Hardware","complex networks;quality of service;resource allocation;software defined networking;virtualisation","traffic routing problem;next-generation network service provisioning;virtual network functions;complex network functions;network function virtualization;quality of service parameters;NFV resource allocation;VNF protection models;VNF placement problem;resource allocation;service function chaining;requested service","",55.0,"",142.0,"IEEE","17 Aug 2020","","","IEEE","IEEE Journals"
"Realizing Best Checkpointing Control in Computing Systems","P. Sigdel; X. Yuan; N. -F. Tzeng","School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA","IEEE Transactions on Parallel and Distributed Systems","31 Aug 2020",2021,32.0,2.0,315,329,"This article considers best checkpointing control realizable in real-world systems, whose mean time between failures (MTBFs) often fluctuate. The considered control scheme is based on equating aggregate checkpointing overhead over an activity sequence of interest (θ) and the expected rework amount after a failure recovery for best checkpointing, called “CHORE” (i.e., checkpointing overhead and rework equated), where θ starts from execution resumption after failure recovery and ends after restore from the following failure. CHORE lets its inter-checkpoint intervals in θ follow a pre-determined sequence independent of MTBF to aim at performance optimality and is shown analytically to keep overall execution time overhead upper bounded. When failure occurrences are tracked during job execution for real-time MTBF estimation, an enhanced CHORE (dubbed En-CHORE) is obtained to lower checkpointing overhead by skipping certain checkpoints at the beginning of each θ before taking checkpoints with the most desirable inter-checkpoint intervals determined on-the-fly for best checkpointing control. En-CHORE can outperform optimal checkpointing (which follows a fixed inter-checkpoint interval optimized for one constant global MTBF known a prior) both under synthetic random failures with local MTBF fluctuating markedly and under real failure traces of 22 real HPC systems (whose failure rates actually fluctuate over their trace time spans).","1558-2183","","10.1109/TPDS.2020.3015805","National Science Foundation(grant numbers:CNS-1527051,III-1652107); Louisiana Board of Regents(grant numbers:LEQSF(2018-21)-RD-A-24); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164989","Absorbing Markov chains;checkpointing control;execution time overhead;mean time between failures (MTBFs);optimal checkpointing;rework after failure recovery","Checkpointing;Optimized production technology;Aggregates;Fluctuations;Estimation;Time measurement;Control systems","checkpointing;failure analysis;fault tolerant computing;optimisation;software fault tolerance;system recovery","synthetic random failures;fixed inter-checkpoint interval;optimal checkpointing;desirable inter-checkpoint intervals;real-time MTBF estimation;failure occurrences;execution time;CHORE;failure recovery;aggregate checkpointing overhead;failure rates","",3.0,"",41.0,"IEEE","11 Aug 2020","","","IEEE","IEEE Journals"
"An Automatic Synthesizer of Advising Tools for High Performance Computing","H. Guan; X. Shen; H. Krim","University of Massachusetts Amherst, USA; North Carolina State University, Raleigh, USA; North Carolina State University, Raleigh, USA","IEEE Transactions on Parallel and Distributed Systems","7 Sep 2020",2021,32.0,2.0,330,341,"This article presents Egeria, the first automatic synthesizer of advising tools for High-Performance Computing (HPC). When one provides it with some HPC programming guides as inputs, Egeria automatically constructs a text retrieval tool that can advise on what to do to improve the performance of a given program. The advising tool provides a concise list of essential rules automatically extracted from the documents and can retrieve relevant optimization knowledge for optimization questions. Egeria is built based on a distinctive multi-layered design that leverages natural language processing (NLP) techniques and extends them with HPC-specific knowledge and considerations. This article presents the design, implementation, and both quantitative and qualitative evaluation results of Egeria.","1558-2183","","10.1109/TPDS.2020.3018636","DOE Early Career Award(grant numbers:DE-SC0013700); National Science Foundation(grant numbers:1455404,1455733 (CAREER),1525609); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173796","Performance tools;natural language processing;code optimization","Tools;Optimization;Programming;Syntactics;Semantics;Guidelines;Natural language processing","information retrieval;natural language processing;parallel processing;text analysis","qualitative evaluation;quantitative evaluation;HPC-specific knowledge;NLP;natural language processing;HPC programming guides;high-performance computing;automatic synthesizer;advising tool;text retrieval tool;Egeria","",3.0,"",47.0,"IEEE","21 Aug 2020","","","IEEE","IEEE Journals"
"CPDE: A Methodology for the Transparent Distribution of Centralized Smart Grid Programs","T. T. Q. Nguyen; C. Bobineau; V. Debusschere; Q. H. Giap; N. Hadjsaid","G2Elab, University Grenoble Alpes, CNRS, Grenoble INP (Institute of Engineering University Grenoble Alpes), Grenoble, France; University Grenoble Alpes, CNRS, Grenoble INP (Institute of Engineering University Grenoble Alpes), LIG, Grenoble, France; G2Elab, University Grenoble Alpes, CNRS, Grenoble INP (Institute of Engineering University Grenoble Alpes), Grenoble, France; Faculty of Electrical Engineering, The University of Danang - University of Science and Technology, Danang, Vietnam; G2Elab, University Grenoble Alpes, CNRS, Grenoble INP (Institute of Engineering University Grenoble Alpes), Grenoble, France","IEEE Transactions on Parallel and Distributed Systems","4 Sep 2020",2021,32.0,2.0,342,354,"Control and management in smart grids are facing many challenges such as scalability, heterogeneity and technology innovation. This requires a transformation from the traditional centralised paradigm into a distributed one. In this article, a new distributed programming methodology, called Centralised Programming and Distributed Execution (CPDE), is proposed. CPDE relies on (i) the abstraction of the whole system as a distributed database; (ii) the use of the Smartlog declarative and reactive rule based language for expressing data manipulation; and (iii) the automatic Smartlog rule distribution according to data distribution. It thus provides a simple and straightforward mean for distributed programming. A centralised algorithm of fair over-voltage regulation of PV systems is used as a typical smart grids study case to validate the methodology and to compare it with centralized implementations. The experiments are implemented in a real-time simulation platform with a network of Raspberry Pis. In addition to showing its correctnes and ease of use, the performance of the CPDE implementation is studied, as well as its sensitivity to the increasing number of computing units and the data distribution. Results are promising and show the clear benefits of this methodology compared to more classical implementations.","1558-2183","","10.1109/TPDS.2020.3019759","Foundation Grenoble INP; French Embassy in Vietnam; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9178461","Distributed programming;CPDE methodology;distributed database;declarative language;Smartlog;smart grid","Programming;Smart grids;Distributed databases;Computer architecture;Real-time systems;Actuators","centralised control;distributed programming;smart power grids","automatic smartlog rule distribution;centralised programming;CPDE implementation;PV systems;centralised algorithm;data distribution;data manipulation;reactive rule;distributed database;distributed execution;distributed programming methodology;technology innovation;centralized smart grid programs;transparent distribution","","","",28.0,"IEEE","26 Aug 2020","","","IEEE","IEEE Journals"
"A Resource and Performance Optimization Reduction Circuit on FPGAs","L. Tang; G. Cai; Y. Zheng; J. Chen","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","9 Sep 2020",2021,32.0,2.0,355,366,"Reduce is a fundamental computing pattern, which is widely involved in scientific and engineering applications. For example, accumulation, the most common example of reduce pattern, is the core of applications such as dot product, matrix multiplication, and finite impulse response (FIR) filter. However, there is a trade-off between performance and area in the hardware implementation of the reduce pattern. To solve this problem, we propose an optimized reduction method that can handle multiple arbitrary-length sets. The performance of the proposed method is evaluated for both a single data set and numerous data sets. Moreover, to quickly differentiate the data of different sets in the reduction circuit, individual modules are designed to manage the data. We implement the design on FPGAs and present the experimental results. The proposed design with high performance and low resource consumption can achieve at least 1.59 times improvement on area-time product compared with the reported methods.","1558-2183","","10.1109/TPDS.2020.3020117","National Natural Science Foundation of China(grant numbers:61901440); Beijing Municipal Natural Science Foundation(grant numbers:4202080); Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9180054","Pipelined processors;vector reduction;accumulator;field programmable gate arrays","Adders;Field programmable gate arrays;Merging;Clocks;Binary trees;Upper bound;Time-frequency analysis","circuit optimisation;field programmable gate arrays;FIR filters;logic design;matrix multiplication","multiple arbitrary-length sets;FIR filter;FPGA;reduction circuit;numerous data sets;multiple arbitrary-length;reduction method;finite impulse response filter;matrix multiplication;dot product;scientific engineering applications","",2.0,"",20.0,"IEEE","28 Aug 2020","","","IEEE","IEEE Journals"
"A Parallel Structured Divide-and-Conquer Algorithm for Symmetric Tridiagonal Eigenvalue Problems","X. Liao; S. Li; Y. Lu; J. E. Roman","College of Computer Science, National University of Defense Technology, Changsha, China; College of Computer Science, National University of Defense Technology, Changsha, China; National Supercomputer Center in Guangzhou, School of Data and Computer Science, Sun Yatsen University, Guangzhou, China; D. Sistemes Informàtics i Computació, Universitat Politècnica de València, València, Spain","IEEE Transactions on Parallel and Distributed Systems","9 Sep 2020",2021,32.0,2.0,367,378,"In this article, a parallel structured divide-and-conquer (PSDC) eigensolver is proposed for symmetric tridiagonal matrices based on ScaLAPACK and a parallel structured matrix multiplication algorithm, called PSMMA. Computing the eigenvectors via matrix-matrix multiplications is the most computationally expensive part of the divide-and-conquer algorithm, and one of the matrices involved in such multiplications is a rank-structured Cauchy-like matrix. By exploiting this particular property, PSMMA constructs the local matrices by using generators of Cauchy-like matrices without any communication, and further reduces the computation costs by using a structured low-rank approximation algorithm. Thus, both the communication and computation costs are reduced. Experimental results show that both PSMMA and PSDC are highly scalable and scale to 4096 processes at least. PSDC has better scalability than PHDC that was proposed in [16] and only scaled to 300 processes for the same matrices. Comparing with PDSTEDC in ScaLAPACK, PSDC is always faster and achieves 1.4x-1.6x speedup for some matrices with few deflations. PSDC is also comparable with ELPA, with PSDC being faster than ELPA when using few processes and a little slower when using many processes.","1558-2183","","10.1109/TPDS.2020.3019471","National Natural Science Foundation of China(grant numbers:NNW2019ZT6-B20,NNW2019ZT6-B21,NNW2019ZT5-A10,U1611261,61872392,U1811461); National Key RD Program of China(grant numbers:2018YFB0204303); NSF of Hunan(grant numbers:2019JJ40339); NSF of NUDT(grant numbers:ZK18-03-01); Natural Science Foundation of Guangdong Province(grant numbers:2018B030312002); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); Spanish Agencia Estatal de Investigación(grant numbers:PID2019-107379RB-I00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9177298","PSMMA;PUMMA algorithm;ScaLAPACK;divide-and-conquer;rank-structured matrix;cauchy-like matrix","Approximation algorithms;Symmetric matrices;Generators;Eigenvalues and eigenfunctions;Matrix decomposition;Complexity theory;Scalability","approximation theory;divide and conquer methods;eigenvalues and eigenfunctions;matrix multiplication","ELPA;rank-structured Cauchy-like matrix;parallel structured divide-and-conquer algorithm;PSMMA;symmetric tridiagonal eigenvalue problems;PSDC;low-rank approximation algorithm;computation costs;local matrices;computationally expensive part;matrix-matrix multiplications;parallel structured matrix multiplication algorithm;ScaLAPACK;symmetric tridiagonal matrices","",8.0,"",47.0,"IEEE","25 Aug 2020","","","IEEE","IEEE Journals"
"Multi-GPU Design and Performance Evaluation of Homomorphic Encryption on GPU Clusters","A. Al Badawi; B. Veeravalli; J. Lin; N. Xiao; M. Kazuaki; A. Khin Mi Mi","A*STAR, Institute for Infocomm Research (I2R), Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; A*STAR, Institute for Infocomm Research (I2R), Singapore; A*STAR, Institute for Infocomm Research (I2R), Singapore; Barcelona Supercomputing Center (BSC), Barcelona, Spain; A*STAR, Institute for Infocomm Research (I2R), Singapore","IEEE Transactions on Parallel and Distributed Systems","15 Sep 2020",2021,32.0,2.0,379,391,"We present a multi-GPU design, implementation and performance evaluation of the Halevi-Polyakov-Shoup (HPS) variant of the Fan-Vercauteren (FV) levelled Fully Homomorphic Encryption (FHE) scheme. Our design follows a data parallelism approach and uses partitioning methods to distribute the workload in FV primitives evenly across available GPUs. The design is put to address space and runtime requirements of FHE computations. It is also suitable for distributed-memory architectures, and includes efficient GPU-to-GPU data exchange protocols. Moreover, it is user-friendly as user intervention is not required for task decomposition, scheduling or load balancing. We implement and evaluate the performance of our design on two homogeneous and heterogeneous NVIDIA GPU clusters: K80, and a customized P100. We also provide a comparison with a recent shared-memory-based multi-core CPU implementation using two homomorphic circuits as workloads: vector addition and multiplication. Moreover, we use our multi-GPU Levelled-FHE to implement the inference circuit of two Convolutional Neural Networks (CNNs) to perform homomorphically image classification on encrypted images from the MNIST and CIFAR - 10 datasets. Our implementation provides 1 to 3 orders of magnitude speedup compared with the CPU implementation on vector operations. In terms of scalability, our design shows reasonable scalability curves when the GPUs are fully connected.","1558-2183","","10.1109/TPDS.2020.3021238","Agency for Science, Technology and Research(grant numbers:RIE2020); Advanced Manufacturing and Engineering(grant numbers:A19E3b0099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185077","Homomorphic encryption;parallel algorithms;multi-GPU clusters;performance evaluation","Graphics processing units;Task analysis;Encryption;Computational modeling;Parallel processing;Field programmable gate arrays","convolutional neural nets;cryptography;graphics processing units;memory architecture;multiprocessing systems;neural chips;parallel processing;shared memory systems","CNNs;CIFAR-10 datasets;MNIST datasets;scalability curves;inference circuit;convolutional neural networks;vector multiplication;vector addition;scheduling;task decomposition;partitioning methods;HPS variant;GPU clusters;shared-memory-based multicore CPU implementation;Fan-Vercauteren levelled fully homomorphic encryption scheme;Halevi-Polyakov-Shoup variant;performance evaluation;multiGPU design;encrypted images;homomorphically image classification;homomorphic circuits;heterogeneous NVIDIA GPU clusters;homogeneous NVIDIA GPU clusters;load balancing;user intervention;efficient GPU-to-GPU data exchange protocols;distributed-memory architectures;FV primitives;data parallelism approach","",10.0,"",47.0,"IEEE","2 Sep 2020","","","IEEE","IEEE Journals"
"Comment on “Circuit Ciphertext-Policy Attribute-Based Hybrid Encryption With Verifiable Delegation in Cloud Computing”","Z. Cao; O. Markowitch","State key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Computer Sciences Department, Université Libre de Bruxelles, Bruxelles, Belgium","IEEE Transactions on Parallel and Distributed Systems","17 Sep 2020",2021,32.0,2.0,392,393,"The scheme [1] is flawed because: (1) its circuit access structure is confusingly described; (2) the cloud server cannot complete the related computations; (3) some users can conspire to generate new decryption keys, without the help of the key generation authority.","1558-2183","","10.1109/TPDS.2020.3021683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186640","Ciphertext-policy attribute-based encryption;verifiable delegation;multilinear map;hybrid encryption","Authorization;Encryption;Cloud computing;Cryptography","authorisation;cloud computing;private key cryptography","key generation authority;decryption keys;cloud server;circuit access structure;cloud computing;verifiable delegation;circuit ciphertext-policy attribute-based hybrid encryption","",3.0,"",1.0,"IEEE","4 Sep 2020","","","IEEE","IEEE Journals"
"Towards Efficient Scheduling of Federated Mobile Devices Under Computational and Statistical Heterogeneity","C. Wang; Y. Yang; P. Zhou","Department of Computer Science, Old Dominion University, Norfolk, USA; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, USA; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, USA","IEEE Transactions on Parallel and Distributed Systems","25 Sep 2020",2021,32.0,2.0,394,410,"Originated from distributed learning, federated learning enables privacy-preserved collaboration on a new abstracted level by sharing the model parameters only. While the current research mainly focuses on optimizing learning algorithms and minimizing communication overhead left by distributed learning, there is still a considerable gap when it comes to the real implementation on mobile devices. In this article, we start with an empirical experiment to demonstrate computation heterogeneity is a more pronounced bottleneck than communication on the current generation of battery-powered mobile devices, and the existing methods are haunted by mobile stragglers. Further, non-identically distributed data across the mobile users makes the selection of participants critical to the accuracy and convergence. To tackle the computational and statistical heterogeneity, we utilize data as a tuning knob and propose two efficient polynomial-time algorithms to schedule different workloads on various mobile devices, when data is identically or non-identically distributed. For identically distributed data, we combine partitioning and linear bottleneck assignment to achieve near-optimal training time without accuracy loss. For non-identically distributed data, we convert it into an average cost minimization problem and propose a greedy algorithm to find a reasonable balance between computation time and accuracy. We also establish an offline profiler to quantify the runtime behavior of different devices, which serves as the input to the scheduling algorithms. We conduct extensive experiments on a mobile testbed with two datasets and up to 20 devices. Compared with the common benchmarks, the proposed algorithms achieve 2-100× speedup epoch-wise, 2–7 percent accuracy gain and boost the convergence rate by more than 100 percent on CIFAR10.","1558-2183","","10.1109/TPDS.2020.3023905","National Science Foundation(grant numbers:CCF-1850045,IIS-2007386); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9195793","Federated learning;on-device deep learning;scheduling optimization;non-IID data","Mobile handsets;Computational modeling;Training;Task analysis;Distributed databases;Convergence;Servers","computational complexity;data privacy;greedy algorithms;learning (artificial intelligence);minimisation;mobile computing;scheduling","mobile testbed;scheduling algorithms;greedy algorithm;average cost minimization;linear bottleneck assignment;polynomial-time algorithms;nonidentically distributed data;mobile stragglers;battery-powered mobile devices;learning algorithms;privacy-preserved collaboration;federated learning;distributed learning;statistical heterogeneity;federated mobile devices","",24.0,"",51.0,"IEEE","14 Sep 2020","","","IEEE","IEEE Journals"
"Multi-Agent Imitation Learning for Pervasive Edge Computing: A Decentralized Computation Offloading Algorithm","X. Wang; Z. Ning; S. Guo","Chongqing Key Laboratory of Mobile Communications Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Mobile Communications Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","24 Sep 2020",2021,32.0,2.0,411,425,"Pervasive edge computing refers to one kind of edge computing that merely relies on edge devices with sensing, storage and communication abilities to realize peer-to-peer offloading without centralized management. Due to lack of unified coordination, users always pursue profits by maximizing their own utilities. However, on one hand, users may not make appropriate scheduling decisions based on their local observations. On the other hand, how to guarantee the fairness among different edge devices in the fully decentralized environment is rather challenging. To solve the above issues, we propose a decentrailized computation offloading algorithm with the purpose of minimizing average task completion time in the pervasive edge computing networks. We first derive a Nash equilibrium among devices by stochastic game theories based on the full observations of system states. After that, we design a traffic offloading algorithm based on partial observations by integrating general adversarial imitation learning. Multiple experts can provide demonstrations, so that devices can mimic the behaviors of corresponding experts by minimizing the gaps between the distributions of their observation-action pairs. At last, theoretical and performance results show that our solution has a significant advantage compared with other representative algorithms.","1558-2183","","10.1109/TPDS.2020.3023936","Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E); National Natural Science Foundation of China(grant numbers:61872310,61971084,62001073); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197692","Pervasive edge computing;computation offloading;imitation learning;decentralized execution","Edge computing;Task analysis;Performance evaluation;Computational modeling;Games;Processor scheduling;Cloud computing","cloud computing;learning (artificial intelligence);mobile computing;multi-agent systems;peer-to-peer computing;scheduling;stochastic games;telecommunication traffic","multiagent imitation learning;general adversarial imitation learning;traffic offloading algorithm;pervasive edge computing networks;decentrailized computation offloading algorithm;fully decentralized environment;different edge devices;peer-to-peer offloading;communication abilities;decentralized computation offloading algorithm","",60.0,"",42.0,"IEEE","15 Sep 2020","","","IEEE","IEEE Journals"
"Towards Minimizing Resource Usage With QoS Guarantee in Cloud Gaming","Y. Li; C. Zhao; X. Tang; W. Cai; X. Liu; G. Wang; X. Gong","Department of Computer Science, Nankai University, Tianjin, China; Department of Computer Science, Nankai University, Tianjin, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Computer Science, Nankai University, Tianjin, China; Department of Computer Science, Nankai University, Tianjin, China; Department of Computer Science, Nankai University, Tianjin, China","IEEE Transactions on Parallel and Distributed Systems","24 Sep 2020",2021,32.0,2.0,426,440,"Cloud gaming has been very popular recently, but providing satisfactory gaming experiences to players at a modest cost is still challenging. Colocating several games onto one server could improve server utilization. However, prior work regarding colocating games either ignores the performance interference between games or uses simple performance model to charaterize it, which may make inefficient game colocation decisions and cause QoS violations. In this article, we address the resource allocation issues for colocating games in cloud gaming. We first propose a novel machine learning-based performance model, which is able to capture the complex relationship among the performance interference, the contention features of colocated games and resource partition. Guided by the performance model, we then propose efficient and effective algorithms for two resource allocation scenarios in cloud gaming. We evaluate the proposed solutions through extensive experiments using a large number of real popular games. The results show that our performance model is able to identify whether a colocated game satisfies QoS requirement within an average error of 5 percent, which significantly outperforms the alternatives. Our resource allocation algorithms are able to increase the resource utilization by up to 60 percent compared to the state-of-the-art solutions.","1558-2183","","10.1109/TPDS.2020.3024068","Science and Technology Development Plan of Tianjin(grant numbers:17JCYBJC15300,18ZXZNGX00140,18ZXZNGX00200); National Natural Science Foundation of China(grant numbers:61602266,61702521,61872201,U1833114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197611","Cloud gaming;game colocation;performance interference;performance prediction;machine learning","Servers;Resource management;Cloud gaming;Quality of service;Interference;Graphics processing units","cloud computing;computer games;learning (artificial intelligence);quality of service;resource allocation","resource partition;machine learning-based performance model;game colocation decisions;QoS guarantee;resource usage;cloud gaming;performance interference;colocating games","",4.0,"",47.0,"IEEE","15 Sep 2020","","","IEEE","IEEE Journals"
"Adaptive Preference-Aware Co-Location for Improving Resource Utilization of Power Constrained Datacenters","P. Pang; Q. Chen; D. Zeng; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science, China University of Geosciences, Wuhan, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","24 Sep 2020",2021,32.0,2.0,441,456,"Large-scale datacenters often host latency-sensitive services that have stringent Quality-of-Service requirement and experience diurnal load pattern. Co-locating best-effort applications that have no QoS requirement with the latency-sensitive services has been widely used to improve the resource utilization of datacenters with careful shared resource management. However, existing co-location techniques tend to result in the power overload problem on power constrained servers due to the ignorance of the power consumption. To this end, we propose Sturgeon, a runtime system proactively manages resources between co-located applications in a power constrained environment, to ensure the QoS of latency-sensitive services while maximizing the throughput of best-effort applications. Our investigation shows that, at a given load, there are multiple feasible resource configurations to meet both QoS requirement and power budget, while one of them yields the maximum throughput of best-effort applications. To find such a configuration, we establish models to accurately predict the performance and power consumption of the co-located applications. Sturgeon monitors the QoS of the services periodically, in order to eliminate the potential QoS violation caused by the unpredictable interference. Besides, when the datacenter hosts different types of applications to perform co-location, Sturgeon places applications with their preferable candidates to improve the overall throughput. The experimental results show that at server level Sturgeon improves the throughput of the best-effort application by 25.43 percent compared to the state-of-the-art technique, while guaranteeing the 95%-ile latency within the QoS target; at cluster level, Sturgeon improves the overall throughput of best-effort applications by 13.74 percent compared to the baseline.","1558-2183","","10.1109/TPDS.2020.3023997","National R&D Program of China(grant numbers:2018YFB1004800); National Natural Science Foundation of China(grant numbers:61632017,61772480,61872240,61832006,61702328); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197666","QoS;improved utilization;power constrained datacenters","Quality of service;Throughput;Resource management;Power demand;Servers;Load modeling;Runtime","computer centres;computer network management;interference;power aware computing;quality of service;resource allocation","unpredictable interference;colocated applications;Sturgeon;quality of service;adaptive preference-aware colocation;power constrained environment;power constrained servers;power overload problem;diurnal load pattern;large-scale datacenters;power constrained datacenters;resource utilization;power consumption;power budget;QoS;latency-sensitive services","",3.0,"",49.0,"IEEE","15 Sep 2020","","","IEEE","IEEE Journals"
"Minimizing Coflow Completion Time in Optical Circuit Switched Networks","T. Zhang; F. Ren; J. Bao; R. Shu; W. Cheng","Collaborative Innovation Center of Novel Software Technology and Industrialization, Nanjing, China; Department of Computer Science and Techonolgy, Tsinghua University, Beijing, China; North Information Control Research Academy Group Company, Ltd, Nanjing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","30 Sep 2020",2021,32.0,2.0,457,469,"Nowadays, optical circuit switching is becoming an increasingly favored technology in scaling data center networks for its definitive advantages in data rate, power consumption, and device cost. Concurrently, reducing coflow completion time (CCT) is of great significance for improving application-level performance. However, minimizing CCT in circuit switched networks is totally different from that in traditional packet switched networks due to port constraints and circuit reconfiguration delays. To address this issue, this article proposes Grouped Optimization-based Scheduling (GOS), a CCT minimization algorithm for circuit switched networks integrating circuit and coflow scheduling. We first formalize the CCT minimization problem into a 0-1 programming problem, then relax and solve the problem in 2 steps to obtain the coflow order and flow grouping decisions on each circuit. Thus intra-group reconfiguration delays are saved, and small coflows can be prioritized at the group level. Theoretical analysis proves GOS is a 4-approximation algorithm in average CCT. To reduce computing overheads, we further propose a heuristic approximation algorithm. Extensive simulations show that the heuristic algorithm has satisfactory CCT performance (0.12× Varys, 0.36× Sunflow) as well as high throughput (16.74× Varys, 1.32× Sunflow), and well adapts to a wide range of reconfiguration delays and algorithm decision time.","1558-2183","","10.1109/TPDS.2020.3025145","National Natural Science Foundation of China(grant numbers:62002165,61872208); Natural Science Foundation of Jiangsu Province(grant numbers:SBK2020041090); National Key Research and Development Program of China(grant numbers:2018YFB1700203); Nanjing University of Aeronautics and Astronautics(grant numbers:90YAH19095); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200719","Data center;optical circuit switch;coflow completion time;circuit scheduling;coflow scheduling","Optical switches;Switching circuits;Optical packet switching;Scheduling;Data centers;Delays;Optical fiber networks","approximation theory;computational complexity;optical burst switching;optimisation;telecommunication scheduling;telecommunication traffic","4-approximation algorithm;average CCT;heuristic approximation algorithm;group level;intra-group reconfiguration delays;flow grouping decisions;coflow order;0-1 programming problem;CCT minimization problem;coflow scheduling;CCT minimization algorithm;GOS;Grouped Optimization-based Scheduling;circuit reconfiguration delays;application-level performance;device cost;power consumption;data rate;definitive advantages;scaling data center networks;increasingly favored technology;optical circuit switching;optical circuit switched networks;coflow completion time;algorithm decision time;satisfactory CCT performance;heuristic algorithm","",3.0,"",39.0,"IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"Dynamic Load Balancing in Parallel Execution of Cellular Automata","A. Giordano; A. De Rango; R. Rongo; D. D'Ambrosio; W. Spataro","ICAR-CNR, Rende (CS), Italy; DIATIC UNICAL, Rende (CS), Italy; DeMACS UNICAL, Rende (CS), Italy; DeMACS UNICAL, Rende (CS), Italy; DeMACS UNICAL, Rende (CS), Italy","IEEE Transactions on Parallel and Distributed Systems","30 Sep 2020",2021,32.0,2.0,470,484,"The allocation of the computational load across different processing elements is an important issue in parallel computing. Indeed, an unbalanced load distribution can strongly affect the performances of a parallel system caused by an excess of synchronization idle times due to less loaded processes waiting for more loaded ones. In this article, we focus on the load balancing issues in the context of the parallel execution of spatial-related applications where the domain space is partitioned in regions that are assigned to different processing elements. In particular, without loss of generality, we consider the well-known spatial-related Cellular Automata computational paradigm for evaluating the proposed dynamic load balancing approach. The main contribution of this article is the derivation of simple closed-form expressions that allow to compute the optimal workload assignment in a dynamic fashion, with the goal of guaranteeing a fully balanced workload distribution during the parallel execution. Based on these expressions, an algorithm for balanced execution of cellular automata is presented and implemented using the MPI technology. Eventually, an experimental section practically shows the behaviour of the proposed dynamic load balancing approach and proves its performance improvement, compared to the not-balanced version, as witnessed by the appreciable reduction of execution times.","1558-2183","","10.1109/TPDS.2020.3025102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200755","Parallel computing;load balancing;cellular automata","Load management;Automata;Heuristic algorithms;Computational modeling;Parallel processing;Task analysis;Load modeling","cellular automata;message passing;resource allocation","MPI;cellular automata computational paradigm;computational load;fully balanced workload distribution;dynamic load balancing approach;spatial-related applications;parallel execution;synchronization;parallel system;unbalanced load distribution;parallel computing","",11.0,"",36.0,"IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"QShield: Protecting Outsourced Cloud Data Queries With Multi-User Access Control Based on SGX","Y. Chen; Q. Zheng; Z. Yan; D. Liu","School of Computer Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; School of Computer Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Department of Communications and Networking, Aalto University, Espoo, Finland; State Key Lab on Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, Shaanxi, China","IEEE Transactions on Parallel and Distributed Systems","29 Sep 2020",2021,32.0,2.0,485,499,"Due to the concern on cloud security, digital encryption is applied before outsourcing data to the cloud for utilization. This introduces a challenge about how to efficiently perform queries over ciphertexts. Crypto-based solutions currently suffer from limited operation support, high computational complexity, weak generality, and poor verifiability. An alternative method that utilizes hardware-assisted Trusted Execution Environment (TEE), i.e., Intel SGX, has emerged to offer high computational efficiency, generality and flexibility. However, SGX-based solutions lack support on multi-user query control and suffer from security compromises caused by untrustworthy TEE function invocation, e.g., key revocation failure, incorrect query results, and sensitive information leakage. In this article, we leverage SGX and propose a secure and efficient SQL-style query framework named QShield. Notably, we propose a novel lightweight secret sharing scheme in QShield to enable multi-user query control; it effectively circumvents key revocation and avoids cumbersome remote attestation for authentication. We further embed a trust-proof mechanism into QShield to guarantee the trustworthiness of TEE function invocation; it ensures the correctness of query results and alleviates side-channel attacks. Through formal security analysis, proof-of-concept implementation and performance evaluation, we show that QShield can securely query over outsourced data with high efficiency and scalable multi-user support.","1558-2183","","10.1109/TPDS.2020.3024880","National Key Research and Development Program of China(grant numbers:2018YFB1004500,2016YFB1000903); National Natural Science Foundation of China(grant numbers:61721002); Innovation Research Team of Ministry of Education(grant numbers:IRT_17R86); National Natural Science Foundation of China(grant numbers:61502379,61532015,61672410,61672420); China Knowledge Center for Engineering Science and Technology; Academy of Finland(grant numbers:308087,314203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200772","Secure query;outsourced data;secure hardware;Intel SGX;cloud computing;multi-user query control","Encryption;Indexes;Cloud computing;Data models;Authentication","authorisation;cloud computing;cryptography;outsourcing;query processing;SQL;trusted computing","ciphertexts;digital encryption;outsourced cloud data queries protection;formal security analysis;side-channel attacks;TEE function invocation;SQL-style query framework;multiuser query control;Intel SGX;hardware-assisted trusted execution environment;computational complexity;crypto-based solutions;outsourcing data;cloud security;multiuser access control;QShield","",7.0,"",42.0,"IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"Distributed and Dynamic Service Placement in Pervasive Edge Computing Networks","Z. Ning; P. Dong; X. Wang; S. Wang; X. Hu; S. Guo; T. Qiu; B. Hu; R. Y. K. Kwok","School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","12 Jan 2021",2021,32.0,6.0,1277,1292,"The explosive growth of mobile devices promotes the prosperity of novel mobile applications, which can be realized by service offloading with the assistance of edge computing servers. However, due to limited computation and storage capabilities of a single server, long service latency hinders the continuous development of service offloading in mobile networks. By supporting multi-server cooperation, Pervasive Edge Computing (PEC) is promising to enable service migration in highly dynamic mobile networks. With the objective of maximizing the system utility, we formulate the optimization problem by jointly considering the constraints of server storage capability and service execution latency. To enable dynamic service placement, we first utilize Lyapunov optimization method to decompose the long-term optimization problem into a series of instant optimization problems. Then, a sample average approximation-based stochastic algorithm is proposed to approximate the future expected system utility. Afterwards, a distributed Markov approximation algorithm is utilized to determine the service placement configurations. Through theoretical analysis, the time complexity of our proposed algorithm is linear to the number of users, and the backlog queue of PEC servers is stable. Performance evaluations are conducted based on both synthetic and real trace-driven scenarios, with numerical results demonstrating the effectiveness of our proposed algorithm from various aspects.","1558-2183","","10.1109/TPDS.2020.3046000","National Natural Science Foundation of China(grant numbers:61971084,61931019,62001073); Chongqing Talent Program(grant numbers:CQYC2020058659); National Natural Science Foundation of China(grant numbers:cstc2019jcyjmsxmX0208); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301260","Pervasive edge computing;service migration;Lyapunov optimization;distributed Markov approximation","Servers;Markov processes;Heuristic algorithms;Edge computing;Quality of service;Approximation algorithms;Task analysis","approximation theory;cloud computing;Markov processes;mobile computing;optimisation;power aware computing;resource allocation;stochastic processes","mobile devices;mobile applications;service offloading;edge computing servers;storage capabilities;long service;multiserver cooperation;service migration;highly dynamic mobile networks;system utility;server storage capability;service execution;dynamic service placement;Lyapunov optimization method;long-term optimization problem;instant optimization problems;sample average approximation-based stochastic algorithm;distributed Markov approximation algorithm;service placement configurations;PEC servers;pervasive edge computing networks","",50.0,"",42.0,"IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"Distributed Adaptive Consensus Tracking Control for Multi-Agent System With Communication Constraints","P. Zhang; H. Xue; S. Gao; J. Zhang","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China","IEEE Transactions on Parallel and Distributed Systems","21 Jan 2021",2021,32.0,6.0,1293,1306,"Aiming at a class of high-order strict feedback nonlinear multi-agent systems with communication constraints, a novel distributed adaptive back-stepping control method is proposed to cooperatively track the moving targets. First, five agents are used as controlled objects, and all five agents form a “leader-follower” mode with a distributed control structure. Meanwhile, the leader's moving velocity is considered the forward velocity of the whole formation system, and remaining agents follow the leader's movement. Then, each agent tracks the desired formation with a time-varying reference trajectory, thereby achieving the consensus tracking purpose of multi-agent system. Moreover, the multi-agent formation system avoids obstacles with the optimal trajectory and maintains the desired formation movement. Finally, the simulation results show that the designed controller can achieve the lateral and horizontal tracking errors of the multi-agent system to converge quickly, and then keep the system asymptotically stable during tracking process.","1558-2183","","10.1109/TPDS.2020.3048383","National Defense Research Program(grant numbers:N2015KD0152,11504429); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311825","Weak communication;multi-agent system;back-stepping technology;rapid convergence","Multi-agent systems;Topology;Target tracking;Process control;Convergence;Trajectory;Time-varying systems","adaptive control;collision avoidance;control system synthesis;distributed control;feedback;mobile robots;multi-agent systems;multi-robot systems;nonlinear control systems;position control;time-varying systems;tracking","distributed adaptive consensus tracking control;multiagent system;communication constraints;high-order strict feedback nonlinear multiagent systems;novel distributed adaptive back-stepping control method;controlled objects;leader-follower mode;distributed control structure;remaining agents;consensus tracking purpose;multiagent formation system","",14.0,"",31.0,"IEEE","31 Dec 2020","","","IEEE","IEEE Journals"
"E2bird: Enhanced Elastic Batch for Improving Responsiveness and Throughput of Deep Learning Services","W. Cui; Q. Chen; H. Zhao; M. Wei; X. Tang; M. Guo","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Institute for Advanced Communication and Data Science, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, Shanghai University of Finance and Economics, Shanghai, China; Shanghai Institute for Advanced Communication and Data Science, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","14 Jan 2021",2021,32.0,6.0,1307,1321,"We aim to tackle existing problems about deep learning serving on GPUs in the view of the system. GPUs have been widely adopted to serve online deep learning-based services that have stringent QoS(Quality-of-Service) requirements. However, emerging deep learning serving systems often result in poor responsiveness and low throughput of the inferences that damage user experience and increase the number of GPUs required to host an online service. Our investigation shows that the poor batching operation and the lack of data transfer-computation overlap are the root causes of the poor responsiveness and low throughput. To this end, we propose E2bird, a deep learning serving system that is comprised of a GPU-resident memory pool, a multi-granularity inference engine, and an elastic batch scheduler. The memory pool eliminates the unnecessary waiting of the batching operation and enables data transfer-computation overlap. The inference engine enables concurrent execution of different batches, improving the GPU resource utilization. The batch scheduler organizes inferences elasticallyto guarantee the QoS. Our experimental results on an Nvidia Titan RTXGPU show that E2bird reduces the response latency of inferences by up to 82.4 percent and improves the throughput by up to 62.8 percent while guaranteeing the QoS target compared with TensorFlow Serving.","1558-2183","","10.1109/TPDS.2020.3047638","National R&D Program of China(grant numbers:2018YFB1004800); National Natural Science Foundation of China(grant numbers:62022057,61632017,61832006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309326","GPUs;DL serving;latency;throughput;responsiveness","Graphics processing units;Throughput;Deep learning;Birds;Quality of service;Kernel;Engines","graphics processing units;inference mechanisms;learning (artificial intelligence);neural nets;processor scheduling;quality of service","enhanced elastic batch;improving responsiveness;deep learning services;GPU;online deep learning-based services;stringent QoS;quality-of-service;poor responsiveness;low throughput;damage user experience;online service;poor batching operation;data transfer-computation overlap;deep learning serving system;GPU-resident memory pool;multigranularity inference engine;elastic batch scheduler;batch scheduler;TensorFlow serving;Nvidia Titan RTXGPU;E2bird","",7.0,"",45.0,"IEEE","28 Dec 2020","","","IEEE","IEEE Journals"
"Partitioning-Based Scheduling of OpenMP Task Systems With Tied Tasks","Y. Wang; X. Jiang; N. Guan; Z. Guo; X. Liu; W. Yi","Northeastern University, Shenyang, China; Northeastern University, Shenyang, China; Hong Kong Polytechnic University, Hong Kong; University of Central Florida, Orlando, FL, USA; McGill University, Montreal, QC, Canada; Uppsala University, Uppsala, Sweden","IEEE Transactions on Parallel and Distributed Systems","2 Feb 2021",2021,32.0,6.0,1322,1339,"OpenMP is a popular programming framework in both general and high-performance computing and has recently drawn much interest in embedded and real-time computing. Although the execution semantics of OpenMP are similar to the DAG task model, the constraints posed by the OpenMP specification make them significantly more challenging to analyze. A tied task is an important feature in OpenMP that must execute on the same thread throughout its entire life cycle. A previous work [1] succeeded in analyzing the real-time scheduling of tied tasks by modifying the Task Scheduling Constraints (TSCs) in OpenMP specification. In this article, we also study the real-time scheduling of OpenMP task systems with tied tasks but without changing the original TSCs. In particular, we propose a partitioning-based algorithm, P-EDF-omp, by which the tied constraint can be automatically guaranteed as long as an OpenMP task system can be successfully partitioned to a multiprocessor platform. Furthermore, we conduct comprehensive experiments with both synthetic workloads and established OpenMP benchmarks to show that our approach consistently outperforms the work in [1] -even without modifying the TSCs.","1558-2183","","10.1109/TPDS.2020.3048373","National Natural Science Foundation of China(grant numbers:61772123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311807","Multicore;parallel tasks;real-time scheduling;partitioning;OpenMP;tied tasks","Task analysis;Program processors;Real-time systems;Time factors;Synchronization;Sun;Semantics","application program interfaces;multiprocessing systems;processor scheduling;real-time systems;scheduling","OpenMP specification;realtime scheduling;OpenMP task system;tied task;tied constraint;OpenMP benchmarks;OpenMP Task systems;DAG task model;task scheduling constraints","",3.0,"",44.0,"IEEE","31 Dec 2020","","","IEEE","IEEE Journals"
"Rings for Privacy: An Architecture for Large Scale Privacy-Preserving Data Mining","M. L. Merani; D. Croce; I. Tinnirello","Consorzio Nazionale Interuniversitario per le Telecomunicazioni (CNIT), Puteaux, France; Consorzio Nazionale Interuniversitario per le Telecomunicazioni (CNIT), Puteaux, France; Consorzio Nazionale Interuniversitario per le Telecomunicazioni (CNIT), Puteaux, France","IEEE Transactions on Parallel and Distributed Systems","20 Jan 2021",2021,32.0,6.0,1340,1352,"This article proposes a new architecture for privacy-preserving data mining based on Multi Party Computation (MPC) and secure sums. While traditional MPC approaches rely on a small number of aggregation peers replacing a centralized trusted entity, the current study puts forth a distributed solution that involves all data sources in the aggregation process, with the help of a single server for storing intermediate results. A large-scale scenario is examined and the possibility that data become inaccessible during the aggregation process is considered, a possibility that traditional schemes often neglect. Here, it is explicitly examined, as it might be provoked by intermittent network connectivity or sudden user departures. For increasing system reliability, data sources are organized in multiple sets, called rings, which independently work on the aggregation process. Two different protocol schemes are proposed and their failure probability, i.e., the probability that the data mining output cannot guarantee the desired level of accuracy, is analytically modeled. The privacy degree, the communication cost and the computational complexity that the schemes exhibit are also characterized. Finally, the new protocols are applied to some specific use cases, demonstrating their feasibility and attractiveness.","1558-2183","","10.1109/TPDS.2021.3049286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314057","Privacy;secret sharing;data mining;secure multi-party computation;C-means","Peer-to-peer computing;Protocols;Servers;Privacy;Distributed databases;Computer architecture;Data privacy","data mining;data privacy;probability","scale privacy-preserving data mining;secure sums;traditional MPC approaches;aggregation peers;centralized trusted entity;distributed solution;data sources;aggregation process;large-scale scenario;traditional schemes;intermittent network connectivity;sudden user departures;protocol schemes;data mining output;privacy degree","",2.0,"",30.0,"IEEE","5 Jan 2021","","","IEEE","IEEE Journals"
"Reliability and Confidentiality Co-Verification for Parallel Applications in Distributed Systems","G. Xie; K. Yang; H. Luo; R. Li; S. Hu","Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer and Control Engineering, Minjiang University, Fujian, China; Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; School of Electronics and Computer Science, University of Southampton, Southampton, U.K.","IEEE Transactions on Parallel and Distributed Systems","2 Feb 2021",2021,32.0,6.0,1353,1368,"Co-verification of reliability and confidentiality is a necessary process for safety- and security-critical applications. While these two objectives are conflicting, preassignment has emerged as an effective and efficient verification solution. In this article, we propose two preassignment-based co-verification techniques, namely, Blocks-based Vulnerability Preassignment (BVP) and Reversed Blocks-based Time Preassignment (RBTP) for a parallel application in distributed CAN FD systems. BVP can significantly improve reliability under a vulnerability bound, while RBTP can reduce vulnerability over a reliability goal. Real case study with the parallel automotive application and parallelism study with two structures of high-parallelism and low-parallelism applications are demonstrated; the proposed BVP and RBTP can improve the verification acceptance ratio by 19 and 10 percent compared to the state-of-the-art Average Vulnerability Preassignment (AVP) and Average Time Preassignment (ATP) techniques, respectively.","1558-2183","","10.1109/TPDS.2021.3049780","National Natural Science Foundation of China(grant numbers:61972139,61932010,61702172,61672217); Electronic Information and Control of Fujian University Engineering Research Center, Minjiang University(grant numbers:MJXY-KF-EIC1902); Fundamental Research Funds for the Central Universities; Hunan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316956","Co-verification;safety;security","Task analysis;Reliability;Security;Safety;Reliability engineering;Process control;Time factors","controller area networks;protocols;telecommunication network reliability","parallel application;distributed systems;security-critical applications;BVP;RBTP;distributed CAN FD systems;reliability;parallel automotive application;low-parallelism applications;verification acceptance ratio;average time preassignment techniques;average vulnerability preassignment;preassignment-based coverification techniques;ATP techniques;AVP techniques;reversed blocks-based time preassignment;blocks-based vulnerability preassignment;controller area network flexible data-rate system","",6.0,"",39.0,"IEEE","8 Jan 2021","","","IEEE","IEEE Journals"
"On Consortium Blockchain Consistency: A Queueing Network Model Approach","T. Meng; Y. Zhao; K. Wolter; C. -Z. Xu","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Mathematics and Computer Science, Freie Universität Berlin, Berlin, Germany; State Key Lab of IoTSC & Department of Computer and Information Science, University of Macau, Macau, China","IEEE Transactions on Parallel and Distributed Systems","20 Jan 2021",2021,32.0,6.0,1369,1382,"Analyzing blockchain protocols is a notoriously difficult task due to the underlying large scale distributed networks. To address this problem, stochastic model-based approaches are often utilized. However, the abstract models in prior work turn out not to be adoptable to consortium blockchains as the consensus of such a blockchain often consists of multiple processes. To address the lack of efficient analysis tools, we propose a queueing network-based method for analyzing consistency properties of consortium blockchain protocols in this article. Our method provides a way to evaluate the performance of the main stages in blockchain consensus. We apply our framework to the Hyperledger Fabric system and recover key properties of the blockchain network. Using our method, we analyze the security properties of the ordering mechanism and the impact of delaying endorsement messages in consortium blockchain protocols. Then an upper bound is derived of the damage an attacker could cause who is capable of delaying the honest players' messages. Based on the proposed method, we employ analytical derivations to investigate both the security and performance features, and corroborate close agreement with measurements on a wide-area network testbed running the Hyperledger Fabric blockchain. With the proposed method, designers of future blockchains can provide a more rigorous analysis of their consortium blockchain schemes.","1558-2183","","10.1109/TPDS.2021.3049915","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164002); National Natural Science Foundation of China(grant numbers:61801306); Science and Technology Development Fund(grant numbers:0015/2019/AKP); Shenzhen Basic Research Program(grant numbers:JCYJ20170818153016513); Shenzhen Fundamental Research(grant numbers:JCYJ20180302145755311,JCYJ20180302145731531); Guangdong Special Fund for Science and Technology Development(grant numbers:2019A050503001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316952","Consortium blockchain;analyzing framework;consistency;delay attack;queueing networks;permissoned blockchain","Blockchain;Protocols;Queueing analysis;Fabrics;Delays;Analytical models;Distributed ledger","blockchains;protocols;queueing theory;stochastic processes","large scale distributed networks;stochastic model-based approaches;abstract models;analysis tools;queueing network-based method;consistency properties;consortium blockchain protocols;blockchain consensus;blockchain network;security properties;wide-area network;hyperledger fabric blockchain;consortium blockchain schemes;consortium blockchain consistency;hyperledger fabric system","",25.0,"",49.0,"IEEE","8 Jan 2021","","","IEEE","IEEE Journals"
"A Scalable Stateful Approach for Virtual Security Functions Orchestration","N. Moradi; A. Shameli-Sendi; A. Khajouei","Faculty of Computer Science and Engineering, Shahid Beheshti University (SBU), Tehran, Iran; Faculty of Computer Science and Engineering, Shahid Beheshti University (SBU), Tehran, Iran; Faculty of Computer Science and Engineering, Shahid Beheshti University (SBU), Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","27 Jan 2021",2021,32.0,6.0,1383,1394,"Previous works suggested different approaches to implementing service chaining. Their goal is to enhance the performance of the middleboxes and satisfy the expectations of the cloud providers and users. To meet these expectations, the delay factor, i.e., flow through the low-cost paths, as well as the best node processing factor, are considered. Achieving these two goals simultaneously turns the middlebox optimal placement into an NP-hard problem. Therefore, when the problem size is large, it is infeasible to obtain an optimal solution at a reasonable time. One of the important issues which has not been considered in the previous works is stateful optimal placement when receiving a new request. Due to resource constraints as well as financial costs for the customers, it is not possible to create functions for all requests. Therefore, not only it is possible to integrate the same network functions between new flows, but it will also be examined between new on-demand network functions as well as existing ones. Our proposed approach not only reduces the creation of network functions that can be cost-effective for the customer but also because of the migration of previous network functions (integration with on-demand network functions) to optimize new requests, overall, it will optimize the entire network cost over time. We formulated the problem as 0-1 programming problem. The results of this article are based on a fat-tree data center. To show that our stateful solution is scalable in large networks, we use network zoning and topology partitioning heuristics. Our simulations show that we were able to scale our placement model to a network with 54K nodes and 1.5M edges.","1558-2183","","10.1109/TPDS.2021.3049804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316976","NFV;security functions;service chaining;optimal placement;stateful placement","Middleboxes;Security;Delays;Cloud computing;Topology;Network topology;Scalability","cloud computing;computational complexity;computer centres;computer network performance evaluation;computer network security;mathematical programming;telecommunication network topology;tree data structures;virtualisation","network zoning;topology partitioning heuristics;scalable stateful approach;virtual security functions orchestration;service chaining;cloud providers;delay factor;low-cost paths;node processing factor;middlebox optimal placement;NP-hard problem;optimal solution;stateful optimal placement;financial costs;on-demand network functions;network cost;0-1 programming problem;stateful solution;performance enhancement;fat-tree data center","",4.0,"",31.0,"IEEE","8 Jan 2021","","","IEEE","IEEE Journals"
"Optimised Lambda Architecture for Monitoring Scientific Infrastructure","U. Suthakar; L. Magnoni; D. R. Smith; A. Khan","CERN, European Organization for Nuclear Research (CERN), Geneva, Switzerland; CERN, European Organization for Nuclear Research (CERN), Geneva, Switzerland; Department of Electronic and Computer Engineering, College of Engineering, Design and Physical Sciences, Brunel University London, Middlesex, U.K.; Department of Electronic and Computer Engineering, College of Engineering, Design and Physical Sciences, Brunel University London, Middlesex, U.K.","IEEE Transactions on Parallel and Distributed Systems","1 Feb 2021",2021,32.0,6.0,1395,1408,"Within scientific infrastructuscientists execute millions of computational jobs daily, resulting in the movement of petabytes of data over the heterogeneous infrastructure. Monitoring the computing and user activities over such a complex infrastructure is incredibly demanding. Whereas present solutions are traditionally based on a Relational Database Management System (RDBMS) for data storage and processing, recent developments evaluate the Lambda Architecture (LA). In particular these studies have evaluated data storage and batch processing for processing large-scale monitoring datasets using Hadoop and its MapReduce framework. Although LA performed better than the RDBMS following evaluation, it was fairly complex to implement and maintain. This paper presents an Optimised Lambda Architecture (OLA) using the Apache Spark ecosystem, which involves modelling an efficient way of joining batch computation and real-time computation transparently without the need to add complexity. A few models were explored: pure streaming, pure batch computation, and the combination of both batch and streaming. An evaluation of the OLA on the CERN IT on-premises Hadoop cluster and the public Amazon cloud infrastructure for the monitoring WLCG Data acTivities (WDT) use case are both presented, demonstrating how the new architecture can offer benefits by combining both batch and real-time processing to compensate for batch-processing latency.","1558-2183","","10.1109/TPDS.2017.2772241","Brunel University London; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8336995","Big data;distributed systems;lambda architecture;low-latency computation;parallel computing","Computer architecture;Monitoring;Real-time systems;Sparks;Distributed databases;Computational modeling;Complexity theory","batch processing (computers);cloud computing;data analysis;grid computing;high energy physics instrumentation computing;parallel processing;relational databases","data storage;large-scale monitoring datasets;RDBMS;OLA;Apache Spark ecosystem;real-time computation;pure batch computation;CERN IT on-premises Hadoop cluster;public Amazon cloud infrastructure;real-time processing;batch-processing;scientific infrastructure;computational jobs;heterogeneous infrastructure;user activities;complex infrastructure;relational database management system;optimised lambda architecture;WLCG Data acTivities monitoring;scientific infrastructure monitoring","",4.0,"",23.0,"CCBY","12 Apr 2018","","","IEEE","IEEE Journals"
"A Machine-Learning-Based Framework for Productive Locality Exploitation","E. Kayraklioglu; E. Favry; T. El-Ghazawi","Hewlett Packard Enterprise Company, San Jose, CA, USA; Université Paris-Est, Champs-sur-Marne, France; George Washington University, DC, USA","IEEE Transactions on Parallel and Distributed Systems","2 Feb 2021",2021,32.0,6.0,1409,1424,"Data locality is of extreme importance in programming distributed-memory architectures due to its implications on latency and energy consumption. Automated compiler and runtime system optimization studies have attempted to improve data locality exploitation without burdening the programmer. However, due to the difficulty of static code analysis, conservatism in compiler optimizations to avoid errors, and cost of dynamic analysis, the efficacy of automated optimizations is limited. Therefore, programmers need to spend significant effort in optimizing locality while creating applications for distributed memory parallel systems. We present a machine-learning based framework to automatically exploit locality in distributed memory applications. This framework takes application source whose time-critical blocks are marked by pragmas, and produces optimized source code that uses a regressor for efficient data movement. The regressor is trained with automatically-collected application profiles with very small input data sizes. We integrate our prototype in the Chapel language stack. In our experiments, we show that the Elastic Net model is the ideal regressor for our case and applications that utilize Elastic Net can perform very similarly to programmer-optimized versions. We also show that such regressors can be trained within few minutes on a cluster or within 30 minutes on a workstation, including data collection.","1558-2183","","10.1109/TPDS.2021.3051348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321696","Data locality;distributed memory;programming models;machine learning","Optimization;Reactive power;Programming;Runtime;Program processors;Productivity;Prefetching","distributed memory systems;learning (artificial intelligence);memory architecture;message passing;optimisation;parallel processing;program compilers;program diagnostics;program verification;shared memory systems","machine-learning-based framework;productive locality exploitation;extreme importance;distributed-memory architectures;latency energy consumption;runtime system optimization studies;data locality exploitation;static code analysis;compiler optimizations;dynamic analysis;automated optimizations;distributed memory parallel systems;machine-learning based framework;distributed memory applications;application source;optimized source code;regressor;efficient data movement;automatically-collected application profiles;programmer-optimized versions;data collection","",2.0,"",49.0,"IEEE","13 Jan 2021","","","IEEE","IEEE Journals"
"Reversible CSP Computations","C. Galindo; N. Nishida; J. Silva; S. Tamarit","Departamento de Sistemas Informáticos y Computadores, Universitat Politècnica de València, Valencia, Spain; Graduate School of Informatics, Nagoya University, Nagoya, Japan; Departamento de Sistemas Informáticos y Computadores, Universitat Politècnica de València, Valencia, Spain; PFS Tech, Valencia, Spain","IEEE Transactions on Parallel and Distributed Systems","1 Feb 2021",2021,32.0,6.0,1425,1436,"Reversibility enables a program to be executed both forwards and backwards. This ability allows programmers to backtrack the execution to a previous state. This is essential if the computation is not deterministic because re-running the program forwards may not lead to that state of interest. Reversibility of sequential programs has been well studied and a strong theoretical basis exists. Contrarily, reversibility of concurrent programs is still very young, especially in the practical side. For instance, in the particular case of the Communicating Sequential Processes (CSP) language, reversibility is practically missing. In this article, we present a new technique, including its formal definition and its implementation, to reverse CSP computations. Most of the ideas presented can be directly applied to other concurrent specification languages such as Promela or CCS, but we center the discussion and the implementation on CSP. The technique proposes different forms of reversibility, including strict reversibility and causal-consistent reversibility. On the practical side, we provide an implementation of a system to reverse CSP computations that is able to highlight the source code that is being executed in each forwards/backwards computation step, and that has been optimized to be scalable to real systems.","1558-2183","","10.1109/TPDS.2021.3051747","EU (FEDER); Spanish MCI/AEI(grant numbers:TIN2016-76843-C4-1-R,PID2019-104735RB-C41); Generalitat Valenciana(grant numbers:Prometeo/2019/098 (DeepTrust)); JSPS KAKENHI(grant numbers:JP17H01722); TAILOR; EU Horizon 2020 research and innovation programme(grant numbers:GA 952215); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324973","Concurrent programming;tracing;debugging aids;code inspections and walkthroughs","Synchronization;Syntactics;Semantics;History;Standards;Data structures;Debugging","communicating sequential processes;formal specification;program verification;specification languages","strict reversibility;causal-consistent reversibility;reversible CSP computations;program forwards;sequential programs;concurrent programs reversibility;communicating sequential processes language;concurrent specification languages;source code","","","",31.0,"IEEE","14 Jan 2021","","","IEEE","IEEE Journals"
"Co-Active: A Workload-Aware Collaborative Cache Management Scheme for NVMe SSDs","H. Sun; S. Dai; J. Huang; X. Qin","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Science and Technology, Anhui University, Hefei, Anhui, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Department of Computer Science and Software Engineering, Auburn University, Auburn, AL, USA","IEEE Transactions on Parallel and Distributed Systems","3 Feb 2021",2021,32.0,6.0,1437,1451,"When it comes to NAND Flash-based solid-state disks (SSDs), cache can narrow the performance gap between user-level I/Os and flash memory. Cache management schemes impose relentless impacts on the endurance and performance of flash memory. A vast majority of existing cache management techniques adopt a passive data-update style (e.g., GCaR, LCR), thereby undermining response times in burst I/O requests-based applications11.Burst I/O requests must be served in a real-time manner. This type of I/O access pattern is prevalent in data-intensive workloads.. To address this issue, we propose a collaborative active write-back cache management scheme, called Co-Active, customized for I/O access patterns and the usage status of a flash chip. We design a hot/cold separation module to determine whether data is cold or hot in workload. When a flash chip is idle, cold and dirty data in the cache is flushed into the idle flash chip to produce clean data. To curtail cache replacement cost, clean data are preferentially evicted amid the procedure of cache replacement. A maximum write-back threshold is configured according to the level of burst I/O requests in workload. This threshold is intended to avert redundant write I/Os flushing into flash memory, thereby boosting the endurance of flash memory. The experiments are conducted to validate the advantages of Co-Active in terms of average response time, write amplification, and erase count. The findings unveil that compared with the six popular cache management schemes (LRU, CFLRU, GCaR_CFLRU, LCR, and MQSim), Co-Active (1) slashes the average response time by up to 83.89 percent with an average of 32.7 percent; (2) drives up the performance cliff degree by up to 76.4 percent with an average of 42.3 percent; and (3) improves write amplification rate by up to 60.5 percent with an average of 5.4 percent.","1558-2183","","10.1109/TPDS.2021.3052028","National Natural Science Foundation of China(grant numbers:61702004,62072001,61821003); State Key Laboratory of Computer Architecture(grant numbers:CARCH201915); Natural Science Research Projects at Higher Institutions in Anhui Province(grant numbers:KJ2017A015); National Science Foundation(grant numbers:IIS-1618669,OAC-1642133,CCF-0845257); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325932","Cache management;NAND flash;solid state disks;proactive write back","Time factors;Nonvolatile memory;Memory management;Parallel processing;Sun;Protocols;Degradation","cache storage;flash memories;NAND circuits","workload-aware collaborative cache management scheme;NAND Flash-based solid-state disks;flash memory;passive data-update style;access pattern;data-intensive workloads;cold data;dirty data;idle flash chip;clean data;cache replacement cost;average response time;Co-Active;NVMe SSDs;user-level I/Os;burst I/O request-based applications;I/O access patterns;hot-cold separation module;maximum write-back threshold;write amplification;erase count","",6.0,"",49.0,"IEEE","15 Jan 2021","","","IEEE","IEEE Journals"
"A Parallel Jacobi-Embedded Gauss-Seidel Method","A. Ahmadi; F. Manganiello; A. Khademi; M. C. Smith","Holcombe Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA; School of Mathematical and Statistical Sciences, Clemson University, Clemson, SC, USA.; Department of Industrial Engineering, Clemson University, Clemson, SC, USA; Holcombe Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA","IEEE Transactions on Parallel and Distributed Systems","1 Feb 2021",2021,32.0,6.0,1452,1464,"A broad range of scientific simulations involve solving large-scale computationally expensive linear systems of equations. Iterative solvers are typically preferred over direct methods when it comes to large systems due to their lower memory requirements and shorter execution times. However, selecting the appropriate iterative solver is problem-specific and dependent on the type and symmetry of the coefficient matrix. Gauss-Seidel (GS) is an iterative method for solving linear systems that are either strictly diagonally dominant or symmetric positive definite. This technique is an improved version of Jacobi and typically converges in fewer iterations. However, the sequential nature of this algorithm complicates the parallel extraction. In fact, most parallel derivatives of GS rely on the sparsity pattern of the coefficient matrix and require matrix reordering or domain decomposition. In this article, we introduce a new algorithm that exploits the convergence property of GS and adapts the parallel structure of Jacobi. The proposed method works for both dense and sparse systems and is straightforward to implement. We have examined the performance of our method on multicore and many-core architectures. Experimental results demonstrate the superior performance of the proposed algorithm compared with GS and Jacobi. Additionally, performance comparison with built-in Krylov solvers in MATLAB showed that in terms of time per iteration, Krylov methods perform faster on CPUs, but our approach is significantly better when executed on GPUs. Lastly, we apply our method to solve the power flow problem, and the results indicate a significant improvement in runtime, reaching up to 87 times faster speed compared with GS.","1558-2183","","10.1109/TPDS.2021.3052091","NSF-MRI(grant numbers:1725573); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325945","Linear systems;iterative methods;parallel;PJG;Gauss-Seidel;Jacobi;Krylov;SpMV performance;power flow","Jacobian matrices;Convergence;Matrix decomposition;Sparse matrices;Mathematical model;Linear systems;Multicore processing","convergence of numerical methods;iterative methods;linear systems;matrix algebra","large-scale computationally expensive linear systems;iterative solvers;memory requirements;iterative solver;coefficient matrix;GS;iterative method;parallel extraction;parallel derivatives;matrix reordering;convergence property;dense systems;parallel Jacobi-embedded Gauss-Seidel method;Krylov methods;iteration;Krylov solvers;sparse systems","",3.0,"",41.0,"IEEE","15 Jan 2021","","","IEEE","IEEE Journals"
"A High-Throughput FPGA Accelerator for Short-Read Mapping of the Whole Human Genome","Y. -L. Chen; B. -Y. Chang; C. -H. Yang; T. -D. Chiueh","Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; PixArt Imaging, Inc., Taiwan; Department of Electrical Engineering, Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan","IEEE Transactions on Parallel and Distributed Systems","1 Feb 2021",2021,32.0,6.0,1465,1478,"The mapping of DNA subsequences to a known reference genome, referred to as “short-read mapping”, is essential for next-generation sequencing. Hundreds of millions of short reads need to be aligned to a tremendously long reference sequence, making short-read mapping very time consuming. In this article, a high-throughput hardware accelerator is proposed so as to accelerate this task. A Bloom filter-based candidate mapping location (CML) generator and a folded processing element (PE) array are proposed to address CML selection and the Smith-Waterman (SW) alignment algorithm, respectively. It is shown that the proposed CML generator reduces the required memory access by 40 percent by employing a down-sampling scheme when compared to the Ferragina-Manzini index (FM-index) solution. The proposed hierarchical Bloom filter (HBF) that includes optimized parameters achieves a 1.5×104 times acceleration over the conventional Bloom filter. The proposed memory re-allocation scheme further reduces the memory access time for the HBF by a factor of 256. The proposed folded PE array delivers a 1.2-to-3.2 times higher giga cell updates per second (GCUPS). The processing time can be further reduced by 53-to-72 percent by employing a fully pipelined PE array that allows for a tailored shift amount for seeding. The accelerator is realized on a Stratix V GX FPGA with 16GB external SDRAM. Operated at 200MHz, the proposed FPGA accelerator delivers a 2.1-to-11 times higher throughput with the highest 99 percent accuracy and 98 percent sensitivity compared to the state-of-the-art FPGA-based solutions.","1558-2183","","10.1109/TPDS.2021.3051011","Ministry of Science and Technology(grant numbers:NSC99-2221-E-002-208-MY2,MOST 108-2218-E-002-060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320533","DNA sequencing;Bloom filter;short-read mapping;Smith-Waterman alignment;FPGA implementation","Genomics;Bioinformatics;Indexes;Field programmable gate arrays;DNA;Generators;Sequential analysis","bioinformatics;data structures;DNA;field programmable gate arrays;genetics;genomics;query processing","next-generation sequencing;hardware accelerator;Bloom filter-based candidate mapping location generator;folded processing element array;CML selection;Smith-Waterman alignment algorithm;CML generator;memory access;hierarchical Bloom filter;conventional Bloom filter;memory access time;folded PE array;processing time;fully pipelined PE array;FPGA-based solutions;high-throughput FPGA accelerator;short-read mapping;human genome;giga cell updates;memory reallocation scheme;efficiency 40.0 percent;efficiency 72.0 percent;memory size 16.0 GByte;efficiency 99.0 percent;frequency 200.0 MHz;efficiency 98.0 percent","",10.0,"",46.0,"IEEE","12 Jan 2021","","","IEEE","IEEE Journals"
"A Scalable Platform for Distributed Object Tracking Across a Many-Camera Network","A. Khochare; A. Krishnan; Y. Simmhan","Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India; VMWare, Bangalore, India; Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India","IEEE Transactions on Parallel and Distributed Systems","1 Feb 2021",2021,32.0,6.0,1479,1493,"Advances in deep neural networks (DNN) and computer vision (CV) algorithms have made it feasible to extract meaningful insights from large-scale deployments of urban cameras. Tracking an object of interest across the camera network in near real-time is a canonical problem. However, current tracking platforms have two key limitations: 1) They are monolithic, proprietary and lack the ability to rapidly incorporate sophisticated tracking models, and 2) They are less responsive to dynamism across wide-area computing resources that include edge, fog, and cloud abstractions. We address these gaps using Anveshak, a runtime platform for composing and coordinating distributed tracking applications. It provides a domain-specific dataflow programming model to intuitively compose a tracking application, supporting contemporary CV advances like query fusion and re-identification, and enabling dynamic scoping of the camera network's search space to avoid wasted computation. We also offer tunable batching and data-dropping strategies for dataflow blocks deployed on distributed resources to respond to network and compute variability. These balance the tracking accuracy, its real-time performance, and the active camera-set size. We illustrate the concise expressiveness of the programming model for four tracking applications. Our detailed experiments for a network of 1000 camera-feeds on modest resources exhibit the tunable scalability, performance, and quality trade-offs enabled by our dynamic tracking, batching, and dropping strategies.","1558-2183","","10.1109/TPDS.2021.3049450","Ministry of Electronic and Information Technology(grant numbers:4(16)/2019-ITEA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314091","Big data platform;edge and fog computing;video analytics;distributed stream processing;Internet of Things","Cameras;Streaming media;Urban areas;Tracking;Cloud computing;Target tracking;Scalability","cloud computing;computer vision;data flow computing;image fusion;neural nets;object tracking;query processing;video cameras","scalable platform;distributed object tracking;many-camera network;large-scale deployments;urban cameras;canonical problem;wide-area computing resources;cloud abstractions;runtime platform;domain-specific dataflow programming model;dynamic scoping;dynamic tracking;fog abstraction;edge abstraction;deep neural networks;computer visionalgorithms;Anveshak;contemporary CV;query fusion;re-identification;tunable batching;data-dropping strategies","",2.0,"",49.0,"IEEE","5 Jan 2021","","","IEEE","IEEE Journals"
"Network-Aware Locality Scheduling for Distributed Data Operators in Data Centers","L. Cheng; Y. Wang; Q. Liu; D. H. J. Epema; C. Liu; Y. Mao; J. Murphy","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Information Technology Group, Wageningen University & Research, Wageningen, The Netherlands; Distributed Systems Group, Delft University of Technology, Delft, The Netherlands; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Computer and Information Science, Fordham University, New York City, NY, USA; School of Computer Science, University College Dublin, Dublin, Ireland","IEEE Transactions on Parallel and Distributed Systems","9 Feb 2021",2021,32.0,6.0,1494,1510,"Large data centers are currently the mainstream infrastructures for big data processing. As one of the most fundamental tasks in these environments, the efficient execution of distributed data operators (e.g., join and aggregation) are still challenging current data systems, and one of the key performance issues is network communication time. State-of-the-art methods trying to improve that problem focus on either application-layer data locality optimization to reduce network traffic or on network-layer data flow optimization to increase bandwidth utilization. However, the techniques in the two layers are totally independent from each other, and performance gains from a joint optimization perspective have not yet been explored. In this article, we propose a novel approach called NEAL (NEtwork-Aware Locality scheduling) to bridge this gap, and consequently to further reduce communication time for distributed big data operators. We present the detailed design and implementation of NEAL, and our experimental results demonstrate that NEAL always performs better than current approaches for different workloads and network bandwidth configurations.","1558-2183","","10.1109/TPDS.2021.3053241","Beijing Municipal Science and Technology Commission(grant numbers:Z181100005118016); National Natural Science Foundation of China(grant numbers:61874124,61876173); European Union’s Horizon 2020 research and innovation programme(grant numbers:799066); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9329172","Data locality;coflow scheduling;distributed operators;data centers;big data;SDN;metaheuristic","Distributed databases;Bandwidth;Scheduling;Data centers;Optimization;Processor scheduling;Big Data","Big Data;computer centres;optimisation;telecommunication scheduling;telecommunication traffic","network-aware locality scheduling;distributed data operators;data centers;big data processing;key performance issues;network communication time;application-layer data locality optimization;network traffic;network-layer data flow optimization;joint optimization perspective;distributed big data operators;different workloads;network bandwidth configurations","",24.0,"",62.0,"IEEE","20 Jan 2021","","","IEEE","IEEE Journals"
"Editor's Note","M. Parashar","","IEEE Transactions on Parallel and Distributed Systems","11 Nov 2020",2021,32.0,4.0,743,745,"Presents the introductory editorial for this issue of the publication.","1558-2183","","10.1109/TPDS.2020.3035244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9257115","","","","","","","",0.0,"IEEE","11 Nov 2020","","","IEEE","IEEE Journals"
"Accelerating Large-Scale Prioritized Graph Computations by Hotness Balanced Partition","S. Gong; Y. Zhang; G. Yu","School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China","IEEE Transactions on Parallel and Distributed Systems","12 Nov 2020",2021,32.0,4.0,746,759,"Prioritized computation is shown promising performance for a large class of graph algorithms. It prioritizes the execution of some vertices that play important roles in determining convergence. For large-scale distributed graph processing, graph partitioning is an important preprocessing step that aims to balance workload and to reduce communication costs between workers. However, existing graph partitioning methods are designed for round-robin synchronous distributed frameworks. They balance workload without distinction of vertex importance and fail to consider the characteristics of priority-based scheduling, which may limit the benefit of prioritized graph computation. In this article, to accelerate prioritized iterative graph computations, we propose Hotness Balanced Partition (HBP). In prioritized graph computation, high priority vertices are likely to be executed more frequently and are likely to pass more messages, which result in hot vertices. Based on this observation, we partition graph by distributing vertices with distinction according to their hotness rather than blindly distributing vertices with equal weights, aiming to evenly distribute the hot vertices among workers. We further provide two HBP algorithms: a streaming-based algorithm for efficient one-pass processing and a distributed algorithm for distributed processing. Our results show that our proposed partitioning methods outperform the state-of-the-art partitioning methods, Fennel, HotGraph, and SNE.","1558-2183","","10.1109/TPDS.2020.3032709","National Key R&D Program of China(grant numbers:2018YFB1003404); National Natural Science Foundation of China(grant numbers:62072082,61672141,U1811261); Fundamental Research Funds for the Central Universities(grant numbers:N181605017,N181604016); Key R&D Program of Liaoning Province(grant numbers:2020JH 2/10100037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234696","Hotness balanced partition;graph partitioning;distributed computing","Processor scheduling;Partitioning algorithms;Runtime;Scheduling;Computational modeling;Google;Optimal scheduling","distributed algorithms;distributed processing;graph theory;iterative methods;scheduling","large-scale prioritized graph computations;hotness balanced partition;prioritized computation;state-of-the-art partitioning methods;distributed processing;distributed algorithm;partition graph;hot vertices;high priority vertices;prioritized graph computation;vertex importance;round-robin synchronous distributed frameworks;graph partitioning methods;balance workload;large-scale distributed graph processing;play important roles;graph algorithms","",1.0,"",50.0,"IEEE","21 Oct 2020","","","IEEE","IEEE Journals"
"Homomorphic Sorting With Better Scalability","G. S. Çetin; E. Savaş; B. Sunar","Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA","IEEE Transactions on Parallel and Distributed Systems","12 Nov 2020",2021,32.0,4.0,760,771,"Homomorphic sorting is an operation that blindly sorts a given set of encrypted numbers without decrypting them (thus, there is no need for the secret key). In this article, we propose a new, efficient, and scalable method for homomorphic sorting of numbers: polynomial rank sort algorithm. To put the new algorithm in a comparative perspective, we provide an extensive survey of classical sorting algorithms and networks that are not directly suitable for homomorphic computation. We also include, in our discussions, two of our previous algorithms specifically designed for homomorphic sorting operation: direct and greedy sort, and explain how they evolve from classical sorting networks. We theoretically show that the new algorithm is superior in terms of multiplicative depth when compared with all other algorithms. When batched implementation is used, the number of comparisons is reduced from O(N2) to O(N) provided that the number of slots is larger than or equal to the number of elements in the set. Our software implementation results confirm that the new algorithm is several orders of magnitude faster than many methods in the literature. Also, the polynomial sort algorithm scales better than the fastest algorithm in the literature to the best our knowledge although for small sets the execution times are comparable. The proposed algorithm is amenable to parallel implementation as most time consuming operations in the algorithm can naturally be performed concurrently.","1558-2183","","10.1109/TPDS.2020.3030748","National Science Foundation(grant numbers:#1561536); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222337","Private computation;encrypted computing;fully homomorphic encryption;homomorphic sorting","Sorting;Encryption;Software algorithms;Optimization;Indexes;Logic gates","computational complexity;polynomials;public key cryptography;sorting","polynomial rank sort algorithm;homomorphic computation;homomorphic sorting operation;direct sort;greedy sort;sorting networks;polynomial sort algorithm;encrypted numbers;multiplicative depth;software implementation;parallel implementation","",1.0,"",33.0,"IEEE","13 Oct 2020","","","IEEE","IEEE Journals"
"Resettable Encoded Vector Clock for Causality Analysis With an Application to Dynamic Race Detection","T. Pozzetti; A. D. Kshemkalyani","Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA; Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA","IEEE Transactions on Parallel and Distributed Systems","18 Nov 2020",2021,32.0,4.0,772,785,"Causality tracking among events is a fundamental challenge in distributed environments. Much previous work on this subject has focused on designing an efficient and scalable protocol to represent logical time. Several implementations of logical clocks have been proposed, most recently the Encoded Vector Clock (EVC), a protocol to encode Vector Clocks (VC) in scalar numbers through the use of prime numbers, to improve performance and scalability. We propose and formalize the concept of Resettable Encoded Vector Clock (REVC), a new logical clock implementation, which builds on the EVC to tackle its very high growth rate issue. We show how our REVC can be applied in both shared memory systems and message passing systems to achieve a consistent logical clock. We show, through practical examples, the advantage of REVC's growth rate with respect to EVC's growth rate. Finally, we show a practical application of the REVC to the dynamic race detection problem in multi-threaded environments. We compare our tool to the currently existing VC-based tool DJIT+ to show how the REVC can help in achieving higher performance with respect to the Vector Clock.","1558-2183","","10.1109/TPDS.2020.3032293","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234035","Causality;vector clock;prime numbers;encoding;dynamic race detection;clock reset protocol;performance","Clocks;Protocols;Scalability;Tools;Message passing;Encoding;Message systems","clocks;message passing;protocols;shared memory systems","VC-based tool DJIT;message passing systems;shared memory systems;REVC growth rate;EVC growth rate;logical clock implementation;Resettable Encoded Vector Clock;logical clocks;causality tracking;causality analysis;dynamic race detection problem","",3.0,"",38.0,"CCBY","20 Oct 2020","","","IEEE","IEEE Journals"
"BOSSA: A Decentralized System for Proofs of Data Retrievability and Replication","D. Chen; H. Yuan; S. Hu; Q. Wang; C. Wang","State Key Laboratory of Cryptology, Beijing, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China; State Key Laboratory of Cryptology, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","12 Nov 2020",2021,32.0,4.0,786,798,"Proofs of retrievability and proofs of replication are two cryptographic tools that enable a remote server to prove that the users' data has been correctly stored. Nevertheless, the literature either requires the users themselves to perform expensive verification jobs, or relies on a “fully trustworthy” third party auditor (TPA) to execute the public verification. In addition, none of existing solutions consider the underlying incentive issues behind a rational server who is motivated to collect users' data but tries to evade the replication checking in order to save storage resources. In this article, we propose the first decentralized system for proofs of data retrievability and replication-BOSSA, which is incentive-compatible for each party and realizes automated auditing atop off-the-shelf blockchain platforms. We deal with issues such as proof enforcements to catch malicious behaviors, new metrics to measure the contributions, and reward distributions to create a fair reciprocal environment. BOSSA also incorporates privacy-enhancing techniques to prevent decentralized peers (including blockchain nodes) from inferring private information about the outsourced data. Security analysis is presented in the context of integrity, privacy, and reliability. We implement a prototype based on BOSSA leveraging the smart contracts of Ethereum blockchain. Our extensive experimental evaluations demonstrate the practicality of our proposal.","1558-2183","","10.1109/TPDS.2020.3030063","National Key Research and Development Program of China(grant numbers:2020YFB1005500); National Natural Science Foundation of China(grant numbers:61822207,U1636219); Outstanding Youth Foundation of Hubei Province(grant numbers:2017CFA047); Fundamental Research Funds for the Central Universities(grant numbers:2042019kf0210); Research Grants Council of Hong Kong(grant numbers:CityU 11212717,CityU 11217819); Innovation and Technology Commission of Hong Kong(grant numbers:ITS/145/19); Fundamental Research Funds for the Central Universities(grant numbers:2020kfyXJJS075); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9220842","Proofs of retrievability;proofs of replication;decentralized system;blockchain","Blockchain;Cryptography;Servers;Reliability;Peer-to-peer computing;Smart contracts","auditing;cryptography;data privacy;distributed databases;outsourcing;peer-to-peer computing","cryptographic tools;remote server;public verification;rational server;replication checking;decentralized system;data retrievability;replication-BOSSA;blockchain platforms;proof enforcements;decentralized peers;data outsourcing;incentive issues;data replication;security analysis;Ethereum blockchain;automated auditing","",15.0,"",52.0,"IEEE","12 Oct 2020","","","IEEE","IEEE Journals"
"Energy-Aware Inference Offloading for DNN-Driven Applications in Mobile Edge Clouds","Z. Xu; L. Zhao; W. Liang; O. F. Rana; P. Zhou; Q. Xia; W. Xu; G. Wu","Key Laboratory for Ubiquitous Network and Service Software, Liaoning Province, China; Key Laboratory for Ubiquitous Network and Service Software, Liaoning Province, China; Research School of Computer Science, The Australian National University, Canberra, ACT, Australia; Cardiff University, Cardiff, United Kingdom; Hubei Engineering Research Center on Big Data Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Key Laboratory for Ubiquitous Network and Service Software, Liaoning Province, China; School of Sichuan University, Chengdu, China; Key Laboratory for Ubiquitous Network and Service Software, Liaoning Province, China","IEEE Transactions on Parallel and Distributed Systems","13 Nov 2020",2021,32.0,4.0,799,814,"With increasing focus on Artificial Intelligence (AI) applications, Deep Neural Networks (DNNs) have been successfully used in a number of application areas. As the number of layers and neurons in DNNs increases rapidly, significant computational resources are needed to execute a learned DNN model. This ever-increasing resource demand of DNNs is currently met by large-scale data centers with state-of-the-art GPUs. However, increasing availability of mobile edge computing and 5G technologies provide new possibilities for DNN-driven AI applications, especially where these application make use of data sets that are distributed in different locations. One fundamental process of a DNN-driven application in mobile edge clouds is the adoption of “inferencing” - the process of executing a pre-trained DNN based on newly generated image and video data from mobile devices. We investigate offloading DNN inference requests in a 5G-enabled mobile edge cloud (MEC), with the aim to admit as many inference requests as possible. We propose exact and approximate solutions to the problem of inference offloading in MECs. We also consider dynamic task offloading for inference requests, and devise an online algorithm that can be adapted in real time. The proposed algorithms are evaluated through large-scale simulations and using a real world test-bed implementation. The experimental results demonstrate that the empirical performance of the proposed algorithms outperform their theoretical counterparts and other similar heuristics reported in literature.","1558-2183","","10.1109/TPDS.2020.3032443","National Natural Science Foundation of China(grant numbers:61802048,61802047); Fundamental Research Funds for the Central Universities(grant numbers:DUT19RC(4)035); DUT-RU Co-Research Center of Advanced ICT for Active Life; Dalian University of Technology; Australian Research Council(grant numbers:DP200101985); National Natural Science Foundation of China(grant numbers:61972448); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234011","Inference offloading;mobile edge clouds;approximation and online algorithms","Artificial intelligence;Cloud computing;5G mobile communication;Base stations;Task analysis;Mobile handsets;Heuristic algorithms","cloud computing;inference mechanisms;learning (artificial intelligence);mobile computing;neural nets","deep neural networks;artificial intelligence;energy-aware inference offloading;dynamic task offloading;DNN inference requests;mobile devices;pre-trained DNN;mobile edge cloud;data sets;DNN-driven AI applications;mobile edge computing;large-scale data centers;resource demand","",24.0,"",59.0,"IEEE","20 Oct 2020","","","IEEE","IEEE Journals"
"Achieving Probabilistic Atomicity With Well-Bounded Staleness and Low Read Latency in Distributed Datastores","L. Ouyang; Y. Huang; H. Wei; J. Lu","Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, China","IEEE Transactions on Parallel and Distributed Systems","16 Nov 2020",2021,32.0,4.0,815,829,"Although it has been commercially successful to deploy weakly consistent but highly-responsive distributed datastores, the tension between developing complex applications and obtaining only weak consistency guarantees becomes more and more severe. The almost strong consistency tradeoff aims at achieving both strong consistency and low latency in the common case. In distributed storage systems, we investigate the generic notion of almost strong consistency in terms of designing fast read algorithms while guaranteeing Probabilistic Atomicity with well-Bounded staleness (PAB). This problem has been explored in the case where only one client can write the data. However, the more general case where multiple clients can write the data has not been studied. In this article, we study the fast read algorithm for PAB in the multi-writer case. We show the bound of data staleness and the probability of atomicity violation by decomposing inconsistent reads into the read inversion and the write inversion patterns. We implement the fast read algorithm and evaluate the consistency-latency tradeoffs based on the instrumentation of Cassandra and the YCSB benchmark framework. The theoretical analysis and the experimental evaluations show that our fast read algorithm guarantees PAB, even when faced with dynamic changes in the computing environment.","1558-2183","","10.1109/TPDS.2020.3034328","National Natural Science Foundation of China(grant numbers:61932021,61772258); Fundamental Research Funds for the Central Universities(grant numbers:14380063); Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9242251","Probabilistic atomicity;well-bounded staleness;fast read algorithm;quorum-replicated datastore","Registers;Heuristic algorithms;Servers;History;Probabilistic logic;Distributed databases;Collaboration","data integrity;distributed databases;probability;storage management","fast read algorithm;PAB;multiwriter case;data staleness;atomicity violation;read inversion;consistency-latency tradeoffs;distributed datastores;distributed storage systems;probabilistic atomicity;well-bounded staleness","","","",68.0,"CCBY","28 Oct 2020","","","IEEE","IEEE Journals"
"Cuttlefish: Neural Configuration Adaptation for Video Analysis in Live Augmented Reality","N. Chen; S. Quan; S. Zhang; Z. Qian; Y. Jin; J. Wu; W. Li; S. Lu","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Center for Networked Computing, Temple University, Philadelphia, PA, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","13 Nov 2020",2021,32.0,4.0,830,841,"Instead of relying on remote clouds, today's Augmented Reality (AR) applications usually send videos to nearby edge servers for analysis (such as objection detection) so as to optimize the user's quality of experience (QoE), which is often determined by not only detection latency but also detection accuracy, playback fluency, etc. Therefore, many studies have been conducted to help adaptively choose best video configuration, e.g., resolution and frame per second (fps), based on network bandwidth to further improve QoE. However, we notice that the video content itself has significant impacts on the configuration selection, e.g., the videos with high-speed objects must be encoded with a high fps to meet the user's fluency requirement. In this article, we aim to adaptively select configurations that match the time-varying network condition as well as the video content. We design Cuttlefish, a system that generates video configuration decisions using reinforcement learning (RL). Cuttlefish trains a neural network model that picks a configuration for the next encoding slot based on observations collected by AR devices. Cuttlefish does not rely on any pre-programmed models or specific assumptions on the environments. Instead, it learns to make configuration decisions solely through observations of the resulting performance of historical decisions. Cuttlefish automatically learns the adaptive configuration policy for diverse AR video streams and obtains a gratifying QoE. We compared Cuttlefish to several state-of-the-art bandwidth-based and velocity-based methods using trace-driven and real world experiments. The results show that Cuttlefish achieves a 18.4-25.8 percent higher QoE than the others.","1558-2183","","10.1109/TPDS.2020.3035044","National Key R&D Program of China(grant numbers:2017YFB1001801); National Natural Science Foundation of China(grant numbers:61872175,61832008); Natural Science Foundation of Jiangsu Province(grant numbers:BK20181252); Jiangsu Key R&D Program(grant numbers:BE2018116); Fundamental Research Funds for the Central Universities(grant numbers:14380060); Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246260","Augmented reality;reinforcement learning;configuration adaption","Bandwidth;Streaming media;Quality of experience;Adaptation models;Image edge detection;Real-time systems;Servers","augmented reality;learning (artificial intelligence);neural nets;object detection;quality of experience;video coding;video streaming","reinforcement learning;Cuttlefish;quality of experience;gratifying QoE;adaptive configuration policy;neural network model;video configuration decisions;time-varying network condition;high-speed objects;video content;improve QoE;network bandwidth;detection latency;objection detection;nearby edge servers;Augmented Reality applications;live Augmented Reality;video analysis;neural configuration adaptation","",7.0,"",47.0,"IEEE","30 Oct 2020","","","IEEE","IEEE Journals"
"SEIZE: Runtime Inspection for Parallel Dataflow Systems","Y. Li; M. Interlandi; F. Psallidas; W. Wang; C. Zaniolo","Department of Computer Science, University of California, Los Angeles, CA, USA; Gray Systems Lab, Microsoft, Redmond, WA, USA; Gray Systems Lab, Microsoft, Redmond, WA, USA; Department of Computer Science, University of California, Los Angeles, CA, USA; Department of Computer Science, University of California, Los Angeles, CA, USA","IEEE Transactions on Parallel and Distributed Systems","13 Nov 2020",2021,32.0,4.0,842,854,"Many Data-Intensive Scalable Computing (DISC) Systems provide easy-to-use functional APIs, and efficient scheduling and execution strategies allowing users to build concise data-parallel programs. In these systems, data transformations are concealed by exposed APIs, and intermediate execution states are masked under dataflow transitions. Consequently, many crucial features and optimizations (e.g., debugging, data provenance, runtime skew detection), which require runtime datafow states, are not well-supported. Inspired by our experience in implementing features and optimizations over DISC systems, we present SEIZE, a unified framework that enables dataflow inspection-wiretapping the data-path with listening logic-in MapReduce-style programming model. We generalize our lessons learned by providing a set of primitives defining dataflow inspection, orchestration options for different inspection granularities, and operator decomposition and dataflow punctuation strategy for dataflow intervention. We demonstrate the generality and flexibility of the approach by deploying SEIZE in both Apache Spark and Apache Flink, and by implementing a prototype runtime query optimizer for Spark. Our experiments show that, the overhead introduced by the inspection logic is most of the time negligible (less than 5 percent in Spark and 10 percent in Flink).","1558-2183","","10.1109/TPDS.2020.3035170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246701","Parallel dataflow;runtime inspection;user desired moments","Inspection;Sparks;Runtime;Programming;Data models;Task analysis;Cluster computing","application program interfaces;data handling;parallel languages;parallel programming;query processing","data provenance;runtime skew detection;runtime datafow states;optimizations;DISC systems;SEIZE;dataflow inspection-wiretapping;data-path;logic-in MapReduce-style programming model;operator decomposition;dataflow punctuation strategy;dataflow intervention;prototype runtime query optimizer;inspection logic;runtime inspection;parallel dataflow Systems;easy-to-use functional APIs;execution strategies;concise data-parallel programs;data transformations;intermediate execution states;dataflow transitions;crucial features;data-intensive scalable computing systems;inspection granularities","",1.0,"",53.0,"IEEE","2 Nov 2020","","","IEEE","IEEE Journals"
"MO-Tree: An Efficient Forwarding Engine for Spatiotemporal-Aware Pub/Sub Systems","T. Ding; S. Qian; J. Cao; G. Xue; Y. Zhu; J. Yu; M. Li","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; College of Mathematics and Computer Science, Zhejiang Normal University, Zhejiang, China","IEEE Transactions on Parallel and Distributed Systems","1 Dec 2020",2021,32.0,4.0,855,866,"For large-scale spatiotemporal-aware publish/subscribe systems, it is critical to design an efficient forwarding engine to achieve fast matching and maintenance of events and subscriptions. For this goal, we propose a novel data structure called MO-Tree to index both subscriptions and events in a unified way. The design philosophy behind MO-Tree is to keep the data structure concise, which manifests in three aspects: limiting the height of MO-Tree, trading space for time, and avoiding node merging and splitting. The difficulty in designing MO-Tree is how to efficiently index width-variable intervals. We present a multi-level cell-overlapping partition scheme and build a theoretical model to optimize the cell width in each level. To evaluate the performance of MO-Tree, a series of experiments is conducted on real-world trace datasets. The experiment results show MO-Tree significantly outperforms the state-of-the-art in terms of matching speed and maintenance cost.","1558-2183","","10.1109/TPDS.2020.3036014","National Key R&D Program of China(grant numbers:2018YFB2101100); National Natural Science Foundation of China(grant numbers:61772334,61702151); National Natural Science Foundation of China(grant numbers:U1736207); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9250646","Spatiotemporal-aware;publish/subscribe;event matching;event maintenance;subscription matching;subscription maintenance","Indexes;Spatiotemporal phenomena;Data structures;Maintenance engineering;Engines;Computational modeling;Limiting","data structures;indexing;message passing;middleware;trees (mathematics)","efficient forwarding engine;design philosophy;data structure;multilevel cell-overlapping partition scheme;MO-Tree;large-scale spatiotemporal-aware publish/subscribe systems;width-variable interval indexing;real-world trace datasets;cell width","",5.0,"",26.0,"IEEE","6 Nov 2020","","","IEEE","IEEE Journals"
"Towards Efficient Large-Scale Interprocedural Program Static Analysis on Distributed Data-Parallel Computation","R. Gu; Z. Zuo; X. Jiang; H. Yin; Z. Wang; L. Wang; X. Li; Y. Huang","State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","23 Nov 2020",2021,32.0,4.0,867,883,"Static program analysis has been widely applied along the whole process of the program development for bug detection, code optimization, testing, etc. Although researchers have made significant work in static program analysis, it is still challenging to perform sophisticated interprocedural analysis on large-scale modern software. The underlying reason is that interprocedural analysis for large-scale modern software is highly computation- and memory-intensive, leading to poor efficiency and scalability. In this article, we introduce an efficient distributed and scalable solution for sophisticated static analysis. Specifically, we propose a data-parallel algorithm and a join-process-filter computation model for the CFL-reachability-based interprocedural analysis. Based on that, an efficient distributed static analysis engine called BigSpa is developed, which is composed of an offline batch static program analysis system and an online incremental static program analysis system. The BigSpa system has high generality and can support all kinds of static analysis tasks that can be expressed as CFL reachability problems. The performance of BigSpa is evaluated on real-world large-scale software datasets. Our experiments show that the offline batch system can exceed an order of magnitude compared with the most advanced analysis tools available on performance, and for incremental analysis with small batch updates on the same data sets, the online analysis system can achieve near real-time response, which is very fast and flexible.","1558-2183","","10.1109/TPDS.2020.3036190","National Key R&D Program of China(grant numbers:2019YFC1711000,2017YFA0700604); China NSF(grant numbers:61802168,61932021,61702254,62072230,U1811461); Natural Science Foundation of Jiangsu Province(grant numbers:BK20170651,BK20191247); Fundamental Research Funds for the Central Universities(grant numbers:14380065); Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252147","Interprocedural static analysis;distributed systems;data-parallel computation","Static analysis;Software;Scalability;Task analysis;Optimization;Big Data;Computational modeling","parallel algorithms;program diagnostics;reachability analysis;software engineering","data-parallel algorithm;join-process-filter computation model;CFL-reachability-based interprocedural analysis;distributed static analysis engine;offline batch static program analysis system;online incremental static program analysis system;distributed data-parallel computation;program development;BigSpa;interprocedural program static analysis","",5.0,"",52.0,"IEEE","9 Nov 2020","","","IEEE","IEEE Journals"
"Coarse-Grained Parallel Routing With Recursive Partitioning for FPGAs","M. Shen; G. Luo; N. Xiao","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","24 Nov 2020",2021,32.0,4.0,884,899,"Routing is a very time-consuming stage in the FPGA design flow, significantly hindering the productivity. This article proposes CPRS, a coarse-grained parallel routing scheme in a distributed computing environment. First, we partition entire routing region to guide the assignment of nets for parallel processing. The partitioning is a recursive fashion, and at each recursive partitioning, the region is partitioned into two subregions forming three subsets of nets. The first subset consists of potentially dependent nets and they are distributed in different subregions. The remaining two subsets consist of potentially independent nets and they are distributed in their own subregions. Second, we route the nets of first subset in serial and process the remaining two subsets in parallel. The parallel processing is a coarse-grained fashion, which is implemented by MPI parallel programming model. Finally, we explore the optimization of both partitioning and parallel processing to further improve the overall speedup of parallel routing. In addition, we adopt MPI message to synchronize the intermediate results between different cores in parallel routing for a feasible solution. Experiments use a set of commonly used benchmarks to demonstrate the effectiveness of CPRS. Notably, CPRS achieves about 18× speedup on average using 32 processor cores with minor loss of quality, compared with the VTR 7.0 serial router. There is about 1.6× improvement over the state-of-the-art parallel router.","1558-2183","","10.1109/TPDS.2020.3035787","National Natural Science Foundation of China(grant numbers:62072479,61433019,61802446); Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); Guangdong Basic and Application Basic Research Teams(grant numbers:2018B030312002); Fundamental Research Funds for the Central Universities(grant numbers:19lgpy215); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248621","Distributed computing;field programmable gate array (FPGA);routing;parallel processing;recursive partitioning","Routing;Field programmable gate arrays;Pins;Nickel;Parallel processing;Optimization;Synchronization","circuit optimisation;field programmable gate arrays;integrated circuit design;network routing;parallel processing;parallel programming","CPRS;coarse-grained parallel routing scheme;distributed computing environment;partition entire routing region;parallel processing;recursive partitioning;potentially dependent nets;potentially independent nets;MPI parallel programming model;parallel router;MPI message;processor cores;FPGA design flow;VTR 7.0 serial router","",4.0,"",44.0,"IEEE","4 Nov 2020","","","IEEE","IEEE Journals"
"Canary: Decentralized Distributed Deep Learning Via Gradient Sketch and Partition in Multi-Interface Networks","Q. Zhou; K. Wang; H. Lu; W. Xu; Y. Sun; S. Guo","School of Automation and School of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Electrical and Computer Engineering, University of California, Los Angeles, Los Angeles, CA, USA; School of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Computer Science and Engineering, University at Buffalo, the State University of New York, Buffalo, NY, USA; School of Automation and School of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Parallel and Distributed Systems","26 Nov 2020",2021,32.0,4.0,900,917,"The multi-interface networks are efficient infrastructures to deploy distributed Deep Learning (DL) tasks as the model gradients generated by each worker can be exchanged to others via different links in parallel. Although this decentralized parameter synchronization mechanism can reduce the time of gradient exchange, building a high-performance distributed DL architecture still requires the balance of communication efficiency and computational utilization, i.e., addressing the issues of traffic burst, data consistency, and programming convenience. To achieve this goal, we intend to asynchronously exchange gradient pieces without the central control in multi-interface networks. We propose the Piece-level Gradient Exchange and Multi-interface Collective Communication to handle parameter synchronization and traffic transmission, respectively. Specifically, we design the gradient sketch approach based on 8-bit uniform quantization to compress gradient tensors and introduce the colayerabstraction to better handle gradient partition, exchange and pipelining. Also, we provide general programming interfaces to capture the synchronization semantics and build the Gradient Exchange Index (GEI) data structures to make our approach online applicable. We implement our algorithms into a prototype system called Canary by using PyTorch-1.4.0. Experiments conducted in Alibaba Cloud demonstrate that Canary reduces 56.28 percent traffic on average and completes the training by up to 1.61x, 2.28x, and 2.84x faster than BML, Ako on PyTorch, and PS on TensorFlow, respectively.","1558-2183","","10.1109/TPDS.2020.3036738","Hong Kong RGC Research Impact Fund(grant numbers:R5060-19,R5034-18); General Research Fund(grant numbers:152221/19E); Collaborative Research Fund(grant numbers:C5026-18G); National Natural Science Foundation of China(grant numbers:61872310,61772286,61802208,61872195); Jiangsu Key Research and Development Program(grant numbers:BE2019742); Natural Science Foundation of Jiangsu Province(grant numbers:BK20191381); Natural Science Foundation for Distinguished Young Scholar of Jiangsu(grant numbers:BK2020010062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252115","Distributed systems;multi-interface network;deep learning;gradient sketch;decentralized architecture","Training;Synchronization;Computer architecture;Convergence;Tensors;Task analysis;Partitioning algorithms","cache storage;cloud computing;data structures;dynamic programming;gradient methods;learning (artificial intelligence);message passing;microprocessor chips;minimisation;parallel processing;quantisation (signal);synchronisation","multiinterface networks;model gradients;decentralized parameter synchronization mechanism;high-performance distributed DL architecture;communication efficiency;gradient pieces;traffic transmission;gradient sketch approach;gradient tensors;pipelining;decentralized distributed deep learning;gradient sketch;multiinterface collective communication;piece-level gradient exchange;gradient partition;gradient exchange index","",3.0,"",62.0,"IEEE","9 Nov 2020","","","IEEE","IEEE Journals"
"Large-Scale Analysis of Docker Images and Performance Implications for Container Storage Systems","N. Zhao; V. Tarasov; H. Albahar; A. Anwar; L. Rupprecht; D. Skourtis; A. K. Paul; K. Chen; A. R. Butt","Key Laboratory of Big Data Storage and Management of MIIT, and National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; IBM Research-Almaden, San Jose, CA, USA; Department of Computer Science, Virginia Tech., Blacksburg, VA, USA; IBM Research-Almaden, San Jose, CA, USA; IBM Research-Almaden, San Jose, CA, USA; IBM Research-Almaden, San Jose, CA, USA; Department of Computer Science, Virginia Tech., Blacksburg, VA, USA; Department of Computer Science, Virginia Tech., Blacksburg, VA, USA; Department of Computer Science, Virginia Tech., Blacksburg, VA, USA","IEEE Transactions on Parallel and Distributed Systems","1 Dec 2020",2021,32.0,4.0,918,930,"Docker containers have become a prominent solution for supporting modern enterprise applications due to the highly desirable features of isolation, low overhead, and efficient packaging of the application’s execution environment. Containers are created from images which are shared between users via a registry. The amount of data registries store is massive. For example, Docker Hub, a popular public registry, stores at least half a million public images. In this article, we analyze over 167 TB of uncompressed Docker Hub images, characterize them using multiple metrics and evaluate the potential of file-level deduplication. Our analysis helps to make conscious decisions when designing storage for containers in general and Docker registries in particular. For example, only 3 percent of the files in images are unique while others are redundant file copies, which means file-level deduplication has a great potential to save storage space. Furthermore, we carry out a comprehensive analysis of both small I/O request performance and copy-on-write performance for multiple popular container storage drivers. Our findings can motivate and help improve the design of data reduction and caching methods for images, pulling optimizations for registries, and storage drivers.","1558-2183","","10.1109/TPDS.2020.3034517","National Science Foundation(grant numbers:CCF-1919113,CNS-1405697,CNS-1615411,OAC-2004751); Chinese National Key Research and Development Program(grant numbers:2018YFB1004401); Natural Science Foundation of Beijing Municipality(grant numbers:L192027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9242268","Containers;Docker;container images;container registry;deduplication;Docker hub;container storage drivers","Containers;Image coding;Cows;Crawlers;Measurement;Libraries;Ecosystems","cache storage;cloud computing;data reduction;image processing;public domain software;service-oriented architecture;software packages","caching methods;data reduction;multiple popular container storage drivers;copy-on-write performance;storage space;redundant file copies;general Docker registries;conscious decisions;file-level deduplication;multiple metrics;uncompressed Docker Hub images;public registry;data registries store;enterprise applications;Docker containers;container storage systems;performance implications;scale analysis","",11.0,"",49.0,"IEEE","28 Oct 2020","","","IEEE","IEEE Journals"
"Towards Greening MapReduce Clusters Considering Both Computation Energy and Cooling Energy","T. R. Toha; A. S. M. Rizvi; J. Noor; M. A. Adnan; A. B. M. A. Al Islam","Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh","IEEE Transactions on Parallel and Distributed Systems","1 Dec 2020",2021,32.0,4.0,931,942,"Increased processing power of MapReduce clusters generally enhances performance and availability at the cost of substantial energy consumption that often incurs higher operational costs (e.g., electricity bills) and negative environmental impacts (e.g., carbon dioxide emissions). There exist a few greening methods for computing clusters in the literature that focus mainly on computational energy consumption leaving cooling energy, which occupies a significant portion of the total energy consumed by the clusters. To this extent, in this article, we propose a machine learning-based approach that reduces the total energy consumption of a MapReduce cluster considering both computational energy and cooling energy. Our approach predicts the number of machines that results in minimum total energy consumption. We perform the prediction through applying different machine learning techniques over year-long data collected from a real setup. We evaluate performance of our approach through both real test-bed experimentation and simulation. Our evaluation reveals that our approach achieves substantial reduction in total energy consumption compared to other state-of-the-art alternatives while experiencing marginal performance degradation in a few cases.","1558-2183","","10.1109/TPDS.2020.3029724","ICT Division, Government of the People; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217959","Green parallel computing;MapReduce;hadoop;machine learning","Cooling;Energy consumption;Time factors;Task analysis;Machine learning algorithms;Cloud computing;Temperature distribution","computer centres;data handling;energy conservation;energy consumption;green computing;learning (artificial intelligence);parallel processing;pattern clustering;power aware computing","higher operational costs;computational energy consumption;cooling energy;machine learning-based approach;MapReduce cluster;minimum total energy consumption;computation energy;computing clusters","",1.0,"",37.0,"IEEE","8 Oct 2020","","","IEEE","IEEE Journals"
"High-Performance Routing With Multipathing and Path Diversity in Ethernet and HPC Networks","M. Besta; J. Domke; M. Schneider; M. Konieczny; S. D. Girolamo; T. Schneider; A. Singla; T. Hoefler","Department of Computer Science, ETH Zurich, Zürich, Switzerland; RIKEN Center for Computational Science (R-CCS), Kobe, Hyogo, Japan; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Faculty of Computer Science, Electronics and Telecommunications, AGH-UST, Kraków, Poland; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Zürich, Switzerland","IEEE Transactions on Parallel and Distributed Systems","2 Dec 2020",2021,32.0,4.0,943,959,"The recent line of research into topology design focuses on lowering network diameter. Many low-diameter topologies such as Slim Fly or Jellyfish that substantially reduce cost, power consumption, and latency have been proposed. A key challenge in realizing the benefits of these topologies is routing. On one hand, these networks provide shorter path lengths than established topologies such as Clos or torus, leading to performance improvements. On the other hand, the number of shortest paths between each pair of endpoints is much smaller than in Clos, but there is a large number of non-minimal paths between router pairs. This hampers or even makes it impossible to use established multipath routing schemes such as ECMP. In this article, to facilitate high-performance routing in modern networks, we analyze existing routing protocols and architectures, focusing on how well they exploit the diversity of minimal and non-minimal paths. We first develop a taxonomy of different forms of support for multipathing and overall path diversity. Then, we analyze how existing routing schemes support this diversity. Among others, we consider multipathing with both shortest and non-shortest paths, support for disjoint paths, or enabling adaptivity. To address the ongoing convergence of HPC and “Big Data” domains, we consider routing protocols developed for both HPC systems and for data centers as well as general clusters. Thus, we cover architectures and protocols based on Ethernet, InfiniBand, and other HPC networks such as Myrinet. Our review will foster developing future high-performance multipathing routing protocols in supercomputers and data centers.","1558-2183","","10.1109/TPDS.2020.3035761","JSPS KAKENHI(grant numbers:JP19H04119); Eidgenössische Technische Hochschule Zürich; Google Doctoral European Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248644","Routing;multipath routing;high-performance routing;path diversity;network architectures;high-performance networks;data center networks;ethernet;TCP/IP;InfiniBand","Routing;Topology;Network topology;Routing protocols;Data centers;Fats;Ethernet","parallel processing;routing protocols;telecommunication network reliability;telecommunication network topology","high-performance routing;path diversity;Ethernet;HPC networks;topology design;network diameter;low-diameter topologies;power consumption;shorter path lengths;established topologies;Clos;performance improvements;shortest paths;nonminimal paths;router pairs;established multipath routing schemes;modern networks;routing protocols;existing routing schemes support;nonshortest paths;disjoint paths;HPC systems;future high-performance multipathing","",9.0,"",176.0,"IEEE","4 Nov 2020","","","IEEE","IEEE Journals"
"A Generic Stochastic Model for Resource Availability in Fog Computing Environments","S. K. Battula; M. M. O'Reilly; S. Garg; J. Montgomery","School of Information and Communication Technology, College of Sciences and Engineering, University of Tasmania, Hobart, Australia; School of Natural Sciences, College of Sciences and Engineering, University of Tasmania, Hobart, Australia; School of Information and Communication Technology, College of Sciences and Engineering, University of Tasmania, Hobart, Australia; School of Information and Communication Technology, College of Sciences and Engineering, University of Tasmania, Hobart, Australia","IEEE Transactions on Parallel and Distributed Systems","2 Dec 2020",2021,32.0,4.0,960,974,"Fog computing is an increasingly popular method with which to process the huge amount of data generated by the Internet of Things (IoT) devices and applications at the edge-level, using the heterogeneous autonomous end-devices of the participating users. To meet the requirements of the IoT and time-sensitive applications, a Fog computing platform needs to select appropriate resources, the availability of which can be guaranteed during the execution of the application. For the proper selection of resources, the platform must be able to predict future availability. Hence, a proper resource availability model which provides knowledge about the future availability of resources in the Fog computing environment is required. However, designing an efficient resource availability model, in a highly distributed and mobile environment like the Fog, is a complex task due to the multidimensional characteristics of Fog devices, such as mobility, lack of centralised control, limited resources, and being battery powered. Existing resource availability models did not consider all the characteristics of a real Fog environment. Therefore, this study aims to provide a generic continuous-time Markov chain (CTMC), based resource availability model for Fog computing environments. The applicability of the model is shown by integrating the model input with the nearest-location best fit (NLBF) and Best-Fit resource selection policies.","1558-2183","","10.1109/TPDS.2020.3037247","Australian Research Council(grant numbers:LP140100152); ARC Center of Excellence for Mathematical and Statistical Frontiers; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253552","Fog computing;resource availability;stochastic model;Markov chain;mobility and Internet of Things","Computational modeling;Edge computing;Mathematical model;Cloud computing;Internet of Things;Stochastic processes;Dynamic scheduling","cloud computing;Internet of Things;Markov processes;resource allocation","time-sensitive applications;fog computing platform;appropriate resources;future availability;proper resource availability model;fog computing environment;efficient resource availability model;highly distributed environment;mobile environment;Fog devices;resource availability models;Fog environment;based resource availability model;generic stochastic model;IoT;heterogeneous autonomous end-devices","",7.0,"",29.0,"IEEE","10 Nov 2020","","","IEEE","IEEE Journals"
"Parallelization and Optimization of NSGA-II on Sunway TaihuLight System","X. Liu; J. Sun; L. Zheng; S. Wang; Y. Liu; T. Wei","National Research Centre of Parallel Computer Engineering and Technology, Beijing, China; Faculty of Information, East China Normal University, Shanghai, China; Faculty of Information, East China Normal University, Shanghai, China; Faculty of Information, East China Normal University, Shanghai, China; Faculty of Information, East China Normal University, Shanghai, China; Faculty of Information, East China Normal University, Shanghai, China","IEEE Transactions on Parallel and Distributed Systems","2 Dec 2020",2021,32.0,4.0,975,987,"Sunway TaihuLight system is the first supercomputer offering a peak performance over 100 PFlops, which can be utilized to parallelize Non-dominated Sorting Genetic Algorithm II (NSGA-II), a standard approach to multi-objective optimization. However, insufficient off-chip memory bandwidth and limited scratchpad memory capacity of the supercomputer hinder the performance improvement of parallellizing NSGA-II. In this article, we propose an optimized parallel NSGA-II on Sunway TaihuLight system, called swNSGA-II, by utilizing process- and thread-level parallelism of the system based on an improved island/master-slave model. To overcome the hurdles of low memory bandwidth and capacity, we propose a data sharing scheme based on register-level communication that can efficiently parallelize non-dominated sorting and crowding-distance computation of NSGA-II. Several optimization techniques including vectorization, direct memory accessing, and double buffering are also adopted to further accelerate swNSGA-II. Experiment results show that the proposed swNSGA-II can achieve a speedup of 41284 on a use case of path planning, and a speedup of 62692 on ZDT1 as compared to conventional NSGA-II.","1558-2183","","10.1109/TPDS.2020.3037082","National Key Research and Development Program of China(grant numbers:2020YFA0607902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253690","Sunway TaihuLight;many-core processor;NSGA-II;multi-objective optimization","Computational modeling;Instruction sets;Bandwidth;Path planning;Supercomputers;Optimization;Sorting","genetic algorithms;multiprocessing systems;parallel machines;Pareto optimisation;sorting","off-chip memory bandwidth;limited scratchpad memory capacity;optimized parallel NSGA-II;Sunway TaihuLight system;thread-level parallelism;low memory bandwidth;vectorization;nondominated sorting genetic algorithm II;multiobjective optimization;swNSGA-II;parallelization","",9.0,"",30.0,"IEEE","10 Nov 2020","","","IEEE","IEEE Journals"
"Editor's Note","M. Parashar","","IEEE Transactions on Parallel and Distributed Systems","19 Apr 2021",2021,32.0,10.0,2381,2385,"Presents the introductory editorial for this issue of the publication.","1558-2183","","10.1109/TPDS.2021.3066313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408530","","","","","","","",0.0,"IEEE","19 Apr 2021","","","IEEE","IEEE Journals"
"gIM: GPU Accelerated RIS-Based Influence Maximization Algorithm","S. Shahrouz; S. Salehkaleybar; M. Hashemi","Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","8 Apr 2021",2021,32.0,10.0,2386,2399,"Given a social network modeled as a weighted graph GG, the influence maximization problem seeks kk vertices to become initially influenced, to maximize the expected number of influenced nodes under a particular diffusion model. The influence maximization problem has been proven to be NP-hard, and most proposed solutions to the problem are approximate greedy algorithms, which can guarantee a tunable approximation ratio for their results with respect to the optimal solution. The state-of-the-art algorithms are based on Reverse Influence Sampling (RIS) technique, which can offer both computational efficiency and non-trivial (1-1/e-ε)(1-1/e-ε)-approximation ratio guarantee for any ε > 0ε>0. RIS-based algorithms, despite their lower computational cost compared to other methods, still require long running times to solve the problem in large-scale graphs with low values of ε. In this article, we present a novel and efficient parallel implementation of a RIS-based algorithm, namely IMM, on GPU. The proposed GPU-accelerated influence maximization algorithm, named gIM, can significantly reduce the running time on large-scale graphs with low values of ε. Furthermore, we show that gIM algorithm can solve other variations of the IM problem, only by applying minor modifications. Experimental results show that the proposed solution reduces the runtime by a factor up to 220 ×. The source code of gIM is publicly available online.","1558-2183","","10.1109/TPDS.2021.3066215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380701","CUDA;GPGPU;graph diffusion process;influence maximization (IM);parallel processing;reverse influence sampling (RIS)","Graphics processing units;Integrated circuit modeling;Computational modeling;Acceleration;Greedy algorithms;Diffusion processes;Social networking (online)","approximation theory;computational complexity;graph theory;graphics processing units;greedy algorithms;network theory (graphs);optimisation;parallel algorithms;sampling methods;social networking (online)","social network;NP-hard problem;GPU accelerated influence maximization algorithm;reverse influence sampling technique;approximation ratio;approximate greedy algorithms;particular diffusion model;influenced nodes;influence maximization problem;weighted graph;IM problem;gIM algorithm;large-scale graphs;RIS","",3.0,"",45.0,"IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"Analysis of GPU Data Access Patterns on Complex Geometries for the D3Q19 Lattice Boltzmann Algorithm","G. Herschlag; S. Lee; J. S. Vetter; A. Randles","Department of Biomedical Engineering, Duke University, Durham, NC, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Department of Biomedical Engineering, Duke University, Durham, NC, USA","IEEE Transactions on Parallel and Distributed Systems","12 Apr 2021",2021,32.0,10.0,2400,2414,"GPU performance of the lattice Boltzmann method (LBM) depends heavily on memory access patterns. When implemented with GPUs on complex domains, typically, geometric data is accessed indirectly and lattice data is accessed lexicographically. Although there are a variety of other options, no study has examined the relative efficacy between them. Here, we examine a suite of memory access schemes via empirical testing and performance modeling. We find strong evidence that semi-direct is often better suited than the more common indirect addressing, providing increased computational speed and reducing memory consumption. For the layout, we find that the Collected Structure of Arrays (CSoA) and bundling layouts outperform the common Structure of Array layout; on V100 and P100 devices, CSoA consistently outperforms bundling, however the relationship is more complicated on K40 devices. When compared to state-of-the-art practices, our recommendations lead to speedups of 10-40 percent and reduce memory consumption up to 17 percent. Using performance modeling and computational experimentation, we determine the mechanisms behind the accelerations. We demonstrate that our results hold across multiple GPUs on two leadership class systems, and present the first near-optimal strong results for LBM with arterial geometries run on GPUs.","1558-2183","","10.1109/TPDS.2021.3061895","National Institutes of Health(grant numbers:DP5OD019876); UT-Battelle(grant numbers:DE-AC05-00OR22725); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373915","Lattice boltzmann;graphics processing units;memory access;complex geometries;high performance computing","Lattices;Layout;Geometry;Memory management;Computational modeling;Acceleration;Standards","graphics processing units;lattice Boltzmann methods;performance evaluation","arterial geometries;colected structure of arrays;array layout;bundling layouts;CSoA;memory consumption reduction;increased computational speed;performance modeling;memory access schemes;geometric data;complex domains;memory access patterns;LBM method;lattice Boltzmann method;GPU performance;D3Q19 Lattice Boltzmann Algorithm;complex geometries;GPU data access patterns","",3.0,"",54.0,"IEEE","9 Mar 2021","","","IEEE","IEEE Journals"
"ETICA: Efficient Two-Level I/O Caching Architecture for Virtualized Platforms","S. Ahmadian; R. Salkhordeh; O. Mutlu; H. Asadi","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; ETH Zurich, Zurich, Switzerland; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Parallel and Distributed Systems","12 Apr 2021",2021,32.0,10.0,2415,2433,"In recent years, increased I/O demand of Virtual Machines (VMs) in large-scale data centers and cloud computing has encouraged system architects to design high-performance storage systems. One common approach to improving performance is to employ fast storage devices such as Solid-State Drives (SSDs) as an I/O caching layer for slower storage devices. SSDs provide high performance, especially on random requests, but they also have limited endurance: they support only a limited number of write operations and can therefore wear out relatively fast due to write operations. In addition to the write requests generated by the applications, each read miss in the SSD cache is served at the cost of imposing a write operation to the SSD (to copy the data block into the cache), resulting in an even larger number of writes into the SSD. Previous I/O caching schemes on virtualized platforms only partially mitigate the endurance limitations of SSD-based I/O caches; they mainly focus on assigning efficient cache write policies and cache space to the VMs. Moreover, existing cache space allocation schemes have inefficiencies: they do not take into account the impact of cache write policy in reuse distance calculation of the running workloads and hence, reserve cache blocks for accesses that would not be served by cache. In this article, we propose an Efficient Two-Level I/O Caching Architecture (ETICA) for virtualized platforms that can significantly improve I/O latency, endurance, and cost (in terms of cache size) while preserving the reliability of write-pending data blocks. As opposed to previous one-level I/O caching schemes in virtualized platforms, our proposed architecture 1) provides two levels of cache by employing both Dynamic Random-Access Memory (DRAM) and SSD in the I/O caching layer of virtualized platforms and 2) effectively partitions the cache space between running VMs to achieve maximum performance and minimum cache size. To manage the two-level cache, unlike the previous reuse distance calculation schemes such as Useful Reuse Distance (URD), which only consider the request type and neglect the impact of cache write policy, we propose a new metric, Policy Optimized reuse Distance (POD). The key idea of POD is to effectively calculate the reuse distance and estimate the amount of two-level DRAM+SSD cache space to allocate by considering both 1) the request type and 2) the cache write policy. Doing so results in enhanced performance and reduced cache size due to the allocation of cache blocks only for the requests that would be served by the I/O cache. ETICA maintains the reliability of write-pending data blocks and improves performance by 1) assigning an effective and fixed write policy at each level of the I/O cache hierarchy and 2) employing effective promotion and eviction methods between cache levels. Our extensive experiments conducted with a real implementation of the proposed two-level storage caching architecture show that ETICA provides 45 percent higher performance, compared to the state-of-the-art caching schemes in virtualized platforms, while improving both cache size and SSD endurance by 51.7 and 33.8 percent, respectively.","1558-2183","","10.1109/TPDS.2021.3066308","Sharif University of Technology; Eidgenössische Technische Hochschule Zürich; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380565","Virtualization;cloud computing;I/O caching;solid-state drives;data storage systems;performance;endurance;reuse distance","Random access memory;Reliability;Measurement;Performance evaluation;Resource management;Computer architecture;Power system reliability","cache storage;cloud computing;DRAM chips;virtual machines","virtualized platforms;efficient cache;cache space allocation schemes;reserve cache blocks;data block;caching layer;minimum cache size;two-level cache;two-level DRAM+SSD cache space;cache levels;two-level storage caching architecture;state-of-the-art caching schemes;high-performance storage systems;write operations;useful reuse distance;URD;policy optimized reuse distance;POD;cache blocks;write-pending data blocks;efficiency 45.0 percent;efficiency 33.8 percent","",4.0,"",89.0,"IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"3D Perception With Slanted Stixels on GPU","D. Hernandez-Juarez; A. Espinosa; D. Vazquez; A. M. Lopez; J. C. Moure","SLAMcore Ltd., London, SE1 1JA, U.K.; Universitat Autonoma de Barcelona, Bellaterra, Spain; Element AI, Montreal, QC, Canada; Computer Vision Center, Universitat Autonoma de Barcelona, Bellaterra, Spain; Universitat Autonoma de Barcelona, Bellaterra, Spain","IEEE Transactions on Parallel and Distributed Systems","12 Apr 2021",2021,32.0,10.0,2434,2447,"This article presents a GPU-accelerated software design of the recently proposed model of Slanted Stixels, which represents the geometric and semantic information of a scene in a compact and accurate way. We reformulate the measurement depth model to reduce the computational complexity of the algorithm, relying on the confidence of the depth estimation and the identification of invalid values to handle outliers. The proposed massively parallel scheme and data layout for the irregular computation pattern that corresponds to a Dynamic Programming paradigm is described and carefully analyzed in performance terms. Performance is shown to scale gracefully on current generation embedded GPUs. We assess the proposed methods in terms of semantic and geometric accuracy as well as run-time performance on three publicly available benchmark datasets. Our approach achieves real-time performance with high accuracy for 2048 × 1024 image sizes and 4 × 4 Stixel resolution on the low-power embedded GPU of an NVIDIA Tegra Xavier.","1558-2183","","10.1109/TPDS.2021.3067836","Ministerio de Economía, Industria y Competitividad(grant numbers:TIN2017-84553-C2-1-R); Antonio M. López(grant numbers:TIN2017-88709-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382880","Stereo vision;stixel world;autonomous vehicles;scene understanding;computer vision;embedded systems;GPU acceleration","Graphics processing units;Semantics;Computational modeling;Real-time systems;Mathematical model;Image segmentation;Indexes","computational complexity;dynamic programming;embedded systems;graphics processing units;image resolution","Slanted Stixels;geometric information;semantic information;measurement depth model;computational complexity;depth estimation;invalid values;massively parallel scheme;data layout;irregular computation pattern;Dynamic Programming paradigm;performance terms;semantic accuracy;geometric accuracy;run-time performance;real-time performance;4 × 4 Stixel resolution;3d perception;GPU-accelerated software design","","","",34.0,"CCBY","22 Mar 2021","","","IEEE","IEEE Journals"
"Spartan: A Sparsity-Adaptive Framework to Accelerate Deep Neural Network Training on GPUs","S. Dong; Y. Sun; N. B. Agostini; E. Karimi; D. Lowell; J. Zhou; J. Cano; J. L. Abellán; D. Kaeli","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; William & Mary, Sadler Center, Williamsburg, VA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Advanced Micro Devices Inc., Santa Clara, CA, USA; Advanced Micro Devices Inc., Santa Clara, CA, USA; University of Glasgow, Glasgow, U.K.; UCAM Universidad Católica de Murcia, Murcia, Spain; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA","IEEE Transactions on Parallel and Distributed Systems","19 Apr 2021",2021,32.0,10.0,2448,2463,"Deep Neural Networks (DNNs) have emerged as an important class of machine learning algorithms, providing accurate solutions to a broad range of applications. Sparsity in activation maps in DNN training presents an opportunity to reduce computations. However, exploiting activation sparsity presents two major challenges: i) profiling activation sparsity during training comes with significant overhead due to computing the degree of sparsity and the data movement; ii) the dynamic nature of activation maps requires dynamic dense-to-sparse conversion during training, leading to significant overhead. In this article, we present Spartan, a lightweight hardware/software framework to accelerate DNN training on a GPU. Spartan provides a cost-effective and programmer-transparent microarchitectural solution to exploit activation sparsity detected during training. Spartan provides an efficient sparsity monitor, a tile-based sparse GEMM algorithm, and a novel compaction engine designed for GPU workloads. Spartan can reduce sparsity profiling overhead by 52.5× on average. For the most compute-intensive layers, i.e., convolutional layers, we can speedup AlexNet by 3.4×, VGGNet-16 by 2.14×, and ResNet-18 by 2.02×, when training on the ImageNet dataset.","1558-2183","","10.1109/TPDS.2021.3067825","AMD; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382871","DNN;sparsity;GPU","Training;Monitoring;Sparse matrices;Graphics processing units;Acceleration;Market research;Engines","graphics processing units;learning (artificial intelligence);neural nets","data movement;hardware framework;software framework;GPU;programmer-transparent microarchitectural solution;ImageNet dataset;dense-to-sparse conversion;activation sparsity;DNN training;activation maps;machine learning algorithms;deep neural networks;sparsity-adaptive framework;sparsity profiling overhead;tile-based sparse GEMM algorithm;Spartan","",2.0,"",53.0,"IEEE","22 Mar 2021","","","IEEE","IEEE Journals"
"The Case for Cross-Component Power Coordination on Power Bounded Systems","R. Ge; X. Feng; T. Allen; P. Zou","School of Computing, Clemson University, Clemson, SC, USA; School of Computing, Clemson University, Clemson, SC, USA; School of Computing, Clemson University, Clemson, SC, USA; School of Computing, Clemson University, Clemson, SC, USA","IEEE Transactions on Parallel and Distributed Systems","16 Apr 2021",2021,32.0,10.0,2464,2476,"Modern computer systems are increasingly bounded by the available or permissible power at multiple layers from components to systems. To cope with this reality, it is necessary to understand how power bounds impact the design and performance of emergent computer systems. Prior work mainly focuses on power capping and budgeting on individual components without coordinating them to achieve the best possible performance. In this article, we study the problem of power bounded computing and power allocation across computer components on CPU and GPU-accelerated systems. We investigate the dynamics between cross-component power allocation and generalize the performance impacts, and propose lightweight heuristics to maximize performance. We draw multiple insights: (1) for a given application and power bound, there exists a maximum achievable performance which requires coordinated power allocation among components for balanced computation and memory access; (2) the max performance increases with the total power bound but only in a definite range specific to applications; (3) the dynamics of power allocations has categorical patterns with regard to performance trends and actual power use; and (4) the categorical patterns can be leveraged to design coordinated power allocations. These findings suggest the promises of cross-component coordination in forthcoming power bounded high performance computing.","1558-2183","","10.1109/TPDS.2021.3068235","National Science Foundation(grant numbers:CCF-1551511,CNS-1551262); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384293","Power-bounded computing;cross-component power coordination;power capping;performance analysis;power management","Graphics processing units;Resource management;Central Processing Unit;Memory modules;Memory management;Hardware;Power demand","graphics processing units;power aware computing","power bounded high performance computing;categorical patterns;lightweight heuristics;GPU-accelerated systems;CPU;cross-component power allocation;computer components;budgeting;power capping;modern computer systems;power bounded systems;cross-component power coordination;cross-component coordination;memory access;balanced computation;coordinated power allocation;maximum achievable performance","",1.0,"",36.0,"IEEE","23 Mar 2021","","","IEEE","IEEE Journals"
"Group Reassignment for Dynamic Edge Partitioning","H. Li; H. Yuan; J. Huang; J. Cui; X. Ma; S. Wang; J. Yoo; P. S. Yu","School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Engineering, Central South University, Changsha, China; Department of Information and Communication Engineering, Chungbuk National University, Cheongju, Korea; Institute for Data Science, Tsinghua University, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","16 Apr 2021",2021,32.0,10.0,2477,2490,"Graph partitioning is a mandatory step in large-scale distributed graph processing. When partitioning real-world power-law graphs, the edge partitioning algorithm performs better than the traditional vertex partitioning algorithm, because it can cut a single vertex into multiple replicas to apportion the computation. Many advanced edge partitioning methods are designed for partitioning a static graph from scratch. However, the real-world graph structure changes continuously, which leads to a decrease in partition quality and affects the performance of the graph applications. Some studies are devoted to offline repartitioning or batch incremental partitioning, but how to deal with dynamics in real-time is still worthy of in-depth study. In this article, we discuss the impact of dynamic change on partition and discover that both insertion and deletion will lead to local suboptimal partitioning, which is the reason for the degradation of partition quality. As a solution, a dynamic edge partitioning algorithm is proposed to partition dynamics in real-time. Specifically, we deal with dynamics by a distributed stream and improve partition quality by reassigning some closely connected edges. Experiments show that it is robust to initial partition quality, dynamic scale and type, and distributed scale. Compared with the state-of-the-art dynamic partitioner, it can reduce vertex-cuts by 29.5 percent. Compared with the repartitioning algorithms, it can save the partitioning time by 91.0 percent. Applied on the graph task, it can reduce the increase of communication cost and the increase of the total time of task by 41.5 and 71.4 percent.","1558-2183","","10.1109/TPDS.2021.3069292","Natural Science Foundation of China(grant numbers:61602354,61876138,61976168,61772394); Natural Science Foundation of Shaanxi Province(grant numbers:2019JM-227); National Science Foundation(grant numbers:III-1763325,III-1909323,SaTC-1930941); National Research Foundation of Korea(grant numbers:2019R1A2C2084257); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9388864","Dynamic graph;edge partitioning;distributed system;edge group","Heuristic algorithms;Partitioning algorithms;Real-time systems;Task analysis;Electronic mail;Aerodynamics;Social networking (online)","graph theory","static graph;real-world graph structure changes;graph applications;offline repartitioning;local suboptimal partitioning;dynamic edge partitioning algorithm;partition dynamics;closely connected edges;initial partition quality;dynamic scale;state-of-the-art dynamic partitioner;partitioning time;graph task;graph partitioning;large-scale distributed graph processing;partitioning real-world power-law graphs;traditional vertex partitioning algorithm;advanced edge partitioning methods","",4.0,"",49.0,"IEEE","29 Mar 2021","","","IEEE","IEEE Journals"
"FRATO: Fog Resource Based Adaptive Task Offloading for Delay-Minimizing IoT Service Provisioning","H. Tran-Dang; D. -S. Kim","Department of IT Convergence Engineering, Kumoh National Institute of Technology, Gumi, Korea; Department of IT Convergence Engineering, Kumoh National Institute of Technology, Gumi, Korea","IEEE Transactions on Parallel and Distributed Systems","16 Apr 2021",2021,32.0,10.0,2491,2508,"In the IoT-based systems, the fog computing allows the fog nodes to offload and process tasks requested from IoT-enabled devices in a distributed manner instead of the centralized cloud servers to reduce the response delay. However, achieving such a benefit is still challenging in the systems with high rate of requests, which imply long queues of tasks in the fog nodes, thus exposing probably an inefficiency in terms of latency to offload the tasks. In addition, a complicated heterogeneous degree in the fog environment introduces an additional issue that many of single fogs can not process heavy tasks due to lack of available resources or limited computing capabilities. To cope with the situation, this article introduces FRATO (Fog Resource aware Adaptive Task Offloading) - a framework for the IoT-fog-cloud systems to offer the minimal service provisioning delay through an adaptive task offloading mechanism. Fundamentally, FRATO is based on the fog resource to select flexibly the optimal offloading policy, which in particular includes a collaborative task offloading solution based on the data fragment concept. In addition, two distributed fog resource allocation algorithms, namely TPRA and MaxRU are developed to deploy the optimized offloading solutions efficiently in cases of resource competition. Through the extensive simulation analysis, the FRATO-based service provisioning approaches show potential advantages in reducing the average delay significantly in the systems with high rate of service requests and heterogeneous fog environment compared with the existing solutions.","1558-2183","","10.1109/TPDS.2021.3067654","Ministry of Science and ICT, South Korea(grant numbers:IITP-2020-2020-0-01612); National Research Foundation of Korea (NRF)(grant numbers:2018R1A6A1A03024003,NRF-2020R1I1A1A01073019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382907","IoT-fog-cloud systems;IoT services;fog computing;task offloading;task division","Task analysis;Delays;Internet of Things;Cloud computing;Servers;Resource management;Quality of service","cloud computing;Internet of Things;resource allocation","data fragment concept;MaxRU;TPRA;fog resource aware adaptive task offloading;heterogeneous fog environment;service requests;FRATO-based service provisioning approaches;resource competition;distributed fog resource allocation algorithms;collaborative task offloading solution;optimal offloading policy;minimal service provisioning delay;IoT-fog-cloud systems;response delay;centralized cloud servers;fog nodes;fog computing;delay-minimizing IoT service provisioning","",22.0,"",43.0,"IEEE","22 Mar 2021","","","IEEE","IEEE Journals"
"DTransE: Distributed Translating Embedding for Knowledge Graph","D. Song; F. Zhang; M. Lu; S. Yang; H. Huang","Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China; Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Parallel and Distributed Systems","19 Apr 2021",2021,32.0,10.0,2509,2523,"Knowledge graphs play an important role in many applications, such as link prediction and question answering. Translating embedding for knowledge graphs is done with the aim of encoding structured information on entities and their rich relations in a low-dimensional embedding space. TransE is one of the most important methods in translation-based models, and uses translation invariance to implement translating embedding for knowledge graphs. In this line of work, translating embedding models represent the relation as a translation from the head entity to the tail entity and have achieved impressive results. Currently, the TransE model is only developed on single-node machines. Unfortunately, the computing and storage capacities of a single machine can easily reach their limits as knowledge graphs become larger and more complex, which limits the application scope of TransE. In order to solve this problem, we propose a distributed TransE method, known as DTransE, which can utilize distributed computing resources to calculate knowledge graph embeddings. However, building a distributed TransE is complicated and involves challenges of knowledge graph partitioning and computation. To solve these challenges, we provide a high-quality edge partitioning algorithm for the power-law graph by considering the high-degree and low-degree vertices with adaptive weights, which can balance the workload. By using the unactivated Gather-Apply-Scatter model on TransE, the processes periodically exchange messages in a loop. The irregular data distribution among the processes is also optimized to further accelerate communication. As far as we know, this is the first work on a distributed TransE method. We use link prediction to evaluate the DTransE in a distributed environment. Experiments show that, compared to the original TransE method, our proposed DTransE is, on average, 24.5 times faster with a minimum loss of accuracy; compared to the state-of-the-art parallel TransE implementation, DTransE is two times faster on average.","1558-2183","","10.1109/TPDS.2021.3066442","National Key Research and Development Program of China(grant numbers:2020YFC0832606); National Natural Science Foundation of China(grant numbers:61976021,61802412,U1811262); Beijing Natural Science Foundation(grant numbers:L192027); State Key Laboratory of Computer Architecture (ICT, CAS)(grant numbers:CARCHA202007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380698","TransE;distributed computing;workload partitioning;knowledge graph and representation","Computational modeling;Training;Partitioning algorithms;Mathematical model;Parallel processing;Memory management;Prediction algorithms","graph theory;semantic networks","parallel TransE implementation;irregular data distribution;Gather-Apply-Scatter model;high-quality edge partitioning algorithm;distributed computing resources;single-node machines;encoding structured information;translation invariance;translation-based models;low-dimensional embedding space;distributed translating embedding;DTransE;distributed TransE method;power-law graph;knowledge graph partitioning;knowledge graph embeddings","",6.0,"",42.0,"IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"VeriML: Enabling Integrity Assurances and Fair Payments for Machine Learning as a Service","L. Zhao; Q. Wang; C. Wang; Q. Li; C. Shen; B. Feng","Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; School of Cyber Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Khoury College of Computer Sciences, Northeastern University, Boston, MA, USA","IEEE Transactions on Parallel and Distributed Systems","4 May 2021",2021,32.0,10.0,2524,2540,"Machine Learning as a Service (MLaaS) allows clients with limited resources to outsource their expensive ML tasks to powerful servers. Despite the huge benefits, current MLaaS solutions still lack strong assurances on: 1) service correctness (i.e., whether the MLaaS works as expected); 2) trustworthy accounting (i.e., whether the bill for the MLaaS resource consumption is correctly accounted); 3) fair payment (i.e., whether a client gets the entire MLaaS result before making the payment). Without these assurances, unfaithful service providers can return improperly-executed ML task results or partially-trained ML models while asking for over-claimed rewards. Moreover, it is hard to argue for wide adoption of MLaaS to both the client and the service provider, especially in the open market without a trusted third party. In this article, we present VeriML, a novel and efficient framework to bring integrity assurances and fair payments to MLaaS. With VeriML, clients can be assured that ML tasks are correctly executed on an untrusted server, and the resource consumption claimed by the service provider equals to the actual workload. We strategically use succinct non-interactive arguments of knowledge (SNARK) on randomly-selected iterations during the ML training phase for efficiency with tunable probabilistic assurance. We also develop multiple ML-specific optimizations to the arithmetic circuit required by SNARK. Our system implements six common algorithms: linear regression, logistic regression, neural network, support vector machine, K-means and decision tree. The experimental results have validated the practical performance of VeriML.","1558-2183","","10.1109/TPDS.2021.3068195","National Key Research and Development Program of China(grant numbers:2020AAA0107700); National Natural Science Foundation of China(grant numbers:U20B2049,61822207,61572412,61572278,61822309,61773310,U1736205); Research Grants Council of Hong Kong(grant numbers:11217819,11217620); Innovation and Technology Commission of Hong Kong(grant numbers:ITS/145/19); BNRist(grant numbers:BNR2020RC01013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384314","Verifiable computation;machine learning;secure outsourcing","Training;Task analysis;Computational modeling;Servers;Machine learning;Predictive models;Optimization","cloud computing;data mining;decision trees;learning (artificial intelligence);outsourcing;regression analysis;support vector machines","current MLaaS solutions;powerful servers;expensive ML tasks;machine learning;support vector machine;multiple ML-specific optimizations;tunable probabilistic assurance;ML training phase;service provider equals;fair payments;integrity assurances;VeriML;ML models;ML task results;unfaithful service providers;payment;entire MLaaS result;MLaaS resource consumption;strong assurances","",7.0,"",64.0,"IEEE","23 Mar 2021","","","IEEE","IEEE Journals"
"Efficient Data Loader for Fast Sampling-Based GNN Training on Large Graphs","Y. Bai; C. Li; Z. Lin; Y. Wu; Y. Miao; Y. Liu; Y. Xu","School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Anhui Province Key Laboratory of High Performance Computing, Hefei, Anhui, China","IEEE Transactions on Parallel and Distributed Systems","16 Apr 2021",2021,32.0,10.0,2541,2556,"Emerging graph neural networks (GNNs) have extended the successes of deep learning techniques against datasets like images and texts to more complex graph-structured data. By leveraging GPU accelerators, existing frameworks combine mini-batch and sampling for effective and efficient model training on large graphs. However, this setup faces a scalability issue since loading rich vertex features from CPU to GPU through a limited bandwidth link usually dominates the training cycle. In this article, we propose PaGraph, a novel, efficient data loader that supports general and efficient sampling-based GNN training on single-server with multi-GPU. PaGraph significantly reduces the data loading time by exploiting available GPU resources to keep frequently-accessed graph data with a cache. It also embodies a lightweight yet effective caching policy that takes into account graph structural information and data access patterns of sampling-based GNN training simultaneously. Furthermore, to scale out on multiple GPUs, PaGraph develops a fast GNN-computation-aware partition algorithm to avoid cross-partition access during data-parallel training and achieves better cache efficiency. Finally, it overlaps data loading and GNN computation for further hiding loading costs. Evaluations on two representative GNN models, GCN and GraphSAGE, using two sampling methods, Neighbor and Layer-wise, show that PaGraph could eliminate the data loading time from the GNN training pipeline, and achieve up to 4.8× performance speedup over the state-of-the-art baselines. Together with preprocessing optimization, PaGraph further delivers up to 16.0× end-to-end speedup.","1558-2183","","10.1109/TPDS.2021.3065737","National Key R&D Program of China(grant numbers:2018YFB1003204); National Natural Science Foundation of China(grant numbers:61 802 358,61 772 484); USTC Research Funds of the Double First-Class Initiative(grant numbers:YD2150002006); Huawei and Microsoft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376972","Graph neural network;cache;large graph;graph partition;pipeline;multi-GPU","Training;Graphics processing units;Loading;Computational modeling;Load modeling;Partitioning algorithms;Deep learning","cache storage;data mining;data structures;deep learning (artificial intelligence);graph theory;graphics processing units;neural nets;sampling methods","deep learning techniques;complex graph-structured data;GPU accelerators;effective model;training cycle;rich vertex features loading;graph neural networks;fast sampling-based GNN training;efficient data loader;GNN training pipeline;sampling methods;representative GNN models;hiding loading costs;GNN computation;cache efficiency;data-parallel training;fast GNN-computation-aware partition algorithm;data access patterns;account graph structural information;lightweight yet effective caching policy;graph data;available GPU resources;data loading time;PaGraph;multiGPU","",2.0,"",60.0,"IEEE","12 Mar 2021","","","IEEE","IEEE Journals"
"Virtualization Overhead of Multithreading in X86 State-of-the-Art & Remaining Challenges","S. Schildermans; J. Shan; K. Aerts; J. Jackrel; X. Ding","Department of Computer Science, KU Leuven, Diepenbeek, Limburg, Belgium; Department of Computer Science, Hofstra University, Heampstead, NY, USA; Department of Computer Science, KU Leuven, Diepenbeek, Limburg, Belgium; Department of Computer Science, Hofstra University, Heampstead, NY, USA; Department of Computer Science, New Jersey Institute of Technology, Newark, NJ, USA","IEEE Transactions on Parallel and Distributed Systems","5 May 2021",2021,32.0,10.0,2557,2570,"Despite great advancements in hardware-assisted virtualization of the x86 architecture, certain workloads still suffer significant overhead. This article dissects said overhead in the context of multi-threading. We describe the state-of-the-art, pinpoint challenges, and suggest improvements, aiming to provide a valuable reference to developers and users of virtualization systems alike. We study the virtualization overhead of the PARSEC and SPLASH2X multithreaded benchmarks in a variety of scenarios using a state-of-the-art system. Through controlled experiments, source code analysis and literature review, we quantify the virtualization overhead multithreading still induces and link it to its root causes, after which we suggest possible mitigation strategies. Multithreading still induces high virtualization overhead, mainly caused by synchronization, spinning at user level and NUMA management. The overhead is diverse in nature and embodiment as it is a function of many system and workload properties. System-level solutions are feasible, but often imply difficult trade-offs. Systematic workload optimization is a promising alternative.","1558-2183","","10.1109/TPDS.2021.3064709","National Science Foundation(grant numbers:CCF 1617749); Flemish FWO(grant numbers:V433819N); KU Leuven(grant numbers:19005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373922","Multi-threading;virtualization;overhead;performance;guidelines;classification","Virtualization;Virtual machine monitors;Hardware;Synchronization;Multithreading;Spinning;Degradation","multiprocessing systems;multi-threading;optimisation;virtualisation","systematic workload optimization;NUMA management;source code analysis;PARSEC multithreaded benchmarks;SPLASH2X multithreaded benchmarks;x86 architecture;system-level solutions;workload properties;high virtualization overhead;virtualization overhead multithreading;virtualization systems;pinpoint challenges;hardware-assisted virtualization","",4.0,"",66.0,"IEEE","9 Mar 2021","","","IEEE","IEEE Journals"
"Data Life Aware Model Updating Strategy for Stream-Based Online Deep Learning","W. Rang; D. Yang; D. Cheng; Y. Wang","Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA","IEEE Transactions on Parallel and Distributed Systems","7 May 2021",2021,32.0,10.0,2571,2581,"Many deep learning applications deployed in dynamic environments change over time, in which the training models are supposed to be continuously updated with streaming data to guarantee better descriptions of data trends. However, most state-of-the-art learning frameworks support well in offline training methods while omitting online model updating strategies. In this work, we propose and implement iDlaLayer, a thin middleware layer on top of existing training frameworks that streamlines the support and implementation of online deep learning applications. In pursuit of good model quality and fast data incorporation, we design a Data Life Aware model updating strategy (DLA), which builds training data samples according to contributions of data from different life stages, and considers the training cost consumed in model updating. We evaluate iDlaLayer's performance through simulations and experiments based on TensorflowOnSpark with three representative online learning workloads. Our experimental results demonstrate that iDlaLayer reduces the overall elapsed time of ResNet, DeepFM and PageRank by 11.3, 28.2, and 15.2 percent compared to the periodic update strategy, respectively. It further achieves an average 20 percent decrease in training cost and brings about a 5 percent improvement in model quality against the traditional continuous training method.","1558-2183","","10.1109/TPDS.2021.3071939","National Science Foundation(grant numbers:CCF-1908843,CNS-2008265); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399304","Online learning;model updating strategy;data life cycle","Training;Data models;Predictive models;Distributed databases;Deep learning;Middleware;Inference algorithms","","","",2.0,"",35.0,"IEEE","8 Apr 2021","","","IEEE","IEEE Journals"
"LightChain: Scalable DHT-Based Blockchain","Y. Hassanzadeh-Nazarabadi; A. Küpçü; Ö. Özkasap","Department of Computer Engineering, Koç University, İstanbul, Turkey; Department of Computer Engineering, Koç University, İstanbul, Turkey; Department of Computer Engineering, Koç University, İstanbul, Turkey","IEEE Transactions on Parallel and Distributed Systems","5 May 2021",2021,32.0,10.0,2582,2593,"As an append-only distributed database, blockchain is utilized in a vast variety of applications including the cryptocurrency and Internet-of-Things (IoT). The existing blockchain solutions show downsides in communication and storage scalability, as well as decentralization. In this article, we propose LightChain, which is the first blockchain architecture that operates over a Distributed Hash Table (DHT) of participating peers. LightChain is a permissionless blockchain that provides addressable blocks and transactions within the network, which makes them efficiently accessible by all peers. Each block and transaction is replicated within the DHT of peers and is retrieved in an on-demand manner. Hence, peers in LightChain are not required to retrieve or keep the entire ledger. LightChain is fair as all of the participating peers have a uniform chance of being involved in the consensus regardless of their influence such as hashing power or stake. We provide formal mathematical analysis and experimental results (simulations and cloud deployment) to demonstrate the security, efficiency, and fairness of LightChain, and show that LightChain is the only existing blockchain that can provide integrity under the corrupted majority power of peers. As we experimentally demonstrate, compared to the mainstream blockchains such as Bitcoin and Ethereum, LightChain requires around 66 times smaller per node storage, and is around 380 times faster on bootstrapping a new node to the system, and each LightChain node is rewarded equally likely for participating in the protocol.","1558-2183","","10.1109/TPDS.2021.3071176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397334","Blockchain;permissionless;DHT;consensus;storage efficiency;communication efficiency;scalability;skip graph","Peer-to-peer computing;Blockchain;Complexity theory;Decision making;Security;Time complexity;Scalability","blockchains;cryptographic protocols;mathematical analysis;statistical analysis","protocol;bootstrapping;Ethereum;Bitcoin;Internet-of-Things;IoT;scalable DHT-based blockchain architecture;permissionless blockchain;distributed hash table;storage scalability;append-only distributed database;LightChain node","",11.0,"",53.0,"IEEE","6 Apr 2021","","","IEEE","IEEE Journals"
"Decentralized Dual Proximal Gradient Algorithms for Non-Smooth Constrained Composite Optimization Problems","H. Li; J. Hu; L. Ran; Z. Wang; Q. Lü; Z. Du; T. Huang","Chongqing Key Laboratory of Nonlinear Circuits and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China; Chongqing Key Laboratory of Nonlinear Circuits and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China; Chongqing Key Laboratory of Nonlinear Circuits and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; College of Computer, Chongqing University, Chongqing, China; Chongqing Key Laboratory of Nonlinear Circuits and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China; Science Program, Texas A&M University at Qatar, Doha, Qatar","IEEE Transactions on Parallel and Distributed Systems","5 May 2021",2021,32.0,10.0,2594,2605,"Decentralized dual methods play significant roles in large-scale optimization, which effectively resolve many constrained optimization problems in machine learning and power systems. In this article, we focus on studying a class of totally non-smooth constrained composite optimization problems over multi-agent systems, where the mutual goal of agents in the system is to optimize a sum of two separable non-smooth functions consisting of a strongly-convex function and another convex (not necessarily strongly-convex) function. Agents in the system conduct parallel local computation and communication in the overall process without leaking their private information. In order to resolve the totally non-smooth constrained composite optimization problem in a fully decentralized manner, we devise a synchronous decentralized dual proximal (SynDe-DuPro) gradient algorithm and its asynchronous version (AsynDe-DuPro) based on the randomized block-coordinate method. Both SynDe-DuPro and AsynDe-DuPro algorithms are theoretically proved to achieve the globally optimal solution to the totally non-smooth constrained composite optimization problem relied on the quasi-Fejér monotone theorem. As a main result, AsynDe-DuPro algorithm attains the globally optimal solution without requiring all agents to be activated at each iteration and thus is more robust than most existing synchronous algorithms. The practicability of the proposed algorithms and correctness of the theoretical findings are demonstrated by the experiments on a constrained Decentralized Sparse Logistic Regression (DSLR) problem in machine learning and a Decentralized Energy Resources Coordination (DERC) problem in power systems.","1558-2183","","10.1109/TPDS.2021.3072373","Fundamental Research Funds for the Central Universities(grant numbers:XDJK2019AC001); Venture and Innovation Support Program for Chongqing Overseas Returnees(grant numbers:cx2019005); National Natural Science Foundation of China(grant numbers:61773321); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9400765","Convex optimization;synchronous and asynchronous decentralized algorithms;multi-agent systems;non-smooth constrained composite optimization problems;decentralized machine learning;DSLR problems;DERC problems","Optimization;Signal processing algorithms;Power systems;Machine learning;Machine learning algorithms;Linear programming;Multi-agent systems","convex programming;gradient methods;iterative methods;learning (artificial intelligence);multi-agent systems;power systems;regression analysis;synchronisation","nonsmooth constrained composite optimization problem;constrained optimization problems;AsynDe-DuPro algorithm;globally optimal solution;constrained decentralized sparse logistic regression problem;decentralized dual proximal gradient algorithms;strongly-convex function;DSLR problem;DERC problem;decentralized energy resource coordination problem;machine learning;randomized block-coordinate method;quasi-Fejér monotone theorem;asynchronous version","",10.0,"",44.0,"IEEE","12 Apr 2021","","","IEEE","IEEE Journals"
"Analysis of Global and Local Synchronization in Parallel Computing","F. Cicirelli; A. Giordano; C. Mastroianni","ICAR-CNR, Rende, Italy; ICAR-CNR, Rende, Italy; ICAR-CNR, Rende, Italy","IEEE Transactions on Parallel and Distributed Systems","1 Dec 2020",2021,32.0,5.0,988,1000,"In a parallel computing scenario, the synchronization overhead, needed to coordinate the execution on the parallel computing nodes, can significantly impair the overall execution performance. Typically, synchronization is achieved by adopting a global synchronization schema involving all the nodes. In many application domains, though, a looser synchronization schema, namely, local synchronization, can be exploited, in which each node needs to synchronize only with a subset of the other nodes. In this work, we compare the performance of global and local synchronization using the efficiency, i.e., the ratio between the useful computing time and the total computing time, including the synchronization overhead, as a key performance indicator. We present an analytical study of the asymptotic behavior of the efficiency when the number of nodes increases. As an original contribution, we prove, using the Max-Plus algebra, that there is a non-zero lower bound on the efficiency in the case of local synchronization and we present a statistical procedure to find a value of this bound. This outcome marks a significant advantage of local synchronization with respect to global synchronization, for which the efficiency tends to zero when increasing the number of nodes.","1558-2183","","10.1109/TPDS.2020.3037469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9257592","Parallel computing;efficiency;synchronization","Algebra;Computational modeling;Simulation;Parallel processing;Probabilistic logic;Random variables;Synchronization","algebra;parallel processing;statistical analysis;synchronisation","statistical procedure;Max-Plus algebra;key performance indicator;global synchronization;synchronization overhead;parallel computing;local synchronization","",6.0,"",63.0,"IEEE","12 Nov 2020","","","IEEE","IEEE Journals"
"Boosting Parallel Influence-Maximization Kernels for Undirected Networks With Fusing and Vectorization","G. Göktürk; K. Kaya","Computer Science and Engineering, Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, TR, Turkey; Computer Science and Engineering, Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, TR, Turkey","IEEE Transactions on Parallel and Distributed Systems","1 Dec 2020",2021,32.0,5.0,1001,1013,"Influence maximization (IM) is the problem of finding a seed vertex set which is expected to incur the maximum influence spread on a graph. It has various applications in practice such as devising an effective and efficient approach to disseminate information, news or ad within a social network. The problem is shown to be NP-hard and approximation algorithms with provable quality guarantees exist in the literature. However, these algorithms are computationally expensive even for medium-scaled graphs. Furthermore, graph algorithms usually suffer from spatial and temporal irregularities during memory accesses, and this adds an extra cost on top of the already expensive IM kernels. In this article we leverage fused sampling, memoization, and vectorization to restructure, parallelize and boost their performance on undirected networks. The proposed approach employs a pseudo-random function and performs multiple Monte-Carlo simulations in parallel to exploit the SIMD lanes effectively and efficiently. In addition, it significantly reduces the number of edge traversals, hence the amount of data brought from the memory, which is critical for almost all memory-bound graph kernels. We apply the proposed approach to the traditional MIXGREEDY algorithm and propose INFUSER-MG which is more than 3000χ fasterthan the greedy approaches and can run on large graphs that have been considered as too large in the literature. For instance, the new algorithm runs in 2.09, 0.08, 0.36 seconds on graphs Amazon, NetHEP, NetPhy with 16 threads where the sequential baseline takes 141.3, 259.1 and 1725.2 seconds, respectively. To compare INFUSER-MG with the state-of-the-art approximation algorithms, we conduct a thorough experimental analysis with various influence settings. The results on real-life, undirected networks show that on 16 threads, INFUSER-MG is 2:3χ-173:8χ faster than state-of-the-art while being superior in terms of influence scores, and using a comparable amount of memory.","1558-2183","","10.1109/TPDS.2020.3038376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9261128","","Monte Carlo methods;Social networking (online);Instruction sets;Memory management;Approximation algorithms;Boosting;Kernel","computational complexity;directed graphs;greedy algorithms;Monte Carlo methods;network theory (graphs);optimisation;parallel algorithms;set theory","undirected networks;vectorization;influence maximization;seed vertex set;NP hard problem;graph algorithms;pseudorandom function;parallel Monte Carlo simulations;INFUSER-MG;MIXGREEDY algorithm;memory bound graph kernels;parallel influence maximization kernels;fused sampling;SIMD lanes;instruction level parallelism","",3.0,"",38.0,"IEEE","16 Nov 2020","","","IEEE","IEEE Journals"
"Transformations of High-Level Synthesis Codes for High-Performance Computing","J. de Fine Licht; M. Besta; S. Meierhans; T. Hoefler","Department of Computer Science, ETH Zurich, Universita, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Universita, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Universita, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Universita, Zürich, Switzerland","IEEE Transactions on Parallel and Distributed Systems","8 Dec 2020",2021,32.0,5.0,1014,1029,"Spatial computing architectures promise a major stride in performance and energy efficiency over the traditional load/store devices currently employed in large scale computing systems. The adoption of high-level synthesis (HLS) from languages such as C++ and OpenCL has greatly increased programmer productivity when designing for such platforms. While this has enabled a wider audience to target spatial computing architectures, the optimization principles known from traditional software design are no longer sufficient to implement high-performance codes, due to fundamentally distinct aspects of hardware design, such as programming for deep pipelines, distributed memory resources, and scalable routing. To alleviate this, we present a collection of optimizing transformations for HLS, targeting scalable and efficient architectures for high-performance computing (HPC) applications. We systematically identify classes of transformations (pipelining, scalability, and memory), the characteristics of their effect on the HLS code and the resulting hardware (e.g., increasing data reuse or resource consumption), and the objectives that each transformation can target (e.g., resolve interface contention, or increase parallelism). We show how these can be used to efficiently exploit pipelining, on-chip distributed fast memory, and on-chip dataflow, allowing for massively parallel architectures. To quantify the effect of various transformations, we cover the optimization process of a sample set of HPC kernels, provided as open source reference codes. We aim to establish a common toolbox to guide both performance engineers and compiler engineers in tapping into the performance potential offered by spatial computing architectures using HLS.","1558-2183","","10.1109/TPDS.2020.3039409","European Research Council(grant numbers:678880); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264692","Parallel architectures;parallel programming;high performance computing","Pipeline processing;Hardware;Computer architecture;Optimization;Software;Performance evaluation;Registers","high level synthesis;logic design;parallel architectures;pipeline processing;power aware computing;program compilers;reconfigurable architectures","compiler engineers;on-chip dataflow;high-level synthesis code transformations;open source reference codes;massively parallel architectures;on-chip distributed fast memory;HLS code;high-performance computing applications;distributed memory resources;hardware design;high-performance codes;software design;scale computing systems;energy efficiency;spatial computing architectures;performance engineers","",30.0,"",103.0,"IEEE","19 Nov 2020","","","IEEE","IEEE Journals"
"Petrel: Heterogeneity-Aware Distributed Deep Learning Via Hybrid Synchronization","Q. Zhou; S. Guo; Z. Qu; P. Li; L. Li; M. Guo; K. Wang","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Computer Science and Engineering, The University of Aizu, Fukushima-ken, Japan; School of Software, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical and Computer Engineering, University of California, Los Angeles, Los Angeles, CA, USA","IEEE Transactions on Parallel and Distributed Systems","11 Dec 2020",2021,32.0,5.0,1030,1043,"The parameter server (PS) paradigm has achieved great success in deploying large-scale distributed Deep Learning (DL) systems. However, these systems implicitly assume that the cluster is homogeneous and this assumption does not hold in many realworld cases. Although the previous efforts are paid to address heterogeneity, they mainly prioritize the contribution of fast workers and reduce the involvement of slow workers, resulting in the limitations of workload imbalance and computation inefficiency. We reveal that grouping workers into communities, an abstraction proposed by us, and handling parameter synchronization at the community level can conquer these limitations and accelerate the training convergence progress. The inspiration of community comes from our exploration of prior knowledge about the similarity between workers, which is often neglected by previous work. These observations motivate us to propose a new synchronization mechanism named Community-aware Synchronous Parallel (CASP), which uses the Asynchronous Advantage Actor-Critic (A3C)-based algorithm to intelligently determine community configuration and fully improve the synchronization performance. The whole idea has been implemented in a prototype system called Petrel that achieves a good balance between convergence efficiency and communication overhead. The evaluation under various benchmarks with multiple metrics and baseline comparison demonstrates the effectiveness of Petrel. Specifically, Petrel accelerates the training convergence speed by up to 1.87 x faster and reduces communication traffic by up to 26.85 percent, on average, over the non-community synchronization mechanisms.","1558-2183","","10.1109/TPDS.2020.3040601","Hong Kong RGC Research Impact Fund(grant numbers:R5060-19,R5034-18); General Research Fund(grant numbers:152221/19E); Collaborative Research Fund(grant numbers:C5026-18G); National Natural Science Foundation of China(grant numbers:61872310); China Postdoctoral Science Foundation(grant numbers:2019M661709); Shanghai Trusted Industrial Control Platform; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9271915","Distributed systems;deep learning;parameter server;heterogeneous environment","Synchronization;Training;Convergence;Servers;Task analysis;Acceleration;Measurement","learning (artificial intelligence);neural nets;synchronisation","heterogeneity-aware distributed deep learning;community configuration;Asynchronous Advantage Actor-Critic-based algorithm;Community-aware Synchronous Parallel;parameter synchronization;parameter server paradigm;hybrid synchronization;Petrel","",10.0,"",52.0,"IEEE","25 Nov 2020","","","IEEE","IEEE Journals"
"Thermal Prediction for Efficient Energy Management of Clouds Using Machine Learning","S. Ilager; K. Ramamohanarao; R. Buyya","Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia","IEEE Transactions on Parallel and Distributed Systems","11 Dec 2020",2021,32.0,5.0,1044,1056,"Thermal management in the hyper-scale cloud data centers is a critical problem. Increased host temperature creates hotspots which significantly increases cooling cost and affects reliability. Accurate prediction of host temperature is crucial for managing the resources effectively. Temperature estimation is a non-trivial problem due to thermal variations in the data center. Existing solutions for temperature estimation are inefficient due to their computational complexity and lack of accurate prediction. However, data-driven machine learning methods for temperature prediction is a promising approach. In this regard, we collect and study data from a private cloud and show the presence of thermal variations. We investigate several machine learning models to accurately predict the host temperature. Specifically, we propose a gradient boosting machine learning model for temperature prediction. The experiment results show that our model accurately predicts the temperature with the average RMSE value of 0.05 or an average prediction error of 2.38 °C, which is 6 °C less as compared to an existing theoretical model. In addition, we propose a dynamic scheduling algorithm to minimize the peak temperature of hosts. The results show that our algorithm reduces the peak temperature by 6.5 °C and consumes 34.5 percent less energy as compared to the baseline algorithm.","1558-2183","","10.1109/TPDS.2020.3040800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272657","Cloud computing;machine learning;energy efficiency in a data center;datacenter cooling;hotspots","Temperature distribution;Data centers;Predictive models;Cloud computing;Data models;Temperature sensors;Cooling","cloud computing;computer centres;cooling;energy consumption;energy management systems;gradient methods;learning (artificial intelligence);power aware computing;scheduling","dynamic scheduling;energy consumption;gradient boosting machine learning;cooling cost;private cloud;data-driven machine learning;computational complexity;thermal variations;temperature estimation;host temperature;hyper-scale cloud data centers;thermal management;energy management;thermal prediction;peak temperature;average prediction error;temperature prediction;temperature 6.5 degC;efficiency 34.5 percent","",21.0,"",42.0,"IEEE","26 Nov 2020","","","IEEE","IEEE Journals"
"IPPTS: An Efficient Algorithm for Scientific Workflow Scheduling in Heterogeneous Computing Systems","H. Djigal; J. Feng; J. Lu; J. Ge","School of Computer and Information, Hohai University, Nanjing, China; School of Computer and Information, Hohai University, Nanjing, China; School of Computer and Information, Hohai University, Nanjing, China; Software Institute, Nanjing University, Nanjing, China","IEEE Transactions on Parallel and Distributed Systems","16 Dec 2020",2021,32.0,5.0,1057,1071,"Efficient scheduling algorithms are key for attaining high performance in heterogeneous computing systems. In this article, we propose a new list scheduling algorithm for assigning task graphs to fully connected heterogeneous processors with an aim to minimize the scheduling length. The proposed algorithm, called Improved Predict Priority Task Scheduling (IPPTS) algorithm has two phases: task prioritization phase, which gives priority to tasks, and processor selection phase, which selects a processor for a task. The IPPTS algorithm has a quadratic time complexity as the related algorithms for the same goal, that is $O(t^{2} \times p)$O(t2×p), for $t$t tasks and $p$p processors. Our algorithm reduces the scheduling length significantly by looking ahead in both task prioritization phase and processor selection phase. In this way, the algorithm is looking ahead to schedule a task and its heaviest successor task to the optimistic processor, i.e., the processor that minimizes their computation and communication costs. The experiments based on both randomly generated graphs and graphs of real-world applications show that the IPPTS algorithm significantly outperforms previous list scheduling algorithms in terms of makespan, speedup, makespan standard deviation, efficiency, and frequency of best results.","1558-2183","","10.1109/TPDS.2020.3041829","National Key R&D Program of China(grant numbers:2018YFC0407901); National Natural Science Foundation of China(grant numbers:61370091,61602151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9276443","Workflow scheduling;list scheduling;random graphs generator;scientific workflows;heterogeneous systems","Task analysis;Program processors;Scheduling algorithms;Prediction algorithms;Heuristic algorithms;Heterogeneous networks;Dynamic scheduling","","","",23.0,"",41.0,"IEEE","2 Dec 2020","","","IEEE","IEEE Journals"
"Privacy-Preserving Similarity Search With Efficient Updates in Distributed Key-Value Stores","W. Lin; H. Cui; B. Li; C. Wang","Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Department of Computer Science, City University of Hong Kong, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","16 Dec 2020",2021,32.0,5.0,1072,1084,"Privacy-preserving similarity search plays an essential role in data analytics, especially when very large encrypted datasets are stored in the cloud. Existing mechanisms on privacy-preserving similarity search were not able to support secure updates (addition and deletion) efficiently when frequent updates are needed. In this article, we propose a new mechanism to support parallel privacypreserving similarity search in a distributed key-value store in the cloud, with a focus on efficient addition and deletion operations, both executed with sublinear time complexity. If search accuracy is the top priority, we further leverage Yao's garbled circuits and the homomorphic property of Hash-ElGamal encryption to build a secure evaluation protocol, which can obtain the top-R most accurate results without extensive client-side post-processing. We have formally analyzed the security strength of our proposed approach, and performed an extensive array of experiments to show its superior performance as compared to existing mechanisms in the literature. In particular, we evaluate the performance of our proposed protocol with respect to the time it takes to build the index and perform similarity queries. Extensive experimental results demonstrated that our protocol can speedup the index building process by up to 800x with 2 threads and the similarity queries by up to -7x with comparable accuracy, as compared to the state-of-the-art in the literature.","1558-2183","","10.1109/TPDS.2020.3042695","Natural Science Basic Research Program of Shaanxi(grant numbers:2020JQ-215); Research Grants Council of Hong Kong(grant numbers:CityU 11217819,CityU 11217620); Fundamental Research Funds for the Central Universities(grant numbers:3102019QD1001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288779","Searchable symmetric encryption;key-value stores;data privacy;similarity search;cloud storage;efficient updates","Cryptography;Servers;Indexes;Protocols;Cloud computing;Encryption;Search problems","cloud computing;computational complexity;cryptographic protocols;data privacy;file organisation;query processing","similarity queries;efficient updates;distributed key-value store;secure updates;deletion operations;search accuracy;parallel privacy preserving similarity search;homomorphic property;Hash-ElGamal encryption;secure evaluation protocol;client-side post-processing;security strength;index building process","",1.0,"",36.0,"IEEE","9 Dec 2020","","","IEEE","IEEE Journals"
"Distributed and Collective Deep Reinforcement Learning for Computation Offloading: A Practical Perspective","X. Qiu; W. Zhang; W. Chen; Z. Zheng","School of Data and Computer Science, National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","16 Dec 2020",2021,32.0,5.0,1085,1101,"Mobile edge computing (MEC) is a promising solution to support resource-constrained devices by offloading tasks to the edge servers. However, traditional approaches (e.g., linear programming and game-theory methods) for computation offloading mainly focus on the immediate performance, potentially leading to performance degradation in the long run. Recent breakthroughs regarding deep reinforcement learning (DRL) provide alternative methods, which focus on maximizing the cumulative reward. Nonetheless, there exists a large gap to deploy real DRL applications in MEC. This is because: 1) training a well-performed DRL agent typically requires data with large quantities and high diversity, and 2) DRL training is usually accompanied by huge costs caused by trial-and-error. To address this mismatch, we study the applications of DRL on the multi-user computation offloading problem from a more practical perspective. In particular, we propose a distributed and collective DRL algorithm called DC-DRL with several improvements: 1) a distributed and collective training scheme that assimilates knowledge from multiple MEC environments, which not only greatly increases data amount and diversity but also spreads the exploration costs, 2) an updating method called adaptive n-step learning, which can improve training efficiency without suffering from the high variance caused by distributed training, and 3) combining the advantages of deep neuroevolution and policy gradient to maximize the utilization of multiple environments and prevent the premature convergence. Lastly, evaluation results demonstrate the effectiveness of our proposed algorithm. Compared with the baselines, the exploration costs and final system costs are reduced by at least 43 and 9.4 percent, respectively.","1558-2183","","10.1109/TPDS.2020.3042599","National Key Research and Development Plan(grant numbers:2018YFB1003803); National Natural Science Foundation of China(grant numbers:61802450,61722214); Natural Science Foundation of Guangdong Province(grant numbers:2018A030313005); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Guangdong Provincial Pearl River Talents Program(grant numbers:2019QN01X130); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288861","Mobile edge computing;computation offloading;deep reinforcement learning;distributed and collective training;n-step return;deep neuroevolution;policy gradient","Training;Task analysis;Computational modeling;Optimization;Resource management;Performance evaluation;Games","cloud computing;learning (artificial intelligence);mobile computing;neural nets","DC-DRL;MEC environments;exploration costs;updating method;adaptive n-step learning;deep neuroevolution;policy gradient;mobile edge computing;resource-constrained devices;edge servers;cumulative reward;DRL agent;multiuser computation offloading;DRL training;distributed and collective DRL algorithm;distributed and collective deep reinforcement learning;distributed and collective training scheme","",29.0,"",42.0,"IEEE","9 Dec 2020","","","IEEE","IEEE Journals"
"Subutai: Speeding Up Legacy Parallel Applications Through Data Synchronization","R. Cataldo; R. Fernandes; K. J. M. Martin; J. Silveira; G. Sanchez; J. Sepúlveda; C. Marcon; J. -P. Diguet","School of Technology, Pontificia Universidade Catolica do Rio Grande do Sul (PUCRS), Porto Alegre, RS, Brazil; Faculty of Informatics, Pontificia Universidade Catolica do Rio Grande do Sul, Porto Alegre, RS, Brazil; Lab-STICC, Université Bretagne Sud (UBS), Lorient, France; Department of Computer Science, Universidade Federal do Ceará, Fortaleza, CE, Brazil; School of Technology, Pontificia Universidade Catolica do Rio Grande do Sul (PUCRS), Porto Alegre, RS, Brazil; Lehrstuhl für Sicherheit in der Informationstechnik, Technical University of Munich, München, Germany; School of Technology, Pontificia Universidade Catolica do Rio Grande do Sul (PUCRS), Porto Alegre, RS, Brazil; Lab-STICC, CNRS / UBS University, Lorient, France","IEEE Transactions on Parallel and Distributed Systems","16 Dec 2020",2021,32.0,5.0,1102,1116,"The decrease of the performance gain dictated by Moore's Law boosted the development of manycore architectures to replace single-core architectures. These new architectures must employ parallel applications and distribute its workload over a multitude of cores to reach the desired performance. Parallel applications are harder to develop than sequential ones since the developer must guarantee data integrity using synchronization primitives. While multiple novel solutions have been proposed to speed up parallel applications through handling one type of data synchronization primitive, exceptionally few works support multiple types of synchronization primitives and legacy code. This article proposes Subutai, a hardware/software co-design solution for accelerating multiple synchronization primitives without modifying the application source code. By providing a new user library, while retaining an existing synchronization API, legacy and novel applications can benefit from our solution. Our experimental evaluation, which provides a POSIX Threads implementation, demonstrates Subutai speeds up to 2.71× and 4.61× the execution of single- and multiple-application executions, respectively.","1558-2183","","10.1109/TPDS.2020.3040066","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268118","Legacy parallel applications;PThreads;network-on-chip;distributed scheduler","Synchronization;Software;Libraries;Hardware;Programming;Optimization;Computational modeling","application program interfaces;data integrity;hardware-software codesign;multi-threading;parallel processing;software libraries;source code (software);synchronisation;Unix","Subutai;legacy parallel applications;manycore architectures;single core architectures;data integrity;data synchronization;application source code;multiple application executions;hardware/software co-design solution;user library;synchronization API;POSIX Threads","","","",40.0,"IEEE","24 Nov 2020","","","IEEE","IEEE Journals"
"Design and Implementation of a Criticality- and Heterogeneity-Aware Runtime System for Task-Parallel Applications","M. Han; J. Park; W. Baek","Department of Computer Science and Engineering, UNIST, Ulsan, Republic of Korea; Department of Computer Science and Engineering, UNIST, Ulsan, Republic of Korea; Department of Computer Science and Engineering, Graduate School of Artificial Intelligence, UNIST, Ulsan, Republic of Korea","IEEE Transactions on Parallel and Distributed Systems","24 Dec 2020",2021,32.0,5.0,1117,1132,"Heterogeneous multiprocessing (HMP) is an emerging technology for high-performance and energy-efficient computing. While task parallelism is widely used in various computing domains, such as embedded, big-data, and machine-learning computing domains, it still remains unexplored to investigate the efficient runtime support that effectively utilizes the criticality of the tasks of the target application and the heterogeneity of the underlying HMP system with full resource management. To bridge this gap, we propose CHRT, a criticality- and heterogeneity-aware runtime system for task-parallel applications. CHRT dynamically estimates the performance and power consumption of the target task-parallel application and robustly manages the full HMP system resources (i.e., core types, counts, and voltage/frequency levels) to maximize the overall efficiency. Our quantitative evaluation based on widely-used task parallel benchmarks and two full HMP systems (i.e., the XU3 and HiKey970 HMP systems) demonstrates the effectiveness of CHRT in that CHRT achieves significantly higher energy (e.g., 60.4 and 57.2 percent on average on the XU3 system) and energy-delay product (e.g., 52.2 and 44.0 percent on average on the HiKey970 system) efficiency than the baseline runtime system that employs the breadth-first scheduler and the state-of-the-art criticality-aware runtime system and incurs low performance overheads.","1558-2183","","10.1109/TPDS.2020.3031911","Institute of Information & Communications Technology Planning & Evaluation(grant numbers:2018-0-00769); National Research Foundation of Korea(grant numbers:NRF-2018R1C1B6005961); Institute of Information & Communications Technology Planning & Evaluation(grant numbers:2020-0-01336); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9266082","Criticality- and heterogeneity-aware runtime system;task-parallel applications","Task analysis;Runtime;Parallel processing;Heart beat;Benchmark testing;Prototypes;Power demand","energy conservation;parallel processing;power aware computing;power consumption;resource allocation;scheduling","CHRT;task parallel applications;HMP system resources;HiKey970 HMP systems;XU3 system;HiKey970 system;heterogeneous multiprocessing;energy efficient computing;criticality aware runtime system;heterogeneity aware runtime system;high performance computing;power consumption;breadth first scheduler","",3.0,"",45.0,"IEEE","23 Nov 2020","","","IEEE","IEEE Journals"
"Multi-Hop Multi-Task Partial Computation Offloading in Collaborative Edge Computing","Y. Sahni; J. Cao; L. Yang; Y. Ji","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Software Engineering, South China University of Technology, Guangzhou, China; Department of Informatics, SOKENDAI (The Graduate University for Advanced Studies), Tokyo, Japan","IEEE Transactions on Parallel and Distributed Systems","16 Dec 2020",2021,32.0,5.0,1133,1145,"Collaborative edge computing (CEC) is a recent popular paradigm where different edge devices collaborate by sharing data and computation resources. One of the fundamental issues in CEC is to make task offloading decision. However, it is a challenging problem to solve as tasks can be offloaded to a device at multi-hop distance leading to conflicting network flows due to limited bandwidth constraint. There are some works on multi-hop computation offloading problem in the literature. However, existing works have not jointly considered multi-hop partial computation offloading and network flow scheduling that can cause network congestion and inefficient performance in terms of completion time. This article formulates the joint multi-task partial computation offloading and network flow scheduling problem to minimize the average completion time of all tasks. The formulated problem optimizes several dependent decision variables including partial offloading ratio, remote offloading device, start time of tasks, routing path, and start time of network flows. The problem is formulated as an MINLP optimization problem and shown to be NP-hard. We propose a joint partial offloading and flow scheduling heuristic (JPOFH) that decides partial offloading ratio by considering both waiting times at the devices and start time of network flows. We also do the relaxation of formulated MINLP problem to an LP problem using McCormick envelope to give a lower bound solution. Performance comparison done using simulation shows that JPOFH leads to up to 32 percent improvement in average completion time compared to benchmark solutions which do not make a joint decision.","1558-2183","","10.1109/TPDS.2020.3042224","Research Grants Council(grant numbers:PolyU 15217919); RGC Research Impact Fund(grant numbers:R5034-18); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9279235","Scheduling and task partitioning;network flow scheduling;collaborative edge computing;Internet of Things","Task analysis;Processor scheduling;Collaboration;Edge computing;Spread spectrum communication;Routing;Cloud computing","computational complexity;Internet of Things;optimisation;scheduling","MINLP optimization problem;multihop multitask partial computation offloading;collaborative edge computing;CEC;task offloading decision;network flow scheduling problem;joint partial offloading and flow scheduling heuristic;JPOFH;Internet of Things;NP-hard","",28.0,"",35.0,"IEEE","3 Dec 2020","","","IEEE","IEEE Journals"
"A Scalable Multi-Layer PBFT Consensus for Blockchain","W. Li; C. Feng; L. Zhang; H. Xu; B. Cao; M. A. Imran","James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom; James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom; James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom; James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom; Beijing University of Posts and Telecommunications, Beijing, China; James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom","IEEE Transactions on Parallel and Distributed Systems","24 Dec 2020",2021,32.0,5.0,1146,1160,"Practical Byzantine Fault Tolerance (PBFT) consensus mechanism shows a great potential to break the performance bottleneck of the Proof-of-Work (PoW)-based blockchain systems, which typically support only dozens of transactions per second and require minutes to hours for transaction confirmation. However, due to frequent inter-node communications, PBFT mechanism has a poor node scalability and thus it is typically adopted in small networks. To enable PBFT in large systems such as massive Internet of Things (IoT) ecosystems and blockchain, in this article, a scalable multi-layer PBFT-based consensus mechanism is proposed by hierarchically grouping nodes into different layers and limiting the communication within the group. We first propose an optimal double-layer PBFT and show that the communication complexity is significantly reduced. Specifically, we prove that when the nodes are evenly distributed within the sub-groups in the second layer, the communication complexity is minimized. The security threshold is analyzed based on faulty probability determined (FPD) and faulty number determined (FND) models, respectively. We also provide a practical protocol for the proposed double-layer PBFT system. Finally, the results are extended to arbitrary-layer PBFT systems with communication complexity and security analysis. Simulation results verify the effectiveness of the analytical results.","1558-2183","","10.1109/TPDS.2020.3042392","UK EPSRC(grant numbers:EP/S02476X/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9279277","PBFT;communication complexity;node scalability;consensus mechanism;blockchain","Blockchain;Complexity theory;Scalability;Security;Peer-to-peer computing;Throughput;Internet of Things","communication complexity;cryptocurrencies;cryptographic protocols;fault tolerance;probability","security analysis;faulty number determined models;faulty probability determined models;security threshold;hierarchically grouping nodes;scalable multilayer PBFT consensus;multilayer PBFT-based consensus mechanism;Internet of Things ecosystems;poor node scalability;frequent inter-node communications;transaction confirmation;proof-of-work-based blockchain systems;practical Byzantine fault tolerance consensus mechanism;communication complexity;arbitrary-layer PBFT systems;double-layer PBFT system","",83.0,"",34.0,"CCBY","3 Dec 2020","","","IEEE","IEEE Journals"
"Efficient Buffer Overflow Detection on GPU","B. Di; J. Sun; H. Chen; D. Li","College of Computer Science and Electrical Engineering, Hunan University, Changsha, China; College of Computer Science and Electrical Engineering, Hunan University, Changsha, China; College of Computer Science and Electrical Engineering, Hunan University, Changsha, China; The Department of Electrical Engineering and Computer Science, University of California Merced, Merced, CA, USA","IEEE Transactions on Parallel and Distributed Systems","8 Jan 2021",2021,32.0,5.0,1161,1177,"Rich thread-level parallelism of GPU has motivated co-running GPU kernels on a single GPU. However, when GPU kernels co-run, it is possible that one kernel can leverage buffer overflow to attack another kernel running on the same GPU. There is very limited work aiming to detect buffer overflow for GPU. Existing work has either large performance overhead or limited capability in detecting buffer overflow. In this article, we introduce GMODx, a runtime software system that can detect GPU buffer overflow. GMODx performs always-on monitoring on allocated memory based on a canary-based design. First, for the fine-grained memory management, GMODx introduces a set of byte arrays to store buffer information for overflow detection. Techniques, such as lock-free accesses to the byte arrays, delayed memory free, efficient memory reallocation, and garbage collection for the byte arrays, are proposed to achieve high performance. Second, for the coarse-grained memory management, GMODx utilizes unified memory to delegate the always-on monitoring to the CPU. To reduce performance overhead, we propose several techniques, including customized list data structure and specific optimizations against the unified memory. For micro-benchmarking, our experiments show that GMODx is capable of detecting buffer overflow for the fine-grained memory management without performance loss, and that it incurs small runtime overhead (4.2 percent on average and up to 9.7 percent) for the coarse-grained memory management. For real workloads, we deploy GMODx on the TensorFlow framework, it only causes 0.8 percent overhead on average (up to 1.8 percent).","1558-2183","","10.1109/TPDS.2020.3042965","National Natural Science Foundation of China(grant numbers:61972137,61772183); Natural Science Foundation of Hunan Province(grant numbers:2016JJ3042); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286775","Buffer overflows;CUDA;GPGPU;unified memory","Graphics processing units;Kernel;Memory management;Runtime;Resource management;Performance evaluation;Instruction sets","benchmark testing;buffer storage;data structures;graphics processing units;multi-threading;performance evaluation","TensorFlow framework;microbenchmarking;optimizations;customized list data structure;garbage collection;delayed memory free;canary-based design;runtime software system;thread-level parallelism;unified memory;coarse-grained memory management;efficient memory reallocation;buffer information;byte arrays;fine-grained memory management;GPU buffer overflow;GMODx;performance overhead;GPU kernels;efficient buffer overflow detection","",1.0,"",57.0,"IEEE","8 Dec 2020","","","IEEE","IEEE Journals"
"Profiles of Upcoming HPC Applications and Their Impact on Reservation Strategies","A. Gainaru; B. Goglin; V. Honoré; G. Pallez","Oak Ridge National Laboratory, Knoxville, TN, USA; Inria & Université de Bordeaux, Talence, France; Inria & Université de Bordeaux, Talence, France; Inria & Université de Bordeaux, Talence, France","IEEE Transactions on Parallel and Distributed Systems","24 Dec 2020",2021,32.0,5.0,1178,1190,"With the expected convergence between HPC, BigData and AI, new applications with different profiles are coming to HPC infrastructures. We aim at better understanding the features and needs of these applications in order to be able to run them efficiently on HPC platforms. The approach followed is bottom-up: we study thoroughly an emerging application, Spatially Localized Atlas Network Tiles (SLANT, originating from the neuroscience community) to understand its behavior. Based on these observations, we derive a generic, yet simple, application model (namely, a linear sequence of stochastic jobs). We expect this model to be representative for a large set of upcoming applications from emerging fields that start to require the computational power of HPC clusters without fitting the typical behavior of large-scale traditional applications. In a second step, we show how one can use this generic model in a scheduling framework. Specifically we consider the problem of making reservations (both time and memory) for an execution on an HPC platform based on the application expected resource requirements. We derive solutions using the model provided by the first step of this work. We experimentally show the robustness of the model, even with very few data points or using another application, to generate the model, and provide performance gains with regards to standard and more recent approaches used in the neuroscience community.","1558-2183","","10.1109/TPDS.2020.3039728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9266751","Stochastic application;execution time;memory footprint;scheduling;checkpointing","Task analysis;Memory management;Neuroscience;Computational modeling;Magnetic resonance imaging;Correlation;Three-dimensional displays","application program interfaces;artificial intelligence;Big Data;parallel processing;processor scheduling;stochastic processes","HPC applications;reservation strategies;HPC platform;spatially localized atlas network tiles;neuroscience community;linear sequence;stochastic jobs;HPC clusters;SLANT;Big Data;artificial intelligence","",2.0,"",50.0,"IEEE","23 Nov 2020","","","IEEE","IEEE Journals"
"PaKman: A Scalable Algorithm for Generating Genomic Contigs on Distributed Memory Machines","P. Ghosh; S. Krishnamoorthy; A. Kalyanaraman","Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA; Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA; School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA","IEEE Transactions on Parallel and Distributed Systems","11 Jan 2021",2021,32.0,5.0,1191,1209,"De novo genome assembly is a fundamental problem in the field of bioinformatics, that aims to assemble the DNA sequence of an unknown genome from numerous short DNA fragments (aka reads) obtained from it. With the advent of high-throughput sequencing technologies, billions of reads can be generated in a matter of hours, necessitating efficient parallelization of the assembly process. While multiple parallel solutions have been proposed in the past, conducting a large-scale assembly at scale remains a challenging problem because of the inherent complexities associated with data movement, and irregular access footprints of memory and I/O operations. In this article, we present a novel algorithm, called PaKman, to address the problem of performing large-scale genome assemblies on a distributed memory parallel computer. Our approach focuses on improving performance through a combination of novel data structures and algorithmic strategies for reducing the communication and I/O footprint during the assembly process. PaKman presents a solution for the two most time-consuming phases in the full genome assembly pipeline, namely, k-mer counting and contig generation. A key aspect of our algorithm is its graph data structure (PaK-Graph), which comprises fat nodes (or what we call “macro-nodes”) that reduce the communication burden during contig generation. We present an extensive performance and qualitative evaluation of our algorithm across a wide range of genomes (varying in both size and species group), including comparisons to other state-of-the-art parallel assemblers. Our results demonstrate the ability to achieve near-linear speedups on up to 16K cores (tested) on the NERSC Cori supercomputer; perform better than or comparable to other state-of-the-art distributed memory and shared memory tools in terms of performance while delivering comparable (if not better) quality; and reduce time to solution significantly. For instance, PaKman is able to generate a high-quality set of assembled contigs for complex genomes such as the human and bread wheat genomes in under a minute on 16K cores. In addition, PaKman was able to successfully process a 3.1 TB simulated dataset of one of the largest known genomes (to date)-Ambystoma mexicanum (the axolotl), in just over 200 seconds on 16K cores.","1558-2183","","10.1109/TPDS.2020.3043241","U.S. Department of Energy(grant numbers:DE-AC02-05CH11231); U.S. Department of Energy(grant numbers:63823); National Science Foundation(grant numbers:CCF 1815467,OAC 1910213,CCF 1919122); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286880","Genome assembly;distributed memory;de bruijn graphs;k-mer counting","Bioinformatics;Genomics;Data structures;Sequential analysis;DNA;Compaction;Optimization","bioinformatics;data structures;distributed memory systems;DNA;genomics;graph theory;parallel algorithms;shared memory systems","PaK-Graph;qualitative evaluation;NERSC Cori supercomputer;high-throughput sequencing technologies;bioinformatics;PaKman;genomic contig generation;genomes-Ambystoma mexicanum;data movement;large-scale assembly;multiple parallel solutions;sequencing technologies;DNA fragments;DNA sequence;distributed memory machines;scalable algorithm;human bread wheat genomes;complex genomes;assembled contigs;shared memory tools;parallel assemblers;graph data structure;contig generation;genome assembly pipeline;assembly process;novel data structures;distributed memory parallel computer","",1.0,"",33.0,"CCBY","8 Dec 2020","","","IEEE","IEEE Journals"
"Auditing Cache Data Integrity in the Edge Computing Environment","B. Li; Q. He; F. Chen; H. Jin; Y. Xiang; Y. Yang","School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia","IEEE Transactions on Parallel and Distributed Systems","11 Jan 2021",2021,32.0,5.0,1210,1223,"Edge computing allows app vendors to deploy their applications and relevant data on distributed edge servers to serve nearby users. Caching data on edge servers can minimize users' data retrieval latency. However, such cache data are subject to both intentional and accidental corruption in the highly distributed, dynamic, and volatile edge computing environment. Given a large number of edge servers and their limited computing resources, how to effectively and efficiently audit the integrity of app vendors' cache data is a critical and challenging problem. This article makes the first attempt to tackle this Edge Data Integrity (EDI) problem. We first analyze the threat model and the audit objectives, then propose a lightweight sampling-based probabilistic approach, namely EDI-V, to help app vendors audit the integrity of their data cached on a large scale of edge servers. We propose a new data structure named variable Merkle hash tree (VMHT) for generating the integrity proofs of those data replicas during the audit. VMHT can ensure the audit accuracy of EDI-V by maintaining sampling uniformity. EDI-V allows app vendors to inspect their cache data and locate the corrupted ones efficiently and effectively. Both theoretical analysis and comprehensively experimental evaluation demonstrate the efficiency and effectiveness of EDI-V.","1558-2183","","10.1109/TPDS.2020.3043755","Australian Research Council(grant numbers:DP180100212,DP200102491); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9290124","Edge computing;data integrity;data cache;data replica;integrity audit;merkle hash tree","Servers;Data integrity;Edge computing;Cloud computing;Image edge detection;Distributed databases;Computer hacking","cache storage;cloud computing;data integrity;information retrieval;mobile computing;probability;sampling methods;security of data;tree data structures","audit accuracy;data replicas;integrity proofs;data structure;EDI-V;audit objectives;Edge Data Integrity problem;computing resources;volatile edge computing environment;dynamic, edge computing environment;highly distributed edge computing environment;distributed edge servers;app vendors;auditing cache Data Integrity","",53.0,"",50.0,"IEEE","10 Dec 2020","","","IEEE","IEEE Journals"
"Collaborative Heterogeneity-Aware OS Scheduler for Asymmetric Multicore Processors","T. Yu; R. Zhong; V. Janjic; P. Petoumenos; J. Zhai; H. Leather; J. Thomson","Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; University of Dundee, Dundee, United Kingdom; University of Manchester, Manchester, United Kingdom; Tsinghua University, Beijing, China; University of Edinburgh, Edinburgh, United Kingdom; University of St Andrews, St Andrews, United Kingdom","IEEE Transactions on Parallel and Distributed Systems","31 Dec 2020",2021,32.0,5.0,1224,1237,"Asymmetric multicore processors (AMP) offer multiple types of cores under the same programming interface. Extracting the full potential of AMPs requires intelligent scheduling decisions, matching each thread with the right kind of core, the core that will maximize performance or minimize wasted energy for this thread. Existing OS schedulers are not up to this task. While they may handle certain aspects of asymmetry in the system, none can handle all runtime factors affecting AMPs for the general case of multi-threaded multi-programmed workloads. We address this problem by introducing COLAB, a general purpose asymmetry-aware scheduler targeting multi-threaded multi-programmed workloads. It estimates the performance and power of each thread on each type of core and identifies communication patterns and bottleneck threads. With this information, the scheduler makes coordinated core assignment and thread selection decisions that still provide each application its fair share of the processor's time. We evaluate our approach using both the GEM5 simulator on four distinct big.LITTLE configurations and a development board with ARM Cortex-A73/A53 processors and mixed workloads composed of PARSEC and SPLASH2 benchmarks. Compared to the state-of-the art Linux CFS and AMP-aware schedulers, we demonstrate performance gains of up to 25 and 5 to 15 percent on average, together with an average 5 percent energy saving depending on the hardware setup.","1558-2183","","10.1109/TPDS.2020.3045279","China Postdoctoral Science Foundation(grant numbers:2020TQ0169); ShuiMu Tsinghua Scholar fellowship(grant numbers:2019SM131); National Key R&D Program of China(grant numbers:2020AAA0105200); National Natural Science Foundation of China(grant numbers:U20A20226); Natural Science Foundation of Beijing Municipality(grant numbers:4202031); Beijing Academy of Artificial Intelligence(grant numbers:EP/P020631/1); Royal Academy of Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9296343","Asymmetric multicore processors;operating system;scheduling;performance model;energy efficiency","Instruction sets;Multicore processing;Runtime;Message systems;Acceleration;Sensitivity;Processor scheduling","multiprocessing systems;multi-threading;operating systems (computers);power aware computing;scheduling","asymmetric multicore processors;programming interface;intelligent scheduling decisions;OS schedulers;multithreaded multiprogrammed workloads;general purpose asymmetry-aware scheduler;bottleneck threads;coordinated core assignment;AMP-aware schedulers;collaborative heterogeneity-aware OS scheduler","",5.0,"",35.0,"IEEE","16 Dec 2020","","","IEEE","IEEE Journals"
"On the Effective Parallelization and Near-Optimal Deployment of Service Function Chains","J. Luo; J. Li; L. Jiao; J. Cai","School of Cyber Security, Guangdong Polytechnic Normal University, Guangzhou, China; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; School of Cyber Security, Guangdong Polytechnic Normal University, Guangzhou, China","IEEE Transactions on Parallel and Distributed Systems","31 Dec 2020",2021,32.0,5.0,1238,1255,"Network operators compose Service Function Chains (SFCs) by tying different network functions (e.g., packet inspection, flow shaping, network address translation) together and process traffic flows in the order the network functions are chained. Leveraging the technique of Network Function Virtualization (NFV), each network function can be “virtualized” and decoupled from its dedicated hardware, and therefore can be deployed flexibly for better performance at any appropriate location of the underlying network infrastructure. However, an SFC often incurs high latency as traffic goes through the virtual network functions one after another. In this article, we first design an algorithm that leverages virtual network function dependency to convert an original SFC into a parallelized SFC (p-SFC). Then, to deploy multiple p-SFCs over the network for serving a large number of users, we model the deployment problem as an Integer Linear Program and propose a heuristic, ParaSFC, based on the Viterbi dynamic programming algorithm to estimate each p-SFC’s occupation of the bottleneck resources and adjust the processing order of the p-SFCs in order to approximate the optimal solution. Finally, we conduct extensive trace-driven evaluations and exhibit that, compared to the Greedy method and the state-of-the-art CoordVNF method, ParaSFC reduces the average service latency of all the deployed p-SFCs by about 15 percent through parallelization while accommodating more SFC deployment requests over resource-limited networks.","1558-2183","","10.1109/TPDS.2020.3043768","National Key Research and Development Program of China(grant numbers:SQ2019YFB180098); National Natural Science Foundation of China(grant numbers:61972104,61902080,61702120); Key Areas of Guangdong Province(grant numbers:2019B010118001); science and technology project in Guangzhou(grant numbers:201803010081); National key R & D plan(grant numbers:SQ2019YFB180098,2018YFB1802200); Foshan Science and Technology Innovation Project, China(grant numbers:2018IT100283); Science and Technology Program of Guangzhou, China(grant numbers:202002020035); Ripple Faculty Fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9300184","Network function virtualization;service function chain;parallelization;deployment;quality of service","Heuristic algorithms;Servers;Quality of service;Parallel processing;Network function virtualization;Approximation algorithms;Hardware","computer network performance evaluation;integer programming;linear programming;telecommunication computing;telecommunication network topology;telecommunication traffic;virtualisation","near-optimal deployment;service function chains;network operators;network address translation;traffic flows;network function virtualization;network infrastructure;virtual network function;parallelized SFC;deployment problem;SFC deployment requests;resource-limited networks;deployed p-SFC;effective parallelization;packet inspection;flow shaping;integer linear program;ParaSFC heuristic;Viterbi dynamic programming;bottleneck resources;greedy method;CoordVNF method","",26.0,"",55.0,"IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"A Case for Pricing Bandwidth: Sharing Datacenter Networks With Cost Dominant Fairness","L. Chen; Y. Feng; B. Li; B. Li","School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China","IEEE Transactions on Parallel and Distributed Systems","11 Jan 2021",2021,32.0,5.0,1256,1269,"Unlike other resources such as CPU or memory in a virtual machine, inter-virtual-machine (inter-VM) bandwidth has not been explicitly priced in datacenter networks. In this article, we argue that tenants of an IaaS cloud computing platform should be given the flexibility to pay more for explicitly priced datacenter bandwidth beyond traditional virtual machines, in order to achieve better (or more predictable) application performance. We show that a much simpler design principle can be followed to allocate bandwidth fairly, and desirable properties related to fairness can be more easily achieved, compared with state-of-the-art proposals. We call such a design principle cost dominant fairness, which stipulates that bandwidth should be allocated based on the total cost that a tenant incurs for running its applications in the cloud. Guided by the principle of cost dominant fairness, we explore the design space of pricing inter-VM bandwidth, as well as achieving fair bandwidth sharing among multiple tenants. Through our study, we believe that it is best to assign per-VM-pair weights based on individualized prices. We present a distributed bandwidth allocation algorithm that is theoretically supported by a network utility maximization formulation, and practically implemented as a shim layer at each virtual machine. We are also concerned with practical issues of billing, where discounts are needed to ensure that a tenant only pays for the bandwidth share that it is allocated. Finally, we have evaluated our pricing framework and per-VM-pair weighted fair bandwidth allocation in the Mininet emulation testbed and simulations.","1558-2183","","10.1109/TPDS.2020.3045709","BoRSF-RCS(grant numbers:LEQSF(2019-22)-RD-A-21); RGC GRF(grant numbers:16206417,16207818); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9298928","Datacenter networks;bandwidth allocation;fairness;pricing","Bandwidth;Pricing;Cloud computing;Virtual machining;Space exploration;Resource management;Channel allocation","bandwidth allocation;cloud computing;computer centres;pricing;resource allocation;telecommunication traffic;virtual machines;virtualisation","per-VM-pair weights;multiple tenants;pricing inter-VM bandwidth;design space;design principle cost dominant fairness;application performance;explicitly priced datacenter bandwidth;IaaS cloud computing platform;inter-virtual-machine bandwidth;datacenter networks;pricing bandwidth;per-VM-pair weighted fair bandwidth allocation;pricing framework;bandwidth share;virtual machine;network utility maximization formulation;distributed bandwidth allocation algorithm;individualized prices","",3.0,"",40.0,"IEEE","18 Dec 2020","","","IEEE","IEEE Journals"
